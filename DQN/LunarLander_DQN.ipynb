{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57601915",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "57601915",
    "outputId": "acbfb83c-129a-4baa-b394-4c0449342d27",
    "ExecuteTime": {
     "end_time": "2024-03-26T18:38:20.256294Z",
     "start_time": "2024-03-26T18:35:35.910824Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting shap\r\n",
      "  Using cached shap-0.44.1-cp38-cp38-macosx_10_9_x86_64.whl.metadata (24 kB)\r\n",
      "Collecting numpy (from shap)\r\n",
      "  Using cached numpy-1.24.4-cp38-cp38-macosx_10_9_x86_64.whl.metadata (5.6 kB)\r\n",
      "Collecting scipy (from shap)\r\n",
      "  Using cached scipy-1.10.1-cp38-cp38-macosx_10_9_x86_64.whl.metadata (53 kB)\r\n",
      "Collecting scikit-learn (from shap)\r\n",
      "  Using cached scikit_learn-1.3.2-cp38-cp38-macosx_10_9_x86_64.whl.metadata (11 kB)\r\n",
      "Collecting pandas (from shap)\r\n",
      "  Using cached pandas-2.0.3-cp38-cp38-macosx_10_9_x86_64.whl.metadata (18 kB)\r\n",
      "Collecting tqdm>=4.27.0 (from shap)\r\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\r\n",
      "Requirement already satisfied: packaging>20.9 in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from shap) (23.2)\r\n",
      "Collecting slicer==0.0.7 (from shap)\r\n",
      "  Using cached slicer-0.0.7-py3-none-any.whl.metadata (3.7 kB)\r\n",
      "Collecting numba (from shap)\r\n",
      "  Using cached numba-0.58.1-cp38-cp38-macosx_10_9_x86_64.whl.metadata (2.7 kB)\r\n",
      "Collecting cloudpickle (from shap)\r\n",
      "  Using cached cloudpickle-3.0.0-py3-none-any.whl.metadata (7.0 kB)\r\n",
      "Collecting llvmlite<0.42,>=0.41.0dev0 (from numba->shap)\r\n",
      "  Using cached llvmlite-0.41.1-cp38-cp38-macosx_10_9_x86_64.whl.metadata (4.8 kB)\r\n",
      "Requirement already satisfied: importlib-metadata in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from numba->shap) (7.0.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from pandas->shap) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from pandas->shap) (2023.3.post1)\r\n",
      "Collecting tzdata>=2022.1 (from pandas->shap)\r\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\r\n",
      "Collecting joblib>=1.1.1 (from scikit-learn->shap)\r\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\r\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn->shap)\r\n",
      "  Using cached threadpoolctl-3.4.0-py3-none-any.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from importlib-metadata->numba->shap) (3.17.0)\r\n",
      "Using cached shap-0.44.1-cp38-cp38-macosx_10_9_x86_64.whl (453 kB)\r\n",
      "Using cached slicer-0.0.7-py3-none-any.whl (14 kB)\r\n",
      "Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\r\n",
      "Using cached cloudpickle-3.0.0-py3-none-any.whl (20 kB)\r\n",
      "Using cached numba-0.58.1-cp38-cp38-macosx_10_9_x86_64.whl (2.6 MB)\r\n",
      "Using cached numpy-1.24.4-cp38-cp38-macosx_10_9_x86_64.whl (19.8 MB)\r\n",
      "Using cached pandas-2.0.3-cp38-cp38-macosx_10_9_x86_64.whl (11.7 MB)\r\n",
      "Using cached scikit_learn-1.3.2-cp38-cp38-macosx_10_9_x86_64.whl (10.1 MB)\r\n",
      "Using cached scipy-1.10.1-cp38-cp38-macosx_10_9_x86_64.whl (35.0 MB)\r\n",
      "Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\r\n",
      "Using cached llvmlite-0.41.1-cp38-cp38-macosx_10_9_x86_64.whl (31.0 MB)\r\n",
      "Using cached threadpoolctl-3.4.0-py3-none-any.whl (17 kB)\r\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\r\n",
      "Installing collected packages: tzdata, tqdm, threadpoolctl, slicer, numpy, llvmlite, joblib, cloudpickle, scipy, pandas, numba, scikit-learn, shap\r\n",
      "Successfully installed cloudpickle-3.0.0 joblib-1.3.2 llvmlite-0.41.1 numba-0.58.1 numpy-1.24.4 pandas-2.0.3 scikit-learn-1.3.2 scipy-1.10.1 shap-0.44.1 slicer-0.0.7 threadpoolctl-3.4.0 tqdm-4.66.2 tzdata-2024.1\r\n",
      "Collecting opencv-python\r\n",
      "  Using cached opencv_python-4.9.0.80-cp37-abi3-macosx_10_16_x86_64.whl.metadata (20 kB)\r\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from opencv-python) (1.24.4)\r\n",
      "Using cached opencv_python-4.9.0.80-cp37-abi3-macosx_10_16_x86_64.whl (55.7 MB)\r\n",
      "Installing collected packages: opencv-python\r\n",
      "Successfully installed opencv-python-4.9.0.80\r\n",
      "Collecting swig\r\n",
      "  Using cached swig-4.2.1-py2.py3-none-macosx_10_9_x86_64.whl.metadata (3.6 kB)\r\n",
      "Using cached swig-4.2.1-py2.py3-none-macosx_10_9_x86_64.whl (1.8 MB)\r\n",
      "Installing collected packages: swig\r\n",
      "Successfully installed swig-4.2.1\r\n",
      "Collecting Box2D\r\n",
      "  Using cached Box2D-2.3.10-cp38-cp38-macosx_10_9_x86_64.whl.metadata (505 bytes)\r\n",
      "Using cached Box2D-2.3.10-cp38-cp38-macosx_10_9_x86_64.whl (1.3 MB)\r\n",
      "Installing collected packages: Box2D\r\n",
      "Successfully installed Box2D-2.3.10\r\n",
      "Collecting gym\r\n",
      "  Using cached gym-0.26.2-py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy>=1.18.0 in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from gym) (1.24.4)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from gym) (3.0.0)\r\n",
      "Collecting gym-notices>=0.0.4 (from gym)\r\n",
      "  Using cached gym_notices-0.0.8-py3-none-any.whl.metadata (1.0 kB)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from gym) (7.0.1)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gym) (3.17.0)\r\n",
      "Using cached gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\r\n",
      "Installing collected packages: gym-notices, gym\r\n",
      "Successfully installed gym-0.26.2 gym-notices-0.0.8\r\n",
      "Collecting pyglet==1.5.27\r\n",
      "  Downloading pyglet-1.5.27-py3-none-any.whl.metadata (7.6 kB)\r\n",
      "Downloading pyglet-1.5.27-py3-none-any.whl (1.1 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m7.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: pyglet\r\n",
      "Successfully installed pyglet-1.5.27\r\n",
      "\u001B[31mERROR: Could not find a version that satisfies the requirement stable-baseline3 (from versions: none)\u001B[0m\u001B[31m\r\n",
      "\u001B[0m\u001B[31mERROR: No matching distribution found for stable-baseline3\u001B[0m\u001B[31m\r\n",
      "\u001B[0mCollecting gymnasium[all]\r\n",
      "  Using cached gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\r\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from gymnasium[all]) (1.24.4)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from gymnasium[all]) (3.0.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from gymnasium[all]) (4.9.0)\r\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium[all])\r\n",
      "  Using cached Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from gymnasium[all]) (7.0.1)\r\n",
      "Collecting shimmy<1.0,>=0.1.0 (from shimmy[atari]<1.0,>=0.1.0; extra == \"all\"->gymnasium[all])\r\n",
      "  Using cached Shimmy-0.2.1-py3-none-any.whl.metadata (2.3 kB)\r\n",
      "Collecting box2d-py==2.3.5 (from gymnasium[all])\r\n",
      "  Using cached box2d-py-2.3.5.tar.gz (374 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hCollecting pygame>=2.1.3 (from gymnasium[all])\r\n",
      "  Using cached pygame-2.5.2-cp38-cp38-macosx_10_9_x86_64.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: swig==4.* in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from gymnasium[all]) (4.2.1)\r\n",
      "Collecting mujoco-py<2.2,>=2.1 (from gymnasium[all])\r\n",
      "  Using cached mujoco_py-2.1.2.14-py3-none-any.whl.metadata (669 bytes)\r\n",
      "Collecting cython<3 (from gymnasium[all])\r\n",
      "  Using cached Cython-0.29.37-py2.py3-none-any.whl.metadata (3.1 kB)\r\n",
      "Collecting mujoco>=2.3.3 (from gymnasium[all])\r\n",
      "  Using cached mujoco-3.1.3-cp38-cp38-macosx_10_16_x86_64.whl.metadata (44 kB)\r\n",
      "Collecting imageio>=2.14.1 (from gymnasium[all])\r\n",
      "  Using cached imageio-2.34.0-py3-none-any.whl.metadata (4.9 kB)\r\n",
      "Collecting jax>=0.4.0 (from gymnasium[all])\r\n",
      "  Using cached jax-0.4.13-py3-none-any.whl\r\n",
      "Collecting jaxlib>=0.4.0 (from gymnasium[all])\r\n",
      "  Using cached jaxlib-0.4.13-cp38-cp38-macosx_10_14_x86_64.whl.metadata (2.1 kB)\r\n",
      "Collecting lz4>=3.1.0 (from gymnasium[all])\r\n",
      "  Using cached lz4-4.3.3-cp38-cp38-macosx_10_9_x86_64.whl.metadata (3.7 kB)\r\n",
      "Requirement already satisfied: opencv-python>=3.0 in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from gymnasium[all]) (4.9.0.80)\r\n",
      "Collecting matplotlib>=3.0 (from gymnasium[all])\r\n",
      "  Using cached matplotlib-3.7.5-cp38-cp38-macosx_10_12_x86_64.whl.metadata (5.7 kB)\r\n",
      "Collecting moviepy>=1.0.0 (from gymnasium[all])\r\n",
      "  Using cached moviepy-1.0.3-py3-none-any.whl\r\n",
      "Collecting torch>=1.0.0 (from gymnasium[all])\r\n",
      "  Using cached torch-2.2.1-cp38-none-macosx_10_9_x86_64.whl.metadata (25 kB)\r\n",
      "Collecting pillow>=8.3.2 (from imageio>=2.14.1->gymnasium[all])\r\n",
      "  Using cached pillow-10.2.0-cp38-cp38-macosx_10_10_x86_64.whl.metadata (9.7 kB)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gymnasium[all]) (3.17.0)\r\n",
      "Collecting ml-dtypes>=0.1.0 (from jax>=0.4.0->gymnasium[all])\r\n",
      "  Using cached ml_dtypes-0.2.0-cp38-cp38-macosx_10_9_universal2.whl.metadata (20 kB)\r\n",
      "Collecting opt-einsum (from jax>=0.4.0->gymnasium[all])\r\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\r\n",
      "Requirement already satisfied: scipy>=1.7 in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from jax>=0.4.0->gymnasium[all]) (1.10.1)\r\n",
      "Collecting contourpy>=1.0.1 (from matplotlib>=3.0->gymnasium[all])\r\n",
      "  Using cached contourpy-1.1.1-cp38-cp38-macosx_10_9_x86_64.whl.metadata (5.9 kB)\r\n",
      "Collecting cycler>=0.10 (from matplotlib>=3.0->gymnasium[all])\r\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\r\n",
      "Collecting fonttools>=4.22.0 (from matplotlib>=3.0->gymnasium[all])\r\n",
      "  Using cached fonttools-4.50.0-cp38-cp38-macosx_10_9_x86_64.whl.metadata (159 kB)\r\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib>=3.0->gymnasium[all])\r\n",
      "  Using cached kiwisolver-1.4.5-cp38-cp38-macosx_10_9_x86_64.whl.metadata (6.4 kB)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from matplotlib>=3.0->gymnasium[all]) (23.2)\r\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib>=3.0->gymnasium[all])\r\n",
      "  Using cached pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from matplotlib>=3.0->gymnasium[all]) (2.8.2)\r\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from matplotlib>=3.0->gymnasium[all]) (6.1.1)\r\n",
      "Collecting decorator<5.0,>=4.0.2 (from moviepy>=1.0.0->gymnasium[all])\r\n",
      "  Using cached decorator-4.4.2-py2.py3-none-any.whl.metadata (4.2 kB)\r\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from moviepy>=1.0.0->gymnasium[all]) (4.66.2)\r\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from moviepy>=1.0.0->gymnasium[all]) (2.31.0)\r\n",
      "Collecting proglog<=1.0.0 (from moviepy>=1.0.0->gymnasium[all])\r\n",
      "  Using cached proglog-0.1.10-py3-none-any.whl.metadata (639 bytes)\r\n",
      "Collecting imageio-ffmpeg>=0.2.0 (from moviepy>=1.0.0->gymnasium[all])\r\n",
      "  Using cached imageio_ffmpeg-0.4.9-py3-none-macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting absl-py (from mujoco>=2.3.3->gymnasium[all])\r\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\r\n",
      "Collecting etils[epath] (from mujoco>=2.3.3->gymnasium[all])\r\n",
      "  Using cached etils-1.3.0-py3-none-any.whl.metadata (5.5 kB)\r\n",
      "Collecting glfw (from mujoco>=2.3.3->gymnasium[all])\r\n",
      "  Using cached glfw-2.7.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-macosx_10_6_intel.whl.metadata (5.4 kB)\r\n",
      "Collecting pyopengl (from mujoco>=2.3.3->gymnasium[all])\r\n",
      "  Using cached PyOpenGL-3.1.7-py3-none-any.whl.metadata (3.2 kB)\r\n",
      "Requirement already satisfied: cffi>=1.10 in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from mujoco-py<2.2,>=2.1->gymnasium[all]) (1.16.0)\r\n",
      "Collecting fasteners~=0.15 (from mujoco-py<2.2,>=2.1->gymnasium[all])\r\n",
      "  Using cached fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)\r\n",
      "Collecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0; extra == \"all\"->gymnasium[all])\r\n",
      "  Using cached ale_py-0.8.1-cp38-cp38-macosx_10_15_x86_64.whl.metadata (8.1 kB)\r\n",
      "Collecting filelock (from torch>=1.0.0->gymnasium[all])\r\n",
      "  Using cached filelock-3.13.3-py3-none-any.whl.metadata (2.8 kB)\r\n",
      "Collecting sympy (from torch>=1.0.0->gymnasium[all])\r\n",
      "  Using cached sympy-1.12-py3-none-any.whl.metadata (12 kB)\r\n",
      "Collecting networkx (from torch>=1.0.0->gymnasium[all])\r\n",
      "  Using cached networkx-3.1-py3-none-any.whl.metadata (5.3 kB)\r\n",
      "Requirement already satisfied: jinja2 in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from torch>=1.0.0->gymnasium[all]) (3.1.3)\r\n",
      "Collecting fsspec (from torch>=1.0.0->gymnasium[all])\r\n",
      "  Using cached fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\r\n",
      "Requirement already satisfied: pycparser in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from cffi>=1.10->mujoco-py<2.2,>=2.1->gymnasium[all]) (2.21)\r\n",
      "Requirement already satisfied: setuptools in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from imageio-ffmpeg>=0.2.0->moviepy>=1.0.0->gymnasium[all]) (68.2.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib>=3.0->gymnasium[all]) (1.16.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[all]) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[all]) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[all]) (2.1.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[all]) (2024.2.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from jinja2->torch>=1.0.0->gymnasium[all]) (2.1.3)\r\n",
      "Collecting mpmath>=0.19 (from sympy->torch>=1.0.0->gymnasium[all])\r\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\r\n",
      "Using cached Cython-0.29.37-py2.py3-none-any.whl (989 kB)\r\n",
      "Using cached Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\r\n",
      "Using cached imageio-2.34.0-py3-none-any.whl (313 kB)\r\n",
      "Using cached jaxlib-0.4.13-cp38-cp38-macosx_10_14_x86_64.whl (75.0 MB)\r\n",
      "Using cached lz4-4.3.3-cp38-cp38-macosx_10_9_x86_64.whl (254 kB)\r\n",
      "Using cached matplotlib-3.7.5-cp38-cp38-macosx_10_12_x86_64.whl (7.4 MB)\r\n",
      "Using cached mujoco-3.1.3-cp38-cp38-macosx_10_16_x86_64.whl (5.6 MB)\r\n",
      "Using cached mujoco_py-2.1.2.14-py3-none-any.whl (2.4 MB)\r\n",
      "Using cached pygame-2.5.2-cp38-cp38-macosx_10_9_x86_64.whl (12.8 MB)\r\n",
      "Using cached Shimmy-0.2.1-py3-none-any.whl (25 kB)\r\n",
      "Using cached gymnasium-0.29.1-py3-none-any.whl (953 kB)\r\n",
      "Using cached torch-2.2.1-cp38-none-macosx_10_9_x86_64.whl (150.6 MB)\r\n",
      "Using cached ale_py-0.8.1-cp38-cp38-macosx_10_15_x86_64.whl (1.1 MB)\r\n",
      "Using cached contourpy-1.1.1-cp38-cp38-macosx_10_9_x86_64.whl (247 kB)\r\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\r\n",
      "Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\r\n",
      "Using cached fasteners-0.19-py3-none-any.whl (18 kB)\r\n",
      "Using cached fonttools-4.50.0-cp38-cp38-macosx_10_9_x86_64.whl (2.3 MB)\r\n",
      "Using cached glfw-2.7.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-macosx_10_6_intel.whl (97 kB)\r\n",
      "Using cached imageio_ffmpeg-0.4.9-py3-none-macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (22.5 MB)\r\n",
      "Using cached kiwisolver-1.4.5-cp38-cp38-macosx_10_9_x86_64.whl (68 kB)\r\n",
      "Using cached ml_dtypes-0.2.0-cp38-cp38-macosx_10_9_universal2.whl (1.2 MB)\r\n",
      "Using cached pillow-10.2.0-cp38-cp38-macosx_10_10_x86_64.whl (3.5 MB)\r\n",
      "Using cached proglog-0.1.10-py3-none-any.whl (6.1 kB)\r\n",
      "Using cached pyparsing-3.1.2-py3-none-any.whl (103 kB)\r\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\r\n",
      "Using cached filelock-3.13.3-py3-none-any.whl (11 kB)\r\n",
      "Using cached fsspec-2024.3.1-py3-none-any.whl (171 kB)\r\n",
      "Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\r\n",
      "Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\r\n",
      "Using cached PyOpenGL-3.1.7-py3-none-any.whl (2.4 MB)\r\n",
      "Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\r\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\r\n",
      "Using cached etils-1.3.0-py3-none-any.whl (126 kB)\r\n",
      "Building wheels for collected packages: box2d-py\r\n",
      "  Building wheel for box2d-py (setup.py) ... \u001B[?25lerror\r\n",
      "  \u001B[1;31merror\u001B[0m: \u001B[1msubprocess-exited-with-error\u001B[0m\r\n",
      "  \r\n",
      "  \u001B[31m×\u001B[0m \u001B[32mpython setup.py bdist_wheel\u001B[0m did not run successfully.\r\n",
      "  \u001B[31m│\u001B[0m exit code: \u001B[1;36m1\u001B[0m\r\n",
      "  \u001B[31m╰─>\u001B[0m \u001B[31m[40 lines of output]\u001B[0m\r\n",
      "  \u001B[31m   \u001B[0m Using setuptools (version 68.2.2).\r\n",
      "  \u001B[31m   \u001B[0m running bdist_wheel\r\n",
      "  \u001B[31m   \u001B[0m running build\r\n",
      "  \u001B[31m   \u001B[0m running build_py\r\n",
      "  \u001B[31m   \u001B[0m creating build\r\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-10.9-x86_64-cpython-38\r\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-10.9-x86_64-cpython-38/Box2D\r\n",
      "  \u001B[31m   \u001B[0m copying library/Box2D/Box2D.py -> build/lib.macosx-10.9-x86_64-cpython-38/Box2D\r\n",
      "  \u001B[31m   \u001B[0m copying library/Box2D/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-38/Box2D\r\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-10.9-x86_64-cpython-38/Box2D/b2\r\n",
      "  \u001B[31m   \u001B[0m copying library/Box2D/b2/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-38/Box2D/b2\r\n",
      "  \u001B[31m   \u001B[0m running build_ext\r\n",
      "  \u001B[31m   \u001B[0m building 'Box2D._Box2D' extension\r\n",
      "  \u001B[31m   \u001B[0m swigging Box2D/Box2D.i to Box2D/Box2D_wrap.cpp\r\n",
      "  \u001B[31m   \u001B[0m swig -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library/Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D/Box2D_wrap.cpp Box2D/Box2D.i\r\n",
      "  \u001B[31m   \u001B[0m Box2D/Common/b2Math.h:67: Warning 302: Redefinition of identifier 'b2Vec2' by %extend ignored,\r\n",
      "  \u001B[31m   \u001B[0m Box2D/Box2D_math.i:47: Warning 302: %extend definition of 'b2Vec2'.\r\n",
      "  \u001B[31m   \u001B[0m Box2D/Common/b2Math.h:158: Warning 302: Redefinition of identifier 'b2Vec3' by %extend ignored,\r\n",
      "  \u001B[31m   \u001B[0m Box2D/Box2D_math.i:168: Warning 302: %extend definition of 'b2Vec3'.\r\n",
      "  \u001B[31m   \u001B[0m Box2D/Common/b2Math.h:197: Warning 302: Redefinition of identifier 'b2Mat22' by %extend ignored,\r\n",
      "  \u001B[31m   \u001B[0m Box2D/Box2D_math.i:301: Warning 302: %extend definition of 'b2Mat22'.\r\n",
      "  \u001B[31m   \u001B[0m Box2D/Common/b2Math.h:271: Warning 302: Redefinition of identifier 'b2Mat33' by %extend ignored,\r\n",
      "  \u001B[31m   \u001B[0m Box2D/Box2D_math.i:372: Warning 302: %extend definition of 'b2Mat33'.\r\n",
      "  \u001B[31m   \u001B[0m Box2D/Collision/b2DynamicTree.h:44: Warning 312: Nested union not currently supported (ignored).\r\n",
      "  \u001B[31m   \u001B[0m Box2D/Common/b2Settings.h:144: Warning 506: Can't wrap varargs with keyword arguments enabled\r\n",
      "  \u001B[31m   \u001B[0m Box2D/Common/b2Math.h:91: Warning 509: Overloaded method b2Vec2::operator ()(int32) effectively ignored,\r\n",
      "  \u001B[31m   \u001B[0m Box2D/Common/b2Math.h:85: Warning 509: as it is shadowed by b2Vec2::operator ()(int32) const.\r\n",
      "  \u001B[31m   \u001B[0m creating build/temp.macosx-10.9-x86_64-cpython-38\r\n",
      "  \u001B[31m   \u001B[0m creating build/temp.macosx-10.9-x86_64-cpython-38/Box2D\r\n",
      "  \u001B[31m   \u001B[0m creating build/temp.macosx-10.9-x86_64-cpython-38/Box2D/Collision\r\n",
      "  \u001B[31m   \u001B[0m creating build/temp.macosx-10.9-x86_64-cpython-38/Box2D/Collision/Shapes\r\n",
      "  \u001B[31m   \u001B[0m creating build/temp.macosx-10.9-x86_64-cpython-38/Box2D/Common\r\n",
      "  \u001B[31m   \u001B[0m creating build/temp.macosx-10.9-x86_64-cpython-38/Box2D/Dynamics\r\n",
      "  \u001B[31m   \u001B[0m creating build/temp.macosx-10.9-x86_64-cpython-38/Box2D/Dynamics/Contacts\r\n",
      "  \u001B[31m   \u001B[0m creating build/temp.macosx-10.9-x86_64-cpython-38/Box2D/Dynamics/Joints\r\n",
      "  \u001B[31m   \u001B[0m gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/Users/dagm/anaconda3/envs/rl_project_final/include -arch x86_64 -I/Users/dagm/anaconda3/envs/rl_project_final/include -arch x86_64 -I/Users/dagm/anaconda3/envs/rl_project_final/include/python3.8 -c Box2D/Box2D_wrap.cpp -o build/temp.macosx-10.9-x86_64-cpython-38/Box2D/Box2D_wrap.o -I. -Wno-unused\r\n",
      "  \u001B[31m   \u001B[0m xcrun: error: active developer path (\"/Applications/Xcode.app/Contents/Developer\") does not exist\r\n",
      "  \u001B[31m   \u001B[0m Use `sudo xcode-select --switch path/to/Xcode.app` to specify the Xcode that you wish to use for command line developer tools, or use `xcode-select --install` to install the standalone command line developer tools.\r\n",
      "  \u001B[31m   \u001B[0m See `man xcode-select` for more details.\r\n",
      "  \u001B[31m   \u001B[0m error: command '/usr/bin/gcc' failed with exit code 1\r\n",
      "  \u001B[31m   \u001B[0m \u001B[31m[end of output]\u001B[0m\r\n",
      "  \r\n",
      "  \u001B[1;35mnote\u001B[0m: This error originates from a subprocess, and is likely not a problem with pip.\r\n",
      "\u001B[31m  ERROR: Failed building wheel for box2d-py\u001B[0m\u001B[31m\r\n",
      "\u001B[0m\u001B[?25h  Running setup.py clean for box2d-py\r\n",
      "Failed to build box2d-py\r\n",
      "\u001B[31mERROR: Could not build wheels for box2d-py, which is required to install pyproject.toml-based projects\u001B[0m\u001B[31m\r\n",
      "\u001B[0mCollecting stable_baselines3\r\n",
      "  Using cached stable_baselines3-2.2.1-py3-none-any.whl.metadata (5.0 kB)\r\n",
      "Collecting gymnasium<0.30,>=0.28.1 (from stable_baselines3)\r\n",
      "  Using cached gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\r\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from stable_baselines3) (1.24.4)\r\n",
      "Collecting torch>=1.13 (from stable_baselines3)\r\n",
      "  Using cached torch-2.2.1-cp38-none-macosx_10_9_x86_64.whl.metadata (25 kB)\r\n",
      "Requirement already satisfied: cloudpickle in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from stable_baselines3) (3.0.0)\r\n",
      "Requirement already satisfied: pandas in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from stable_baselines3) (2.0.3)\r\n",
      "Collecting matplotlib (from stable_baselines3)\r\n",
      "  Using cached matplotlib-3.7.5-cp38-cp38-macosx_10_12_x86_64.whl.metadata (5.7 kB)\r\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (4.9.0)\r\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium<0.30,>=0.28.1->stable_baselines3)\r\n",
      "  Using cached Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (7.0.1)\r\n",
      "Collecting filelock (from torch>=1.13->stable_baselines3)\r\n",
      "  Using cached filelock-3.13.3-py3-none-any.whl.metadata (2.8 kB)\r\n",
      "Collecting sympy (from torch>=1.13->stable_baselines3)\r\n",
      "  Using cached sympy-1.12-py3-none-any.whl.metadata (12 kB)\r\n",
      "Collecting networkx (from torch>=1.13->stable_baselines3)\r\n",
      "  Using cached networkx-3.1-py3-none-any.whl.metadata (5.3 kB)\r\n",
      "Requirement already satisfied: jinja2 in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from torch>=1.13->stable_baselines3) (3.1.3)\r\n",
      "Collecting fsspec (from torch>=1.13->stable_baselines3)\r\n",
      "  Using cached fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\r\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->stable_baselines3)\r\n",
      "  Using cached contourpy-1.1.1-cp38-cp38-macosx_10_9_x86_64.whl.metadata (5.9 kB)\r\n",
      "Collecting cycler>=0.10 (from matplotlib->stable_baselines3)\r\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\r\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->stable_baselines3)\r\n",
      "  Using cached fonttools-4.50.0-cp38-cp38-macosx_10_9_x86_64.whl.metadata (159 kB)\r\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib->stable_baselines3)\r\n",
      "  Using cached kiwisolver-1.4.5-cp38-cp38-macosx_10_9_x86_64.whl.metadata (6.4 kB)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from matplotlib->stable_baselines3) (23.2)\r\n",
      "Collecting pillow>=6.2.0 (from matplotlib->stable_baselines3)\r\n",
      "  Using cached pillow-10.2.0-cp38-cp38-macosx_10_10_x86_64.whl.metadata (9.7 kB)\r\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib->stable_baselines3)\r\n",
      "  Using cached pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from matplotlib->stable_baselines3) (2.8.2)\r\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from matplotlib->stable_baselines3) (6.1.1)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from pandas->stable_baselines3) (2023.3.post1)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from pandas->stable_baselines3) (2024.1)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gymnasium<0.30,>=0.28.1->stable_baselines3) (3.17.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.16.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/dagm/anaconda3/envs/rl_project_final/lib/python3.8/site-packages (from jinja2->torch>=1.13->stable_baselines3) (2.1.3)\r\n",
      "Collecting mpmath>=0.19 (from sympy->torch>=1.13->stable_baselines3)\r\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\r\n",
      "Using cached stable_baselines3-2.2.1-py3-none-any.whl (181 kB)\r\n",
      "Using cached gymnasium-0.29.1-py3-none-any.whl (953 kB)\r\n",
      "Using cached torch-2.2.1-cp38-none-macosx_10_9_x86_64.whl (150.6 MB)\r\n",
      "Using cached matplotlib-3.7.5-cp38-cp38-macosx_10_12_x86_64.whl (7.4 MB)\r\n",
      "Using cached contourpy-1.1.1-cp38-cp38-macosx_10_9_x86_64.whl (247 kB)\r\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\r\n",
      "Using cached Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\r\n",
      "Using cached fonttools-4.50.0-cp38-cp38-macosx_10_9_x86_64.whl (2.3 MB)\r\n",
      "Using cached kiwisolver-1.4.5-cp38-cp38-macosx_10_9_x86_64.whl (68 kB)\r\n",
      "Using cached pillow-10.2.0-cp38-cp38-macosx_10_10_x86_64.whl (3.5 MB)\r\n",
      "Using cached pyparsing-3.1.2-py3-none-any.whl (103 kB)\r\n",
      "Using cached filelock-3.13.3-py3-none-any.whl (11 kB)\r\n",
      "Using cached fsspec-2024.3.1-py3-none-any.whl (171 kB)\r\n",
      "Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\r\n",
      "Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\r\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\r\n",
      "Installing collected packages: mpmath, farama-notifications, sympy, pyparsing, pillow, networkx, kiwisolver, fsspec, fonttools, filelock, cycler, contourpy, torch, matplotlib, gymnasium, stable_baselines3\r\n",
      "Successfully installed contourpy-1.1.1 cycler-0.12.1 farama-notifications-0.0.4 filelock-3.13.3 fonttools-4.50.0 fsspec-2024.3.1 gymnasium-0.29.1 kiwisolver-1.4.5 matplotlib-3.7.5 mpmath-1.3.0 networkx-3.1 pillow-10.2.0 pyparsing-3.1.2 stable_baselines3-2.2.1 sympy-1.12 torch-2.2.1\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install shap\n",
    "!pip install opencv-python\n",
    "!pip install swig\n",
    "!pip install Box2D\n",
    "\n",
    "\n",
    "# !pip install box2d pygame\n",
    "\n",
    "\n",
    "!pip install gym\n",
    "!pip install pyglet==1.5.27\n",
    "!pip install stable-baseline3\n",
    "!pip install \"gymnasium[all]\"\n",
    "\n",
    "!pip install stable_baselines3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "PtTyQewr3gxt",
   "metadata": {
    "id": "PtTyQewr3gxt"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b00a128f",
   "metadata": {
    "id": "b00a128f",
    "ExecuteTime": {
     "end_time": "2024-03-26T18:44:05.340622Z",
     "start_time": "2024-03-26T18:43:56.512013Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imageio'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mimageio\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mstable_baselines3\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DQN, A2C\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'imageio'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import imageio\n",
    "import os\n",
    "from stable_baselines3 import DQN, A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack, SubprocVecEnv, VecNormalize\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common import results_plotter\n",
    "import gymnasium  as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import scipy.stats as stats\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
    "import tensorflow as tf\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "YtZN-eC7NwuS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YtZN-eC7NwuS",
    "outputId": "c8163864-1647-471c-c554-51d085ca9703"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "GPU not found. Please ensure that GPU is enabled in Colab.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-26 11:46:13.866719: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "# seeds\n",
    "# Set seed for numpy\n",
    "np.random.seed(100)\n",
    "\n",
    "# Set seed for Python random module\n",
    "import random\n",
    "random.seed(100)\n",
    "\n",
    "# Set seed for TensorFlow\n",
    "tf.random.set_seed(100)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device:', tf.test.gpu_device_name())\n",
    "else:\n",
    "    print(\"GPU not found. Please ensure that GPU is enabled in Colab.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780afb92",
   "metadata": {
    "id": "780afb92"
   },
   "source": [
    "<h1> Important Libraries To Install </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2826cd85",
   "metadata": {
    "id": "2826cd85"
   },
   "source": [
    "<h1> Parameter & Environment Information </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ef75ca",
   "metadata": {
    "id": "87ef75ca"
   },
   "source": [
    "<p>\n",
    "    This environment is part of the Box2D environments.\n",
    "</p>\n",
    "\n",
    "<ul>\n",
    "    <li> Action Space Discrete(4) </li>\n",
    "    <li> Observation Shape (8,) </li>\n",
    "    <li> Observation High [1.5 1.5 5. 5. 3.14 5. 1. 1. ] </li>\n",
    "    <li> Observation Low [-1.5 -1.5 -5. -5. -3.14 -5. -0. -0. ] </li>\n",
    "    <li> Import gymnasium.make(\"LunarLander-v2\") </li>\n",
    "</ul>\n",
    "\n",
    "<h3> Description </h3>\n",
    "<p>This environment is a classic rocket trajectory optimization problem. According to Pontryagin’s maximum principle, it is optimal to fire the engine at full throttle or turn it off. This is the reason why this environment has discrete actions: engine on or off.\n",
    "\n",
    "There are two environment versions: discrete or continuous. The landing pad is always at coordinates (0,0). The coordinates are the first two numbers in the state vector. Landing outside of the landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt.</p>\n",
    "\n",
    "<h3> Action Space </h3>\n",
    "<p>\n",
    "There are four discrete actions available:\n",
    "\n",
    "* 0: do nothing\n",
    "* 1: fire left orientation engine\n",
    "* 2: fire main engine\n",
    "* 3: fire right orientation engine\n",
    "\n",
    "</p>\n",
    "\n",
    "<h3> Observation Space </h3>\n",
    "<p>\n",
    "The state is an 8-dimensional vector: the coordinates of the lander in x & y, its linear velocities in x & y, its angle, its angular velocity, and two booleans that represent whether each leg is in contact with the ground or not.\n",
    "</p>\n",
    "\n",
    "<h3> Reward </h3>\n",
    "<p>\n",
    "After every step a reward is granted. The total reward of an episode is the sum of the rewards for all the steps within that episode.\n",
    "\n",
    "For each step, the reward:\n",
    "\n",
    "* is increased/decreased the closer/further the lander is to the landing pad.\n",
    "* is increased/decreased the slower/faster the lander is moving.\n",
    "* is decreased the more the lander is tilted (angle not horizontal).\n",
    "* is increased by 10 points for each leg that is in contact with the ground.\n",
    "* is decreased by 0.03 points each frame a side engine is firing.\n",
    "* is decreased by 0.3 points each frame the main engine is firing.\n",
    "\n",
    "The episode receive an additional reward of -100 or +100 points for crashing or landing safely respectively.\n",
    "\n",
    "An episode is considered a solution if it scores at least 200 points.\n",
    "</p>\n",
    "\n",
    "<h3> Starting State </h3>\n",
    "\n",
    "<p>The lander starts at the top center of the viewport with a random initial force applied to its center of mass.</p>\n",
    "\n",
    "<h3> Episode Termination </h3>\n",
    "<p> The episode finishes if:<br>\n",
    "    \n",
    "1. the lander crashes (the lander body gets in contact with the moon);<br>\n",
    "2. the lander gets outside of the viewport (x coordinate is greater than 1);<br>\n",
    "3. the lander is not awake. From the Box2D docs, a body which is not awake is a body which doesn’t move and doesn’t collide with any other body:<br>\n",
    "\n",
    "When Box2D determines that a body (or group of bodies) has come to rest, the body enters a sleep state which has very little CPU overhead. If a body is awake and collides with a sleeping body, then the sleeping body wakes up. Bodies will also wake up if a joint or contact attached to them is destroyed.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d0543a5",
   "metadata": {
    "id": "3d0543a5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3fb45b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b3fb45b2",
    "outputId": "e9ce14ef-0046-4982-82ef-219ca2c3c49a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Action inter is descrete 4\n",
      "Shape of Observation is (8,)\n"
     ]
    }
   ],
   "source": [
    "print(\"The Action inter is descrete {}\".format(env.action_space.n))\n",
    "print(\"Shape of Observation is {}\".format(env.observation_space.sample().shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9ff9c9",
   "metadata": {
    "id": "8b9ff9c9"
   },
   "source": [
    "<h1> Baseline Model. </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d35101af",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d35101af",
    "outputId": "2e2a8917-fad9-4b49-a560-06aa6026d67b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward after 10 max run is -0.7557301819098438\n"
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "obs = env.reset()\n",
    "done = False\n",
    "MAX_RUN = 10\n",
    "\n",
    "for i in range(MAX_RUN):\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action_sample = env.action_space.sample()\n",
    "        # let's take a step in the environment\n",
    "        obs, rwd, done, info ,_  = env.step(action_sample)\n",
    "        rewards.append(rwd)\n",
    "env.close()\n",
    "print(\"Mean Reward after {} max run is {}\".format(MAX_RUN, np.mean(np.array(rewards))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac737551",
   "metadata": {
    "id": "ac737551"
   },
   "source": [
    "<h1> Reinforcement Learning For Training The Model </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdaa0e55",
   "metadata": {
    "id": "bdaa0e55"
   },
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model')\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "          # Retrieve training reward\n",
    "          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "          if len(x) > 0:\n",
    "              # Mean training reward over the last 100 episodes\n",
    "              mean_reward = np.mean(y[-100:])\n",
    "              if self.verbose > 0:\n",
    "                print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n",
    "\n",
    "              # New best model, you could save the agent here\n",
    "              if mean_reward > self.best_mean_reward:\n",
    "                  self.best_mean_reward = mean_reward\n",
    "                  # Example for saving best model\n",
    "                  if self.verbose > 0:\n",
    "                    print(f\"Saving new best model to {self.save_path}.zip\")\n",
    "                  self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d311d6a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2d311d6a",
    "outputId": "415468ec-76cf-4059-90e4-175d94da9be1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to ./TensorBoardLog/DQN_6\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 88.2     |\n",
      "|    ep_rew_mean      | -138     |\n",
      "|    exploration_rate | 0.997    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 353      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.08     |\n",
      "|    n_updates        | 352      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 87.5     |\n",
      "|    ep_rew_mean      | -183     |\n",
      "|    exploration_rate | 0.995    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 328      |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 700      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.82     |\n",
      "|    n_updates        | 696      |\n",
      "----------------------------------\n",
      "Num timesteps: 1000\n",
      "Best mean reward: -inf - Last mean reward per episode: -203.36\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 86.8     |\n",
      "|    ep_rew_mean      | -222     |\n",
      "|    exploration_rate | 0.992    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 315      |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 1042     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.18     |\n",
      "|    n_updates        | 1040     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 86.8     |\n",
      "|    ep_rew_mean      | -189     |\n",
      "|    exploration_rate | 0.99     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 315      |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1389     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.38     |\n",
      "|    n_updates        | 1388     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 87       |\n",
      "|    ep_rew_mean      | -177     |\n",
      "|    exploration_rate | 0.987    |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 313      |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 1740     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.64     |\n",
      "|    n_updates        | 1736     |\n",
      "----------------------------------\n",
      "Num timesteps: 2000\n",
      "Best mean reward: -203.36 - Last mean reward per episode: -177.15\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 90.2     |\n",
      "|    ep_rew_mean      | -177     |\n",
      "|    exploration_rate | 0.984    |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 307      |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 2166     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.48     |\n",
      "|    n_updates        | 2164     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 91.3     |\n",
      "|    ep_rew_mean      | -171     |\n",
      "|    exploration_rate | 0.981    |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 309      |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 2556     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.23     |\n",
      "|    n_updates        | 2552     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 91.1     |\n",
      "|    ep_rew_mean      | -170     |\n",
      "|    exploration_rate | 0.978    |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 2914     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2        |\n",
      "|    n_updates        | 2912     |\n",
      "----------------------------------\n",
      "Num timesteps: 3000\n",
      "Best mean reward: -177.15 - Last mean reward per episode: -167.35\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 91.4     |\n",
      "|    ep_rew_mean      | -169     |\n",
      "|    exploration_rate | 0.975    |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 3290     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.81     |\n",
      "|    n_updates        | 3288     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 91.8     |\n",
      "|    ep_rew_mean      | -163     |\n",
      "|    exploration_rate | 0.972    |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 3674     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.54     |\n",
      "|    n_updates        | 3672     |\n",
      "----------------------------------\n",
      "Num timesteps: 4000\n",
      "Best mean reward: -167.35 - Last mean reward per episode: -164.83\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 92.2     |\n",
      "|    ep_rew_mean      | -163     |\n",
      "|    exploration_rate | 0.97     |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 311      |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 4059     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.14     |\n",
      "|    n_updates        | 4056     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 91.3     |\n",
      "|    ep_rew_mean      | -162     |\n",
      "|    exploration_rate | 0.967    |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 4382     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.76     |\n",
      "|    n_updates        | 4380     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 92.2     |\n",
      "|    ep_rew_mean      | -161     |\n",
      "|    exploration_rate | 0.964    |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 311      |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 4794     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.79     |\n",
      "|    n_updates        | 4792     |\n",
      "----------------------------------\n",
      "Num timesteps: 5000\n",
      "Best mean reward: -164.83 - Last mean reward per episode: -159.84\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 93.3     |\n",
      "|    ep_rew_mean      | -157     |\n",
      "|    exploration_rate | 0.961    |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 309      |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 5223     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.34     |\n",
      "|    n_updates        | 5220     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 94       |\n",
      "|    ep_rew_mean      | -162     |\n",
      "|    exploration_rate | 0.958    |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 309      |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 5640     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.28     |\n",
      "|    n_updates        | 5636     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 93.5     |\n",
      "|    ep_rew_mean      | -162     |\n",
      "|    exploration_rate | 0.955    |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 5984     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.12     |\n",
      "|    n_updates        | 5980     |\n",
      "----------------------------------\n",
      "Num timesteps: 6000\n",
      "Best mean reward: -159.84 - Last mean reward per episode: -162.38\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 92.4     |\n",
      "|    ep_rew_mean      | -160     |\n",
      "|    exploration_rate | 0.953    |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 6282     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.33     |\n",
      "|    n_updates        | 6280     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 91.8     |\n",
      "|    ep_rew_mean      | -158     |\n",
      "|    exploration_rate | 0.95     |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 6610     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.41     |\n",
      "|    n_updates        | 6608     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 91.3     |\n",
      "|    ep_rew_mean      | -154     |\n",
      "|    exploration_rate | 0.948    |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 6936     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.25     |\n",
      "|    n_updates        | 6932     |\n",
      "----------------------------------\n",
      "Num timesteps: 7000\n",
      "Best mean reward: -159.84 - Last mean reward per episode: -154.04\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 90.8     |\n",
      "|    ep_rew_mean      | -151     |\n",
      "|    exploration_rate | 0.946    |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 7260     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.5      |\n",
      "|    n_updates        | 7256     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 90.1     |\n",
      "|    ep_rew_mean      | -150     |\n",
      "|    exploration_rate | 0.943    |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 7566     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.35     |\n",
      "|    n_updates        | 7564     |\n",
      "----------------------------------\n",
      "Num timesteps: 8000\n",
      "Best mean reward: -154.04 - Last mean reward per episode: -151.42\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 90.9     |\n",
      "|    ep_rew_mean      | -151     |\n",
      "|    exploration_rate | 0.94     |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.09     |\n",
      "|    n_updates        | 7996     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 90.8     |\n",
      "|    ep_rew_mean      | -149     |\n",
      "|    exploration_rate | 0.937    |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8350     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.07     |\n",
      "|    n_updates        | 8348     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 90.1     |\n",
      "|    ep_rew_mean      | -146     |\n",
      "|    exploration_rate | 0.935    |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 8650     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.04     |\n",
      "|    n_updates        | 8648     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 89.8     |\n",
      "|    ep_rew_mean      | -144     |\n",
      "|    exploration_rate | 0.933    |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 8975     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.59     |\n",
      "|    n_updates        | 8972     |\n",
      "----------------------------------\n",
      "Num timesteps: 9000\n",
      "Best mean reward: -151.42 - Last mean reward per episode: -144.36\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 89.4     |\n",
      "|    ep_rew_mean      | -145     |\n",
      "|    exploration_rate | 0.93     |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9293     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.51     |\n",
      "|    n_updates        | 9292     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 89.7     |\n",
      "|    ep_rew_mean      | -139     |\n",
      "|    exploration_rate | 0.927    |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 309      |\n",
      "|    time_elapsed     | 31       |\n",
      "|    total_timesteps  | 9667     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.52     |\n",
      "|    n_updates        | 9664     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 89.2     |\n",
      "|    ep_rew_mean      | -133     |\n",
      "|    exploration_rate | 0.925    |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 32       |\n",
      "|    total_timesteps  | 9964     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.96     |\n",
      "|    n_updates        | 9960     |\n",
      "----------------------------------\n",
      "Num timesteps: 10000\n",
      "Best mean reward: -144.36 - Last mean reward per episode: -132.62\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 89.1     |\n",
      "|    ep_rew_mean      | -133     |\n",
      "|    exploration_rate | 0.923    |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 10300    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.87     |\n",
      "|    n_updates        | 10296    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 89.7     |\n",
      "|    ep_rew_mean      | -136     |\n",
      "|    exploration_rate | 0.92     |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 10706    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.81     |\n",
      "|    n_updates        | 10704    |\n",
      "----------------------------------\n",
      "Num timesteps: 11000\n",
      "Best mean reward: -132.62 - Last mean reward per episode: -133.93\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 89       |\n",
      "|    ep_rew_mean      | -132     |\n",
      "|    exploration_rate | 0.917    |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 311      |\n",
      "|    time_elapsed     | 35       |\n",
      "|    total_timesteps  | 11064    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.09     |\n",
      "|    n_updates        | 11060    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 89       |\n",
      "|    ep_rew_mean      | -131     |\n",
      "|    exploration_rate | 0.914    |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 311      |\n",
      "|    time_elapsed     | 36       |\n",
      "|    total_timesteps  | 11461    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.665    |\n",
      "|    n_updates        | 11460    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 89.3     |\n",
      "|    ep_rew_mean      | -135     |\n",
      "|    exploration_rate | 0.911    |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 311      |\n",
      "|    time_elapsed     | 38       |\n",
      "|    total_timesteps  | 11840    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.836    |\n",
      "|    n_updates        | 11836    |\n",
      "----------------------------------\n",
      "Num timesteps: 12000\n",
      "Best mean reward: -132.62 - Last mean reward per episode: -134.56\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 88.7     |\n",
      "|    ep_rew_mean      | -132     |\n",
      "|    exploration_rate | 0.909    |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 311      |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 12161    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.9      |\n",
      "|    n_updates        | 12160    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 88.8     |\n",
      "|    ep_rew_mean      | -134     |\n",
      "|    exploration_rate | 0.906    |\n",
      "| time/               |          |\n",
      "|    episodes         | 140      |\n",
      "|    fps              | 311      |\n",
      "|    time_elapsed     | 40       |\n",
      "|    total_timesteps  | 12556    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.781    |\n",
      "|    n_updates        | 12552    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 88.1     |\n",
      "|    ep_rew_mean      | -132     |\n",
      "|    exploration_rate | 0.903    |\n",
      "| time/               |          |\n",
      "|    episodes         | 144      |\n",
      "|    fps              | 311      |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total_timesteps  | 12867    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.29     |\n",
      "|    n_updates        | 12864    |\n",
      "----------------------------------\n",
      "Num timesteps: 13000\n",
      "Best mean reward: -132.62 - Last mean reward per episode: -132.28\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 88.9     |\n",
      "|    ep_rew_mean      | -132     |\n",
      "|    exploration_rate | 0.9      |\n",
      "| time/               |          |\n",
      "|    episodes         | 148      |\n",
      "|    fps              | 311      |\n",
      "|    time_elapsed     | 42       |\n",
      "|    total_timesteps  | 13270    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.14     |\n",
      "|    n_updates        | 13268    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 88.5     |\n",
      "|    ep_rew_mean      | -130     |\n",
      "|    exploration_rate | 0.898    |\n",
      "| time/               |          |\n",
      "|    episodes         | 152      |\n",
      "|    fps              | 311      |\n",
      "|    time_elapsed     | 43       |\n",
      "|    total_timesteps  | 13644    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.67     |\n",
      "|    n_updates        | 13640    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 87.7     |\n",
      "|    ep_rew_mean      | -129     |\n",
      "|    exploration_rate | 0.895    |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 311      |\n",
      "|    time_elapsed     | 44       |\n",
      "|    total_timesteps  | 13997    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.969    |\n",
      "|    n_updates        | 13996    |\n",
      "----------------------------------\n",
      "Num timesteps: 14000\n",
      "Best mean reward: -132.28 - Last mean reward per episode: -128.90\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 87.1     |\n",
      "|    ep_rew_mean      | -123     |\n",
      "|    exploration_rate | 0.892    |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 46       |\n",
      "|    total_timesteps  | 14352    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.72     |\n",
      "|    n_updates        | 14348    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 86.9     |\n",
      "|    ep_rew_mean      | -120     |\n",
      "|    exploration_rate | 0.89     |\n",
      "| time/               |          |\n",
      "|    episodes         | 164      |\n",
      "|    fps              | 311      |\n",
      "|    time_elapsed     | 47       |\n",
      "|    total_timesteps  | 14678    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.41     |\n",
      "|    n_updates        | 14676    |\n",
      "----------------------------------\n",
      "Num timesteps: 15000\n",
      "Best mean reward: -128.90 - Last mean reward per episode: -117.80\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 87.8     |\n",
      "|    ep_rew_mean      | -118     |\n",
      "|    exploration_rate | 0.887    |\n",
      "| time/               |          |\n",
      "|    episodes         | 168      |\n",
      "|    fps              | 311      |\n",
      "|    time_elapsed     | 48       |\n",
      "|    total_timesteps  | 15058    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.22     |\n",
      "|    n_updates        | 15056    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 87.8     |\n",
      "|    ep_rew_mean      | -118     |\n",
      "|    exploration_rate | 0.885    |\n",
      "| time/               |          |\n",
      "|    episodes         | 172      |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 49       |\n",
      "|    total_timesteps  | 15390    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.16     |\n",
      "|    n_updates        | 15388    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 88.7     |\n",
      "|    ep_rew_mean      | -119     |\n",
      "|    exploration_rate | 0.881    |\n",
      "| time/               |          |\n",
      "|    episodes         | 176      |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 50       |\n",
      "|    total_timesteps  | 15804    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.25     |\n",
      "|    n_updates        | 15800    |\n",
      "----------------------------------\n",
      "Num timesteps: 16000\n",
      "Best mean reward: -117.80 - Last mean reward per episode: -118.76\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 88.8     |\n",
      "|    ep_rew_mean      | -119     |\n",
      "|    exploration_rate | 0.879    |\n",
      "| time/               |          |\n",
      "|    episodes         | 180      |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 51       |\n",
      "|    total_timesteps  | 16137    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.64     |\n",
      "|    n_updates        | 16136    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 89.5     |\n",
      "|    ep_rew_mean      | -119     |\n",
      "|    exploration_rate | 0.876    |\n",
      "| time/               |          |\n",
      "|    episodes         | 184      |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 53       |\n",
      "|    total_timesteps  | 16516    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.48     |\n",
      "|    n_updates        | 16512    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 89       |\n",
      "|    ep_rew_mean      | -117     |\n",
      "|    exploration_rate | 0.873    |\n",
      "| time/               |          |\n",
      "|    episodes         | 188      |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 54       |\n",
      "|    total_timesteps  | 16896    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.16     |\n",
      "|    n_updates        | 16892    |\n",
      "----------------------------------\n",
      "Num timesteps: 17000\n",
      "Best mean reward: -117.80 - Last mean reward per episode: -116.66\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 88.2     |\n",
      "|    ep_rew_mean      | -116     |\n",
      "|    exploration_rate | 0.871    |\n",
      "| time/               |          |\n",
      "|    episodes         | 192      |\n",
      "|    fps              | 309      |\n",
      "|    time_elapsed     | 55       |\n",
      "|    total_timesteps  | 17170    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.987    |\n",
      "|    n_updates        | 17168    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 89.2     |\n",
      "|    ep_rew_mean      | -117     |\n",
      "|    exploration_rate | 0.868    |\n",
      "| time/               |          |\n",
      "|    episodes         | 196      |\n",
      "|    fps              | 309      |\n",
      "|    time_elapsed     | 56       |\n",
      "|    total_timesteps  | 17567    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.74     |\n",
      "|    n_updates        | 17564    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 90.1     |\n",
      "|    ep_rew_mean      | -118     |\n",
      "|    exploration_rate | 0.865    |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 309      |\n",
      "|    time_elapsed     | 58       |\n",
      "|    total_timesteps  | 17984    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.22     |\n",
      "|    n_updates        | 17980    |\n",
      "----------------------------------\n",
      "Num timesteps: 18000\n",
      "Best mean reward: -116.66 - Last mean reward per episode: -117.97\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 90.6     |\n",
      "|    ep_rew_mean      | -115     |\n",
      "|    exploration_rate | 0.862    |\n",
      "| time/               |          |\n",
      "|    episodes         | 204      |\n",
      "|    fps              | 309      |\n",
      "|    time_elapsed     | 59       |\n",
      "|    total_timesteps  | 18349    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.15     |\n",
      "|    n_updates        | 18348    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 90.3     |\n",
      "|    ep_rew_mean      | -113     |\n",
      "|    exploration_rate | 0.86     |\n",
      "| time/               |          |\n",
      "|    episodes         | 208      |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 60       |\n",
      "|    total_timesteps  | 18699    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.29     |\n",
      "|    n_updates        | 18696    |\n",
      "----------------------------------\n",
      "Num timesteps: 19000\n",
      "Best mean reward: -116.66 - Last mean reward per episode: -112.66\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 90.5     |\n",
      "|    ep_rew_mean      | -113     |\n",
      "|    exploration_rate | 0.857    |\n",
      "| time/               |          |\n",
      "|    episodes         | 212      |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 61       |\n",
      "|    total_timesteps  | 19011    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.39     |\n",
      "|    n_updates        | 19008    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 89.8     |\n",
      "|    ep_rew_mean      | -112     |\n",
      "|    exploration_rate | 0.855    |\n",
      "| time/               |          |\n",
      "|    episodes         | 216      |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 62       |\n",
      "|    total_timesteps  | 19284    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.18     |\n",
      "|    n_updates        | 19280    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 88.7     |\n",
      "|    ep_rew_mean      | -108     |\n",
      "|    exploration_rate | 0.853    |\n",
      "| time/               |          |\n",
      "|    episodes         | 220      |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 63       |\n",
      "|    total_timesteps  | 19574    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.13     |\n",
      "|    n_updates        | 19572    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 88.8     |\n",
      "|    ep_rew_mean      | -109     |\n",
      "|    exploration_rate | 0.85     |\n",
      "| time/               |          |\n",
      "|    episodes         | 224      |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 64       |\n",
      "|    total_timesteps  | 19940    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.4      |\n",
      "|    n_updates        | 19936    |\n",
      "----------------------------------\n",
      "Num timesteps: 20000\n",
      "Best mean reward: -112.66 - Last mean reward per episode: -108.38\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 88.2     |\n",
      "|    ep_rew_mean      | -108     |\n",
      "|    exploration_rate | 0.848    |\n",
      "| time/               |          |\n",
      "|    episodes         | 228      |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 65       |\n",
      "|    total_timesteps  | 20276    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.9      |\n",
      "|    n_updates        | 20272    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 88       |\n",
      "|    ep_rew_mean      | -103     |\n",
      "|    exploration_rate | 0.845    |\n",
      "| time/               |          |\n",
      "|    episodes         | 232      |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 66       |\n",
      "|    total_timesteps  | 20639    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.1      |\n",
      "|    n_updates        | 20636    |\n",
      "----------------------------------\n",
      "Num timesteps: 21000\n",
      "Best mean reward: -108.38 - Last mean reward per episode: -101.74\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 88.5     |\n",
      "|    ep_rew_mean      | -102     |\n",
      "|    exploration_rate | 0.842    |\n",
      "| time/               |          |\n",
      "|    episodes         | 236      |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 67       |\n",
      "|    total_timesteps  | 21014    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.52     |\n",
      "|    n_updates        | 21012    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 87.9     |\n",
      "|    ep_rew_mean      | -99.8    |\n",
      "|    exploration_rate | 0.84     |\n",
      "| time/               |          |\n",
      "|    episodes         | 240      |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 68       |\n",
      "|    total_timesteps  | 21349    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.29     |\n",
      "|    n_updates        | 21348    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 89       |\n",
      "|    ep_rew_mean      | -98.2    |\n",
      "|    exploration_rate | 0.837    |\n",
      "| time/               |          |\n",
      "|    episodes         | 244      |\n",
      "|    fps              | 309      |\n",
      "|    time_elapsed     | 70       |\n",
      "|    total_timesteps  | 21767    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.05     |\n",
      "|    n_updates        | 21764    |\n",
      "----------------------------------\n",
      "Num timesteps: 22000\n",
      "Best mean reward: -101.74 - Last mean reward per episode: -95.76\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 88.6     |\n",
      "|    ep_rew_mean      | -95.5    |\n",
      "|    exploration_rate | 0.834    |\n",
      "| time/               |          |\n",
      "|    episodes         | 248      |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 71       |\n",
      "|    total_timesteps  | 22133    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.993    |\n",
      "|    n_updates        | 22132    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 88.3     |\n",
      "|    ep_rew_mean      | -94.6    |\n",
      "|    exploration_rate | 0.831    |\n",
      "| time/               |          |\n",
      "|    episodes         | 252      |\n",
      "|    fps              | 309      |\n",
      "|    time_elapsed     | 72       |\n",
      "|    total_timesteps  | 22479    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.16     |\n",
      "|    n_updates        | 22476    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 89.2     |\n",
      "|    ep_rew_mean      | -95.8    |\n",
      "|    exploration_rate | 0.828    |\n",
      "| time/               |          |\n",
      "|    episodes         | 256      |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 73       |\n",
      "|    total_timesteps  | 22912    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.51     |\n",
      "|    n_updates        | 22908    |\n",
      "----------------------------------\n",
      "Num timesteps: 23000\n",
      "Best mean reward: -95.76 - Last mean reward per episode: -95.76\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 89       |\n",
      "|    ep_rew_mean      | -94.1    |\n",
      "|    exploration_rate | 0.826    |\n",
      "| time/               |          |\n",
      "|    episodes         | 260      |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 74       |\n",
      "|    total_timesteps  | 23253    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.44     |\n",
      "|    n_updates        | 23252    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 88.9     |\n",
      "|    ep_rew_mean      | -94.1    |\n",
      "|    exploration_rate | 0.823    |\n",
      "| time/               |          |\n",
      "|    episodes         | 264      |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 76       |\n",
      "|    total_timesteps  | 23570    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.94     |\n",
      "|    n_updates        | 23568    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 89       |\n",
      "|    ep_rew_mean      | -95.5    |\n",
      "|    exploration_rate | 0.82     |\n",
      "| time/               |          |\n",
      "|    episodes         | 268      |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 77       |\n",
      "|    total_timesteps  | 23959    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.23     |\n",
      "|    n_updates        | 23956    |\n",
      "----------------------------------\n",
      "Num timesteps: 24000\n",
      "Best mean reward: -95.76 - Last mean reward per episode: -95.52\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 90       |\n",
      "|    ep_rew_mean      | -94.9    |\n",
      "|    exploration_rate | 0.817    |\n",
      "| time/               |          |\n",
      "|    episodes         | 272      |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 78       |\n",
      "|    total_timesteps  | 24386    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.41     |\n",
      "|    n_updates        | 24384    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 89.6     |\n",
      "|    ep_rew_mean      | -95.7    |\n",
      "|    exploration_rate | 0.814    |\n",
      "| time/               |          |\n",
      "|    episodes         | 276      |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 79       |\n",
      "|    total_timesteps  | 24761    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.913    |\n",
      "|    n_updates        | 24760    |\n",
      "----------------------------------\n",
      "Num timesteps: 25000\n",
      "Best mean reward: -95.52 - Last mean reward per episode: -98.09\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 90.5     |\n",
      "|    ep_rew_mean      | -100     |\n",
      "|    exploration_rate | 0.811    |\n",
      "| time/               |          |\n",
      "|    episodes         | 280      |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 81       |\n",
      "|    total_timesteps  | 25191    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.806    |\n",
      "|    n_updates        | 25188    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 90.7     |\n",
      "|    ep_rew_mean      | -98.2    |\n",
      "|    exploration_rate | 0.808    |\n",
      "| time/               |          |\n",
      "|    episodes         | 284      |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 82       |\n",
      "|    total_timesteps  | 25590    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.961    |\n",
      "|    n_updates        | 25588    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 90.9     |\n",
      "|    ep_rew_mean      | -98.4    |\n",
      "|    exploration_rate | 0.805    |\n",
      "| time/               |          |\n",
      "|    episodes         | 288      |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 83       |\n",
      "|    total_timesteps  | 25988    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.883    |\n",
      "|    n_updates        | 25984    |\n",
      "----------------------------------\n",
      "Num timesteps: 26000\n",
      "Best mean reward: -95.52 - Last mean reward per episode: -98.36\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 92.3     |\n",
      "|    ep_rew_mean      | -99.2    |\n",
      "|    exploration_rate | 0.802    |\n",
      "| time/               |          |\n",
      "|    episodes         | 292      |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 85       |\n",
      "|    total_timesteps  | 26404    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.988    |\n",
      "|    n_updates        | 26400    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 92.3     |\n",
      "|    ep_rew_mean      | -99.5    |\n",
      "|    exploration_rate | 0.799    |\n",
      "| time/               |          |\n",
      "|    episodes         | 296      |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 86       |\n",
      "|    total_timesteps  | 26793    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.09     |\n",
      "|    n_updates        | 26792    |\n",
      "----------------------------------\n",
      "Num timesteps: 27000\n",
      "Best mean reward: -95.52 - Last mean reward per episode: -97.34\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 92.1     |\n",
      "|    ep_rew_mean      | -97.3    |\n",
      "|    exploration_rate | 0.796    |\n",
      "| time/               |          |\n",
      "|    episodes         | 300      |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 87       |\n",
      "|    total_timesteps  | 27193    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.53     |\n",
      "|    n_updates        | 27192    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 92.1     |\n",
      "|    ep_rew_mean      | -97.6    |\n",
      "|    exploration_rate | 0.793    |\n",
      "| time/               |          |\n",
      "|    episodes         | 304      |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 88       |\n",
      "|    total_timesteps  | 27555    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.66     |\n",
      "|    n_updates        | 27552    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 92.5     |\n",
      "|    ep_rew_mean      | -99.3    |\n",
      "|    exploration_rate | 0.79     |\n",
      "| time/               |          |\n",
      "|    episodes         | 308      |\n",
      "|    fps              | 310      |\n",
      "|    time_elapsed     | 90       |\n",
      "|    total_timesteps  | 27950    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.05     |\n",
      "|    n_updates        | 27948    |\n",
      "----------------------------------\n",
      "Num timesteps: 28000\n",
      "Best mean reward: -95.52 - Last mean reward per episode: -99.28\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 93.4     |\n",
      "|    ep_rew_mean      | -97.1    |\n",
      "|    exploration_rate | 0.787    |\n",
      "| time/               |          |\n",
      "|    episodes         | 312      |\n",
      "|    fps              | 309      |\n",
      "|    time_elapsed     | 91       |\n",
      "|    total_timesteps  | 28347    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.39     |\n",
      "|    n_updates        | 28344    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 94.1     |\n",
      "|    ep_rew_mean      | -96.8    |\n",
      "|    exploration_rate | 0.785    |\n",
      "| time/               |          |\n",
      "|    episodes         | 316      |\n",
      "|    fps              | 309      |\n",
      "|    time_elapsed     | 92       |\n",
      "|    total_timesteps  | 28690    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.845    |\n",
      "|    n_updates        | 28688    |\n",
      "----------------------------------\n",
      "Num timesteps: 29000\n",
      "Best mean reward: -95.52 - Last mean reward per episode: -96.32\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 95.2     |\n",
      "|    ep_rew_mean      | -95.6    |\n",
      "|    exploration_rate | 0.782    |\n",
      "| time/               |          |\n",
      "|    episodes         | 320      |\n",
      "|    fps              | 309      |\n",
      "|    time_elapsed     | 93       |\n",
      "|    total_timesteps  | 29096    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.1      |\n",
      "|    n_updates        | 29092    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 94.7     |\n",
      "|    ep_rew_mean      | -95.7    |\n",
      "|    exploration_rate | 0.779    |\n",
      "| time/               |          |\n",
      "|    episodes         | 324      |\n",
      "|    fps              | 309      |\n",
      "|    time_elapsed     | 94       |\n",
      "|    total_timesteps  | 29408    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.62     |\n",
      "|    n_updates        | 29404    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 95.8     |\n",
      "|    ep_rew_mean      | -94.6    |\n",
      "|    exploration_rate | 0.776    |\n",
      "| time/               |          |\n",
      "|    episodes         | 328      |\n",
      "|    fps              | 309      |\n",
      "|    time_elapsed     | 96       |\n",
      "|    total_timesteps  | 29858    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.804    |\n",
      "|    n_updates        | 29856    |\n",
      "----------------------------------\n",
      "Num timesteps: 30000\n",
      "Best mean reward: -95.52 - Last mean reward per episode: -93.70\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 96.7     |\n",
      "|    ep_rew_mean      | -93.5    |\n",
      "|    exploration_rate | 0.773    |\n",
      "| time/               |          |\n",
      "|    episodes         | 332      |\n",
      "|    fps              | 309      |\n",
      "|    time_elapsed     | 97       |\n",
      "|    total_timesteps  | 30309    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.54     |\n",
      "|    n_updates        | 30308    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 97.1     |\n",
      "|    ep_rew_mean      | -97.3    |\n",
      "|    exploration_rate | 0.77     |\n",
      "| time/               |          |\n",
      "|    episodes         | 336      |\n",
      "|    fps              | 309      |\n",
      "|    time_elapsed     | 99       |\n",
      "|    total_timesteps  | 30728    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.996    |\n",
      "|    n_updates        | 30724    |\n",
      "----------------------------------\n",
      "Num timesteps: 31000\n",
      "Best mean reward: -93.70 - Last mean reward per episode: -97.23\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 97.4     |\n",
      "|    ep_rew_mean      | -97.3    |\n",
      "|    exploration_rate | 0.767    |\n",
      "| time/               |          |\n",
      "|    episodes         | 340      |\n",
      "|    fps              | 309      |\n",
      "|    time_elapsed     | 100      |\n",
      "|    total_timesteps  | 31093    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.12     |\n",
      "|    n_updates        | 31092    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 98       |\n",
      "|    ep_rew_mean      | -98.5    |\n",
      "|    exploration_rate | 0.763    |\n",
      "| time/               |          |\n",
      "|    episodes         | 344      |\n",
      "|    fps              | 309      |\n",
      "|    time_elapsed     | 102      |\n",
      "|    total_timesteps  | 31569    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.17     |\n",
      "|    n_updates        | 31568    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 98.3     |\n",
      "|    ep_rew_mean      | -98.1    |\n",
      "|    exploration_rate | 0.76     |\n",
      "| time/               |          |\n",
      "|    episodes         | 348      |\n",
      "|    fps              | 309      |\n",
      "|    time_elapsed     | 103      |\n",
      "|    total_timesteps  | 31964    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.07     |\n",
      "|    n_updates        | 31960    |\n",
      "----------------------------------\n",
      "Num timesteps: 32000\n",
      "Best mean reward: -93.70 - Last mean reward per episode: -98.09\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 100      |\n",
      "|    ep_rew_mean      | -102     |\n",
      "|    exploration_rate | 0.756    |\n",
      "| time/               |          |\n",
      "|    episodes         | 352      |\n",
      "|    fps              | 308      |\n",
      "|    time_elapsed     | 105      |\n",
      "|    total_timesteps  | 32487    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.31     |\n",
      "|    n_updates        | 32484    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 99.5     |\n",
      "|    ep_rew_mean      | -102     |\n",
      "|    exploration_rate | 0.754    |\n",
      "| time/               |          |\n",
      "|    episodes         | 356      |\n",
      "|    fps              | 309      |\n",
      "|    time_elapsed     | 106      |\n",
      "|    total_timesteps  | 32860    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.17     |\n",
      "|    n_updates        | 32856    |\n",
      "----------------------------------\n",
      "Num timesteps: 33000\n",
      "Best mean reward: -93.70 - Last mean reward per episode: -102.52\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 100      |\n",
      "|    ep_rew_mean      | -103     |\n",
      "|    exploration_rate | 0.75     |\n",
      "| time/               |          |\n",
      "|    episodes         | 360      |\n",
      "|    fps              | 309      |\n",
      "|    time_elapsed     | 107      |\n",
      "|    total_timesteps  | 33301    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.21     |\n",
      "|    n_updates        | 33300    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 102      |\n",
      "|    ep_rew_mean      | -103     |\n",
      "|    exploration_rate | 0.747    |\n",
      "| time/               |          |\n",
      "|    episodes         | 364      |\n",
      "|    fps              | 309      |\n",
      "|    time_elapsed     | 109      |\n",
      "|    total_timesteps  | 33772    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.15     |\n",
      "|    n_updates        | 33768    |\n",
      "----------------------------------\n",
      "Num timesteps: 34000\n",
      "Best mean reward: -93.70 - Last mean reward per episode: -101.48\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 102      |\n",
      "|    ep_rew_mean      | -102     |\n",
      "|    exploration_rate | 0.744    |\n",
      "| time/               |          |\n",
      "|    episodes         | 368      |\n",
      "|    fps              | 309      |\n",
      "|    time_elapsed     | 110      |\n",
      "|    total_timesteps  | 34147    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.25     |\n",
      "|    n_updates        | 34144    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 102      |\n",
      "|    ep_rew_mean      | -101     |\n",
      "|    exploration_rate | 0.741    |\n",
      "| time/               |          |\n",
      "|    episodes         | 372      |\n",
      "|    fps              | 309      |\n",
      "|    time_elapsed     | 111      |\n",
      "|    total_timesteps  | 34571    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.18     |\n",
      "|    n_updates        | 34568    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 102      |\n",
      "|    ep_rew_mean      | -97.7    |\n",
      "|    exploration_rate | 0.738    |\n",
      "| time/               |          |\n",
      "|    episodes         | 376      |\n",
      "|    fps              | 309      |\n",
      "|    time_elapsed     | 113      |\n",
      "|    total_timesteps  | 34987    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.946    |\n",
      "|    n_updates        | 34984    |\n",
      "----------------------------------\n",
      "Num timesteps: 35000\n",
      "Best mean reward: -93.70 - Last mean reward per episode: -97.69\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 103      |\n",
      "|    ep_rew_mean      | -93      |\n",
      "|    exploration_rate | 0.734    |\n",
      "| time/               |          |\n",
      "|    episodes         | 380      |\n",
      "|    fps              | 309      |\n",
      "|    time_elapsed     | 114      |\n",
      "|    total_timesteps  | 35470    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.93     |\n",
      "|    n_updates        | 35468    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 103      |\n",
      "|    ep_rew_mean      | -94.7    |\n",
      "|    exploration_rate | 0.731    |\n",
      "| time/               |          |\n",
      "|    episodes         | 384      |\n",
      "|    fps              | 308      |\n",
      "|    time_elapsed     | 116      |\n",
      "|    total_timesteps  | 35906    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.96     |\n",
      "|    n_updates        | 35904    |\n",
      "----------------------------------\n",
      "Num timesteps: 36000\n",
      "Best mean reward: -93.70 - Last mean reward per episode: -94.74\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 104      |\n",
      "|    ep_rew_mean      | -93      |\n",
      "|    exploration_rate | 0.727    |\n",
      "| time/               |          |\n",
      "|    episodes         | 388      |\n",
      "|    fps              | 308      |\n",
      "|    time_elapsed     | 117      |\n",
      "|    total_timesteps  | 36342    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.06     |\n",
      "|    n_updates        | 36340    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 103      |\n",
      "|    ep_rew_mean      | -93.2    |\n",
      "|    exploration_rate | 0.725    |\n",
      "| time/               |          |\n",
      "|    episodes         | 392      |\n",
      "|    fps              | 308      |\n",
      "|    time_elapsed     | 118      |\n",
      "|    total_timesteps  | 36703    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.47     |\n",
      "|    n_updates        | 36700    |\n",
      "----------------------------------\n",
      "Num timesteps: 37000\n",
      "Best mean reward: -93.70 - Last mean reward per episode: -91.72\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 103      |\n",
      "|    ep_rew_mean      | -91.2    |\n",
      "|    exploration_rate | 0.722    |\n",
      "| time/               |          |\n",
      "|    episodes         | 396      |\n",
      "|    fps              | 308      |\n",
      "|    time_elapsed     | 120      |\n",
      "|    total_timesteps  | 37110    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.01     |\n",
      "|    n_updates        | 37108    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 103      |\n",
      "|    ep_rew_mean      | -92.1    |\n",
      "|    exploration_rate | 0.719    |\n",
      "| time/               |          |\n",
      "|    episodes         | 400      |\n",
      "|    fps              | 308      |\n",
      "|    time_elapsed     | 121      |\n",
      "|    total_timesteps  | 37449    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.29     |\n",
      "|    n_updates        | 37448    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 103      |\n",
      "|    ep_rew_mean      | -91.7    |\n",
      "|    exploration_rate | 0.716    |\n",
      "| time/               |          |\n",
      "|    episodes         | 404      |\n",
      "|    fps              | 308      |\n",
      "|    time_elapsed     | 122      |\n",
      "|    total_timesteps  | 37892    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.985    |\n",
      "|    n_updates        | 37888    |\n",
      "----------------------------------\n",
      "Num timesteps: 38000\n",
      "Best mean reward: -91.72 - Last mean reward per episode: -91.72\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 104      |\n",
      "|    ep_rew_mean      | -90.2    |\n",
      "|    exploration_rate | 0.712    |\n",
      "| time/               |          |\n",
      "|    episodes         | 408      |\n",
      "|    fps              | 308      |\n",
      "|    time_elapsed     | 124      |\n",
      "|    total_timesteps  | 38389    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.32     |\n",
      "|    n_updates        | 38388    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 104      |\n",
      "|    ep_rew_mean      | -90.5    |\n",
      "|    exploration_rate | 0.709    |\n",
      "| time/               |          |\n",
      "|    episodes         | 412      |\n",
      "|    fps              | 308      |\n",
      "|    time_elapsed     | 125      |\n",
      "|    total_timesteps  | 38755    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.14     |\n",
      "|    n_updates        | 38752    |\n",
      "----------------------------------\n",
      "Num timesteps: 39000\n",
      "Best mean reward: -91.72 - Last mean reward per episode: -89.84\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 105      |\n",
      "|    ep_rew_mean      | -89.7    |\n",
      "|    exploration_rate | 0.706    |\n",
      "| time/               |          |\n",
      "|    episodes         | 416      |\n",
      "|    fps              | 308      |\n",
      "|    time_elapsed     | 127      |\n",
      "|    total_timesteps  | 39232    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.74     |\n",
      "|    n_updates        | 39228    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 105      |\n",
      "|    ep_rew_mean      | -89.6    |\n",
      "|    exploration_rate | 0.703    |\n",
      "| time/               |          |\n",
      "|    episodes         | 420      |\n",
      "|    fps              | 308      |\n",
      "|    time_elapsed     | 128      |\n",
      "|    total_timesteps  | 39598    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.55     |\n",
      "|    n_updates        | 39596    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 105      |\n",
      "|    ep_rew_mean      | -88.1    |\n",
      "|    exploration_rate | 0.7      |\n",
      "| time/               |          |\n",
      "|    episodes         | 424      |\n",
      "|    fps              | 308      |\n",
      "|    time_elapsed     | 129      |\n",
      "|    total_timesteps  | 39937    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.15     |\n",
      "|    n_updates        | 39936    |\n",
      "----------------------------------\n",
      "Num timesteps: 40000\n",
      "Best mean reward: -89.84 - Last mean reward per episode: -88.10\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 105      |\n",
      "|    ep_rew_mean      | -89.2    |\n",
      "|    exploration_rate | 0.698    |\n",
      "| time/               |          |\n",
      "|    episodes         | 428      |\n",
      "|    fps              | 308      |\n",
      "|    time_elapsed     | 130      |\n",
      "|    total_timesteps  | 40318    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.13     |\n",
      "|    n_updates        | 40316    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 104      |\n",
      "|    ep_rew_mean      | -88      |\n",
      "|    exploration_rate | 0.694    |\n",
      "| time/               |          |\n",
      "|    episodes         | 432      |\n",
      "|    fps              | 308      |\n",
      "|    time_elapsed     | 132      |\n",
      "|    total_timesteps  | 40746    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.52     |\n",
      "|    n_updates        | 40744    |\n",
      "----------------------------------\n",
      "Num timesteps: 41000\n",
      "Best mean reward: -88.10 - Last mean reward per episode: -87.51\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 105      |\n",
      "|    ep_rew_mean      | -83.6    |\n",
      "|    exploration_rate | 0.691    |\n",
      "| time/               |          |\n",
      "|    episodes         | 436      |\n",
      "|    fps              | 308      |\n",
      "|    time_elapsed     | 133      |\n",
      "|    total_timesteps  | 41245    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.47     |\n",
      "|    n_updates        | 41244    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 105      |\n",
      "|    ep_rew_mean      | -83      |\n",
      "|    exploration_rate | 0.688    |\n",
      "| time/               |          |\n",
      "|    episodes         | 440      |\n",
      "|    fps              | 308      |\n",
      "|    time_elapsed     | 135      |\n",
      "|    total_timesteps  | 41599    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.43     |\n",
      "|    n_updates        | 41596    |\n",
      "----------------------------------\n",
      "Num timesteps: 42000\n",
      "Best mean reward: -87.51 - Last mean reward per episode: -81.88\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 105      |\n",
      "|    ep_rew_mean      | -81.8    |\n",
      "|    exploration_rate | 0.685    |\n",
      "| time/               |          |\n",
      "|    episodes         | 444      |\n",
      "|    fps              | 307      |\n",
      "|    time_elapsed     | 136      |\n",
      "|    total_timesteps  | 42046    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.12     |\n",
      "|    n_updates        | 42044    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 105      |\n",
      "|    ep_rew_mean      | -81.8    |\n",
      "|    exploration_rate | 0.681    |\n",
      "| time/               |          |\n",
      "|    episodes         | 448      |\n",
      "|    fps              | 308      |\n",
      "|    time_elapsed     | 137      |\n",
      "|    total_timesteps  | 42501    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.02     |\n",
      "|    n_updates        | 42500    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 104      |\n",
      "|    ep_rew_mean      | -77.6    |\n",
      "|    exploration_rate | 0.678    |\n",
      "| time/               |          |\n",
      "|    episodes         | 452      |\n",
      "|    fps              | 308      |\n",
      "|    time_elapsed     | 139      |\n",
      "|    total_timesteps  | 42878    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.3      |\n",
      "|    n_updates        | 42876    |\n",
      "----------------------------------\n",
      "Num timesteps: 43000\n",
      "Best mean reward: -81.88 - Last mean reward per episode: -77.59\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 104      |\n",
      "|    ep_rew_mean      | -76.7    |\n",
      "|    exploration_rate | 0.676    |\n",
      "| time/               |          |\n",
      "|    episodes         | 456      |\n",
      "|    fps              | 307      |\n",
      "|    time_elapsed     | 140      |\n",
      "|    total_timesteps  | 43244    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.27     |\n",
      "|    n_updates        | 43240    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 104      |\n",
      "|    ep_rew_mean      | -77      |\n",
      "|    exploration_rate | 0.672    |\n",
      "| time/               |          |\n",
      "|    episodes         | 460      |\n",
      "|    fps              | 307      |\n",
      "|    time_elapsed     | 141      |\n",
      "|    total_timesteps  | 43669    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.21     |\n",
      "|    n_updates        | 43668    |\n",
      "----------------------------------\n",
      "Num timesteps: 44000\n",
      "Best mean reward: -77.59 - Last mean reward per episode: -77.47\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 103      |\n",
      "|    ep_rew_mean      | -75.5    |\n",
      "|    exploration_rate | 0.669    |\n",
      "| time/               |          |\n",
      "|    episodes         | 464      |\n",
      "|    fps              | 307      |\n",
      "|    time_elapsed     | 143      |\n",
      "|    total_timesteps  | 44120    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.68     |\n",
      "|    n_updates        | 44116    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 105      |\n",
      "|    ep_rew_mean      | -75.4    |\n",
      "|    exploration_rate | 0.665    |\n",
      "| time/               |          |\n",
      "|    episodes         | 468      |\n",
      "|    fps              | 307      |\n",
      "|    time_elapsed     | 145      |\n",
      "|    total_timesteps  | 44651    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.51     |\n",
      "|    n_updates        | 44648    |\n",
      "----------------------------------\n",
      "Num timesteps: 45000\n",
      "Best mean reward: -77.47 - Last mean reward per episode: -74.80\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 106      |\n",
      "|    ep_rew_mean      | -76.1    |\n",
      "|    exploration_rate | 0.661    |\n",
      "| time/               |          |\n",
      "|    episodes         | 472      |\n",
      "|    fps              | 307      |\n",
      "|    time_elapsed     | 146      |\n",
      "|    total_timesteps  | 45168    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.29     |\n",
      "|    n_updates        | 45164    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 106      |\n",
      "|    ep_rew_mean      | -76.1    |\n",
      "|    exploration_rate | 0.658    |\n",
      "| time/               |          |\n",
      "|    episodes         | 476      |\n",
      "|    fps              | 307      |\n",
      "|    time_elapsed     | 148      |\n",
      "|    total_timesteps  | 45598    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.87     |\n",
      "|    n_updates        | 45596    |\n",
      "----------------------------------\n",
      "Num timesteps: 46000\n",
      "Best mean reward: -74.80 - Last mean reward per episode: -75.83\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 106      |\n",
      "|    ep_rew_mean      | -75.2    |\n",
      "|    exploration_rate | 0.655    |\n",
      "| time/               |          |\n",
      "|    episodes         | 480      |\n",
      "|    fps              | 307      |\n",
      "|    time_elapsed     | 149      |\n",
      "|    total_timesteps  | 46047    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.72     |\n",
      "|    n_updates        | 46044    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 105      |\n",
      "|    ep_rew_mean      | -72.7    |\n",
      "|    exploration_rate | 0.652    |\n",
      "| time/               |          |\n",
      "|    episodes         | 484      |\n",
      "|    fps              | 307      |\n",
      "|    time_elapsed     | 150      |\n",
      "|    total_timesteps  | 46384    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.885    |\n",
      "|    n_updates        | 46380    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 105      |\n",
      "|    ep_rew_mean      | -71.2    |\n",
      "|    exploration_rate | 0.649    |\n",
      "| time/               |          |\n",
      "|    episodes         | 488      |\n",
      "|    fps              | 307      |\n",
      "|    time_elapsed     | 152      |\n",
      "|    total_timesteps  | 46854    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.11     |\n",
      "|    n_updates        | 46852    |\n",
      "----------------------------------\n",
      "Num timesteps: 47000\n",
      "Best mean reward: -74.80 - Last mean reward per episode: -70.33\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 106      |\n",
      "|    ep_rew_mean      | -67.6    |\n",
      "|    exploration_rate | 0.645    |\n",
      "| time/               |          |\n",
      "|    episodes         | 492      |\n",
      "|    fps              | 307      |\n",
      "|    time_elapsed     | 153      |\n",
      "|    total_timesteps  | 47297    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.04     |\n",
      "|    n_updates        | 47296    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 105      |\n",
      "|    ep_rew_mean      | -67.5    |\n",
      "|    exploration_rate | 0.643    |\n",
      "| time/               |          |\n",
      "|    episodes         | 496      |\n",
      "|    fps              | 307      |\n",
      "|    time_elapsed     | 154      |\n",
      "|    total_timesteps  | 47639    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.38     |\n",
      "|    n_updates        | 47636    |\n",
      "----------------------------------\n",
      "Num timesteps: 48000\n",
      "Best mean reward: -70.33 - Last mean reward per episode: -66.83\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 107      |\n",
      "|    ep_rew_mean      | -65.5    |\n",
      "|    exploration_rate | 0.639    |\n",
      "| time/               |          |\n",
      "|    episodes         | 500      |\n",
      "|    fps              | 307      |\n",
      "|    time_elapsed     | 156      |\n",
      "|    total_timesteps  | 48113    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.55     |\n",
      "|    n_updates        | 48112    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 106      |\n",
      "|    ep_rew_mean      | -64.1    |\n",
      "|    exploration_rate | 0.636    |\n",
      "| time/               |          |\n",
      "|    episodes         | 504      |\n",
      "|    fps              | 307      |\n",
      "|    time_elapsed     | 157      |\n",
      "|    total_timesteps  | 48531    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.04     |\n",
      "|    n_updates        | 48528    |\n",
      "----------------------------------\n",
      "Num timesteps: 49000\n",
      "Best mean reward: -66.83 - Last mean reward per episode: -64.07\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 107      |\n",
      "|    ep_rew_mean      | -64.1    |\n",
      "|    exploration_rate | 0.632    |\n",
      "| time/               |          |\n",
      "|    episodes         | 508      |\n",
      "|    fps              | 307      |\n",
      "|    time_elapsed     | 159      |\n",
      "|    total_timesteps  | 49043    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.41     |\n",
      "|    n_updates        | 49040    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 108      |\n",
      "|    ep_rew_mean      | -63.6    |\n",
      "|    exploration_rate | 0.628    |\n",
      "| time/               |          |\n",
      "|    episodes         | 512      |\n",
      "|    fps              | 307      |\n",
      "|    time_elapsed     | 161      |\n",
      "|    total_timesteps  | 49557    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.46     |\n",
      "|    n_updates        | 49556    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 108      |\n",
      "|    ep_rew_mean      | -62.4    |\n",
      "|    exploration_rate | 0.625    |\n",
      "| time/               |          |\n",
      "|    episodes         | 516      |\n",
      "|    fps              | 306      |\n",
      "|    time_elapsed     | 162      |\n",
      "|    total_timesteps  | 49998    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.21     |\n",
      "|    n_updates        | 49996    |\n",
      "----------------------------------\n",
      "Num timesteps: 50000\n",
      "Best mean reward: -64.07 - Last mean reward per episode: -62.44\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 108      |\n",
      "|    ep_rew_mean      | -61.6    |\n",
      "|    exploration_rate | 0.622    |\n",
      "| time/               |          |\n",
      "|    episodes         | 520      |\n",
      "|    fps              | 306      |\n",
      "|    time_elapsed     | 164      |\n",
      "|    total_timesteps  | 50408    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.32     |\n",
      "|    n_updates        | 50404    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 108      |\n",
      "|    ep_rew_mean      | -62.1    |\n",
      "|    exploration_rate | 0.619    |\n",
      "| time/               |          |\n",
      "|    episodes         | 524      |\n",
      "|    fps              | 307      |\n",
      "|    time_elapsed     | 165      |\n",
      "|    total_timesteps  | 50766    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.08     |\n",
      "|    n_updates        | 50764    |\n",
      "----------------------------------\n",
      "Num timesteps: 51000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -61.95\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 108      |\n",
      "|    ep_rew_mean      | -61.2    |\n",
      "|    exploration_rate | 0.616    |\n",
      "| time/               |          |\n",
      "|    episodes         | 528      |\n",
      "|    fps              | 307      |\n",
      "|    time_elapsed     | 166      |\n",
      "|    total_timesteps  | 51151    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.47     |\n",
      "|    n_updates        | 51148    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 109      |\n",
      "|    ep_rew_mean      | -59.6    |\n",
      "|    exploration_rate | 0.613    |\n",
      "| time/               |          |\n",
      "|    episodes         | 532      |\n",
      "|    fps              | 306      |\n",
      "|    time_elapsed     | 168      |\n",
      "|    total_timesteps  | 51613    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.6      |\n",
      "|    n_updates        | 51612    |\n",
      "----------------------------------\n",
      "Num timesteps: 52000\n",
      "Best mean reward: -61.95 - Last mean reward per episode: -60.51\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 109      |\n",
      "|    ep_rew_mean      | -59.4    |\n",
      "|    exploration_rate | 0.609    |\n",
      "| time/               |          |\n",
      "|    episodes         | 536      |\n",
      "|    fps              | 306      |\n",
      "|    time_elapsed     | 170      |\n",
      "|    total_timesteps  | 52171    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.71     |\n",
      "|    n_updates        | 52168    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 111      |\n",
      "|    ep_rew_mean      | -57.6    |\n",
      "|    exploration_rate | 0.605    |\n",
      "| time/               |          |\n",
      "|    episodes         | 540      |\n",
      "|    fps              | 306      |\n",
      "|    time_elapsed     | 171      |\n",
      "|    total_timesteps  | 52676    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.13     |\n",
      "|    n_updates        | 52672    |\n",
      "----------------------------------\n",
      "Num timesteps: 53000\n",
      "Best mean reward: -60.51 - Last mean reward per episode: -56.63\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 111      |\n",
      "|    ep_rew_mean      | -55.3    |\n",
      "|    exploration_rate | 0.601    |\n",
      "| time/               |          |\n",
      "|    episodes         | 544      |\n",
      "|    fps              | 306      |\n",
      "|    time_elapsed     | 173      |\n",
      "|    total_timesteps  | 53171    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.86     |\n",
      "|    n_updates        | 53168    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 111      |\n",
      "|    ep_rew_mean      | -54.5    |\n",
      "|    exploration_rate | 0.598    |\n",
      "| time/               |          |\n",
      "|    episodes         | 548      |\n",
      "|    fps              | 306      |\n",
      "|    time_elapsed     | 174      |\n",
      "|    total_timesteps  | 53603    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.1      |\n",
      "|    n_updates        | 53600    |\n",
      "----------------------------------\n",
      "Num timesteps: 54000\n",
      "Best mean reward: -56.63 - Last mean reward per episode: -54.15\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 111      |\n",
      "|    ep_rew_mean      | -53.5    |\n",
      "|    exploration_rate | 0.595    |\n",
      "| time/               |          |\n",
      "|    episodes         | 552      |\n",
      "|    fps              | 306      |\n",
      "|    time_elapsed     | 176      |\n",
      "|    total_timesteps  | 54025    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.1      |\n",
      "|    n_updates        | 54024    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 112      |\n",
      "|    ep_rew_mean      | -50.6    |\n",
      "|    exploration_rate | 0.592    |\n",
      "| time/               |          |\n",
      "|    episodes         | 556      |\n",
      "|    fps              | 306      |\n",
      "|    time_elapsed     | 177      |\n",
      "|    total_timesteps  | 54452    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.25     |\n",
      "|    n_updates        | 54448    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 112      |\n",
      "|    ep_rew_mean      | -49.8    |\n",
      "|    exploration_rate | 0.588    |\n",
      "| time/               |          |\n",
      "|    episodes         | 560      |\n",
      "|    fps              | 306      |\n",
      "|    time_elapsed     | 179      |\n",
      "|    total_timesteps  | 54916    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.73     |\n",
      "|    n_updates        | 54912    |\n",
      "----------------------------------\n",
      "Num timesteps: 55000\n",
      "Best mean reward: -54.15 - Last mean reward per episode: -49.76\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 113      |\n",
      "|    ep_rew_mean      | -48      |\n",
      "|    exploration_rate | 0.585    |\n",
      "| time/               |          |\n",
      "|    episodes         | 564      |\n",
      "|    fps              | 306      |\n",
      "|    time_elapsed     | 180      |\n",
      "|    total_timesteps  | 55392    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.779    |\n",
      "|    n_updates        | 55388    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 112      |\n",
      "|    ep_rew_mean      | -45.5    |\n",
      "|    exploration_rate | 0.581    |\n",
      "| time/               |          |\n",
      "|    episodes         | 568      |\n",
      "|    fps              | 306      |\n",
      "|    time_elapsed     | 182      |\n",
      "|    total_timesteps  | 55839    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.23     |\n",
      "|    n_updates        | 55836    |\n",
      "----------------------------------\n",
      "Num timesteps: 56000\n",
      "Best mean reward: -49.76 - Last mean reward per episode: -45.35\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 112      |\n",
      "|    ep_rew_mean      | -42.7    |\n",
      "|    exploration_rate | 0.578    |\n",
      "| time/               |          |\n",
      "|    episodes         | 572      |\n",
      "|    fps              | 306      |\n",
      "|    time_elapsed     | 183      |\n",
      "|    total_timesteps  | 56321    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.57     |\n",
      "|    n_updates        | 56320    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 112      |\n",
      "|    ep_rew_mean      | -41.8    |\n",
      "|    exploration_rate | 0.574    |\n",
      "| time/               |          |\n",
      "|    episodes         | 576      |\n",
      "|    fps              | 306      |\n",
      "|    time_elapsed     | 185      |\n",
      "|    total_timesteps  | 56812    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.98     |\n",
      "|    n_updates        | 56808    |\n",
      "----------------------------------\n",
      "Num timesteps: 57000\n",
      "Best mean reward: -45.35 - Last mean reward per episode: -40.60\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 112      |\n",
      "|    ep_rew_mean      | -40.5    |\n",
      "|    exploration_rate | 0.57     |\n",
      "| time/               |          |\n",
      "|    episodes         | 580      |\n",
      "|    fps              | 305      |\n",
      "|    time_elapsed     | 187      |\n",
      "|    total_timesteps  | 57267    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.55     |\n",
      "|    n_updates        | 57264    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 114      |\n",
      "|    ep_rew_mean      | -39.8    |\n",
      "|    exploration_rate | 0.567    |\n",
      "| time/               |          |\n",
      "|    episodes         | 584      |\n",
      "|    fps              | 305      |\n",
      "|    time_elapsed     | 188      |\n",
      "|    total_timesteps  | 57761    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.29     |\n",
      "|    n_updates        | 57760    |\n",
      "----------------------------------\n",
      "Num timesteps: 58000\n",
      "Best mean reward: -40.60 - Last mean reward per episode: -40.24\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 114      |\n",
      "|    ep_rew_mean      | -41.6    |\n",
      "|    exploration_rate | 0.563    |\n",
      "| time/               |          |\n",
      "|    episodes         | 588      |\n",
      "|    fps              | 305      |\n",
      "|    time_elapsed     | 190      |\n",
      "|    total_timesteps  | 58231    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.25     |\n",
      "|    n_updates        | 58228    |\n",
      "----------------------------------\n",
      "Num timesteps: 59000\n",
      "Best mean reward: -40.24 - Last mean reward per episode: -43.70\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 118      |\n",
      "|    ep_rew_mean      | -43.1    |\n",
      "|    exploration_rate | 0.557    |\n",
      "| time/               |          |\n",
      "|    episodes         | 592      |\n",
      "|    fps              | 305      |\n",
      "|    time_elapsed     | 193      |\n",
      "|    total_timesteps  | 59053    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.53     |\n",
      "|    n_updates        | 59052    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 118      |\n",
      "|    ep_rew_mean      | -42.9    |\n",
      "|    exploration_rate | 0.554    |\n",
      "| time/               |          |\n",
      "|    episodes         | 596      |\n",
      "|    fps              | 305      |\n",
      "|    time_elapsed     | 194      |\n",
      "|    total_timesteps  | 59488    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.6      |\n",
      "|    n_updates        | 59484    |\n",
      "----------------------------------\n",
      "Num timesteps: 60000\n",
      "Best mean reward: -40.24 - Last mean reward per episode: -41.80\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 121      |\n",
      "|    ep_rew_mean      | -41.8    |\n",
      "|    exploration_rate | 0.548    |\n",
      "| time/               |          |\n",
      "|    episodes         | 600      |\n",
      "|    fps              | 305      |\n",
      "|    time_elapsed     | 197      |\n",
      "|    total_timesteps  | 60237    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.05     |\n",
      "|    n_updates        | 60236    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 123      |\n",
      "|    ep_rew_mean      | -41.6    |\n",
      "|    exploration_rate | 0.544    |\n",
      "| time/               |          |\n",
      "|    episodes         | 604      |\n",
      "|    fps              | 305      |\n",
      "|    time_elapsed     | 199      |\n",
      "|    total_timesteps  | 60832    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.49     |\n",
      "|    n_updates        | 60828    |\n",
      "----------------------------------\n",
      "Num timesteps: 61000\n",
      "Best mean reward: -40.24 - Last mean reward per episode: -42.80\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 124      |\n",
      "|    ep_rew_mean      | -41      |\n",
      "|    exploration_rate | 0.54     |\n",
      "| time/               |          |\n",
      "|    episodes         | 608      |\n",
      "|    fps              | 305      |\n",
      "|    time_elapsed     | 201      |\n",
      "|    total_timesteps  | 61399    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.81     |\n",
      "|    n_updates        | 61396    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 122      |\n",
      "|    ep_rew_mean      | -39.9    |\n",
      "|    exploration_rate | 0.537    |\n",
      "| time/               |          |\n",
      "|    episodes         | 612      |\n",
      "|    fps              | 304      |\n",
      "|    time_elapsed     | 202      |\n",
      "|    total_timesteps  | 61797    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.69     |\n",
      "|    n_updates        | 61796    |\n",
      "----------------------------------\n",
      "Num timesteps: 62000\n",
      "Best mean reward: -40.24 - Last mean reward per episode: -40.21\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 122      |\n",
      "|    ep_rew_mean      | -39.6    |\n",
      "|    exploration_rate | 0.533    |\n",
      "| time/               |          |\n",
      "|    episodes         | 616      |\n",
      "|    fps              | 304      |\n",
      "|    time_elapsed     | 204      |\n",
      "|    total_timesteps  | 62223    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.58     |\n",
      "|    n_updates        | 62220    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 124      |\n",
      "|    ep_rew_mean      | -37.8    |\n",
      "|    exploration_rate | 0.529    |\n",
      "| time/               |          |\n",
      "|    episodes         | 620      |\n",
      "|    fps              | 304      |\n",
      "|    time_elapsed     | 206      |\n",
      "|    total_timesteps  | 62769    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.31     |\n",
      "|    n_updates        | 62768    |\n",
      "----------------------------------\n",
      "Num timesteps: 63000\n",
      "Best mean reward: -40.21 - Last mean reward per episode: -35.25\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 124      |\n",
      "|    ep_rew_mean      | -35.3    |\n",
      "|    exploration_rate | 0.526    |\n",
      "| time/               |          |\n",
      "|    episodes         | 624      |\n",
      "|    fps              | 304      |\n",
      "|    time_elapsed     | 207      |\n",
      "|    total_timesteps  | 63154    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.62     |\n",
      "|    n_updates        | 63152    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 125      |\n",
      "|    ep_rew_mean      | -34.6    |\n",
      "|    exploration_rate | 0.522    |\n",
      "| time/               |          |\n",
      "|    episodes         | 628      |\n",
      "|    fps              | 304      |\n",
      "|    time_elapsed     | 209      |\n",
      "|    total_timesteps  | 63670    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.71     |\n",
      "|    n_updates        | 63668    |\n",
      "----------------------------------\n",
      "Num timesteps: 64000\n",
      "Best mean reward: -35.25 - Last mean reward per episode: -34.49\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 125      |\n",
      "|    ep_rew_mean      | -35      |\n",
      "|    exploration_rate | 0.519    |\n",
      "| time/               |          |\n",
      "|    episodes         | 632      |\n",
      "|    fps              | 303      |\n",
      "|    time_elapsed     | 210      |\n",
      "|    total_timesteps  | 64103    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.93     |\n",
      "|    n_updates        | 64100    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 125      |\n",
      "|    ep_rew_mean      | -33.1    |\n",
      "|    exploration_rate | 0.515    |\n",
      "| time/               |          |\n",
      "|    episodes         | 636      |\n",
      "|    fps              | 303      |\n",
      "|    time_elapsed     | 212      |\n",
      "|    total_timesteps  | 64647    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.49     |\n",
      "|    n_updates        | 64644    |\n",
      "----------------------------------\n",
      "Num timesteps: 65000\n",
      "Best mean reward: -34.49 - Last mean reward per episode: -34.33\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 124      |\n",
      "|    ep_rew_mean      | -33.5    |\n",
      "|    exploration_rate | 0.512    |\n",
      "| time/               |          |\n",
      "|    episodes         | 640      |\n",
      "|    fps              | 303      |\n",
      "|    time_elapsed     | 214      |\n",
      "|    total_timesteps  | 65043    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.49     |\n",
      "|    n_updates        | 65040    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 124      |\n",
      "|    ep_rew_mean      | -33.1    |\n",
      "|    exploration_rate | 0.509    |\n",
      "| time/               |          |\n",
      "|    episodes         | 644      |\n",
      "|    fps              | 303      |\n",
      "|    time_elapsed     | 216      |\n",
      "|    total_timesteps  | 65524    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.73     |\n",
      "|    n_updates        | 65520    |\n",
      "----------------------------------\n",
      "Num timesteps: 66000\n",
      "Best mean reward: -34.33 - Last mean reward per episode: -34.03\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 124      |\n",
      "|    ep_rew_mean      | -33.8    |\n",
      "|    exploration_rate | 0.505    |\n",
      "| time/               |          |\n",
      "|    episodes         | 648      |\n",
      "|    fps              | 303      |\n",
      "|    time_elapsed     | 217      |\n",
      "|    total_timesteps  | 66033    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.33     |\n",
      "|    n_updates        | 66032    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 125      |\n",
      "|    ep_rew_mean      | -32.8    |\n",
      "|    exploration_rate | 0.501    |\n",
      "| time/               |          |\n",
      "|    episodes         | 652      |\n",
      "|    fps              | 302      |\n",
      "|    time_elapsed     | 219      |\n",
      "|    total_timesteps  | 66542    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.73     |\n",
      "|    n_updates        | 66540    |\n",
      "----------------------------------\n",
      "Num timesteps: 67000\n",
      "Best mean reward: -34.03 - Last mean reward per episode: -32.60\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 125      |\n",
      "|    ep_rew_mean      | -32.8    |\n",
      "|    exploration_rate | 0.497    |\n",
      "| time/               |          |\n",
      "|    episodes         | 656      |\n",
      "|    fps              | 302      |\n",
      "|    time_elapsed     | 221      |\n",
      "|    total_timesteps  | 67001    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.45     |\n",
      "|    n_updates        | 67000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 130      |\n",
      "|    ep_rew_mean      | -31.9    |\n",
      "|    exploration_rate | 0.491    |\n",
      "| time/               |          |\n",
      "|    episodes         | 660      |\n",
      "|    fps              | 302      |\n",
      "|    time_elapsed     | 224      |\n",
      "|    total_timesteps  | 67871    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.42     |\n",
      "|    n_updates        | 67868    |\n",
      "----------------------------------\n",
      "Num timesteps: 68000\n",
      "Best mean reward: -32.60 - Last mean reward per episode: -31.92\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 69000\n",
      "Best mean reward: -31.92 - Last mean reward per episode: -31.87\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 139      |\n",
      "|    ep_rew_mean      | -32.7    |\n",
      "|    exploration_rate | 0.48     |\n",
      "| time/               |          |\n",
      "|    episodes         | 664      |\n",
      "|    fps              | 299      |\n",
      "|    time_elapsed     | 231      |\n",
      "|    total_timesteps  | 69273    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2        |\n",
      "|    n_updates        | 69272    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 139      |\n",
      "|    ep_rew_mean      | -33.2    |\n",
      "|    exploration_rate | 0.477    |\n",
      "| time/               |          |\n",
      "|    episodes         | 668      |\n",
      "|    fps              | 299      |\n",
      "|    time_elapsed     | 232      |\n",
      "|    total_timesteps  | 69733    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.71     |\n",
      "|    n_updates        | 69732    |\n",
      "----------------------------------\n",
      "Num timesteps: 70000\n",
      "Best mean reward: -31.87 - Last mean reward per episode: -33.33\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 144      |\n",
      "|    ep_rew_mean      | -34.7    |\n",
      "|    exploration_rate | 0.47     |\n",
      "| time/               |          |\n",
      "|    episodes         | 672      |\n",
      "|    fps              | 298      |\n",
      "|    time_elapsed     | 236      |\n",
      "|    total_timesteps  | 70671    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.97     |\n",
      "|    n_updates        | 70668    |\n",
      "----------------------------------\n",
      "Num timesteps: 71000\n",
      "Best mean reward: -31.87 - Last mean reward per episode: -34.54\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 146      |\n",
      "|    ep_rew_mean      | -35.3    |\n",
      "|    exploration_rate | 0.465    |\n",
      "| time/               |          |\n",
      "|    episodes         | 676      |\n",
      "|    fps              | 298      |\n",
      "|    time_elapsed     | 239      |\n",
      "|    total_timesteps  | 71384    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.93     |\n",
      "|    n_updates        | 71380    |\n",
      "----------------------------------\n",
      "Num timesteps: 72000\n",
      "Best mean reward: -31.87 - Last mean reward per episode: -33.39\n",
      "Num timesteps: 73000\n",
      "Best mean reward: -31.87 - Last mean reward per episode: -35.32\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 159      |\n",
      "|    ep_rew_mean      | -35.2    |\n",
      "|    exploration_rate | 0.451    |\n",
      "| time/               |          |\n",
      "|    episodes         | 680      |\n",
      "|    fps              | 296      |\n",
      "|    time_elapsed     | 246      |\n",
      "|    total_timesteps  | 73162    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.56     |\n",
      "|    n_updates        | 73160    |\n",
      "----------------------------------\n",
      "Num timesteps: 74000\n",
      "Best mean reward: -31.87 - Last mean reward per episode: -35.60\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 164      |\n",
      "|    ep_rew_mean      | -36.6    |\n",
      "|    exploration_rate | 0.444    |\n",
      "| time/               |          |\n",
      "|    episodes         | 684      |\n",
      "|    fps              | 295      |\n",
      "|    time_elapsed     | 250      |\n",
      "|    total_timesteps  | 74168    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.55     |\n",
      "|    n_updates        | 74164    |\n",
      "----------------------------------\n",
      "Num timesteps: 75000\n",
      "Best mean reward: -31.87 - Last mean reward per episode: -36.52\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 173      |\n",
      "|    ep_rew_mean      | -34.9    |\n",
      "|    exploration_rate | 0.433    |\n",
      "| time/               |          |\n",
      "|    episodes         | 688      |\n",
      "|    fps              | 294      |\n",
      "|    time_elapsed     | 256      |\n",
      "|    total_timesteps  | 75563    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.43     |\n",
      "|    n_updates        | 75560    |\n",
      "----------------------------------\n",
      "Num timesteps: 76000\n",
      "Best mean reward: -31.87 - Last mean reward per episode: -34.05\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 177      |\n",
      "|    ep_rew_mean      | -36.5    |\n",
      "|    exploration_rate | 0.424    |\n",
      "| time/               |          |\n",
      "|    episodes         | 692      |\n",
      "|    fps              | 293      |\n",
      "|    time_elapsed     | 261      |\n",
      "|    total_timesteps  | 76778    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.59     |\n",
      "|    n_updates        | 76776    |\n",
      "----------------------------------\n",
      "Num timesteps: 77000\n",
      "Best mean reward: -31.87 - Last mean reward per episode: -36.60\n",
      "Num timesteps: 78000\n",
      "Best mean reward: -31.87 - Last mean reward per episode: -35.72\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 187      |\n",
      "|    ep_rew_mean      | -35.9    |\n",
      "|    exploration_rate | 0.414    |\n",
      "| time/               |          |\n",
      "|    episodes         | 696      |\n",
      "|    fps              | 292      |\n",
      "|    time_elapsed     | 267      |\n",
      "|    total_timesteps  | 78147    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 4.18     |\n",
      "|    n_updates        | 78144    |\n",
      "----------------------------------\n",
      "Num timesteps: 79000\n",
      "Best mean reward: -31.87 - Last mean reward per episode: -35.36\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 193      |\n",
      "|    ep_rew_mean      | -35.5    |\n",
      "|    exploration_rate | 0.404    |\n",
      "| time/               |          |\n",
      "|    episodes         | 700      |\n",
      "|    fps              | 290      |\n",
      "|    time_elapsed     | 273      |\n",
      "|    total_timesteps  | 79488    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.35     |\n",
      "|    n_updates        | 79484    |\n",
      "----------------------------------\n",
      "Num timesteps: 80000\n",
      "Best mean reward: -31.87 - Last mean reward per episode: -35.20\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 192      |\n",
      "|    ep_rew_mean      | -35.6    |\n",
      "|    exploration_rate | 0.4      |\n",
      "| time/               |          |\n",
      "|    episodes         | 704      |\n",
      "|    fps              | 290      |\n",
      "|    time_elapsed     | 275      |\n",
      "|    total_timesteps  | 80038    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.26     |\n",
      "|    n_updates        | 80036    |\n",
      "----------------------------------\n",
      "Num timesteps: 81000\n",
      "Best mean reward: -31.87 - Last mean reward per episode: -34.40\n",
      "Num timesteps: 82000\n",
      "Best mean reward: -31.87 - Last mean reward per episode: -33.38\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 209      |\n",
      "|    ep_rew_mean      | -33      |\n",
      "|    exploration_rate | 0.383    |\n",
      "| time/               |          |\n",
      "|    episodes         | 708      |\n",
      "|    fps              | 287      |\n",
      "|    time_elapsed     | 286      |\n",
      "|    total_timesteps  | 82323    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.76     |\n",
      "|    n_updates        | 82320    |\n",
      "----------------------------------\n",
      "Num timesteps: 83000\n",
      "Best mean reward: -31.87 - Last mean reward per episode: -33.03\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 219      |\n",
      "|    ep_rew_mean      | -31.6    |\n",
      "|    exploration_rate | 0.372    |\n",
      "| time/               |          |\n",
      "|    episodes         | 712      |\n",
      "|    fps              | 286      |\n",
      "|    time_elapsed     | 292      |\n",
      "|    total_timesteps  | 83671    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.69     |\n",
      "|    n_updates        | 83668    |\n",
      "----------------------------------\n",
      "Num timesteps: 84000\n",
      "Best mean reward: -31.87 - Last mean reward per episode: -31.79\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 85000\n",
      "Best mean reward: -31.79 - Last mean reward per episode: -31.00\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 86000\n",
      "Best mean reward: -31.00 - Last mean reward per episode: -30.46\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 245      |\n",
      "|    ep_rew_mean      | -29.8    |\n",
      "|    exploration_rate | 0.349    |\n",
      "| time/               |          |\n",
      "|    episodes         | 716      |\n",
      "|    fps              | 282      |\n",
      "|    time_elapsed     | 307      |\n",
      "|    total_timesteps  | 86771    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.17     |\n",
      "|    n_updates        | 86768    |\n",
      "----------------------------------\n",
      "Num timesteps: 87000\n",
      "Best mean reward: -30.46 - Last mean reward per episode: -29.81\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 88000\n",
      "Best mean reward: -29.81 - Last mean reward per episode: -30.07\n",
      "Num timesteps: 89000\n",
      "Best mean reward: -29.81 - Last mean reward per episode: -29.96\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 271      |\n",
      "|    ep_rew_mean      | -29.2    |\n",
      "|    exploration_rate | 0.326    |\n",
      "| time/               |          |\n",
      "|    episodes         | 720      |\n",
      "|    fps              | 279      |\n",
      "|    time_elapsed     | 320      |\n",
      "|    total_timesteps  | 89849    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.77     |\n",
      "|    n_updates        | 89848    |\n",
      "----------------------------------\n",
      "Num timesteps: 90000\n",
      "Best mean reward: -29.81 - Last mean reward per episode: -29.30\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 91000\n",
      "Best mean reward: -29.30 - Last mean reward per episode: -28.47\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 92000\n",
      "Best mean reward: -28.47 - Last mean reward per episode: -28.48\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 289      |\n",
      "|    ep_rew_mean      | -27.7    |\n",
      "|    exploration_rate | 0.31     |\n",
      "| time/               |          |\n",
      "|    episodes         | 724      |\n",
      "|    fps              | 278      |\n",
      "|    time_elapsed     | 330      |\n",
      "|    total_timesteps  | 92031    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.9      |\n",
      "|    n_updates        | 92028    |\n",
      "----------------------------------\n",
      "Num timesteps: 93000\n",
      "Best mean reward: -28.47 - Last mean reward per episode: -27.23\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 94000\n",
      "Best mean reward: -27.23 - Last mean reward per episode: -25.55\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 306      |\n",
      "|    ep_rew_mean      | -24.3    |\n",
      "|    exploration_rate | 0.293    |\n",
      "| time/               |          |\n",
      "|    episodes         | 728      |\n",
      "|    fps              | 276      |\n",
      "|    time_elapsed     | 340      |\n",
      "|    total_timesteps  | 94316    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.63     |\n",
      "|    n_updates        | 94312    |\n",
      "----------------------------------\n",
      "Num timesteps: 95000\n",
      "Best mean reward: -25.55 - Last mean reward per episode: -24.31\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 96000\n",
      "Best mean reward: -24.31 - Last mean reward per episode: -23.62\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 97000\n",
      "Best mean reward: -23.62 - Last mean reward per episode: -23.34\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 98000\n",
      "Best mean reward: -23.34 - Last mean reward per episode: -22.64\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 342      |\n",
      "|    ep_rew_mean      | -21.4    |\n",
      "|    exploration_rate | 0.263    |\n",
      "| time/               |          |\n",
      "|    episodes         | 732      |\n",
      "|    fps              | 272      |\n",
      "|    time_elapsed     | 360      |\n",
      "|    total_timesteps  | 98316    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.58     |\n",
      "|    n_updates        | 98312    |\n",
      "----------------------------------\n",
      "Num timesteps: 99000\n",
      "Best mean reward: -22.64 - Last mean reward per episode: -20.95\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 100000\n",
      "Best mean reward: -20.95 - Last mean reward per episode: -19.87\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 101000\n",
      "Best mean reward: -19.87 - Last mean reward per episode: -18.44\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 368      |\n",
      "|    ep_rew_mean      | -17.3    |\n",
      "|    exploration_rate | 0.239    |\n",
      "| time/               |          |\n",
      "|    episodes         | 736      |\n",
      "|    fps              | 270      |\n",
      "|    time_elapsed     | 375      |\n",
      "|    total_timesteps  | 101476   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 4.31     |\n",
      "|    n_updates        | 101472   |\n",
      "----------------------------------\n",
      "Num timesteps: 102000\n",
      "Best mean reward: -18.44 - Last mean reward per episode: -13.70\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 370      |\n",
      "|    ep_rew_mean      | -13.4    |\n",
      "|    exploration_rate | 0.235    |\n",
      "| time/               |          |\n",
      "|    episodes         | 740      |\n",
      "|    fps              | 270      |\n",
      "|    time_elapsed     | 377      |\n",
      "|    total_timesteps  | 102045   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 4.34     |\n",
      "|    n_updates        | 102044   |\n",
      "----------------------------------\n",
      "Num timesteps: 103000\n",
      "Best mean reward: -13.70 - Last mean reward per episode: -13.15\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 104000\n",
      "Best mean reward: -13.15 - Last mean reward per episode: -13.01\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 392      |\n",
      "|    ep_rew_mean      | -8.13    |\n",
      "|    exploration_rate | 0.215    |\n",
      "| time/               |          |\n",
      "|    episodes         | 744      |\n",
      "|    fps              | 268      |\n",
      "|    time_elapsed     | 389      |\n",
      "|    total_timesteps  | 104720   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.61     |\n",
      "|    n_updates        | 104716   |\n",
      "----------------------------------\n",
      "Num timesteps: 105000\n",
      "Best mean reward: -13.01 - Last mean reward per episode: -7.02\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 106000\n",
      "Best mean reward: -7.02 - Last mean reward per episode: -3.54\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 408      |\n",
      "|    ep_rew_mean      | -2.84    |\n",
      "|    exploration_rate | 0.199    |\n",
      "| time/               |          |\n",
      "|    episodes         | 748      |\n",
      "|    fps              | 267      |\n",
      "|    time_elapsed     | 398      |\n",
      "|    total_timesteps  | 106858   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.82     |\n",
      "|    n_updates        | 106856   |\n",
      "----------------------------------\n",
      "Num timesteps: 107000\n",
      "Best mean reward: -3.54 - Last mean reward per episode: -2.84\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 108000\n",
      "Best mean reward: -2.84 - Last mean reward per episode: 0.01\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 417      |\n",
      "|    ep_rew_mean      | 0.668    |\n",
      "|    exploration_rate | 0.189    |\n",
      "| time/               |          |\n",
      "|    episodes         | 752      |\n",
      "|    fps              | 267      |\n",
      "|    time_elapsed     | 404      |\n",
      "|    total_timesteps  | 108200   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.99     |\n",
      "|    n_updates        | 108196   |\n",
      "----------------------------------\n",
      "Num timesteps: 109000\n",
      "Best mean reward: 0.01 - Last mean reward per episode: 0.67\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 425      |\n",
      "|    ep_rew_mean      | 1.24     |\n",
      "|    exploration_rate | 0.179    |\n",
      "| time/               |          |\n",
      "|    episodes         | 756      |\n",
      "|    fps              | 266      |\n",
      "|    time_elapsed     | 410      |\n",
      "|    total_timesteps  | 109494   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.97     |\n",
      "|    n_updates        | 109492   |\n",
      "----------------------------------\n",
      "Num timesteps: 110000\n",
      "Best mean reward: 0.67 - Last mean reward per episode: 2.21\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 430      |\n",
      "|    ep_rew_mean      | 9.07     |\n",
      "|    exploration_rate | 0.169    |\n",
      "| time/               |          |\n",
      "|    episodes         | 760      |\n",
      "|    fps              | 266      |\n",
      "|    time_elapsed     | 415      |\n",
      "|    total_timesteps  | 110865   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.74     |\n",
      "|    n_updates        | 110864   |\n",
      "----------------------------------\n",
      "Num timesteps: 111000\n",
      "Best mean reward: 2.21 - Last mean reward per episode: 9.07\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 112000\n",
      "Best mean reward: 9.07 - Last mean reward per episode: 15.15\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 434      |\n",
      "|    ep_rew_mean      | 16.1     |\n",
      "|    exploration_rate | 0.155    |\n",
      "| time/               |          |\n",
      "|    episodes         | 764      |\n",
      "|    fps              | 266      |\n",
      "|    time_elapsed     | 423      |\n",
      "|    total_timesteps  | 112707   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.6      |\n",
      "|    n_updates        | 112704   |\n",
      "----------------------------------\n",
      "Num timesteps: 113000\n",
      "Best mean reward: 15.15 - Last mean reward per episode: 16.67\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 114000\n",
      "Best mean reward: 16.67 - Last mean reward per episode: 19.49\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 446      |\n",
      "|    ep_rew_mean      | 20.8     |\n",
      "|    exploration_rate | 0.143    |\n",
      "| time/               |          |\n",
      "|    episodes         | 768      |\n",
      "|    fps              | 265      |\n",
      "|    time_elapsed     | 429      |\n",
      "|    total_timesteps  | 114305   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.48     |\n",
      "|    n_updates        | 114304   |\n",
      "----------------------------------\n",
      "Num timesteps: 115000\n",
      "Best mean reward: 19.49 - Last mean reward per episode: 23.64\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 451      |\n",
      "|    ep_rew_mean      | 28       |\n",
      "|    exploration_rate | 0.132    |\n",
      "| time/               |          |\n",
      "|    episodes         | 772      |\n",
      "|    fps              | 265      |\n",
      "|    time_elapsed     | 435      |\n",
      "|    total_timesteps  | 115767   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.18     |\n",
      "|    n_updates        | 115764   |\n",
      "----------------------------------\n",
      "Num timesteps: 116000\n",
      "Best mean reward: 23.64 - Last mean reward per episode: 28.04\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 117000\n",
      "Best mean reward: 28.04 - Last mean reward per episode: 29.85\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 118000\n",
      "Best mean reward: 29.85 - Last mean reward per episode: 30.40\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 471      |\n",
      "|    ep_rew_mean      | 37.2     |\n",
      "|    exploration_rate | 0.111    |\n",
      "| time/               |          |\n",
      "|    episodes         | 776      |\n",
      "|    fps              | 264      |\n",
      "|    time_elapsed     | 447      |\n",
      "|    total_timesteps  | 118516   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.53     |\n",
      "|    n_updates        | 118512   |\n",
      "----------------------------------\n",
      "Num timesteps: 119000\n",
      "Best mean reward: 30.40 - Last mean reward per episode: 37.15\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 120000\n",
      "Best mean reward: 37.15 - Last mean reward per episode: 37.10\n",
      "Num timesteps: 121000\n",
      "Best mean reward: 37.15 - Last mean reward per episode: 41.95\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 480      |\n",
      "|    ep_rew_mean      | 43.7     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 780      |\n",
      "|    fps              | 263      |\n",
      "|    time_elapsed     | 459      |\n",
      "|    total_timesteps  | 121156   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.96     |\n",
      "|    n_updates        | 121152   |\n",
      "----------------------------------\n",
      "Num timesteps: 122000\n",
      "Best mean reward: 41.95 - Last mean reward per episode: 50.68\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 483      |\n",
      "|    ep_rew_mean      | 54.8     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 784      |\n",
      "|    fps              | 263      |\n",
      "|    time_elapsed     | 464      |\n",
      "|    total_timesteps  | 122485   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.53     |\n",
      "|    n_updates        | 122484   |\n",
      "----------------------------------\n",
      "Num timesteps: 123000\n",
      "Best mean reward: 50.68 - Last mean reward per episode: 57.80\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 484      |\n",
      "|    ep_rew_mean      | 67.3     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 788      |\n",
      "|    fps              | 263      |\n",
      "|    time_elapsed     | 470      |\n",
      "|    total_timesteps  | 123987   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.59     |\n",
      "|    n_updates        | 123984   |\n",
      "----------------------------------\n",
      "Num timesteps: 124000\n",
      "Best mean reward: 57.80 - Last mean reward per episode: 67.26\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 125000\n",
      "Best mean reward: 67.26 - Last mean reward per episode: 66.90\n",
      "Num timesteps: 126000\n",
      "Best mean reward: 67.26 - Last mean reward per episode: 73.49\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 498      |\n",
      "|    ep_rew_mean      | 74.9     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 792      |\n",
      "|    fps              | 262      |\n",
      "|    time_elapsed     | 482      |\n",
      "|    total_timesteps  | 126626   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.53     |\n",
      "|    n_updates        | 126624   |\n",
      "----------------------------------\n",
      "Num timesteps: 127000\n",
      "Best mean reward: 73.49 - Last mean reward per episode: 74.93\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 128000\n",
      "Best mean reward: 74.93 - Last mean reward per episode: 76.00\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 129000\n",
      "Best mean reward: 76.00 - Last mean reward per episode: 77.20\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 514      |\n",
      "|    ep_rew_mean      | 78.9     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 796      |\n",
      "|    fps              | 261      |\n",
      "|    time_elapsed     | 494      |\n",
      "|    total_timesteps  | 129500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.03     |\n",
      "|    n_updates        | 129496   |\n",
      "----------------------------------\n",
      "Num timesteps: 130000\n",
      "Best mean reward: 77.20 - Last mean reward per episode: 78.94\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 131000\n",
      "Best mean reward: 78.94 - Last mean reward per episode: 79.13\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 516      |\n",
      "|    ep_rew_mean      | 82.3     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 800      |\n",
      "|    fps              | 261      |\n",
      "|    time_elapsed     | 501      |\n",
      "|    total_timesteps  | 131039   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.783    |\n",
      "|    n_updates        | 131036   |\n",
      "----------------------------------\n",
      "Num timesteps: 132000\n",
      "Best mean reward: 79.13 - Last mean reward per episode: 85.67\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 525      |\n",
      "|    ep_rew_mean      | 88.1     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 804      |\n",
      "|    fps              | 261      |\n",
      "|    time_elapsed     | 507      |\n",
      "|    total_timesteps  | 132526   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.918    |\n",
      "|    n_updates        | 132524   |\n",
      "----------------------------------\n",
      "Num timesteps: 133000\n",
      "Best mean reward: 85.67 - Last mean reward per episode: 88.09\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 134000\n",
      "Best mean reward: 88.09 - Last mean reward per episode: 86.62\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 522      |\n",
      "|    ep_rew_mean      | 87.7     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 808      |\n",
      "|    fps              | 260      |\n",
      "|    time_elapsed     | 515      |\n",
      "|    total_timesteps  | 134478   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.804    |\n",
      "|    n_updates        | 134476   |\n",
      "----------------------------------\n",
      "Num timesteps: 135000\n",
      "Best mean reward: 88.09 - Last mean reward per episode: 90.11\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 136000\n",
      "Best mean reward: 90.11 - Last mean reward per episode: 92.89\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 531      |\n",
      "|    ep_rew_mean      | 96.3     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 812      |\n",
      "|    fps              | 260      |\n",
      "|    time_elapsed     | 525      |\n",
      "|    total_timesteps  | 136806   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.954    |\n",
      "|    n_updates        | 136804   |\n",
      "----------------------------------\n",
      "Num timesteps: 137000\n",
      "Best mean reward: 92.89 - Last mean reward per episode: 96.31\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 512      |\n",
      "|    ep_rew_mean      | 101      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 816      |\n",
      "|    fps              | 260      |\n",
      "|    time_elapsed     | 530      |\n",
      "|    total_timesteps  | 137996   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.67     |\n",
      "|    n_updates        | 137992   |\n",
      "----------------------------------\n",
      "Num timesteps: 138000\n",
      "Best mean reward: 96.31 - Last mean reward per episode: 101.03\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 139000\n",
      "Best mean reward: 101.03 - Last mean reward per episode: 102.97\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 140000\n",
      "Best mean reward: 102.97 - Last mean reward per episode: 106.99\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 502      |\n",
      "|    ep_rew_mean      | 107      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 820      |\n",
      "|    fps              | 259      |\n",
      "|    time_elapsed     | 538      |\n",
      "|    total_timesteps  | 140020   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.622    |\n",
      "|    n_updates        | 140016   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 485      |\n",
      "|    ep_rew_mean      | 108      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 824      |\n",
      "|    fps              | 259      |\n",
      "|    time_elapsed     | 540      |\n",
      "|    total_timesteps  | 140518   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.938    |\n",
      "|    n_updates        | 140516   |\n",
      "----------------------------------\n",
      "Num timesteps: 141000\n",
      "Best mean reward: 106.99 - Last mean reward per episode: 106.28\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 476      |\n",
      "|    ep_rew_mean      | 107      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 828      |\n",
      "|    fps              | 259      |\n",
      "|    time_elapsed     | 546      |\n",
      "|    total_timesteps  | 141945   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.485    |\n",
      "|    n_updates        | 141944   |\n",
      "----------------------------------\n",
      "Num timesteps: 142000\n",
      "Best mean reward: 106.99 - Last mean reward per episode: 107.13\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 143000\n",
      "Best mean reward: 107.13 - Last mean reward per episode: 109.25\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 144000\n",
      "Best mean reward: 109.25 - Last mean reward per episode: 110.10\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 458      |\n",
      "|    ep_rew_mean      | 112      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 832      |\n",
      "|    fps              | 259      |\n",
      "|    time_elapsed     | 556      |\n",
      "|    total_timesteps  | 144165   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.574    |\n",
      "|    n_updates        | 144164   |\n",
      "----------------------------------\n",
      "Num timesteps: 145000\n",
      "Best mean reward: 110.10 - Last mean reward per episode: 114.08\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 441      |\n",
      "|    ep_rew_mean      | 113      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 836      |\n",
      "|    fps              | 258      |\n",
      "|    time_elapsed     | 562      |\n",
      "|    total_timesteps  | 145528   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.91     |\n",
      "|    n_updates        | 145524   |\n",
      "----------------------------------\n",
      "Num timesteps: 146000\n",
      "Best mean reward: 114.08 - Last mean reward per episode: 113.13\n",
      "Num timesteps: 147000\n",
      "Best mean reward: 114.08 - Last mean reward per episode: 117.66\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 455      |\n",
      "|    ep_rew_mean      | 118      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 840      |\n",
      "|    fps              | 258      |\n",
      "|    time_elapsed     | 570      |\n",
      "|    total_timesteps  | 147517   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.605    |\n",
      "|    n_updates        | 147516   |\n",
      "----------------------------------\n",
      "Num timesteps: 148000\n",
      "Best mean reward: 117.66 - Last mean reward per episode: 114.65\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 434      |\n",
      "|    ep_rew_mean      | 115      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 844      |\n",
      "|    fps              | 258      |\n",
      "|    time_elapsed     | 573      |\n",
      "|    total_timesteps  | 148146   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.608    |\n",
      "|    n_updates        | 148144   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 419      |\n",
      "|    ep_rew_mean      | 116      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 848      |\n",
      "|    fps              | 258      |\n",
      "|    time_elapsed     | 575      |\n",
      "|    total_timesteps  | 148746   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.648    |\n",
      "|    n_updates        | 148744   |\n",
      "----------------------------------\n",
      "Num timesteps: 149000\n",
      "Best mean reward: 117.66 - Last mean reward per episode: 114.55\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 413      |\n",
      "|    ep_rew_mean      | 117      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 852      |\n",
      "|    fps              | 258      |\n",
      "|    time_elapsed     | 578      |\n",
      "|    total_timesteps  | 149482   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.484    |\n",
      "|    n_updates        | 149480   |\n",
      "----------------------------------\n",
      "Num timesteps: 150000\n",
      "Best mean reward: 117.66 - Last mean reward per episode: 117.08\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 410      |\n",
      "|    ep_rew_mean      | 120      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 856      |\n",
      "|    fps              | 258      |\n",
      "|    time_elapsed     | 582      |\n",
      "|    total_timesteps  | 150446   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.702    |\n",
      "|    n_updates        | 150444   |\n",
      "----------------------------------\n",
      "Num timesteps: 151000\n",
      "Best mean reward: 117.66 - Last mean reward per episode: 120.58\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 152000\n",
      "Best mean reward: 120.58 - Last mean reward per episode: 122.64\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 153000\n",
      "Best mean reward: 122.64 - Last mean reward per episode: 120.93\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 425      |\n",
      "|    ep_rew_mean      | 121      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 860      |\n",
      "|    fps              | 257      |\n",
      "|    time_elapsed     | 594      |\n",
      "|    total_timesteps  | 153397   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.26     |\n",
      "|    n_updates        | 153396   |\n",
      "----------------------------------\n",
      "Num timesteps: 154000\n",
      "Best mean reward: 122.64 - Last mean reward per episode: 121.35\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 420      |\n",
      "|    ep_rew_mean      | 124      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 864      |\n",
      "|    fps              | 257      |\n",
      "|    time_elapsed     | 600      |\n",
      "|    total_timesteps  | 154756   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.889    |\n",
      "|    n_updates        | 154752   |\n",
      "----------------------------------\n",
      "Num timesteps: 155000\n",
      "Best mean reward: 122.64 - Last mean reward per episode: 123.65\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 156000\n",
      "Best mean reward: 123.65 - Last mean reward per episode: 126.83\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 419      |\n",
      "|    ep_rew_mean      | 126      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 868      |\n",
      "|    fps              | 257      |\n",
      "|    time_elapsed     | 606      |\n",
      "|    total_timesteps  | 156211   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.625    |\n",
      "|    n_updates        | 156208   |\n",
      "----------------------------------\n",
      "Num timesteps: 157000\n",
      "Best mean reward: 126.83 - Last mean reward per episode: 123.63\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 413      |\n",
      "|    ep_rew_mean      | 123      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 872      |\n",
      "|    fps              | 257      |\n",
      "|    time_elapsed     | 609      |\n",
      "|    total_timesteps  | 157038   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.715    |\n",
      "|    n_updates        | 157036   |\n",
      "----------------------------------\n",
      "Num timesteps: 158000\n",
      "Best mean reward: 126.83 - Last mean reward per episode: 122.20\n",
      "Num timesteps: 159000\n",
      "Best mean reward: 126.83 - Last mean reward per episode: 122.65\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 406      |\n",
      "|    ep_rew_mean      | 122      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 876      |\n",
      "|    fps              | 257      |\n",
      "|    time_elapsed     | 618      |\n",
      "|    total_timesteps  | 159135   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.502    |\n",
      "|    n_updates        | 159132   |\n",
      "----------------------------------\n",
      "Num timesteps: 160000\n",
      "Best mean reward: 126.83 - Last mean reward per episode: 122.59\n",
      "Num timesteps: 161000\n",
      "Best mean reward: 126.83 - Last mean reward per episode: 123.37\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 406      |\n",
      "|    ep_rew_mean      | 123      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 880      |\n",
      "|    fps              | 256      |\n",
      "|    time_elapsed     | 629      |\n",
      "|    total_timesteps  | 161708   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.675    |\n",
      "|    n_updates        | 161704   |\n",
      "----------------------------------\n",
      "Num timesteps: 162000\n",
      "Best mean reward: 126.83 - Last mean reward per episode: 123.39\n",
      "Num timesteps: 163000\n",
      "Best mean reward: 126.83 - Last mean reward per episode: 121.89\n",
      "Num timesteps: 164000\n",
      "Best mean reward: 126.83 - Last mean reward per episode: 121.88\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 424      |\n",
      "|    ep_rew_mean      | 125      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 884      |\n",
      "|    fps              | 256      |\n",
      "|    time_elapsed     | 643      |\n",
      "|    total_timesteps  | 164851   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.481    |\n",
      "|    n_updates        | 164848   |\n",
      "----------------------------------\n",
      "Num timesteps: 165000\n",
      "Best mean reward: 126.83 - Last mean reward per episode: 124.69\n",
      "Num timesteps: 166000\n",
      "Best mean reward: 126.83 - Last mean reward per episode: 121.03\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 421      |\n",
      "|    ep_rew_mean      | 116      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 888      |\n",
      "|    fps              | 256      |\n",
      "|    time_elapsed     | 648      |\n",
      "|    total_timesteps  | 166134   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.3      |\n",
      "|    n_updates        | 166132   |\n",
      "----------------------------------\n",
      "Num timesteps: 167000\n",
      "Best mean reward: 126.83 - Last mean reward per episode: 118.46\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 410      |\n",
      "|    ep_rew_mean      | 120      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 892      |\n",
      "|    fps              | 256      |\n",
      "|    time_elapsed     | 654      |\n",
      "|    total_timesteps  | 167615   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.483    |\n",
      "|    n_updates        | 167612   |\n",
      "----------------------------------\n",
      "Num timesteps: 168000\n",
      "Best mean reward: 126.83 - Last mean reward per episode: 119.73\n",
      "Num timesteps: 169000\n",
      "Best mean reward: 126.83 - Last mean reward per episode: 118.93\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 396      |\n",
      "|    ep_rew_mean      | 121      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 896      |\n",
      "|    fps              | 255      |\n",
      "|    time_elapsed     | 661      |\n",
      "|    total_timesteps  | 169140   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.422    |\n",
      "|    n_updates        | 169136   |\n",
      "----------------------------------\n",
      "Num timesteps: 170000\n",
      "Best mean reward: 126.83 - Last mean reward per episode: 123.30\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 390      |\n",
      "|    ep_rew_mean      | 120      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 900      |\n",
      "|    fps              | 255      |\n",
      "|    time_elapsed     | 664      |\n",
      "|    total_timesteps  | 170019   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.387    |\n",
      "|    n_updates        | 170016   |\n",
      "----------------------------------\n",
      "Num timesteps: 171000\n",
      "Best mean reward: 126.83 - Last mean reward per episode: 118.86\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 394      |\n",
      "|    ep_rew_mean      | 124      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 904      |\n",
      "|    fps              | 255      |\n",
      "|    time_elapsed     | 673      |\n",
      "|    total_timesteps  | 171974   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.792    |\n",
      "|    n_updates        | 171972   |\n",
      "----------------------------------\n",
      "Num timesteps: 172000\n",
      "Best mean reward: 126.83 - Last mean reward per episode: 124.02\n",
      "Num timesteps: 173000\n",
      "Best mean reward: 126.83 - Last mean reward per episode: 128.40\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 391      |\n",
      "|    ep_rew_mean      | 129      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 908      |\n",
      "|    fps              | 255      |\n",
      "|    time_elapsed     | 679      |\n",
      "|    total_timesteps  | 173548   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.713    |\n",
      "|    n_updates        | 173544   |\n",
      "----------------------------------\n",
      "Num timesteps: 174000\n",
      "Best mean reward: 128.40 - Last mean reward per episode: 129.45\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 175000\n",
      "Best mean reward: 129.45 - Last mean reward per episode: 126.43\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 386      |\n",
      "|    ep_rew_mean      | 126      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 912      |\n",
      "|    fps              | 255      |\n",
      "|    time_elapsed     | 687      |\n",
      "|    total_timesteps  | 175423   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.604    |\n",
      "|    n_updates        | 175420   |\n",
      "----------------------------------\n",
      "Num timesteps: 176000\n",
      "Best mean reward: 129.45 - Last mean reward per episode: 125.95\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 385      |\n",
      "|    ep_rew_mean      | 126      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 916      |\n",
      "|    fps              | 255      |\n",
      "|    time_elapsed     | 691      |\n",
      "|    total_timesteps  | 176498   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.598    |\n",
      "|    n_updates        | 176496   |\n",
      "----------------------------------\n",
      "Num timesteps: 177000\n",
      "Best mean reward: 129.45 - Last mean reward per episode: 124.14\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 375      |\n",
      "|    ep_rew_mean      | 122      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 920      |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 696      |\n",
      "|    total_timesteps  | 177540   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.48     |\n",
      "|    n_updates        | 177536   |\n",
      "----------------------------------\n",
      "Num timesteps: 178000\n",
      "Best mean reward: 129.45 - Last mean reward per episode: 123.36\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 376      |\n",
      "|    ep_rew_mean      | 123      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 924      |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 698      |\n",
      "|    total_timesteps  | 178110   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.685    |\n",
      "|    n_updates        | 178108   |\n",
      "----------------------------------\n",
      "Num timesteps: 179000\n",
      "Best mean reward: 129.45 - Last mean reward per episode: 125.86\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 380      |\n",
      "|    ep_rew_mean      | 126      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 928      |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 706      |\n",
      "|    total_timesteps  | 179984   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.825    |\n",
      "|    n_updates        | 179980   |\n",
      "----------------------------------\n",
      "Num timesteps: 180000\n",
      "Best mean reward: 129.45 - Last mean reward per episode: 126.06\n",
      "Num timesteps: 181000\n",
      "Best mean reward: 129.45 - Last mean reward per episode: 127.60\n",
      "Num timesteps: 182000\n",
      "Best mean reward: 129.45 - Last mean reward per episode: 127.72\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 383      |\n",
      "|    ep_rew_mean      | 126      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 932      |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 715      |\n",
      "|    total_timesteps  | 182451   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.334    |\n",
      "|    n_updates        | 182448   |\n",
      "----------------------------------\n",
      "Num timesteps: 183000\n",
      "Best mean reward: 129.45 - Last mean reward per episode: 125.67\n",
      "Num timesteps: 184000\n",
      "Best mean reward: 129.45 - Last mean reward per episode: 122.64\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 395      |\n",
      "|    ep_rew_mean      | 128      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 936      |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 725      |\n",
      "|    total_timesteps  | 184993   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.212    |\n",
      "|    n_updates        | 184992   |\n",
      "----------------------------------\n",
      "Num timesteps: 185000\n",
      "Best mean reward: 129.45 - Last mean reward per episode: 127.98\n",
      "Num timesteps: 186000\n",
      "Best mean reward: 129.45 - Last mean reward per episode: 123.25\n",
      "Num timesteps: 187000\n",
      "Best mean reward: 129.45 - Last mean reward per episode: 123.64\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 399      |\n",
      "|    ep_rew_mean      | 126      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 940      |\n",
      "|    fps              | 255      |\n",
      "|    time_elapsed     | 734      |\n",
      "|    total_timesteps  | 187397   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.273    |\n",
      "|    n_updates        | 187396   |\n",
      "----------------------------------\n",
      "Num timesteps: 188000\n",
      "Best mean reward: 129.45 - Last mean reward per episode: 126.02\n",
      "Num timesteps: 189000\n",
      "Best mean reward: 129.45 - Last mean reward per episode: 128.46\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 413      |\n",
      "|    ep_rew_mean      | 131      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 944      |\n",
      "|    fps              | 255      |\n",
      "|    time_elapsed     | 743      |\n",
      "|    total_timesteps  | 189485   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.717    |\n",
      "|    n_updates        | 189484   |\n",
      "----------------------------------\n",
      "Num timesteps: 190000\n",
      "Best mean reward: 129.45 - Last mean reward per episode: 130.58\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 191000\n",
      "Best mean reward: 130.58 - Last mean reward per episode: 131.74\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 192000\n",
      "Best mean reward: 131.74 - Last mean reward per episode: 130.90\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 437      |\n",
      "|    ep_rew_mean      | 134      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 948      |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 754      |\n",
      "|    total_timesteps  | 192405   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.807    |\n",
      "|    n_updates        | 192404   |\n",
      "----------------------------------\n",
      "Num timesteps: 193000\n",
      "Best mean reward: 131.74 - Last mean reward per episode: 133.51\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 194000\n",
      "Best mean reward: 133.51 - Last mean reward per episode: 134.62\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 195000\n",
      "Best mean reward: 134.62 - Last mean reward per episode: 135.42\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 465      |\n",
      "|    ep_rew_mean      | 136      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 952      |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 769      |\n",
      "|    total_timesteps  | 195963   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.806    |\n",
      "|    n_updates        | 195960   |\n",
      "----------------------------------\n",
      "Num timesteps: 196000\n",
      "Best mean reward: 135.42 - Last mean reward per episode: 136.17\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 197000\n",
      "Best mean reward: 136.17 - Last mean reward per episode: 135.22\n",
      "Num timesteps: 198000\n",
      "Best mean reward: 136.17 - Last mean reward per episode: 137.53\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 480      |\n",
      "|    ep_rew_mean      | 143      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 956      |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 779      |\n",
      "|    total_timesteps  | 198423   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.408    |\n",
      "|    n_updates        | 198420   |\n",
      "----------------------------------\n",
      "Num timesteps: 199000\n",
      "Best mean reward: 137.53 - Last mean reward per episode: 142.73\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 200000\n",
      "Best mean reward: 142.73 - Last mean reward per episode: 145.10\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 201000\n",
      "Best mean reward: 145.10 - Last mean reward per episode: 145.08\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 481      |\n",
      "|    ep_rew_mean      | 144      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 960      |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 791      |\n",
      "|    total_timesteps  | 201523   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.359    |\n",
      "|    n_updates        | 201520   |\n",
      "----------------------------------\n",
      "Num timesteps: 202000\n",
      "Best mean reward: 145.10 - Last mean reward per episode: 143.96\n",
      "Num timesteps: 203000\n",
      "Best mean reward: 145.10 - Last mean reward per episode: 140.67\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 490      |\n",
      "|    ep_rew_mean      | 140      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 964      |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 800      |\n",
      "|    total_timesteps  | 203756   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.831    |\n",
      "|    n_updates        | 203752   |\n",
      "----------------------------------\n",
      "Num timesteps: 204000\n",
      "Best mean reward: 145.10 - Last mean reward per episode: 139.88\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 483      |\n",
      "|    ep_rew_mean      | 140      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 968      |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 803      |\n",
      "|    total_timesteps  | 204554   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.371    |\n",
      "|    n_updates        | 204552   |\n",
      "----------------------------------\n",
      "Num timesteps: 205000\n",
      "Best mean reward: 145.10 - Last mean reward per episode: 139.64\n",
      "Num timesteps: 206000\n",
      "Best mean reward: 145.10 - Last mean reward per episode: 140.16\n",
      "Num timesteps: 207000\n",
      "Best mean reward: 145.10 - Last mean reward per episode: 145.08\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 506      |\n",
      "|    ep_rew_mean      | 147      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 972      |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 816      |\n",
      "|    total_timesteps  | 207621   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.268    |\n",
      "|    n_updates        | 207620   |\n",
      "----------------------------------\n",
      "Num timesteps: 208000\n",
      "Best mean reward: 145.10 - Last mean reward per episode: 146.77\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 209000\n",
      "Best mean reward: 146.77 - Last mean reward per episode: 148.59\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 507      |\n",
      "|    ep_rew_mean      | 144      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 976      |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 824      |\n",
      "|    total_timesteps  | 209812   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.556    |\n",
      "|    n_updates        | 209808   |\n",
      "----------------------------------\n",
      "Num timesteps: 210000\n",
      "Best mean reward: 148.59 - Last mean reward per episode: 143.86\n",
      "Num timesteps: 211000\n",
      "Best mean reward: 148.59 - Last mean reward per episode: 144.07\n",
      "Num timesteps: 212000\n",
      "Best mean reward: 148.59 - Last mean reward per episode: 146.43\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 511      |\n",
      "|    ep_rew_mean      | 148      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 980      |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 836      |\n",
      "|    total_timesteps  | 212776   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.695    |\n",
      "|    n_updates        | 212772   |\n",
      "----------------------------------\n",
      "Num timesteps: 213000\n",
      "Best mean reward: 148.59 - Last mean reward per episode: 142.13\n",
      "Num timesteps: 214000\n",
      "Best mean reward: 148.59 - Last mean reward per episode: 139.65\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 492      |\n",
      "|    ep_rew_mean      | 139      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 984      |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 841      |\n",
      "|    total_timesteps  | 214074   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.778    |\n",
      "|    n_updates        | 214072   |\n",
      "----------------------------------\n",
      "Num timesteps: 215000\n",
      "Best mean reward: 148.59 - Last mean reward per episode: 140.82\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 491      |\n",
      "|    ep_rew_mean      | 144      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 988      |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 845      |\n",
      "|    total_timesteps  | 215200   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.183    |\n",
      "|    n_updates        | 215196   |\n",
      "----------------------------------\n",
      "Num timesteps: 216000\n",
      "Best mean reward: 148.59 - Last mean reward per episode: 143.77\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 493      |\n",
      "|    ep_rew_mean      | 142      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 992      |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 852      |\n",
      "|    total_timesteps  | 216963   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.553    |\n",
      "|    n_updates        | 216960   |\n",
      "----------------------------------\n",
      "Num timesteps: 217000\n",
      "Best mean reward: 148.59 - Last mean reward per episode: 142.33\n",
      "Num timesteps: 218000\n",
      "Best mean reward: 148.59 - Last mean reward per episode: 144.33\n",
      "Num timesteps: 219000\n",
      "Best mean reward: 148.59 - Last mean reward per episode: 144.48\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 504      |\n",
      "|    ep_rew_mean      | 148      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 996      |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 862      |\n",
      "|    total_timesteps  | 219573   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.416    |\n",
      "|    n_updates        | 219572   |\n",
      "----------------------------------\n",
      "Num timesteps: 220000\n",
      "Best mean reward: 148.59 - Last mean reward per episode: 148.95\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 221000\n",
      "Best mean reward: 148.95 - Last mean reward per episode: 152.19\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 511      |\n",
      "|    ep_rew_mean      | 155      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1000     |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 868      |\n",
      "|    total_timesteps  | 221073   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.355    |\n",
      "|    n_updates        | 221072   |\n",
      "----------------------------------\n",
      "Num timesteps: 222000\n",
      "Best mean reward: 152.19 - Last mean reward per episode: 155.21\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 223000\n",
      "Best mean reward: 155.21 - Last mean reward per episode: 156.91\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 224000\n",
      "Best mean reward: 156.91 - Last mean reward per episode: 156.04\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 530      |\n",
      "|    ep_rew_mean      | 155      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1004     |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 883      |\n",
      "|    total_timesteps  | 224985   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.164    |\n",
      "|    n_updates        | 224984   |\n",
      "----------------------------------\n",
      "Num timesteps: 225000\n",
      "Best mean reward: 156.91 - Last mean reward per episode: 154.92\n",
      "Num timesteps: 226000\n",
      "Best mean reward: 156.91 - Last mean reward per episode: 151.69\n",
      "Num timesteps: 227000\n",
      "Best mean reward: 156.91 - Last mean reward per episode: 151.64\n",
      "Num timesteps: 228000\n",
      "Best mean reward: 156.91 - Last mean reward per episode: 152.67\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 546      |\n",
      "|    ep_rew_mean      | 154      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1008     |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 896      |\n",
      "|    total_timesteps  | 228118   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.578    |\n",
      "|    n_updates        | 228116   |\n",
      "----------------------------------\n",
      "Num timesteps: 229000\n",
      "Best mean reward: 156.91 - Last mean reward per episode: 153.66\n",
      "Num timesteps: 230000\n",
      "Best mean reward: 156.91 - Last mean reward per episode: 156.82\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 554      |\n",
      "|    ep_rew_mean      | 157      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1012     |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 907      |\n",
      "|    total_timesteps  | 230811   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.381    |\n",
      "|    n_updates        | 230808   |\n",
      "----------------------------------\n",
      "Num timesteps: 231000\n",
      "Best mean reward: 156.91 - Last mean reward per episode: 157.49\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 232000\n",
      "Best mean reward: 157.49 - Last mean reward per episode: 157.01\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 565      |\n",
      "|    ep_rew_mean      | 163      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1016     |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 915      |\n",
      "|    total_timesteps  | 232976   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.163    |\n",
      "|    n_updates        | 232972   |\n",
      "----------------------------------\n",
      "Num timesteps: 233000\n",
      "Best mean reward: 157.49 - Last mean reward per episode: 162.88\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 234000\n",
      "Best mean reward: 162.88 - Last mean reward per episode: 163.81\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 235000\n",
      "Best mean reward: 163.81 - Last mean reward per episode: 162.95\n",
      "Num timesteps: 236000\n",
      "Best mean reward: 163.81 - Last mean reward per episode: 164.69\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 594      |\n",
      "|    ep_rew_mean      | 166      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1020     |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 932      |\n",
      "|    total_timesteps  | 236976   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.51     |\n",
      "|    n_updates        | 236972   |\n",
      "----------------------------------\n",
      "Num timesteps: 237000\n",
      "Best mean reward: 164.69 - Last mean reward per episode: 166.06\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 238000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 165.01\n",
      "Num timesteps: 239000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 163.96\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 611      |\n",
      "|    ep_rew_mean      | 166      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1024     |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 940      |\n",
      "|    total_timesteps  | 239168   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.161    |\n",
      "|    n_updates        | 239164   |\n",
      "----------------------------------\n",
      "Num timesteps: 240000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 163.74\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 606      |\n",
      "|    ep_rew_mean      | 167      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1028     |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 946      |\n",
      "|    total_timesteps  | 240614   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.142    |\n",
      "|    n_updates        | 240612   |\n",
      "----------------------------------\n",
      "Num timesteps: 241000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 159.77\n",
      "Num timesteps: 242000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 160.90\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 596      |\n",
      "|    ep_rew_mean      | 163      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1032     |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 951      |\n",
      "|    total_timesteps  | 242024   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.285    |\n",
      "|    n_updates        | 242020   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 579      |\n",
      "|    ep_rew_mean      | 161      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1036     |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 954      |\n",
      "|    total_timesteps  | 242846   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.179    |\n",
      "|    n_updates        | 242844   |\n",
      "----------------------------------\n",
      "Num timesteps: 243000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 162.62\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 558      |\n",
      "|    ep_rew_mean      | 155      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1040     |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 956      |\n",
      "|    total_timesteps  | 243163   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.146    |\n",
      "|    n_updates        | 243160   |\n",
      "----------------------------------\n",
      "Num timesteps: 244000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 148.19\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 550      |\n",
      "|    ep_rew_mean      | 151      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1044     |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 961      |\n",
      "|    total_timesteps  | 244476   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.501    |\n",
      "|    n_updates        | 244472   |\n",
      "----------------------------------\n",
      "Num timesteps: 245000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 149.08\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 532      |\n",
      "|    ep_rew_mean      | 147      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1048     |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 965      |\n",
      "|    total_timesteps  | 245615   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.134    |\n",
      "|    n_updates        | 245612   |\n",
      "----------------------------------\n",
      "Num timesteps: 246000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 145.60\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 509      |\n",
      "|    ep_rew_mean      | 144      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1052     |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 970      |\n",
      "|    total_timesteps  | 246865   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.135    |\n",
      "|    n_updates        | 246864   |\n",
      "----------------------------------\n",
      "Num timesteps: 247000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 143.71\n",
      "Num timesteps: 248000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 145.20\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 505      |\n",
      "|    ep_rew_mean      | 142      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1056     |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 978      |\n",
      "|    total_timesteps  | 248900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.182    |\n",
      "|    n_updates        | 248896   |\n",
      "----------------------------------\n",
      "Num timesteps: 249000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 141.91\n",
      "Num timesteps: 250000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 142.36\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 492      |\n",
      "|    ep_rew_mean      | 145      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1060     |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 985      |\n",
      "|    total_timesteps  | 250682   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.357    |\n",
      "|    n_updates        | 250680   |\n",
      "----------------------------------\n",
      "Num timesteps: 251000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 142.78\n",
      "Num timesteps: 252000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 145.62\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 485      |\n",
      "|    ep_rew_mean      | 139      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1064     |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 991      |\n",
      "|    total_timesteps  | 252287   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.318    |\n",
      "|    n_updates        | 252284   |\n",
      "----------------------------------\n",
      "Num timesteps: 253000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 138.78\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 493      |\n",
      "|    ep_rew_mean      | 141      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1068     |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 998      |\n",
      "|    total_timesteps  | 253892   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.552    |\n",
      "|    n_updates        | 253888   |\n",
      "----------------------------------\n",
      "Num timesteps: 254000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 140.83\n",
      "Num timesteps: 255000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 138.36\n",
      "Num timesteps: 256000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 137.58\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 489      |\n",
      "|    ep_rew_mean      | 139      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1072     |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 1008     |\n",
      "|    total_timesteps  | 256472   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.311    |\n",
      "|    n_updates        | 256468   |\n",
      "----------------------------------\n",
      "Num timesteps: 257000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 138.77\n",
      "Num timesteps: 258000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 138.32\n",
      "Num timesteps: 259000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 137.88\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 500      |\n",
      "|    ep_rew_mean      | 141      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1076     |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 1021     |\n",
      "|    total_timesteps  | 259822   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.294    |\n",
      "|    n_updates        | 259820   |\n",
      "----------------------------------\n",
      "Num timesteps: 260000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 141.30\n",
      "Num timesteps: 261000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 140.76\n",
      "Num timesteps: 262000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 142.16\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 496      |\n",
      "|    ep_rew_mean      | 142      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1080     |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 1032     |\n",
      "|    total_timesteps  | 262366   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.203    |\n",
      "|    n_updates        | 262364   |\n",
      "----------------------------------\n",
      "Num timesteps: 263000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 141.83\n",
      "Num timesteps: 264000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 150.72\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 506      |\n",
      "|    ep_rew_mean      | 152      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1084     |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 1040     |\n",
      "|    total_timesteps  | 264628   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.562    |\n",
      "|    n_updates        | 264624   |\n",
      "----------------------------------\n",
      "Num timesteps: 265000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 154.01\n",
      "Num timesteps: 266000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 156.25\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 510      |\n",
      "|    ep_rew_mean      | 154      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1088     |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 1046     |\n",
      "|    total_timesteps  | 266168   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.364    |\n",
      "|    n_updates        | 266164   |\n",
      "----------------------------------\n",
      "Num timesteps: 267000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 153.72\n",
      "Num timesteps: 268000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 155.08\n",
      "Num timesteps: 269000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 155.82\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 526      |\n",
      "|    ep_rew_mean      | 154      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1092     |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 1061     |\n",
      "|    total_timesteps  | 269581   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.165    |\n",
      "|    n_updates        | 269580   |\n",
      "----------------------------------\n",
      "Num timesteps: 270000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 154.37\n",
      "Num timesteps: 271000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 153.22\n",
      "Num timesteps: 272000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 153.04\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 527      |\n",
      "|    ep_rew_mean      | 149      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1096     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1071     |\n",
      "|    total_timesteps  | 272234   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.498    |\n",
      "|    n_updates        | 272232   |\n",
      "----------------------------------\n",
      "Num timesteps: 273000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 144.65\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 526      |\n",
      "|    ep_rew_mean      | 144      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1100     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1077     |\n",
      "|    total_timesteps  | 273692   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.367    |\n",
      "|    n_updates        | 273688   |\n",
      "----------------------------------\n",
      "Num timesteps: 274000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 144.17\n",
      "Num timesteps: 275000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 144.09\n",
      "Num timesteps: 276000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 141.90\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 511      |\n",
      "|    ep_rew_mean      | 143      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1104     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1086     |\n",
      "|    total_timesteps  | 276048   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.216    |\n",
      "|    n_updates        | 276044   |\n",
      "----------------------------------\n",
      "Num timesteps: 277000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 145.69\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 497      |\n",
      "|    ep_rew_mean      | 149      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1108     |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 1093     |\n",
      "|    total_timesteps  | 277848   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.177    |\n",
      "|    n_updates        | 277844   |\n",
      "----------------------------------\n",
      "Num timesteps: 278000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 149.07\n",
      "Num timesteps: 279000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 146.72\n",
      "Num timesteps: 280000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 146.56\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 494      |\n",
      "|    ep_rew_mean      | 141      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1112     |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 1103     |\n",
      "|    total_timesteps  | 280230   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.163    |\n",
      "|    n_updates        | 280228   |\n",
      "----------------------------------\n",
      "Num timesteps: 281000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 138.39\n",
      "Num timesteps: 282000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 137.51\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 500      |\n",
      "|    ep_rew_mean      | 136      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1116     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1114     |\n",
      "|    total_timesteps  | 282937   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.438    |\n",
      "|    n_updates        | 282936   |\n",
      "----------------------------------\n",
      "Num timesteps: 283000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 135.66\n",
      "Num timesteps: 284000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 134.70\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 479      |\n",
      "|    ep_rew_mean      | 135      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1120     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1121     |\n",
      "|    total_timesteps  | 284903   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.402    |\n",
      "|    n_updates        | 284900   |\n",
      "----------------------------------\n",
      "Num timesteps: 285000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 134.73\n",
      "Num timesteps: 286000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 133.85\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 471      |\n",
      "|    ep_rew_mean      | 135      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1124     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1127     |\n",
      "|    total_timesteps  | 286317   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.313    |\n",
      "|    n_updates        | 286316   |\n",
      "----------------------------------\n",
      "Num timesteps: 287000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 135.26\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 472      |\n",
      "|    ep_rew_mean      | 135      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1128     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1133     |\n",
      "|    total_timesteps  | 287801   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.658    |\n",
      "|    n_updates        | 287800   |\n",
      "----------------------------------\n",
      "Num timesteps: 288000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 134.59\n",
      "Num timesteps: 289000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 142.62\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 472      |\n",
      "|    ep_rew_mean      | 143      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1132     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1138     |\n",
      "|    total_timesteps  | 289207   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.414    |\n",
      "|    n_updates        | 289204   |\n",
      "----------------------------------\n",
      "Num timesteps: 290000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 144.08\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 480      |\n",
      "|    ep_rew_mean      | 148      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1136     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1145     |\n",
      "|    total_timesteps  | 290805   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.199    |\n",
      "|    n_updates        | 290804   |\n",
      "----------------------------------\n",
      "Num timesteps: 291000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 148.48\n",
      "Num timesteps: 292000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 151.44\n",
      "Num timesteps: 293000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 154.70\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 506      |\n",
      "|    ep_rew_mean      | 159      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1140     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1156     |\n",
      "|    total_timesteps  | 293741   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.628    |\n",
      "|    n_updates        | 293740   |\n",
      "----------------------------------\n",
      "Num timesteps: 294000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 158.89\n",
      "Num timesteps: 295000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 163.96\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 513      |\n",
      "|    ep_rew_mean      | 164      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1144     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1165     |\n",
      "|    total_timesteps  | 295785   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.395    |\n",
      "|    n_updates        | 295784   |\n",
      "----------------------------------\n",
      "Num timesteps: 296000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 163.80\n",
      "Num timesteps: 297000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 162.61\n",
      "Num timesteps: 298000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 165.33\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 525      |\n",
      "|    ep_rew_mean      | 168      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1148     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1174     |\n",
      "|    total_timesteps  | 298102   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.332    |\n",
      "|    n_updates        | 298100   |\n",
      "----------------------------------\n",
      "Num timesteps: 299000\n",
      "Best mean reward: 166.06 - Last mean reward per episode: 167.82\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 300000\n",
      "Best mean reward: 167.82 - Last mean reward per episode: 169.36\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 301000\n",
      "Best mean reward: 169.36 - Last mean reward per episode: 168.05\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 545      |\n",
      "|    ep_rew_mean      | 172      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1152     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1188     |\n",
      "|    total_timesteps  | 301401   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.897    |\n",
      "|    n_updates        | 301400   |\n",
      "----------------------------------\n",
      "Num timesteps: 302000\n",
      "Best mean reward: 169.36 - Last mean reward per episode: 169.82\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 533      |\n",
      "|    ep_rew_mean      | 167      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1156     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1191     |\n",
      "|    total_timesteps  | 302152   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.401    |\n",
      "|    n_updates        | 302148   |\n",
      "----------------------------------\n",
      "Num timesteps: 303000\n",
      "Best mean reward: 169.82 - Last mean reward per episode: 167.15\n",
      "Num timesteps: 304000\n",
      "Best mean reward: 169.82 - Last mean reward per episode: 165.91\n",
      "Num timesteps: 305000\n",
      "Best mean reward: 169.82 - Last mean reward per episode: 165.86\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 548      |\n",
      "|    ep_rew_mean      | 166      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1160     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1205     |\n",
      "|    total_timesteps  | 305516   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.185    |\n",
      "|    n_updates        | 305512   |\n",
      "----------------------------------\n",
      "Num timesteps: 306000\n",
      "Best mean reward: 169.82 - Last mean reward per episode: 165.80\n",
      "Num timesteps: 307000\n",
      "Best mean reward: 169.82 - Last mean reward per episode: 164.85\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 550      |\n",
      "|    ep_rew_mean      | 174      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1164     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1212     |\n",
      "|    total_timesteps  | 307276   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.566    |\n",
      "|    n_updates        | 307272   |\n",
      "----------------------------------\n",
      "Num timesteps: 308000\n",
      "Best mean reward: 169.82 - Last mean reward per episode: 172.88\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 309000\n",
      "Best mean reward: 172.88 - Last mean reward per episode: 171.96\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 551      |\n",
      "|    ep_rew_mean      | 175      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1168     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1219     |\n",
      "|    total_timesteps  | 309031   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.272    |\n",
      "|    n_updates        | 309028   |\n",
      "----------------------------------\n",
      "Num timesteps: 310000\n",
      "Best mean reward: 172.88 - Last mean reward per episode: 176.02\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 544      |\n",
      "|    ep_rew_mean      | 175      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1172     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1226     |\n",
      "|    total_timesteps  | 310873   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.0915   |\n",
      "|    n_updates        | 310872   |\n",
      "----------------------------------\n",
      "Num timesteps: 311000\n",
      "Best mean reward: 176.02 - Last mean reward per episode: 174.79\n",
      "Num timesteps: 312000\n",
      "Best mean reward: 176.02 - Last mean reward per episode: 174.69\n",
      "Num timesteps: 313000\n",
      "Best mean reward: 176.02 - Last mean reward per episode: 176.13\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 314000\n",
      "Best mean reward: 176.13 - Last mean reward per episode: 176.31\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 542      |\n",
      "|    ep_rew_mean      | 177      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1176     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1239     |\n",
      "|    total_timesteps  | 314024   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.212    |\n",
      "|    n_updates        | 314020   |\n",
      "----------------------------------\n",
      "Num timesteps: 315000\n",
      "Best mean reward: 176.31 - Last mean reward per episode: 175.13\n",
      "Num timesteps: 316000\n",
      "Best mean reward: 176.31 - Last mean reward per episode: 173.94\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 538      |\n",
      "|    ep_rew_mean      | 174      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1180     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1248     |\n",
      "|    total_timesteps  | 316139   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.2      |\n",
      "|    n_updates        | 316136   |\n",
      "----------------------------------\n",
      "Num timesteps: 317000\n",
      "Best mean reward: 176.31 - Last mean reward per episode: 171.74\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 524      |\n",
      "|    ep_rew_mean      | 169      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1184     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1251     |\n",
      "|    total_timesteps  | 317059   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.188    |\n",
      "|    n_updates        | 317056   |\n",
      "----------------------------------\n",
      "Num timesteps: 318000\n",
      "Best mean reward: 176.31 - Last mean reward per episode: 169.47\n",
      "Num timesteps: 319000\n",
      "Best mean reward: 176.31 - Last mean reward per episode: 169.40\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 537      |\n",
      "|    ep_rew_mean      | 170      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1188     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1262     |\n",
      "|    total_timesteps  | 319830   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.536    |\n",
      "|    n_updates        | 319828   |\n",
      "----------------------------------\n",
      "Num timesteps: 320000\n",
      "Best mean reward: 176.31 - Last mean reward per episode: 170.33\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 512      |\n",
      "|    ep_rew_mean      | 167      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1192     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1266     |\n",
      "|    total_timesteps  | 320736   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.553    |\n",
      "|    n_updates        | 320732   |\n",
      "----------------------------------\n",
      "Num timesteps: 321000\n",
      "Best mean reward: 176.31 - Last mean reward per episode: 166.92\n",
      "Num timesteps: 322000\n",
      "Best mean reward: 176.31 - Last mean reward per episode: 167.19\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 507      |\n",
      "|    ep_rew_mean      | 165      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1196     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1275     |\n",
      "|    total_timesteps  | 322920   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.563    |\n",
      "|    n_updates        | 322916   |\n",
      "----------------------------------\n",
      "Num timesteps: 323000\n",
      "Best mean reward: 176.31 - Last mean reward per episode: 165.48\n",
      "Num timesteps: 324000\n",
      "Best mean reward: 176.31 - Last mean reward per episode: 168.36\n",
      "Num timesteps: 325000\n",
      "Best mean reward: 176.31 - Last mean reward per episode: 166.14\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 516      |\n",
      "|    ep_rew_mean      | 169      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1200     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1284     |\n",
      "|    total_timesteps  | 325251   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.181    |\n",
      "|    n_updates        | 325248   |\n",
      "----------------------------------\n",
      "Num timesteps: 326000\n",
      "Best mean reward: 176.31 - Last mean reward per episode: 169.55\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 504      |\n",
      "|    ep_rew_mean      | 170      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1204     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1289     |\n",
      "|    total_timesteps  | 326493   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.28     |\n",
      "|    n_updates        | 326492   |\n",
      "----------------------------------\n",
      "Num timesteps: 327000\n",
      "Best mean reward: 176.31 - Last mean reward per episode: 170.08\n",
      "Num timesteps: 328000\n",
      "Best mean reward: 176.31 - Last mean reward per episode: 169.19\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 509      |\n",
      "|    ep_rew_mean      | 163      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1208     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1297     |\n",
      "|    total_timesteps  | 328740   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.205    |\n",
      "|    n_updates        | 328736   |\n",
      "----------------------------------\n",
      "Num timesteps: 329000\n",
      "Best mean reward: 176.31 - Last mean reward per episode: 162.61\n",
      "Num timesteps: 330000\n",
      "Best mean reward: 176.31 - Last mean reward per episode: 164.76\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 500      |\n",
      "|    ep_rew_mean      | 166      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1212     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1303     |\n",
      "|    total_timesteps  | 330214   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.245    |\n",
      "|    n_updates        | 330212   |\n",
      "----------------------------------\n",
      "Num timesteps: 331000\n",
      "Best mean reward: 176.31 - Last mean reward per episode: 167.97\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 482      |\n",
      "|    ep_rew_mean      | 170      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1216     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1307     |\n",
      "|    total_timesteps  | 331144   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.145    |\n",
      "|    n_updates        | 331140   |\n",
      "----------------------------------\n",
      "Num timesteps: 332000\n",
      "Best mean reward: 176.31 - Last mean reward per episode: 169.93\n",
      "Num timesteps: 333000\n",
      "Best mean reward: 176.31 - Last mean reward per episode: 172.80\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 481      |\n",
      "|    ep_rew_mean      | 174      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1220     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1314     |\n",
      "|    total_timesteps  | 333016   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.372    |\n",
      "|    n_updates        | 333012   |\n",
      "----------------------------------\n",
      "Num timesteps: 334000\n",
      "Best mean reward: 176.31 - Last mean reward per episode: 176.86\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 481      |\n",
      "|    ep_rew_mean      | 177      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1224     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1320     |\n",
      "|    total_timesteps  | 334429   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.16     |\n",
      "|    n_updates        | 334428   |\n",
      "----------------------------------\n",
      "Num timesteps: 335000\n",
      "Best mean reward: 176.86 - Last mean reward per episode: 175.86\n",
      "Num timesteps: 336000\n",
      "Best mean reward: 176.86 - Last mean reward per episode: 177.69\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 483      |\n",
      "|    ep_rew_mean      | 180      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1228     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1326     |\n",
      "|    total_timesteps  | 336105   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.153    |\n",
      "|    n_updates        | 336104   |\n",
      "----------------------------------\n",
      "Num timesteps: 337000\n",
      "Best mean reward: 177.69 - Last mean reward per episode: 179.10\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 338000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 174.39\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 497      |\n",
      "|    ep_rew_mean      | 174      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1232     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1338     |\n",
      "|    total_timesteps  | 338905   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.122    |\n",
      "|    n_updates        | 338904   |\n",
      "----------------------------------\n",
      "Num timesteps: 339000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 173.96\n",
      "Num timesteps: 340000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 170.60\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 494      |\n",
      "|    ep_rew_mean      | 166      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1236     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1343     |\n",
      "|    total_timesteps  | 340183   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.87     |\n",
      "|    n_updates        | 340180   |\n",
      "----------------------------------\n",
      "Num timesteps: 341000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 166.29\n",
      "Num timesteps: 342000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 166.16\n",
      "Num timesteps: 343000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 164.30\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 493      |\n",
      "|    ep_rew_mean      | 165      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1240     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1355     |\n",
      "|    total_timesteps  | 343020   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.227    |\n",
      "|    n_updates        | 343016   |\n",
      "----------------------------------\n",
      "Num timesteps: 344000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 164.62\n",
      "Num timesteps: 345000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 162.92\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 492      |\n",
      "|    ep_rew_mean      | 164      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1244     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1363     |\n",
      "|    total_timesteps  | 345021   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.267    |\n",
      "|    n_updates        | 345020   |\n",
      "----------------------------------\n",
      "Num timesteps: 346000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 163.51\n",
      "Num timesteps: 347000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 163.63\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 491      |\n",
      "|    ep_rew_mean      | 159      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1248     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1372     |\n",
      "|    total_timesteps  | 347198   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.323    |\n",
      "|    n_updates        | 347196   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 463      |\n",
      "|    ep_rew_mean      | 152      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1252     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1373     |\n",
      "|    total_timesteps  | 347660   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.57     |\n",
      "|    n_updates        | 347656   |\n",
      "----------------------------------\n",
      "Num timesteps: 348000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 149.90\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 459      |\n",
      "|    ep_rew_mean      | 150      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1256     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1375     |\n",
      "|    total_timesteps  | 348095   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.386    |\n",
      "|    n_updates        | 348092   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 429      |\n",
      "|    ep_rew_mean      | 139      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1260     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1376     |\n",
      "|    total_timesteps  | 348420   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.744    |\n",
      "|    n_updates        | 348416   |\n",
      "----------------------------------\n",
      "Num timesteps: 349000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 135.62\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 425      |\n",
      "|    ep_rew_mean      | 134      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1264     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1382     |\n",
      "|    total_timesteps  | 349740   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.643    |\n",
      "|    n_updates        | 349736   |\n",
      "----------------------------------\n",
      "Num timesteps: 350000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 129.73\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 411      |\n",
      "|    ep_rew_mean      | 124      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1268     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1384     |\n",
      "|    total_timesteps  | 350170   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.65     |\n",
      "|    n_updates        | 350168   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 396      |\n",
      "|    ep_rew_mean      | 117      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1272     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1385     |\n",
      "|    total_timesteps  | 350517   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.831    |\n",
      "|    n_updates        | 350516   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 369      |\n",
      "|    ep_rew_mean      | 109      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1276     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1386     |\n",
      "|    total_timesteps  | 350953   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.989    |\n",
      "|    n_updates        | 350952   |\n",
      "----------------------------------\n",
      "Num timesteps: 351000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 108.94\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 352      |\n",
      "|    ep_rew_mean      | 101      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1280     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1388     |\n",
      "|    total_timesteps  | 351357   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.44     |\n",
      "|    n_updates        | 351356   |\n",
      "----------------------------------\n",
      "Num timesteps: 352000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 95.80\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 355      |\n",
      "|    ep_rew_mean      | 97.6     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1284     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1393     |\n",
      "|    total_timesteps  | 352594   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.809    |\n",
      "|    n_updates        | 352592   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 331      |\n",
      "|    ep_rew_mean      | 87.5     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1288     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1394     |\n",
      "|    total_timesteps  | 352900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.764    |\n",
      "|    n_updates        | 352896   |\n",
      "----------------------------------\n",
      "Num timesteps: 353000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 84.12\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 325      |\n",
      "|    ep_rew_mean      | 83.2     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1292     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1395     |\n",
      "|    total_timesteps  | 353224   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.01     |\n",
      "|    n_updates        | 353220   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 307      |\n",
      "|    ep_rew_mean      | 78.6     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1296     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1397     |\n",
      "|    total_timesteps  | 353639   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.606    |\n",
      "|    n_updates        | 353636   |\n",
      "----------------------------------\n",
      "Num timesteps: 354000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 72.21\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 288      |\n",
      "|    ep_rew_mean      | 69.9     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1300     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1398     |\n",
      "|    total_timesteps  | 354053   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.567    |\n",
      "|    n_updates        | 354052   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 279      |\n",
      "|    ep_rew_mean      | 60.7     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1304     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1400     |\n",
      "|    total_timesteps  | 354393   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.865    |\n",
      "|    n_updates        | 354392   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 261      |\n",
      "|    ep_rew_mean      | 57.5     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1308     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1401     |\n",
      "|    total_timesteps  | 354838   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.309    |\n",
      "|    n_updates        | 354836   |\n",
      "----------------------------------\n",
      "Num timesteps: 355000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 52.06\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 51.2     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1312     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1402     |\n",
      "|    total_timesteps  | 355193   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.48     |\n",
      "|    n_updates        | 355192   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 245      |\n",
      "|    ep_rew_mean      | 41.6     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1316     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1404     |\n",
      "|    total_timesteps  | 355621   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.05     |\n",
      "|    n_updates        | 355620   |\n",
      "----------------------------------\n",
      "Num timesteps: 356000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 41.60\n",
      "Num timesteps: 357000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 36.81\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 240      |\n",
      "|    ep_rew_mean      | 36.5     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1320     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1410     |\n",
      "|    total_timesteps  | 357037   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.25     |\n",
      "|    n_updates        | 357036   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 234      |\n",
      "|    ep_rew_mean      | 32.8     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1324     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1413     |\n",
      "|    total_timesteps  | 357846   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.441    |\n",
      "|    n_updates        | 357844   |\n",
      "----------------------------------\n",
      "Num timesteps: 358000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 32.77\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 222      |\n",
      "|    ep_rew_mean      | 24       |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1328     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1414     |\n",
      "|    total_timesteps  | 358297   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.696    |\n",
      "|    n_updates        | 358296   |\n",
      "----------------------------------\n",
      "Num timesteps: 359000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 24.71\n",
      "Num timesteps: 360000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 26.95\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 214      |\n",
      "|    ep_rew_mean      | 26.8     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1332     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1422     |\n",
      "|    total_timesteps  | 360256   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.621    |\n",
      "|    n_updates        | 360252   |\n",
      "----------------------------------\n",
      "Num timesteps: 361000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 21.98\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 209      |\n",
      "|    ep_rew_mean      | 23.3     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1336     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1425     |\n",
      "|    total_timesteps  | 361070   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.22     |\n",
      "|    n_updates        | 361068   |\n",
      "----------------------------------\n",
      "Num timesteps: 362000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 20.13\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 193      |\n",
      "|    ep_rew_mean      | 19.9     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1340     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1430     |\n",
      "|    total_timesteps  | 362349   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.776    |\n",
      "|    n_updates        | 362348   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 179      |\n",
      "|    ep_rew_mean      | 9.79     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1344     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1433     |\n",
      "|    total_timesteps  | 362939   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.491    |\n",
      "|    n_updates        | 362936   |\n",
      "----------------------------------\n",
      "Num timesteps: 363000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 9.79\n",
      "Num timesteps: 364000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 12.01\n",
      "Num timesteps: 365000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 10.08\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 183      |\n",
      "|    ep_rew_mean      | 13.6     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1348     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1443     |\n",
      "|    total_timesteps  | 365496   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.539    |\n",
      "|    n_updates        | 365492   |\n",
      "----------------------------------\n",
      "Num timesteps: 366000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 14.03\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 188      |\n",
      "|    ep_rew_mean      | 17.5     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1352     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1446     |\n",
      "|    total_timesteps  | 366419   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.593    |\n",
      "|    n_updates        | 366416   |\n",
      "----------------------------------\n",
      "Num timesteps: 367000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 17.49\n",
      "Num timesteps: 368000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 24.34\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 209      |\n",
      "|    ep_rew_mean      | 25.1     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1356     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1457     |\n",
      "|    total_timesteps  | 368980   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.06     |\n",
      "|    n_updates        | 368976   |\n",
      "----------------------------------\n",
      "Num timesteps: 369000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 25.08\n",
      "Num timesteps: 370000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 28.11\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 222      |\n",
      "|    ep_rew_mean      | 35.2     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1360     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1464     |\n",
      "|    total_timesteps  | 370585   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.3      |\n",
      "|    n_updates        | 370584   |\n",
      "----------------------------------\n",
      "Num timesteps: 371000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 35.17\n",
      "Num timesteps: 372000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 40.09\n",
      "Num timesteps: 373000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 41.04\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 234      |\n",
      "|    ep_rew_mean      | 41.6     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1364     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1474     |\n",
      "|    total_timesteps  | 373105   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.901    |\n",
      "|    n_updates        | 373104   |\n",
      "----------------------------------\n",
      "Num timesteps: 374000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 44.54\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 242      |\n",
      "|    ep_rew_mean      | 51.1     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1368     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1479     |\n",
      "|    total_timesteps  | 374398   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.27     |\n",
      "|    n_updates        | 374396   |\n",
      "----------------------------------\n",
      "Num timesteps: 375000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 54.32\n",
      "Num timesteps: 376000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 56.31\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 262      |\n",
      "|    ep_rew_mean      | 57.4     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1372     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1488     |\n",
      "|    total_timesteps  | 376691   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.649    |\n",
      "|    n_updates        | 376688   |\n",
      "----------------------------------\n",
      "Num timesteps: 377000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 57.39\n",
      "Num timesteps: 378000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 62.32\n",
      "Num timesteps: 379000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 64.21\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 284      |\n",
      "|    ep_rew_mean      | 66       |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1376     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1500     |\n",
      "|    total_timesteps  | 379380   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.742    |\n",
      "|    n_updates        | 379376   |\n",
      "----------------------------------\n",
      "Num timesteps: 380000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 65.36\n",
      "Num timesteps: 381000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 70.73\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 302      |\n",
      "|    ep_rew_mean      | 72       |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1380     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1509     |\n",
      "|    total_timesteps  | 381565   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.49     |\n",
      "|    n_updates        | 381564   |\n",
      "----------------------------------\n",
      "Num timesteps: 382000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 72.25\n",
      "Num timesteps: 383000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 74.07\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 306      |\n",
      "|    ep_rew_mean      | 76.1     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1384     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1515     |\n",
      "|    total_timesteps  | 383202   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.16     |\n",
      "|    n_updates        | 383200   |\n",
      "----------------------------------\n",
      "Num timesteps: 384000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 80.02\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 319      |\n",
      "|    ep_rew_mean      | 80.3     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1388     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1522     |\n",
      "|    total_timesteps  | 384805   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.73     |\n",
      "|    n_updates        | 384804   |\n",
      "----------------------------------\n",
      "Num timesteps: 385000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 80.27\n",
      "Num timesteps: 386000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 88.19\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 330      |\n",
      "|    ep_rew_mean      | 91.6     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1392     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1527     |\n",
      "|    total_timesteps  | 386274   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.39     |\n",
      "|    n_updates        | 386272   |\n",
      "----------------------------------\n",
      "Num timesteps: 387000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 92.69\n",
      "Num timesteps: 388000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 95.01\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 348      |\n",
      "|    ep_rew_mean      | 98.3     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1396     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1536     |\n",
      "|    total_timesteps  | 388402   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.43     |\n",
      "|    n_updates        | 388400   |\n",
      "----------------------------------\n",
      "Num timesteps: 389000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 102.67\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 358      |\n",
      "|    ep_rew_mean      | 106      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1400     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1541     |\n",
      "|    total_timesteps  | 389815   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.42     |\n",
      "|    n_updates        | 389812   |\n",
      "----------------------------------\n",
      "Num timesteps: 390000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 106.10\n",
      "Num timesteps: 391000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 110.59\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 373      |\n",
      "|    ep_rew_mean      | 115      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1404     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1548     |\n",
      "|    total_timesteps  | 391667   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.79     |\n",
      "|    n_updates        | 391664   |\n",
      "----------------------------------\n",
      "Num timesteps: 392000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 115.04\n",
      "Num timesteps: 393000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 117.81\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 387      |\n",
      "|    ep_rew_mean      | 121      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1408     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1555     |\n",
      "|    total_timesteps  | 393504   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.386    |\n",
      "|    n_updates        | 393500   |\n",
      "----------------------------------\n",
      "Num timesteps: 394000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 120.98\n",
      "Num timesteps: 395000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 122.99\n",
      "Num timesteps: 396000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 129.87\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 414      |\n",
      "|    ep_rew_mean      | 133      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1412     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1568     |\n",
      "|    total_timesteps  | 396566   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.922    |\n",
      "|    n_updates        | 396564   |\n",
      "----------------------------------\n",
      "Num timesteps: 397000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 132.50\n",
      "Num timesteps: 398000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 133.85\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 433      |\n",
      "|    ep_rew_mean      | 137      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1416     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1577     |\n",
      "|    total_timesteps  | 398879   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.503    |\n",
      "|    n_updates        | 398876   |\n",
      "----------------------------------\n",
      "Num timesteps: 399000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 137.06\n",
      "Num timesteps: 400000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 138.35\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 437      |\n",
      "|    ep_rew_mean      | 141      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1420     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1584     |\n",
      "|    total_timesteps  | 400692   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.08     |\n",
      "|    n_updates        | 400688   |\n",
      "----------------------------------\n",
      "Num timesteps: 401000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 141.11\n",
      "Num timesteps: 402000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 145.83\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 444      |\n",
      "|    ep_rew_mean      | 149      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1424     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1590     |\n",
      "|    total_timesteps  | 402263   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.717    |\n",
      "|    n_updates        | 402260   |\n",
      "----------------------------------\n",
      "Num timesteps: 403000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 151.90\n",
      "Num timesteps: 404000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 155.18\n",
      "Num timesteps: 405000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 156.71\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 472      |\n",
      "|    ep_rew_mean      | 160      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1428     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1603     |\n",
      "|    total_timesteps  | 405543   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.477    |\n",
      "|    n_updates        | 405540   |\n",
      "----------------------------------\n",
      "Num timesteps: 406000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 157.55\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 465      |\n",
      "|    ep_rew_mean      | 156      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1432     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1608     |\n",
      "|    total_timesteps  | 406797   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.686    |\n",
      "|    n_updates        | 406796   |\n",
      "----------------------------------\n",
      "Num timesteps: 407000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 155.95\n",
      "Num timesteps: 408000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 160.45\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 471      |\n",
      "|    ep_rew_mean      | 160      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1436     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1613     |\n",
      "|    total_timesteps  | 408183   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.244    |\n",
      "|    n_updates        | 408180   |\n",
      "----------------------------------\n",
      "Num timesteps: 409000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 159.41\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 469      |\n",
      "|    ep_rew_mean      | 157      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1440     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1617     |\n",
      "|    total_timesteps  | 409275   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.848    |\n",
      "|    n_updates        | 409272   |\n",
      "----------------------------------\n",
      "Num timesteps: 410000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 156.97\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 478      |\n",
      "|    ep_rew_mean      | 167      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1444     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1623     |\n",
      "|    total_timesteps  | 410750   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.501    |\n",
      "|    n_updates        | 410748   |\n",
      "----------------------------------\n",
      "Num timesteps: 411000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 165.39\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 465      |\n",
      "|    ep_rew_mean      | 163      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1448     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1628     |\n",
      "|    total_timesteps  | 411972   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.617    |\n",
      "|    n_updates        | 411968   |\n",
      "----------------------------------\n",
      "Num timesteps: 412000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 163.44\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 461      |\n",
      "|    ep_rew_mean      | 161      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1452     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1630     |\n",
      "|    total_timesteps  | 412490   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.688    |\n",
      "|    n_updates        | 412488   |\n",
      "----------------------------------\n",
      "Num timesteps: 413000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 156.76\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 445      |\n",
      "|    ep_rew_mean      | 156      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1456     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1634     |\n",
      "|    total_timesteps  | 413452   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.671    |\n",
      "|    n_updates        | 413448   |\n",
      "----------------------------------\n",
      "Num timesteps: 414000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 156.16\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 442      |\n",
      "|    ep_rew_mean      | 151      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1460     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1639     |\n",
      "|    total_timesteps  | 414790   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.569    |\n",
      "|    n_updates        | 414788   |\n",
      "----------------------------------\n",
      "Num timesteps: 415000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 150.69\n",
      "Num timesteps: 416000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 148.20\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 432      |\n",
      "|    ep_rew_mean      | 147      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1464     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1645     |\n",
      "|    total_timesteps  | 416288   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.316    |\n",
      "|    n_updates        | 416284   |\n",
      "----------------------------------\n",
      "Num timesteps: 417000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 143.96\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 430      |\n",
      "|    ep_rew_mean      | 144      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1468     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1649     |\n",
      "|    total_timesteps  | 417408   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.806    |\n",
      "|    n_updates        | 417404   |\n",
      "----------------------------------\n",
      "Num timesteps: 418000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 143.14\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 422      |\n",
      "|    ep_rew_mean      | 147      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1472     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1655     |\n",
      "|    total_timesteps  | 418874   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.601    |\n",
      "|    n_updates        | 418872   |\n",
      "----------------------------------\n",
      "Num timesteps: 419000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 147.37\n",
      "Num timesteps: 420000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 147.42\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 414      |\n",
      "|    ep_rew_mean      | 145      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1476     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1662     |\n",
      "|    total_timesteps  | 420810   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.394    |\n",
      "|    n_updates        | 420808   |\n",
      "----------------------------------\n",
      "Num timesteps: 421000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 145.81\n",
      "Num timesteps: 422000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 145.68\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 413      |\n",
      "|    ep_rew_mean      | 146      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1480     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1670     |\n",
      "|    total_timesteps  | 422868   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.303    |\n",
      "|    n_updates        | 422864   |\n",
      "----------------------------------\n",
      "Num timesteps: 423000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 144.83\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 403      |\n",
      "|    ep_rew_mean      | 143      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1484     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1673     |\n",
      "|    total_timesteps  | 423466   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.537    |\n",
      "|    n_updates        | 423464   |\n",
      "----------------------------------\n",
      "Num timesteps: 424000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 142.71\n",
      "Num timesteps: 425000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 144.99\n",
      "Num timesteps: 426000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 144.29\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 419      |\n",
      "|    ep_rew_mean      | 148      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1488     |\n",
      "|    fps              | 253      |\n",
      "|    time_elapsed     | 1686     |\n",
      "|    total_timesteps  | 426703   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.854    |\n",
      "|    n_updates        | 426700   |\n",
      "----------------------------------\n",
      "Num timesteps: 427000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 147.98\n",
      "Num timesteps: 428000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 148.15\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 425      |\n",
      "|    ep_rew_mean      | 147      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1492     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1695     |\n",
      "|    total_timesteps  | 428767   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.698    |\n",
      "|    n_updates        | 428764   |\n",
      "----------------------------------\n",
      "Num timesteps: 429000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 147.28\n",
      "Num timesteps: 430000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 148.30\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 419      |\n",
      "|    ep_rew_mean      | 149      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1496     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1701     |\n",
      "|    total_timesteps  | 430323   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.923    |\n",
      "|    n_updates        | 430320   |\n",
      "----------------------------------\n",
      "Num timesteps: 431000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 147.21\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 420      |\n",
      "|    ep_rew_mean      | 149      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1500     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1707     |\n",
      "|    total_timesteps  | 431855   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.246    |\n",
      "|    n_updates        | 431852   |\n",
      "----------------------------------\n",
      "Num timesteps: 432000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 149.26\n",
      "Num timesteps: 433000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 149.94\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 417      |\n",
      "|    ep_rew_mean      | 150      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1504     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1713     |\n",
      "|    total_timesteps  | 433400   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.604    |\n",
      "|    n_updates        | 433396   |\n",
      "----------------------------------\n",
      "Num timesteps: 434000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 149.98\n",
      "Num timesteps: 435000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 149.23\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 415      |\n",
      "|    ep_rew_mean      | 152      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1508     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1719     |\n",
      "|    total_timesteps  | 435001   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.443    |\n",
      "|    n_updates        | 435000   |\n",
      "----------------------------------\n",
      "Num timesteps: 436000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 152.56\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 402      |\n",
      "|    ep_rew_mean      | 149      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1512     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1726     |\n",
      "|    total_timesteps  | 436782   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.913    |\n",
      "|    n_updates        | 436780   |\n",
      "----------------------------------\n",
      "Num timesteps: 437000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 148.75\n",
      "Num timesteps: 438000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 151.60\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 395      |\n",
      "|    ep_rew_mean      | 154      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1516     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1733     |\n",
      "|    total_timesteps  | 438345   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.475    |\n",
      "|    n_updates        | 438344   |\n",
      "----------------------------------\n",
      "Num timesteps: 439000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 154.36\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 390      |\n",
      "|    ep_rew_mean      | 154      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1520     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1738     |\n",
      "|    total_timesteps  | 439711   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.264    |\n",
      "|    n_updates        | 439708   |\n",
      "----------------------------------\n",
      "Num timesteps: 440000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 154.30\n",
      "Num timesteps: 441000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 150.63\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 391      |\n",
      "|    ep_rew_mean      | 149      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1524     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1745     |\n",
      "|    total_timesteps  | 441330   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.48     |\n",
      "|    n_updates        | 441328   |\n",
      "----------------------------------\n",
      "Num timesteps: 442000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 148.82\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 368      |\n",
      "|    ep_rew_mean      | 150      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1528     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1748     |\n",
      "|    total_timesteps  | 442335   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.129    |\n",
      "|    n_updates        | 442332   |\n",
      "----------------------------------\n",
      "Num timesteps: 443000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 153.45\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 367      |\n",
      "|    ep_rew_mean      | 159      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1532     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1753     |\n",
      "|    total_timesteps  | 443504   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.651    |\n",
      "|    n_updates        | 443500   |\n",
      "----------------------------------\n",
      "Num timesteps: 444000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 161.49\n",
      "Num timesteps: 445000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 162.68\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 370      |\n",
      "|    ep_rew_mean      | 164      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1536     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1759     |\n",
      "|    total_timesteps  | 445177   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.295    |\n",
      "|    n_updates        | 445176   |\n",
      "----------------------------------\n",
      "Num timesteps: 446000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 165.66\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 369      |\n",
      "|    ep_rew_mean      | 168      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1540     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1763     |\n",
      "|    total_timesteps  | 446216   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.657    |\n",
      "|    n_updates        | 446212   |\n",
      "----------------------------------\n",
      "Num timesteps: 447000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 167.80\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 371      |\n",
      "|    ep_rew_mean      | 166      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1544     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1770     |\n",
      "|    total_timesteps  | 447868   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.263    |\n",
      "|    n_updates        | 447864   |\n",
      "----------------------------------\n",
      "Num timesteps: 448000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 165.59\n",
      "Num timesteps: 449000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 164.64\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 378      |\n",
      "|    ep_rew_mean      | 170      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1548     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1778     |\n",
      "|    total_timesteps  | 449822   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.776    |\n",
      "|    n_updates        | 449820   |\n",
      "----------------------------------\n",
      "Num timesteps: 450000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 170.34\n",
      "Num timesteps: 451000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 174.57\n",
      "Num timesteps: 452000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 174.58\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 396      |\n",
      "|    ep_rew_mean      | 176      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1552     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1788     |\n",
      "|    total_timesteps  | 452066   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.313    |\n",
      "|    n_updates        | 452064   |\n",
      "----------------------------------\n",
      "Num timesteps: 453000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 175.87\n",
      "Num timesteps: 454000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 177.17\n",
      "Num timesteps: 455000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 178.88\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 420      |\n",
      "|    ep_rew_mean      | 181      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1556     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1802     |\n",
      "|    total_timesteps  | 455438   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.266    |\n",
      "|    n_updates        | 455436   |\n",
      "----------------------------------\n",
      "Num timesteps: 456000\n",
      "Best mean reward: 179.10 - Last mean reward per episode: 182.19\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 418      |\n",
      "|    ep_rew_mean      | 185      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1560     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1806     |\n",
      "|    total_timesteps  | 456614   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.436    |\n",
      "|    n_updates        | 456612   |\n",
      "----------------------------------\n",
      "Num timesteps: 457000\n",
      "Best mean reward: 182.19 - Last mean reward per episode: 183.44\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "Num timesteps: 458000\n",
      "Best mean reward: 183.44 - Last mean reward per episode: 184.55\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 423      |\n",
      "|    ep_rew_mean      | 185      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1564     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1814     |\n",
      "|    total_timesteps  | 458559   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.539    |\n",
      "|    n_updates        | 458556   |\n",
      "----------------------------------\n",
      "Num timesteps: 459000\n",
      "Best mean reward: 184.55 - Last mean reward per episode: 184.54\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 425      |\n",
      "|    ep_rew_mean      | 187      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1568     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1820     |\n",
      "|    total_timesteps  | 459862   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.449    |\n",
      "|    n_updates        | 459860   |\n",
      "----------------------------------\n",
      "Num timesteps: 460000\n",
      "Best mean reward: 184.55 - Last mean reward per episode: 186.63\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 419      |\n",
      "|    ep_rew_mean      | 181      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1572     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1823     |\n",
      "|    total_timesteps  | 460756   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.519    |\n",
      "|    n_updates        | 460752   |\n",
      "----------------------------------\n",
      "Num timesteps: 461000\n",
      "Best mean reward: 186.63 - Last mean reward per episode: 180.60\n",
      "Num timesteps: 462000\n",
      "Best mean reward: 186.63 - Last mean reward per episode: 185.95\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 412      |\n",
      "|    ep_rew_mean      | 184      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1576     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1828     |\n",
      "|    total_timesteps  | 462020   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.48     |\n",
      "|    n_updates        | 462016   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 400      |\n",
      "|    ep_rew_mean      | 183      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1580     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1831     |\n",
      "|    total_timesteps  | 462844   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.456    |\n",
      "|    n_updates        | 462840   |\n",
      "----------------------------------\n",
      "Num timesteps: 463000\n",
      "Best mean reward: 186.63 - Last mean reward per episode: 184.81\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 402      |\n",
      "|    ep_rew_mean      | 187      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1584     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1834     |\n",
      "|    total_timesteps  | 463625   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.19     |\n",
      "|    n_updates        | 463624   |\n",
      "----------------------------------\n",
      "Num timesteps: 464000\n",
      "Best mean reward: 186.63 - Last mean reward per episode: 187.36\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 382      |\n",
      "|    ep_rew_mean      | 187      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1588     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1839     |\n",
      "|    total_timesteps  | 464929   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.452    |\n",
      "|    n_updates        | 464928   |\n",
      "----------------------------------\n",
      "Num timesteps: 465000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 187.00\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 370      |\n",
      "|    ep_rew_mean      | 183      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1592     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1842     |\n",
      "|    total_timesteps  | 465733   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.195    |\n",
      "|    n_updates        | 465732   |\n",
      "----------------------------------\n",
      "Num timesteps: 466000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 183.33\n",
      "Num timesteps: 467000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 186.87\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 370      |\n",
      "|    ep_rew_mean      | 187      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1596     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1848     |\n",
      "|    total_timesteps  | 467359   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.528    |\n",
      "|    n_updates        | 467356   |\n",
      "----------------------------------\n",
      "Num timesteps: 468000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 186.76\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 362      |\n",
      "|    ep_rew_mean      | 184      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1600     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1851     |\n",
      "|    total_timesteps  | 468097   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.896    |\n",
      "|    n_updates        | 468096   |\n",
      "----------------------------------\n",
      "Num timesteps: 469000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 181.94\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 357      |\n",
      "|    ep_rew_mean      | 182      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1604     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1855     |\n",
      "|    total_timesteps  | 469053   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.249    |\n",
      "|    n_updates        | 469052   |\n",
      "----------------------------------\n",
      "Num timesteps: 470000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 181.50\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 354      |\n",
      "|    ep_rew_mean      | 180      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1608     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1860     |\n",
      "|    total_timesteps  | 470354   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.315    |\n",
      "|    n_updates        | 470352   |\n",
      "----------------------------------\n",
      "Num timesteps: 471000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 178.52\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 343      |\n",
      "|    ep_rew_mean      | 179      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1612     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1863     |\n",
      "|    total_timesteps  | 471059   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.377    |\n",
      "|    n_updates        | 471056   |\n",
      "----------------------------------\n",
      "Num timesteps: 472000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 178.56\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 337      |\n",
      "|    ep_rew_mean      | 179      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1616     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1866     |\n",
      "|    total_timesteps  | 472001   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.17     |\n",
      "|    n_updates        | 472000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 327      |\n",
      "|    ep_rew_mean      | 171      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1620     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1868     |\n",
      "|    total_timesteps  | 472391   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.453    |\n",
      "|    n_updates        | 472388   |\n",
      "----------------------------------\n",
      "Num timesteps: 473000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 170.56\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 323      |\n",
      "|    ep_rew_mean      | 172      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1624     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1873     |\n",
      "|    total_timesteps  | 473610   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.599    |\n",
      "|    n_updates        | 473608   |\n",
      "----------------------------------\n",
      "Num timesteps: 474000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 164.80\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 317      |\n",
      "|    ep_rew_mean      | 162      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1628     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1874     |\n",
      "|    total_timesteps  | 474044   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.215    |\n",
      "|    n_updates        | 474040   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 310      |\n",
      "|    ep_rew_mean      | 152      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1632     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1876     |\n",
      "|    total_timesteps  | 474504   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.122    |\n",
      "|    n_updates        | 474500   |\n",
      "----------------------------------\n",
      "Num timesteps: 475000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 152.25\n",
      "Num timesteps: 476000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 151.51\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 312      |\n",
      "|    ep_rew_mean      | 151      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1636     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1883     |\n",
      "|    total_timesteps  | 476374   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.157    |\n",
      "|    n_updates        | 476372   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 306      |\n",
      "|    ep_rew_mean      | 146      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1640     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1885     |\n",
      "|    total_timesteps  | 476830   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.394    |\n",
      "|    n_updates        | 476828   |\n",
      "----------------------------------\n",
      "Num timesteps: 477000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 146.19\n",
      "Num timesteps: 478000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 148.67\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 306      |\n",
      "|    ep_rew_mean      | 151      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1644     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1891     |\n",
      "|    total_timesteps  | 478420   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.71     |\n",
      "|    n_updates        | 478416   |\n",
      "----------------------------------\n",
      "Num timesteps: 479000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 147.44\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 297      |\n",
      "|    ep_rew_mean      | 147      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1648     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1895     |\n",
      "|    total_timesteps  | 479507   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.427    |\n",
      "|    n_updates        | 479504   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 279      |\n",
      "|    ep_rew_mean      | 142      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1652     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1897     |\n",
      "|    total_timesteps  | 479955   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.425    |\n",
      "|    n_updates        | 479952   |\n",
      "----------------------------------\n",
      "Num timesteps: 480000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 141.62\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 255      |\n",
      "|    ep_rew_mean      | 138      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1656     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1901     |\n",
      "|    total_timesteps  | 480946   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.381    |\n",
      "|    n_updates        | 480944   |\n",
      "----------------------------------\n",
      "Num timesteps: 481000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 138.40\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 139      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1660     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1903     |\n",
      "|    total_timesteps  | 481615   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.555    |\n",
      "|    n_updates        | 481612   |\n",
      "----------------------------------\n",
      "Num timesteps: 482000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 137.86\n",
      "Num timesteps: 483000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 136.71\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 246      |\n",
      "|    ep_rew_mean      | 140      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1664     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1909     |\n",
      "|    total_timesteps  | 483111   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.986    |\n",
      "|    n_updates        | 483108   |\n",
      "----------------------------------\n",
      "Num timesteps: 484000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 140.25\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 249      |\n",
      "|    ep_rew_mean      | 141      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1668     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1916     |\n",
      "|    total_timesteps  | 484715   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.334    |\n",
      "|    n_updates        | 484712   |\n",
      "----------------------------------\n",
      "Num timesteps: 485000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 143.24\n",
      "Num timesteps: 486000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 143.01\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 256      |\n",
      "|    ep_rew_mean      | 145      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1672     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1923     |\n",
      "|    total_timesteps  | 486385   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.714    |\n",
      "|    n_updates        | 486384   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 249      |\n",
      "|    ep_rew_mean      | 139      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1676     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1924     |\n",
      "|    total_timesteps  | 486900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.34     |\n",
      "|    n_updates        | 486896   |\n",
      "----------------------------------\n",
      "Num timesteps: 487000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 138.64\n",
      "Num timesteps: 488000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 138.25\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 258      |\n",
      "|    ep_rew_mean      | 140      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1680     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1931     |\n",
      "|    total_timesteps  | 488661   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.163    |\n",
      "|    n_updates        | 488660   |\n",
      "----------------------------------\n",
      "Num timesteps: 489000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 139.82\n",
      "Num timesteps: 490000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 138.32\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 267      |\n",
      "|    ep_rew_mean      | 141      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1684     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1938     |\n",
      "|    total_timesteps  | 490284   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.421    |\n",
      "|    n_updates        | 490280   |\n",
      "----------------------------------\n",
      "Num timesteps: 491000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 139.47\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 262      |\n",
      "|    ep_rew_mean      | 140      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1688     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1941     |\n",
      "|    total_timesteps  | 491137   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.645    |\n",
      "|    n_updates        | 491136   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 261      |\n",
      "|    ep_rew_mean      | 135      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1692     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1944     |\n",
      "|    total_timesteps  | 491808   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.594    |\n",
      "|    n_updates        | 491804   |\n",
      "----------------------------------\n",
      "Num timesteps: 492000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 134.98\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 256      |\n",
      "|    ep_rew_mean      | 130      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1696     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1948     |\n",
      "|    total_timesteps  | 492993   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.21     |\n",
      "|    n_updates        | 492992   |\n",
      "----------------------------------\n",
      "Num timesteps: 493000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 130.34\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 256      |\n",
      "|    ep_rew_mean      | 126      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1700     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1951     |\n",
      "|    total_timesteps  | 493688   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.361    |\n",
      "|    n_updates        | 493684   |\n",
      "----------------------------------\n",
      "Num timesteps: 494000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 126.44\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 255      |\n",
      "|    ep_rew_mean      | 124      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1704     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1955     |\n",
      "|    total_timesteps  | 494591   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.394    |\n",
      "|    n_updates        | 494588   |\n",
      "----------------------------------\n",
      "Num timesteps: 495000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 124.25\n",
      "Num timesteps: 496000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 123.42\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 262      |\n",
      "|    ep_rew_mean      | 126      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1708     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1963     |\n",
      "|    total_timesteps  | 496587   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.421    |\n",
      "|    n_updates        | 496584   |\n",
      "----------------------------------\n",
      "Num timesteps: 497000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 126.29\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 262      |\n",
      "|    ep_rew_mean      | 122      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1712     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1965     |\n",
      "|    total_timesteps  | 497270   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.56     |\n",
      "|    n_updates        | 497268   |\n",
      "----------------------------------\n",
      "Num timesteps: 498000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 124.41\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 262      |\n",
      "|    ep_rew_mean      | 120      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1716     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1969     |\n",
      "|    total_timesteps  | 498177   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.657    |\n",
      "|    n_updates        | 498176   |\n",
      "----------------------------------\n",
      "Num timesteps: 499000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 122.49\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 268      |\n",
      "|    ep_rew_mean      | 126      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1720     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1973     |\n",
      "|    total_timesteps  | 499162   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.88     |\n",
      "|    n_updates        | 499160   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 260      |\n",
      "|    ep_rew_mean      | 122      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1724     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1975     |\n",
      "|    total_timesteps  | 499637   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.15     |\n",
      "|    n_updates        | 499636   |\n",
      "----------------------------------\n",
      "Num timesteps: 500000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 120.88\n",
      "Num timesteps: 501000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 125.19\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 270      |\n",
      "|    ep_rew_mean      | 127      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1728     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1980     |\n",
      "|    total_timesteps  | 501086   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.905    |\n",
      "|    n_updates        | 501084   |\n",
      "----------------------------------\n",
      "Num timesteps: 502000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 132.02\n",
      "Num timesteps: 503000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 133.88\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 285      |\n",
      "|    ep_rew_mean      | 136      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1732     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1988     |\n",
      "|    total_timesteps  | 503010   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.19     |\n",
      "|    n_updates        | 503008   |\n",
      "----------------------------------\n",
      "Num timesteps: 504000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 135.47\n",
      "Num timesteps: 505000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 134.76\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 287      |\n",
      "|    ep_rew_mean      | 138      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1736     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 1997     |\n",
      "|    total_timesteps  | 505103   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.797    |\n",
      "|    n_updates        | 505100   |\n",
      "----------------------------------\n",
      "Num timesteps: 506000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 139.38\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 300      |\n",
      "|    ep_rew_mean      | 145      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1740     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2003     |\n",
      "|    total_timesteps  | 506869   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.749    |\n",
      "|    n_updates        | 506868   |\n",
      "----------------------------------\n",
      "Num timesteps: 507000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 144.56\n",
      "Num timesteps: 508000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 147.07\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 297      |\n",
      "|    ep_rew_mean      | 144      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1744     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2008     |\n",
      "|    total_timesteps  | 508086   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.602    |\n",
      "|    n_updates        | 508084   |\n",
      "----------------------------------\n",
      "Num timesteps: 509000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 152.03\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 295      |\n",
      "|    ep_rew_mean      | 150      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1748     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2012     |\n",
      "|    total_timesteps  | 509030   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.39     |\n",
      "|    n_updates        | 509028   |\n",
      "----------------------------------\n",
      "Num timesteps: 510000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 154.33\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 307      |\n",
      "|    ep_rew_mean      | 157      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1752     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2018     |\n",
      "|    total_timesteps  | 510625   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.462    |\n",
      "|    n_updates        | 510624   |\n",
      "----------------------------------\n",
      "Num timesteps: 511000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 156.75\n",
      "Num timesteps: 512000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 153.93\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 311      |\n",
      "|    ep_rew_mean      | 154      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1756     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2024     |\n",
      "|    total_timesteps  | 512025   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.365    |\n",
      "|    n_updates        | 512024   |\n",
      "----------------------------------\n",
      "Num timesteps: 513000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 154.05\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 314      |\n",
      "|    ep_rew_mean      | 154      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1760     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2028     |\n",
      "|    total_timesteps  | 513001   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.619    |\n",
      "|    n_updates        | 513000   |\n",
      "----------------------------------\n",
      "Num timesteps: 514000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 156.35\n",
      "Num timesteps: 515000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 157.33\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 323      |\n",
      "|    ep_rew_mean      | 155      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1764     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2038     |\n",
      "|    total_timesteps  | 515419   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.186    |\n",
      "|    n_updates        | 515416   |\n",
      "----------------------------------\n",
      "Num timesteps: 516000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 152.94\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 321      |\n",
      "|    ep_rew_mean      | 156      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1768     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2044     |\n",
      "|    total_timesteps  | 516785   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.4      |\n",
      "|    n_updates        | 516784   |\n",
      "----------------------------------\n",
      "Num timesteps: 517000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 156.12\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 312      |\n",
      "|    ep_rew_mean      | 152      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1772     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2047     |\n",
      "|    total_timesteps  | 517549   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1        |\n",
      "|    n_updates        | 517548   |\n",
      "----------------------------------\n",
      "Num timesteps: 518000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 152.05\n",
      "Num timesteps: 519000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 153.29\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 322      |\n",
      "|    ep_rew_mean      | 154      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1776     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2053     |\n",
      "|    total_timesteps  | 519100   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.195    |\n",
      "|    n_updates        | 519096   |\n",
      "----------------------------------\n",
      "Num timesteps: 520000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 153.98\n",
      "Num timesteps: 521000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 154.81\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 324      |\n",
      "|    ep_rew_mean      | 155      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1780     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2061     |\n",
      "|    total_timesteps  | 521035   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.351    |\n",
      "|    n_updates        | 521032   |\n",
      "----------------------------------\n",
      "Num timesteps: 522000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 155.05\n",
      "Num timesteps: 523000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 157.15\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 335      |\n",
      "|    ep_rew_mean      | 154      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1784     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2072     |\n",
      "|    total_timesteps  | 523781   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.78     |\n",
      "|    n_updates        | 523780   |\n",
      "----------------------------------\n",
      "Num timesteps: 524000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 154.23\n",
      "Num timesteps: 525000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 158.26\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 343      |\n",
      "|    ep_rew_mean      | 156      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1788     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2079     |\n",
      "|    total_timesteps  | 525466   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.325    |\n",
      "|    n_updates        | 525464   |\n",
      "----------------------------------\n",
      "Num timesteps: 526000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 156.31\n",
      "Num timesteps: 527000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 158.78\n",
      "Num timesteps: 528000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 162.96\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 368      |\n",
      "|    ep_rew_mean      | 164      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1792     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2092     |\n",
      "|    total_timesteps  | 528642   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.416    |\n",
      "|    n_updates        | 528640   |\n",
      "----------------------------------\n",
      "Num timesteps: 529000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 159.41\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 361      |\n",
      "|    ep_rew_mean      | 160      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1796     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2093     |\n",
      "|    total_timesteps  | 529044   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.701    |\n",
      "|    n_updates        | 529040   |\n",
      "----------------------------------\n",
      "Num timesteps: 530000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 163.38\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 364      |\n",
      "|    ep_rew_mean      | 164      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1800     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2098     |\n",
      "|    total_timesteps  | 530132   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.363    |\n",
      "|    n_updates        | 530128   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 362      |\n",
      "|    ep_rew_mean      | 164      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1804     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2100     |\n",
      "|    total_timesteps  | 530819   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.984    |\n",
      "|    n_updates        | 530816   |\n",
      "----------------------------------\n",
      "Num timesteps: 531000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 164.29\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 352      |\n",
      "|    ep_rew_mean      | 165      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1808     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2104     |\n",
      "|    total_timesteps  | 531811   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.233    |\n",
      "|    n_updates        | 531808   |\n",
      "----------------------------------\n",
      "Num timesteps: 532000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 165.39\n",
      "Num timesteps: 533000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 164.54\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 364      |\n",
      "|    ep_rew_mean      | 171      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1812     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2112     |\n",
      "|    total_timesteps  | 533623   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.508    |\n",
      "|    n_updates        | 533620   |\n",
      "----------------------------------\n",
      "Num timesteps: 534000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 166.07\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 365      |\n",
      "|    ep_rew_mean      | 168      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1816     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2116     |\n",
      "|    total_timesteps  | 534701   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.334    |\n",
      "|    n_updates        | 534700   |\n",
      "----------------------------------\n",
      "Num timesteps: 535000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 170.22\n",
      "Num timesteps: 536000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 169.50\n",
      "Num timesteps: 537000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 170.85\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 381      |\n",
      "|    ep_rew_mean      | 171      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1820     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2127     |\n",
      "|    total_timesteps  | 537289   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.998    |\n",
      "|    n_updates        | 537288   |\n",
      "----------------------------------\n",
      "Num timesteps: 538000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 170.74\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 393      |\n",
      "|    ep_rew_mean      | 170      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1824     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2133     |\n",
      "|    total_timesteps  | 538979   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.567    |\n",
      "|    n_updates        | 538976   |\n",
      "----------------------------------\n",
      "Num timesteps: 539000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 169.90\n",
      "Num timesteps: 540000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 169.57\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 393      |\n",
      "|    ep_rew_mean      | 170      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1828     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2139     |\n",
      "|    total_timesteps  | 540419   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.138    |\n",
      "|    n_updates        | 540416   |\n",
      "----------------------------------\n",
      "Num timesteps: 541000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 169.86\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 388      |\n",
      "|    ep_rew_mean      | 168      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1832     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2145     |\n",
      "|    total_timesteps  | 541792   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.22     |\n",
      "|    n_updates        | 541788   |\n",
      "----------------------------------\n",
      "Num timesteps: 542000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 167.28\n",
      "Num timesteps: 543000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 168.73\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 383      |\n",
      "|    ep_rew_mean      | 168      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1836     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2151     |\n",
      "|    total_timesteps  | 543399   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.402    |\n",
      "|    n_updates        | 543396   |\n",
      "----------------------------------\n",
      "Num timesteps: 544000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 159.05\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 373      |\n",
      "|    ep_rew_mean      | 159      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1840     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2154     |\n",
      "|    total_timesteps  | 544172   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.969    |\n",
      "|    n_updates        | 544168   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 366      |\n",
      "|    ep_rew_mean      | 146      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1844     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2156     |\n",
      "|    total_timesteps  | 544734   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.764    |\n",
      "|    n_updates        | 544732   |\n",
      "----------------------------------\n",
      "Num timesteps: 545000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 137.00\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 362      |\n",
      "|    ep_rew_mean      | 137      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1848     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2158     |\n",
      "|    total_timesteps  | 545264   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.266    |\n",
      "|    n_updates        | 545260   |\n",
      "----------------------------------\n",
      "Num timesteps: 546000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 137.05\n",
      "Num timesteps: 547000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 137.17\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 369      |\n",
      "|    ep_rew_mean      | 130      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1852     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2168     |\n",
      "|    total_timesteps  | 547560   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.332    |\n",
      "|    n_updates        | 547556   |\n",
      "----------------------------------\n",
      "Num timesteps: 548000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 115.15\n",
      "Num timesteps: 549000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 114.79\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 371      |\n",
      "|    ep_rew_mean      | 116      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1856     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2174     |\n",
      "|    total_timesteps  | 549130   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.439    |\n",
      "|    n_updates        | 549128   |\n",
      "----------------------------------\n",
      "Num timesteps: 550000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 116.20\n",
      "Num timesteps: 551000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 117.20\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 382      |\n",
      "|    ep_rew_mean      | 119      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1860     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2182     |\n",
      "|    total_timesteps  | 551216   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.367    |\n",
      "|    n_updates        | 551212   |\n",
      "----------------------------------\n",
      "Num timesteps: 552000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 118.68\n",
      "Num timesteps: 553000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 119.36\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 377      |\n",
      "|    ep_rew_mean      | 119      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1864     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2190     |\n",
      "|    total_timesteps  | 553131   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.561    |\n",
      "|    n_updates        | 553128   |\n",
      "----------------------------------\n",
      "Num timesteps: 554000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 109.21\n",
      "Num timesteps: 555000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 108.73\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 386      |\n",
      "|    ep_rew_mean      | 107      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1868     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2199     |\n",
      "|    total_timesteps  | 555373   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.235    |\n",
      "|    n_updates        | 555372   |\n",
      "----------------------------------\n",
      "Num timesteps: 556000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 107.45\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 394      |\n",
      "|    ep_rew_mean      | 69.5     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1872     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2206     |\n",
      "|    total_timesteps  | 556964   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.45     |\n",
      "|    n_updates        | 556960   |\n",
      "----------------------------------\n",
      "Num timesteps: 557000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 69.51\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 386      |\n",
      "|    ep_rew_mean      | 47       |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1876     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2209     |\n",
      "|    total_timesteps  | 557678   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.279    |\n",
      "|    n_updates        | 557676   |\n",
      "----------------------------------\n",
      "Num timesteps: 558000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 40.69\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 373      |\n",
      "|    ep_rew_mean      | 32.9     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1880     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2211     |\n",
      "|    total_timesteps  | 558382   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.557    |\n",
      "|    n_updates        | 558380   |\n",
      "----------------------------------\n",
      "Num timesteps: 559000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 18.89\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 361      |\n",
      "|    ep_rew_mean      | 0.994    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1884     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2217     |\n",
      "|    total_timesteps  | 559845   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.521    |\n",
      "|    n_updates        | 559844   |\n",
      "----------------------------------\n",
      "Num timesteps: 560000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -1.62\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 349      |\n",
      "|    ep_rew_mean      | -17      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1888     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2219     |\n",
      "|    total_timesteps  | 560380   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.73     |\n",
      "|    n_updates        | 560376   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 323      |\n",
      "|    ep_rew_mean      | -45.5    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1892     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2222     |\n",
      "|    total_timesteps  | 560963   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.05     |\n",
      "|    n_updates        | 560960   |\n",
      "----------------------------------\n",
      "Num timesteps: 561000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -45.46\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 324      |\n",
      "|    ep_rew_mean      | -67.6    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1896     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2223     |\n",
      "|    total_timesteps  | 561451   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.796    |\n",
      "|    n_updates        | 561448   |\n",
      "----------------------------------\n",
      "Num timesteps: 562000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -78.77\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 319      |\n",
      "|    ep_rew_mean      | -94.8    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1900     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2226     |\n",
      "|    total_timesteps  | 562051   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.45     |\n",
      "|    n_updates        | 562048   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 316      |\n",
      "|    ep_rew_mean      | -113     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1904     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2227     |\n",
      "|    total_timesteps  | 562407   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.767    |\n",
      "|    n_updates        | 562404   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 310      |\n",
      "|    ep_rew_mean      | -139     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1908     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2228     |\n",
      "|    total_timesteps  | 562801   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.29     |\n",
      "|    n_updates        | 562800   |\n",
      "----------------------------------\n",
      "Num timesteps: 563000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -139.35\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 299      |\n",
      "|    ep_rew_mean      | -158     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1912     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2231     |\n",
      "|    total_timesteps  | 563493   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.779    |\n",
      "|    n_updates        | 563492   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 291      |\n",
      "|    ep_rew_mean      | -178     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1916     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2232     |\n",
      "|    total_timesteps  | 563826   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.962    |\n",
      "|    n_updates        | 563824   |\n",
      "----------------------------------\n",
      "Num timesteps: 564000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -184.45\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 269      |\n",
      "|    ep_rew_mean      | -203     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1920     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2234     |\n",
      "|    total_timesteps  | 564189   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.766    |\n",
      "|    n_updates        | 564188   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 255      |\n",
      "|    ep_rew_mean      | -219     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1924     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2235     |\n",
      "|    total_timesteps  | 564513   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.681    |\n",
      "|    n_updates        | 564512   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 245      |\n",
      "|    ep_rew_mean      | -243     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1928     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2236     |\n",
      "|    total_timesteps  | 564874   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.2      |\n",
      "|    n_updates        | 564872   |\n",
      "----------------------------------\n",
      "Num timesteps: 565000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -250.33\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 234      |\n",
      "|    ep_rew_mean      | -267     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1932     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2237     |\n",
      "|    total_timesteps  | 565200   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.756    |\n",
      "|    n_updates        | 565196   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 221      |\n",
      "|    ep_rew_mean      | -290     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1936     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2238     |\n",
      "|    total_timesteps  | 565509   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.41     |\n",
      "|    n_updates        | 565508   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 216      |\n",
      "|    ep_rew_mean      | -299     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1940     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2240     |\n",
      "|    total_timesteps  | 565816   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.925    |\n",
      "|    n_updates        | 565812   |\n",
      "----------------------------------\n",
      "Num timesteps: 566000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -305.59\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 214      |\n",
      "|    ep_rew_mean      | -309     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1944     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2241     |\n",
      "|    total_timesteps  | 566167   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.39     |\n",
      "|    n_updates        | 566164   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 213      |\n",
      "|    ep_rew_mean      | -324     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1948     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2242     |\n",
      "|    total_timesteps  | 566542   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.95     |\n",
      "|    n_updates        | 566540   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 193      |\n",
      "|    ep_rew_mean      | -339     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1952     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2244     |\n",
      "|    total_timesteps  | 566877   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.19     |\n",
      "|    n_updates        | 566876   |\n",
      "----------------------------------\n",
      "Num timesteps: 567000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -331.23\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 181      |\n",
      "|    ep_rew_mean      | -340     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1956     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2245     |\n",
      "|    total_timesteps  | 567212   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.717    |\n",
      "|    n_updates        | 567208   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 164      |\n",
      "|    ep_rew_mean      | -361     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1960     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2246     |\n",
      "|    total_timesteps  | 567602   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.25     |\n",
      "|    n_updates        | 567600   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 148      |\n",
      "|    ep_rew_mean      | -379     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1964     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2248     |\n",
      "|    total_timesteps  | 567977   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.63     |\n",
      "|    n_updates        | 567976   |\n",
      "----------------------------------\n",
      "Num timesteps: 568000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -378.72\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 130      |\n",
      "|    ep_rew_mean      | -387     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1968     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2249     |\n",
      "|    total_timesteps  | 568370   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.75     |\n",
      "|    n_updates        | 568368   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 119      |\n",
      "|    ep_rew_mean      | -367     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1972     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2251     |\n",
      "|    total_timesteps  | 568822   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.15     |\n",
      "|    n_updates        | 568820   |\n",
      "----------------------------------\n",
      "Num timesteps: 569000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -366.61\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 116      |\n",
      "|    ep_rew_mean      | -363     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1976     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2252     |\n",
      "|    total_timesteps  | 569237   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.06     |\n",
      "|    n_updates        | 569236   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 114      |\n",
      "|    ep_rew_mean      | -371     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1980     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2255     |\n",
      "|    total_timesteps  | 569830   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.82     |\n",
      "|    n_updates        | 569828   |\n",
      "----------------------------------\n",
      "Num timesteps: 570000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -369.59\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 106      |\n",
      "|    ep_rew_mean      | -359     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1984     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2257     |\n",
      "|    total_timesteps  | 570407   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.67     |\n",
      "|    n_updates        | 570404   |\n",
      "----------------------------------\n",
      "Num timesteps: 571000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -358.65\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 110      |\n",
      "|    ep_rew_mean      | -361     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1988     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2261     |\n",
      "|    total_timesteps  | 571415   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.9      |\n",
      "|    n_updates        | 571412   |\n",
      "----------------------------------\n",
      "Num timesteps: 572000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -356.36\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 112      |\n",
      "|    ep_rew_mean      | -354     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1992     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2264     |\n",
      "|    total_timesteps  | 572168   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.14     |\n",
      "|    n_updates        | 572164   |\n",
      "----------------------------------\n",
      "Num timesteps: 573000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -351.77\n",
      "Num timesteps: 574000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -347.13\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 131      |\n",
      "|    ep_rew_mean      | -336     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 1996     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2274     |\n",
      "|    total_timesteps  | 574529   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.49     |\n",
      "|    n_updates        | 574528   |\n",
      "----------------------------------\n",
      "Num timesteps: 575000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -333.07\n",
      "Num timesteps: 576000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -334.77\n",
      "Num timesteps: 577000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -332.97\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 151      |\n",
      "|    ep_rew_mean      | -318     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2000     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2286     |\n",
      "|    total_timesteps  | 577141   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.19     |\n",
      "|    n_updates        | 577140   |\n",
      "----------------------------------\n",
      "Num timesteps: 578000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -318.23\n",
      "Num timesteps: 579000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -308.17\n",
      "Num timesteps: 580000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -306.10\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | -302     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2004     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2301     |\n",
      "|    total_timesteps  | 580444   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.93     |\n",
      "|    n_updates        | 580440   |\n",
      "----------------------------------\n",
      "Num timesteps: 581000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -296.62\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 189      |\n",
      "|    ep_rew_mean      | -291     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2008     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2306     |\n",
      "|    total_timesteps  | 581671   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.51     |\n",
      "|    n_updates        | 581668   |\n",
      "----------------------------------\n",
      "Num timesteps: 582000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -294.66\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 191      |\n",
      "|    ep_rew_mean      | -283     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2012     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2310     |\n",
      "|    total_timesteps  | 582560   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.68     |\n",
      "|    n_updates        | 582556   |\n",
      "----------------------------------\n",
      "Num timesteps: 583000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -279.05\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 194      |\n",
      "|    ep_rew_mean      | -274     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2016     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2312     |\n",
      "|    total_timesteps  | 583230   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.28     |\n",
      "|    n_updates        | 583228   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 196      |\n",
      "|    ep_rew_mean      | -268     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2020     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2314     |\n",
      "|    total_timesteps  | 583794   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.04     |\n",
      "|    n_updates        | 583792   |\n",
      "----------------------------------\n",
      "Num timesteps: 584000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -265.40\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 198      |\n",
      "|    ep_rew_mean      | -263     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2024     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2316     |\n",
      "|    total_timesteps  | 584311   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.35     |\n",
      "|    n_updates        | 584308   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | -255     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2028     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2318     |\n",
      "|    total_timesteps  | 584863   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.51     |\n",
      "|    n_updates        | 584860   |\n",
      "----------------------------------\n",
      "Num timesteps: 585000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -255.38\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 202      |\n",
      "|    ep_rew_mean      | -250     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2032     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2320     |\n",
      "|    total_timesteps  | 585439   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 4.43     |\n",
      "|    n_updates        | 585436   |\n",
      "----------------------------------\n",
      "Num timesteps: 586000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -244.62\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 205      |\n",
      "|    ep_rew_mean      | -244     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2036     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2323     |\n",
      "|    total_timesteps  | 586013   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.29     |\n",
      "|    n_updates        | 586012   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 207      |\n",
      "|    ep_rew_mean      | -243     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2040     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2324     |\n",
      "|    total_timesteps  | 586515   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.52     |\n",
      "|    n_updates        | 586512   |\n",
      "----------------------------------\n",
      "Num timesteps: 587000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -241.32\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 208      |\n",
      "|    ep_rew_mean      | -242     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2044     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2326     |\n",
      "|    total_timesteps  | 587012   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 4.52     |\n",
      "|    n_updates        | 587008   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 210      |\n",
      "|    ep_rew_mean      | -237     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2048     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2328     |\n",
      "|    total_timesteps  | 587592   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.58     |\n",
      "|    n_updates        | 587588   |\n",
      "----------------------------------\n",
      "Num timesteps: 588000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -235.70\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 213      |\n",
      "|    ep_rew_mean      | -234     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2052     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2331     |\n",
      "|    total_timesteps  | 588159   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.33     |\n",
      "|    n_updates        | 588156   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 214      |\n",
      "|    ep_rew_mean      | -232     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2056     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2332     |\n",
      "|    total_timesteps  | 588630   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.34     |\n",
      "|    n_updates        | 588628   |\n",
      "----------------------------------\n",
      "Num timesteps: 589000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -230.05\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 216      |\n",
      "|    ep_rew_mean      | -229     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2060     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2335     |\n",
      "|    total_timesteps  | 589175   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.89     |\n",
      "|    n_updates        | 589172   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 217      |\n",
      "|    ep_rew_mean      | -227     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2064     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2336     |\n",
      "|    total_timesteps  | 589690   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.95     |\n",
      "|    n_updates        | 589688   |\n",
      "----------------------------------\n",
      "Num timesteps: 590000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -226.62\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 219      |\n",
      "|    ep_rew_mean      | -226     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2068     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2338     |\n",
      "|    total_timesteps  | 590225   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.88     |\n",
      "|    n_updates        | 590224   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 219      |\n",
      "|    ep_rew_mean      | -221     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2072     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2340     |\n",
      "|    total_timesteps  | 590766   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 4.92     |\n",
      "|    n_updates        | 590764   |\n",
      "----------------------------------\n",
      "Num timesteps: 591000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -220.04\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 221      |\n",
      "|    ep_rew_mean      | -217     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2076     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2343     |\n",
      "|    total_timesteps  | 591366   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.76     |\n",
      "|    n_updates        | 591364   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 221      |\n",
      "|    ep_rew_mean      | -211     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2080     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2345     |\n",
      "|    total_timesteps  | 591965   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.39     |\n",
      "|    n_updates        | 591964   |\n",
      "----------------------------------\n",
      "Num timesteps: 592000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -211.48\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 221      |\n",
      "|    ep_rew_mean      | -207     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2084     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2347     |\n",
      "|    total_timesteps  | 592461   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.3      |\n",
      "|    n_updates        | 592460   |\n",
      "----------------------------------\n",
      "Num timesteps: 593000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -210.05\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 217      |\n",
      "|    ep_rew_mean      | -207     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2088     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2349     |\n",
      "|    total_timesteps  | 593139   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.78     |\n",
      "|    n_updates        | 593136   |\n",
      "----------------------------------\n",
      "Num timesteps: 594000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -207.73\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 223      |\n",
      "|    ep_rew_mean      | -209     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2092     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2354     |\n",
      "|    total_timesteps  | 594425   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.58     |\n",
      "|    n_updates        | 594424   |\n",
      "----------------------------------\n",
      "Num timesteps: 595000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -209.33\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 212      |\n",
      "|    ep_rew_mean      | -215     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2096     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2359     |\n",
      "|    total_timesteps  | 595759   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.73     |\n",
      "|    n_updates        | 595756   |\n",
      "----------------------------------\n",
      "Num timesteps: 596000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -215.11\n",
      "Num timesteps: 597000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -225.52\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 199      |\n",
      "|    ep_rew_mean      | -228     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2100     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2364     |\n",
      "|    total_timesteps  | 597008   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.49     |\n",
      "|    n_updates        | 597004   |\n",
      "----------------------------------\n",
      "Num timesteps: 598000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -230.29\n",
      "Num timesteps: 599000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -234.30\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 188      |\n",
      "|    ep_rew_mean      | -236     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2104     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2374     |\n",
      "|    total_timesteps  | 599242   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.85     |\n",
      "|    n_updates        | 599240   |\n",
      "----------------------------------\n",
      "Num timesteps: 600000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -236.04\n",
      "Num timesteps: 601000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -235.68\n",
      "Num timesteps: 602000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -234.64\n",
      "Num timesteps: 603000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -233.62\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 216      |\n",
      "|    ep_rew_mean      | -232     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2108     |\n",
      "|    fps              | 252      |\n",
      "|    time_elapsed     | 2393     |\n",
      "|    total_timesteps  | 603242   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.09     |\n",
      "|    n_updates        | 603240   |\n",
      "----------------------------------\n",
      "Num timesteps: 604000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -231.70\n",
      "Num timesteps: 605000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -236.18\n",
      "Num timesteps: 606000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -235.10\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 238      |\n",
      "|    ep_rew_mean      | -234     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2112     |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 2409     |\n",
      "|    total_timesteps  | 606359   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.44     |\n",
      "|    n_updates        | 606356   |\n",
      "----------------------------------\n",
      "Num timesteps: 607000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -235.11\n",
      "Num timesteps: 608000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -232.40\n",
      "Num timesteps: 609000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -231.49\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 264      |\n",
      "|    ep_rew_mean      | -230     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2116     |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 2424     |\n",
      "|    total_timesteps  | 609588   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.21     |\n",
      "|    n_updates        | 609584   |\n",
      "----------------------------------\n",
      "Num timesteps: 610000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -229.52\n",
      "Num timesteps: 611000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -227.42\n",
      "Num timesteps: 612000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -226.37\n",
      "Num timesteps: 613000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -224.79\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 294      |\n",
      "|    ep_rew_mean      | -223     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2120     |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 2441     |\n",
      "|    total_timesteps  | 613152   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.19     |\n",
      "|    n_updates        | 613148   |\n",
      "----------------------------------\n",
      "Num timesteps: 614000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -222.63\n",
      "Num timesteps: 615000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -220.91\n",
      "Num timesteps: 616000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -217.39\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 321      |\n",
      "|    ep_rew_mean      | -216     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2124     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2457     |\n",
      "|    total_timesteps  | 616450   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.37     |\n",
      "|    n_updates        | 616448   |\n",
      "----------------------------------\n",
      "Num timesteps: 617000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -215.67\n",
      "Num timesteps: 618000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -220.89\n",
      "Num timesteps: 619000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -218.25\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 347      |\n",
      "|    ep_rew_mean      | -217     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2128     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2472     |\n",
      "|    total_timesteps  | 619591   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.18     |\n",
      "|    n_updates        | 619588   |\n",
      "----------------------------------\n",
      "Num timesteps: 620000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -216.58\n",
      "Num timesteps: 621000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -219.45\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 358      |\n",
      "|    ep_rew_mean      | -217     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2132     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2479     |\n",
      "|    total_timesteps  | 621194   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.02     |\n",
      "|    n_updates        | 621192   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 357      |\n",
      "|    ep_rew_mean      | -213     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2136     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2481     |\n",
      "|    total_timesteps  | 621668   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.77     |\n",
      "|    n_updates        | 621664   |\n",
      "----------------------------------\n",
      "Num timesteps: 622000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -212.05\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 360      |\n",
      "|    ep_rew_mean      | -212     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2140     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2484     |\n",
      "|    total_timesteps  | 622536   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.09     |\n",
      "|    n_updates        | 622532   |\n",
      "----------------------------------\n",
      "Num timesteps: 623000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -209.90\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 361      |\n",
      "|    ep_rew_mean      | -210     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2144     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2487     |\n",
      "|    total_timesteps  | 623141   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.43     |\n",
      "|    n_updates        | 623140   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 360      |\n",
      "|    ep_rew_mean      | -211     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2148     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2488     |\n",
      "|    total_timesteps  | 623579   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.28     |\n",
      "|    n_updates        | 623576   |\n",
      "----------------------------------\n",
      "Num timesteps: 624000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -208.47\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 359      |\n",
      "|    ep_rew_mean      | -210     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2152     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2490     |\n",
      "|    total_timesteps  | 624039   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.2      |\n",
      "|    n_updates        | 624036   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 359      |\n",
      "|    ep_rew_mean      | -211     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2156     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2492     |\n",
      "|    total_timesteps  | 624503   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.19     |\n",
      "|    n_updates        | 624500   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 356      |\n",
      "|    ep_rew_mean      | -217     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2160     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2493     |\n",
      "|    total_timesteps  | 624760   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3        |\n",
      "|    n_updates        | 624756   |\n",
      "----------------------------------\n",
      "Num timesteps: 625000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -215.55\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 355      |\n",
      "|    ep_rew_mean      | -223     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2164     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2494     |\n",
      "|    total_timesteps  | 625227   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.82     |\n",
      "|    n_updates        | 625224   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 352      |\n",
      "|    ep_rew_mean      | -227     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2168     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2495     |\n",
      "|    total_timesteps  | 625460   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 5.86     |\n",
      "|    n_updates        | 625456   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 350      |\n",
      "|    ep_rew_mean      | -240     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2172     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2497     |\n",
      "|    total_timesteps  | 625787   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.6      |\n",
      "|    n_updates        | 625784   |\n",
      "----------------------------------\n",
      "Num timesteps: 626000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -248.78\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 347      |\n",
      "|    ep_rew_mean      | -251     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2176     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2498     |\n",
      "|    total_timesteps  | 626051   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.78     |\n",
      "|    n_updates        | 626048   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 343      |\n",
      "|    ep_rew_mean      | -264     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2180     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2498     |\n",
      "|    total_timesteps  | 626304   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.42     |\n",
      "|    n_updates        | 626300   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 341      |\n",
      "|    ep_rew_mean      | -274     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2184     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2499     |\n",
      "|    total_timesteps  | 626558   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 8.26     |\n",
      "|    n_updates        | 626556   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 337      |\n",
      "|    ep_rew_mean      | -281     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2188     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2500     |\n",
      "|    total_timesteps  | 626803   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.46     |\n",
      "|    n_updates        | 626800   |\n",
      "----------------------------------\n",
      "Num timesteps: 627000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -288.50\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 327      |\n",
      "|    ep_rew_mean      | -293     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2192     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2501     |\n",
      "|    total_timesteps  | 627117   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.15     |\n",
      "|    n_updates        | 627116   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 317      |\n",
      "|    ep_rew_mean      | -305     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2196     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2503     |\n",
      "|    total_timesteps  | 627426   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.33     |\n",
      "|    n_updates        | 627424   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 307      |\n",
      "|    ep_rew_mean      | -311     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2200     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2504     |\n",
      "|    total_timesteps  | 627709   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 6.54     |\n",
      "|    n_updates        | 627708   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 287      |\n",
      "|    ep_rew_mean      | -325     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2204     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2505     |\n",
      "|    total_timesteps  | 627968   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.11     |\n",
      "|    n_updates        | 627964   |\n",
      "----------------------------------\n",
      "Num timesteps: 628000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -324.65\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -340     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2208     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2506     |\n",
      "|    total_timesteps  | 628220   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 6.83     |\n",
      "|    n_updates        | 628216   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 221      |\n",
      "|    ep_rew_mean      | -355     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2212     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2507     |\n",
      "|    total_timesteps  | 628488   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 4.75     |\n",
      "|    n_updates        | 628484   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 192      |\n",
      "|    ep_rew_mean      | -370     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2216     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2508     |\n",
      "|    total_timesteps  | 628778   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 15.6     |\n",
      "|    n_updates        | 628776   |\n",
      "----------------------------------\n",
      "Num timesteps: 629000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -385.57\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 159      |\n",
      "|    ep_rew_mean      | -389     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2220     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2509     |\n",
      "|    total_timesteps  | 629050   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 8.69     |\n",
      "|    n_updates        | 629048   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 129      |\n",
      "|    ep_rew_mean      | -405     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2224     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2510     |\n",
      "|    total_timesteps  | 629306   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 5.17     |\n",
      "|    n_updates        | 629304   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 99.3     |\n",
      "|    ep_rew_mean      | -411     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2228     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2510     |\n",
      "|    total_timesteps  | 629522   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 4.23     |\n",
      "|    n_updates        | 629520   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 85.7     |\n",
      "|    ep_rew_mean      | -418     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2232     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2511     |\n",
      "|    total_timesteps  | 629762   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 4.47     |\n",
      "|    n_updates        | 629760   |\n",
      "----------------------------------\n",
      "Num timesteps: 630000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -429.16\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 83.5     |\n",
      "|    ep_rew_mean      | -432     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2236     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2512     |\n",
      "|    total_timesteps  | 630022   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 4.41     |\n",
      "|    n_updates        | 630020   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 77.8     |\n",
      "|    ep_rew_mean      | -447     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2240     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2513     |\n",
      "|    total_timesteps  | 630312   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 13.3     |\n",
      "|    n_updates        | 630308   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 74.5     |\n",
      "|    ep_rew_mean      | -459     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2244     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2514     |\n",
      "|    total_timesteps  | 630595   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.08     |\n",
      "|    n_updates        | 630592   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 72.8     |\n",
      "|    ep_rew_mean      | -469     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2248     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2515     |\n",
      "|    total_timesteps  | 630862   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 6.92     |\n",
      "|    n_updates        | 630860   |\n",
      "----------------------------------\n",
      "Num timesteps: 631000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -471.84\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 70.9     |\n",
      "|    ep_rew_mean      | -481     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2252     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2516     |\n",
      "|    total_timesteps  | 631130   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 4.99     |\n",
      "|    n_updates        | 631128   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 68.9     |\n",
      "|    ep_rew_mean      | -491     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2256     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2517     |\n",
      "|    total_timesteps  | 631392   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 4.14     |\n",
      "|    n_updates        | 631388   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 69       |\n",
      "|    ep_rew_mean      | -495     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2260     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2518     |\n",
      "|    total_timesteps  | 631662   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 5.35     |\n",
      "|    n_updates        | 631660   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 67.1     |\n",
      "|    ep_rew_mean      | -501     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2264     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2519     |\n",
      "|    total_timesteps  | 631937   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 9.19     |\n",
      "|    n_updates        | 631936   |\n",
      "----------------------------------\n",
      "Num timesteps: 632000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -500.61\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 67.3     |\n",
      "|    ep_rew_mean      | -507     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2268     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2520     |\n",
      "|    total_timesteps  | 632194   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 5.46     |\n",
      "|    n_updates        | 632192   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 66.8     |\n",
      "|    ep_rew_mean      | -506     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2272     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2521     |\n",
      "|    total_timesteps  | 632469   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 5.39     |\n",
      "|    n_updates        | 632468   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 66.5     |\n",
      "|    ep_rew_mean      | -505     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2276     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2522     |\n",
      "|    total_timesteps  | 632700   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.88     |\n",
      "|    n_updates        | 632696   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 66.5     |\n",
      "|    ep_rew_mean      | -502     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2280     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2523     |\n",
      "|    total_timesteps  | 632949   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.76     |\n",
      "|    n_updates        | 632948   |\n",
      "----------------------------------\n",
      "Num timesteps: 633000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -502.28\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 66.5     |\n",
      "|    ep_rew_mean      | -505     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2284     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2524     |\n",
      "|    total_timesteps  | 633211   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 6.39     |\n",
      "|    n_updates        | 633208   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 66.8     |\n",
      "|    ep_rew_mean      | -509     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2288     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2525     |\n",
      "|    total_timesteps  | 633482   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 8.12     |\n",
      "|    n_updates        | 633480   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 66.2     |\n",
      "|    ep_rew_mean      | -505     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2292     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2526     |\n",
      "|    total_timesteps  | 633740   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 6.74     |\n",
      "|    n_updates        | 633736   |\n",
      "----------------------------------\n",
      "Num timesteps: 634000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -505.31\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 65.9     |\n",
      "|    ep_rew_mean      | -505     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2296     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2527     |\n",
      "|    total_timesteps  | 634020   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 12.8     |\n",
      "|    n_updates        | 634016   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 65.8     |\n",
      "|    ep_rew_mean      | -504     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2300     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2528     |\n",
      "|    total_timesteps  | 634290   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 6.25     |\n",
      "|    n_updates        | 634288   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 66.2     |\n",
      "|    ep_rew_mean      | -506     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2304     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2529     |\n",
      "|    total_timesteps  | 634585   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 6.69     |\n",
      "|    n_updates        | 634584   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 66       |\n",
      "|    ep_rew_mean      | -506     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2308     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2530     |\n",
      "|    total_timesteps  | 634817   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 10.7     |\n",
      "|    n_updates        | 634816   |\n",
      "----------------------------------\n",
      "Num timesteps: 635000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -508.48\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 66       |\n",
      "|    ep_rew_mean      | -511     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2312     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2531     |\n",
      "|    total_timesteps  | 635090   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.75     |\n",
      "|    n_updates        | 635088   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 65.6     |\n",
      "|    ep_rew_mean      | -510     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2316     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2532     |\n",
      "|    total_timesteps  | 635342   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.72     |\n",
      "|    n_updates        | 635340   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 65.7     |\n",
      "|    ep_rew_mean      | -510     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2320     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2533     |\n",
      "|    total_timesteps  | 635624   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 9.61     |\n",
      "|    n_updates        | 635620   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 65.5     |\n",
      "|    ep_rew_mean      | -510     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2324     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2534     |\n",
      "|    total_timesteps  | 635858   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 5.49     |\n",
      "|    n_updates        | 635856   |\n",
      "----------------------------------\n",
      "Num timesteps: 636000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -511.38\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 65.7     |\n",
      "|    ep_rew_mean      | -511     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2328     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2535     |\n",
      "|    total_timesteps  | 636088   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 6.09     |\n",
      "|    n_updates        | 636084   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 66.2     |\n",
      "|    ep_rew_mean      | -518     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2332     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2536     |\n",
      "|    total_timesteps  | 636383   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 8.86     |\n",
      "|    n_updates        | 636380   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 66.8     |\n",
      "|    ep_rew_mean      | -524     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2336     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2537     |\n",
      "|    total_timesteps  | 636698   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 4.41     |\n",
      "|    n_updates        | 636696   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 66.5     |\n",
      "|    ep_rew_mean      | -523     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2340     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2538     |\n",
      "|    total_timesteps  | 636962   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 11.5     |\n",
      "|    n_updates        | 636960   |\n",
      "----------------------------------\n",
      "Num timesteps: 637000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -522.84\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 66.4     |\n",
      "|    ep_rew_mean      | -521     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2344     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2539     |\n",
      "|    total_timesteps  | 637233   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 10.4     |\n",
      "|    n_updates        | 637232   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 66.5     |\n",
      "|    ep_rew_mean      | -521     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2348     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2540     |\n",
      "|    total_timesteps  | 637513   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 9.02     |\n",
      "|    n_updates        | 637512   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 66.7     |\n",
      "|    ep_rew_mean      | -523     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2352     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2541     |\n",
      "|    total_timesteps  | 637800   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 6.13     |\n",
      "|    n_updates        | 637796   |\n",
      "----------------------------------\n",
      "Num timesteps: 638000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -521.18\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 67.1     |\n",
      "|    ep_rew_mean      | -521     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2356     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2542     |\n",
      "|    total_timesteps  | 638100   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 6.82     |\n",
      "|    n_updates        | 638096   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 67.2     |\n",
      "|    ep_rew_mean      | -525     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2360     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2543     |\n",
      "|    total_timesteps  | 638387   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 10.6     |\n",
      "|    n_updates        | 638384   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 67.4     |\n",
      "|    ep_rew_mean      | -526     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2364     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2544     |\n",
      "|    total_timesteps  | 638678   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 4.33     |\n",
      "|    n_updates        | 638676   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 67.5     |\n",
      "|    ep_rew_mean      | -529     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2368     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2545     |\n",
      "|    total_timesteps  | 638941   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 4.82     |\n",
      "|    n_updates        | 638940   |\n",
      "----------------------------------\n",
      "Num timesteps: 639000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -529.29\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 67.6     |\n",
      "|    ep_rew_mean      | -531     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2372     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2546     |\n",
      "|    total_timesteps  | 639231   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 4.31     |\n",
      "|    n_updates        | 639228   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 68.2     |\n",
      "|    ep_rew_mean      | -537     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2376     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2548     |\n",
      "|    total_timesteps  | 639523   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 8.31     |\n",
      "|    n_updates        | 639520   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 68.8     |\n",
      "|    ep_rew_mean      | -540     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2380     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2549     |\n",
      "|    total_timesteps  | 639824   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 7.65     |\n",
      "|    n_updates        | 639820   |\n",
      "----------------------------------\n",
      "Num timesteps: 640000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -539.98\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 68.8     |\n",
      "|    ep_rew_mean      | -535     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2384     |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 2550     |\n",
      "|    total_timesteps  | 640087   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.24     |\n",
      "|    n_updates        | 640084   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 68.8     |\n",
      "|    ep_rew_mean      | -532     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2388     |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 2551     |\n",
      "|    total_timesteps  | 640358   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 5.36     |\n",
      "|    n_updates        | 640356   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 68.8     |\n",
      "|    ep_rew_mean      | -532     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2392     |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 2552     |\n",
      "|    total_timesteps  | 640624   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 7.27     |\n",
      "|    n_updates        | 640620   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 69       |\n",
      "|    ep_rew_mean      | -533     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2396     |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 2553     |\n",
      "|    total_timesteps  | 640919   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 8.01     |\n",
      "|    n_updates        | 640916   |\n",
      "----------------------------------\n",
      "Num timesteps: 641000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -532.15\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 69.1     |\n",
      "|    ep_rew_mean      | -531     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2400     |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 2554     |\n",
      "|    total_timesteps  | 641199   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 13       |\n",
      "|    n_updates        | 641196   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 68.9     |\n",
      "|    ep_rew_mean      | -530     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2404     |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 2555     |\n",
      "|    total_timesteps  | 641472   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 6.83     |\n",
      "|    n_updates        | 641468   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 69.3     |\n",
      "|    ep_rew_mean      | -532     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2408     |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 2556     |\n",
      "|    total_timesteps  | 641750   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 9.68     |\n",
      "|    n_updates        | 641748   |\n",
      "----------------------------------\n",
      "Num timesteps: 642000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -527.96\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 69.2     |\n",
      "|    ep_rew_mean      | -526     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2412     |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 2557     |\n",
      "|    total_timesteps  | 642006   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 13.9     |\n",
      "|    n_updates        | 642004   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 70       |\n",
      "|    ep_rew_mean      | -529     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2416     |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 2558     |\n",
      "|    total_timesteps  | 642339   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 4.99     |\n",
      "|    n_updates        | 642336   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 69.9     |\n",
      "|    ep_rew_mean      | -525     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2420     |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 2559     |\n",
      "|    total_timesteps  | 642614   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 6.6      |\n",
      "|    n_updates        | 642612   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 70.7     |\n",
      "|    ep_rew_mean      | -529     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2424     |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 2560     |\n",
      "|    total_timesteps  | 642923   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 6.9      |\n",
      "|    n_updates        | 642920   |\n",
      "----------------------------------\n",
      "Num timesteps: 643000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -527.68\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 71.2     |\n",
      "|    ep_rew_mean      | -526     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2428     |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 2561     |\n",
      "|    total_timesteps  | 643205   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 9.04     |\n",
      "|    n_updates        | 643204   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 71       |\n",
      "|    ep_rew_mean      | -519     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2432     |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 2562     |\n",
      "|    total_timesteps  | 643481   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 5.86     |\n",
      "|    n_updates        | 643480   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 71       |\n",
      "|    ep_rew_mean      | -514     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2436     |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 2564     |\n",
      "|    total_timesteps  | 643794   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 4.94     |\n",
      "|    n_updates        | 643792   |\n",
      "----------------------------------\n",
      "Num timesteps: 644000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -510.17\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 71.5     |\n",
      "|    ep_rew_mean      | -509     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2440     |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 2565     |\n",
      "|    total_timesteps  | 644113   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 7.01     |\n",
      "|    n_updates        | 644112   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 71.8     |\n",
      "|    ep_rew_mean      | -499     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2444     |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 2566     |\n",
      "|    total_timesteps  | 644418   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 4.95     |\n",
      "|    n_updates        | 644416   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 71.9     |\n",
      "|    ep_rew_mean      | -489     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2448     |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 2567     |\n",
      "|    total_timesteps  | 644699   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 10.2     |\n",
      "|    n_updates        | 644696   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 71.6     |\n",
      "|    ep_rew_mean      | -479     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2452     |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 2568     |\n",
      "|    total_timesteps  | 644957   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 4.91     |\n",
      "|    n_updates        | 644956   |\n",
      "----------------------------------\n",
      "Num timesteps: 645000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -478.96\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 71.7     |\n",
      "|    ep_rew_mean      | -470     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2456     |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 2569     |\n",
      "|    total_timesteps  | 645274   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 5.94     |\n",
      "|    n_updates        | 645272   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 72       |\n",
      "|    ep_rew_mean      | -457     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2460     |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 2570     |\n",
      "|    total_timesteps  | 645592   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.35     |\n",
      "|    n_updates        | 645588   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 72.9     |\n",
      "|    ep_rew_mean      | -447     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2464     |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 2572     |\n",
      "|    total_timesteps  | 645965   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.34     |\n",
      "|    n_updates        | 645964   |\n",
      "----------------------------------\n",
      "Num timesteps: 646000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -446.55\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 74.5     |\n",
      "|    ep_rew_mean      | -432     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2468     |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 2573     |\n",
      "|    total_timesteps  | 646394   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 5.29     |\n",
      "|    n_updates        | 646392   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 76.9     |\n",
      "|    ep_rew_mean      | -416     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2472     |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 2575     |\n",
      "|    total_timesteps  | 646923   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.51     |\n",
      "|    n_updates        | 646920   |\n",
      "----------------------------------\n",
      "Num timesteps: 647000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -415.58\n",
      "Num timesteps: 648000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -405.69\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 85.1     |\n",
      "|    ep_rew_mean      | -400     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2476     |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 2580     |\n",
      "|    total_timesteps  | 648037   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 5.4      |\n",
      "|    n_updates        | 648036   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 88.2     |\n",
      "|    ep_rew_mean      | -380     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2480     |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 2582     |\n",
      "|    total_timesteps  | 648645   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3        |\n",
      "|    n_updates        | 648644   |\n",
      "----------------------------------\n",
      "Num timesteps: 649000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -380.23\n",
      "Num timesteps: 650000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -372.21\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 104      |\n",
      "|    ep_rew_mean      | -374     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2484     |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 2590     |\n",
      "|    total_timesteps  | 650440   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.61     |\n",
      "|    n_updates        | 650436   |\n",
      "----------------------------------\n",
      "Num timesteps: 651000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -369.97\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 116      |\n",
      "|    ep_rew_mean      | -361     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2488     |\n",
      "|    fps              | 251      |\n",
      "|    time_elapsed     | 2596     |\n",
      "|    total_timesteps  | 651980   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.93     |\n",
      "|    n_updates        | 651976   |\n",
      "----------------------------------\n",
      "Num timesteps: 652000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -360.61\n",
      "Num timesteps: 653000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -357.90\n",
      "Num timesteps: 654000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -356.19\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 138      |\n",
      "|    ep_rew_mean      | -347     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2492     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2608     |\n",
      "|    total_timesteps  | 654400   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.99     |\n",
      "|    n_updates        | 654396   |\n",
      "----------------------------------\n",
      "Num timesteps: 655000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -340.88\n",
      "Num timesteps: 656000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -338.75\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 156      |\n",
      "|    ep_rew_mean      | -337     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2496     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2617     |\n",
      "|    total_timesteps  | 656563   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 4.42     |\n",
      "|    n_updates        | 656560   |\n",
      "----------------------------------\n",
      "Num timesteps: 657000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -337.43\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 166      |\n",
      "|    ep_rew_mean      | -322     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2500     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2623     |\n",
      "|    total_timesteps  | 657811   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 7.26     |\n",
      "|    n_updates        | 657808   |\n",
      "----------------------------------\n",
      "Num timesteps: 658000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -313.55\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 167      |\n",
      "|    ep_rew_mean      | -303     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2504     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2624     |\n",
      "|    total_timesteps  | 658167   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 6.49     |\n",
      "|    n_updates        | 658164   |\n",
      "----------------------------------\n",
      "Num timesteps: 659000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -299.47\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 181      |\n",
      "|    ep_rew_mean      | -291     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2508     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2632     |\n",
      "|    total_timesteps  | 659850   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 10.7     |\n",
      "|    n_updates        | 659848   |\n",
      "----------------------------------\n",
      "Num timesteps: 660000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -290.78\n",
      "Num timesteps: 661000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -287.33\n",
      "Num timesteps: 662000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -282.06\n",
      "Num timesteps: 663000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -279.31\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 218      |\n",
      "|    ep_rew_mean      | -276     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2512     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2652     |\n",
      "|    total_timesteps  | 663850   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 6.72     |\n",
      "|    n_updates        | 663848   |\n",
      "----------------------------------\n",
      "Num timesteps: 664000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -275.93\n",
      "Num timesteps: 665000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -274.49\n",
      "Num timesteps: 666000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -271.37\n",
      "Num timesteps: 667000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -268.16\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 248      |\n",
      "|    ep_rew_mean      | -266     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2516     |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 2668     |\n",
      "|    total_timesteps  | 667162   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 15.1     |\n",
      "|    n_updates        | 667160   |\n",
      "----------------------------------\n",
      "Num timesteps: 668000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -265.77\n",
      "Num timesteps: 669000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -261.55\n",
      "Num timesteps: 670000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -258.87\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 280      |\n",
      "|    ep_rew_mean      | -251     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2520     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2685     |\n",
      "|    total_timesteps  | 670584   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 10.1     |\n",
      "|    n_updates        | 670580   |\n",
      "----------------------------------\n",
      "Num timesteps: 671000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -250.92\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 287      |\n",
      "|    ep_rew_mean      | -235     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2524     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2689     |\n",
      "|    total_timesteps  | 671596   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 6.42     |\n",
      "|    n_updates        | 671592   |\n",
      "----------------------------------\n",
      "Num timesteps: 672000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -226.81\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 291      |\n",
      "|    ep_rew_mean      | -223     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2528     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2691     |\n",
      "|    total_timesteps  | 672292   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 12.5     |\n",
      "|    n_updates        | 672288   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 293      |\n",
      "|    ep_rew_mean      | -212     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2532     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2693     |\n",
      "|    total_timesteps  | 672774   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 7.27     |\n",
      "|    n_updates        | 672772   |\n",
      "----------------------------------\n",
      "Num timesteps: 673000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -206.62\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 295      |\n",
      "|    ep_rew_mean      | -195     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2536     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2695     |\n",
      "|    total_timesteps  | 673258   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 7.97     |\n",
      "|    n_updates        | 673256   |\n",
      "----------------------------------\n",
      "Num timesteps: 674000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -188.49\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 302      |\n",
      "|    ep_rew_mean      | -185     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2540     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2699     |\n",
      "|    total_timesteps  | 674324   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 6.48     |\n",
      "|    n_updates        | 674320   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 304      |\n",
      "|    ep_rew_mean      | -176     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2544     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2701     |\n",
      "|    total_timesteps  | 674781   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 7.87     |\n",
      "|    n_updates        | 674780   |\n",
      "----------------------------------\n",
      "Num timesteps: 675000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -175.10\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 306      |\n",
      "|    ep_rew_mean      | -177     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2548     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2703     |\n",
      "|    total_timesteps  | 675338   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 4.42     |\n",
      "|    n_updates        | 675336   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 307      |\n",
      "|    ep_rew_mean      | -169     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2552     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2705     |\n",
      "|    total_timesteps  | 675682   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 5.88     |\n",
      "|    n_updates        | 675680   |\n",
      "----------------------------------\n",
      "Num timesteps: 676000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -173.90\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 310      |\n",
      "|    ep_rew_mean      | -177     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2556     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2707     |\n",
      "|    total_timesteps  | 676237   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 5.35     |\n",
      "|    n_updates        | 676236   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 310      |\n",
      "|    ep_rew_mean      | -175     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2560     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2708     |\n",
      "|    total_timesteps  | 676593   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 4.33     |\n",
      "|    n_updates        | 676592   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 310      |\n",
      "|    ep_rew_mean      | -172     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2564     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2709     |\n",
      "|    total_timesteps  | 676919   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 6.72     |\n",
      "|    n_updates        | 676916   |\n",
      "----------------------------------\n",
      "Num timesteps: 677000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -171.85\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 310      |\n",
      "|    ep_rew_mean      | -174     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2568     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2711     |\n",
      "|    total_timesteps  | 677344   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 4.45     |\n",
      "|    n_updates        | 677340   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 309      |\n",
      "|    ep_rew_mean      | -177     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2572     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2712     |\n",
      "|    total_timesteps  | 677812   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 5.82     |\n",
      "|    n_updates        | 677808   |\n",
      "----------------------------------\n",
      "Num timesteps: 678000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -173.16\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 301      |\n",
      "|    ep_rew_mean      | -176     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2576     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2714     |\n",
      "|    total_timesteps  | 678162   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 5.57     |\n",
      "|    n_updates        | 678160   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 299      |\n",
      "|    ep_rew_mean      | -184     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2580     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2715     |\n",
      "|    total_timesteps  | 678507   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 8.5      |\n",
      "|    n_updates        | 678504   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 284      |\n",
      "|    ep_rew_mean      | -183     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2584     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2716     |\n",
      "|    total_timesteps  | 678801   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 5.15     |\n",
      "|    n_updates        | 678800   |\n",
      "----------------------------------\n",
      "Num timesteps: 679000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -184.84\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 272      |\n",
      "|    ep_rew_mean      | -185     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2588     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2718     |\n",
      "|    total_timesteps  | 679208   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 4.2      |\n",
      "|    n_updates        | 679204   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 252      |\n",
      "|    ep_rew_mean      | -186     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2592     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2719     |\n",
      "|    total_timesteps  | 679567   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 4.3      |\n",
      "|    n_updates        | 679564   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 234      |\n",
      "|    ep_rew_mean      | -183     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2596     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2721     |\n",
      "|    total_timesteps  | 679998   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.09     |\n",
      "|    n_updates        | 679996   |\n",
      "----------------------------------\n",
      "Num timesteps: 680000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -182.69\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 225      |\n",
      "|    ep_rew_mean      | -190     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2600     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2722     |\n",
      "|    total_timesteps  | 680345   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.78     |\n",
      "|    n_updates        | 680344   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 225      |\n",
      "|    ep_rew_mean      | -194     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2604     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2723     |\n",
      "|    total_timesteps  | 680713   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 5.59     |\n",
      "|    n_updates        | 680712   |\n",
      "----------------------------------\n",
      "Num timesteps: 681000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -193.19\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 213      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2608     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2725     |\n",
      "|    total_timesteps  | 681149   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 4.32     |\n",
      "|    n_updates        | 681148   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 176      |\n",
      "|    ep_rew_mean      | -209     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2612     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2726     |\n",
      "|    total_timesteps  | 681499   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 6.18     |\n",
      "|    n_updates        | 681496   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 148      |\n",
      "|    ep_rew_mean      | -209     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2616     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2728     |\n",
      "|    total_timesteps  | 681954   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 6.88     |\n",
      "|    n_updates        | 681952   |\n",
      "----------------------------------\n",
      "Num timesteps: 682000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -208.56\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 117      |\n",
      "|    ep_rew_mean      | -215     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2620     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2730     |\n",
      "|    total_timesteps  | 682331   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 5.81     |\n",
      "|    n_updates        | 682328   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 113      |\n",
      "|    ep_rew_mean      | -222     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2624     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2732     |\n",
      "|    total_timesteps  | 682938   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.48     |\n",
      "|    n_updates        | 682936   |\n",
      "----------------------------------\n",
      "Num timesteps: 683000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -222.02\n",
      "Num timesteps: 684000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -224.24\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 117      |\n",
      "|    ep_rew_mean      | -226     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2628     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2736     |\n",
      "|    total_timesteps  | 684013   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 4.1      |\n",
      "|    n_updates        | 684012   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 120      |\n",
      "|    ep_rew_mean      | -229     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2632     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2739     |\n",
      "|    total_timesteps  | 684806   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 4.2      |\n",
      "|    n_updates        | 684804   |\n",
      "----------------------------------\n",
      "Num timesteps: 685000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -228.81\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 125      |\n",
      "|    ep_rew_mean      | -234     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2636     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2743     |\n",
      "|    total_timesteps  | 685794   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 4.45     |\n",
      "|    n_updates        | 685792   |\n",
      "----------------------------------\n",
      "Num timesteps: 686000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -233.91\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 127      |\n",
      "|    ep_rew_mean      | -233     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2640     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2748     |\n",
      "|    total_timesteps  | 686992   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.93     |\n",
      "|    n_updates        | 686988   |\n",
      "----------------------------------\n",
      "Num timesteps: 687000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -232.86\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 132      |\n",
      "|    ep_rew_mean      | -237     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2644     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2752     |\n",
      "|    total_timesteps  | 687991   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 4.74     |\n",
      "|    n_updates        | 687988   |\n",
      "----------------------------------\n",
      "Num timesteps: 688000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -236.84\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 132      |\n",
      "|    ep_rew_mean      | -228     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2648     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2754     |\n",
      "|    total_timesteps  | 688534   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 4.27     |\n",
      "|    n_updates        | 688532   |\n",
      "----------------------------------\n",
      "Num timesteps: 689000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -225.74\n",
      "Num timesteps: 690000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -223.81\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 144      |\n",
      "|    ep_rew_mean      | -223     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2652     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2761     |\n",
      "|    total_timesteps  | 690054   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 4.81     |\n",
      "|    n_updates        | 690052   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 144      |\n",
      "|    ep_rew_mean      | -209     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2656     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2763     |\n",
      "|    total_timesteps  | 690603   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.61     |\n",
      "|    n_updates        | 690600   |\n",
      "----------------------------------\n",
      "Num timesteps: 691000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -207.51\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 148      |\n",
      "|    ep_rew_mean      | -204     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2660     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2766     |\n",
      "|    total_timesteps  | 691360   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 5.51     |\n",
      "|    n_updates        | 691356   |\n",
      "----------------------------------\n",
      "Num timesteps: 692000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -202.26\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 158      |\n",
      "|    ep_rew_mean      | -199     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2664     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2772     |\n",
      "|    total_timesteps  | 692693   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.43     |\n",
      "|    n_updates        | 692692   |\n",
      "----------------------------------\n",
      "Num timesteps: 693000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -195.65\n",
      "Num timesteps: 694000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -193.52\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 168      |\n",
      "|    ep_rew_mean      | -192     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2668     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2778     |\n",
      "|    total_timesteps  | 694112   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 4.71     |\n",
      "|    n_updates        | 694108   |\n",
      "----------------------------------\n",
      "Num timesteps: 695000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -184.33\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 173      |\n",
      "|    ep_rew_mean      | -183     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2672     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2782     |\n",
      "|    total_timesteps  | 695130   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 4.21     |\n",
      "|    n_updates        | 695128   |\n",
      "----------------------------------\n",
      "Num timesteps: 696000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -177.98\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | -178     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2676     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2786     |\n",
      "|    total_timesteps  | 696166   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.03     |\n",
      "|    n_updates        | 696164   |\n",
      "----------------------------------\n",
      "Num timesteps: 697000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -174.43\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 187      |\n",
      "|    ep_rew_mean      | -172     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2680     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2790     |\n",
      "|    total_timesteps  | 697216   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.91     |\n",
      "|    n_updates        | 697212   |\n",
      "----------------------------------\n",
      "Num timesteps: 698000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -170.43\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 197      |\n",
      "|    ep_rew_mean      | -166     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2684     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2795     |\n",
      "|    total_timesteps  | 698454   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.91     |\n",
      "|    n_updates        | 698452   |\n",
      "----------------------------------\n",
      "Num timesteps: 699000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -165.30\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 202      |\n",
      "|    ep_rew_mean      | -165     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2688     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2798     |\n",
      "|    total_timesteps  | 699368   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.29     |\n",
      "|    n_updates        | 699364   |\n",
      "----------------------------------\n",
      "Num timesteps: 700000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -160.44\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 205      |\n",
      "|    ep_rew_mean      | -160     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2692     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2801     |\n",
      "|    total_timesteps  | 700113   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.81     |\n",
      "|    n_updates        | 700112   |\n",
      "----------------------------------\n",
      "Num timesteps: 701000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -158.70\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 219      |\n",
      "|    ep_rew_mean      | -154     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2696     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2809     |\n",
      "|    total_timesteps  | 701898   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 4.01     |\n",
      "|    n_updates        | 701896   |\n",
      "----------------------------------\n",
      "Num timesteps: 702000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -154.24\n",
      "Num timesteps: 703000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -149.03\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 231      |\n",
      "|    ep_rew_mean      | -143     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2700     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2816     |\n",
      "|    total_timesteps  | 703473   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.25     |\n",
      "|    n_updates        | 703472   |\n",
      "----------------------------------\n",
      "Num timesteps: 704000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -143.10\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 241      |\n",
      "|    ep_rew_mean      | -146     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2704     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2821     |\n",
      "|    total_timesteps  | 704802   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.69     |\n",
      "|    n_updates        | 704800   |\n",
      "----------------------------------\n",
      "Num timesteps: 705000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -145.10\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 247      |\n",
      "|    ep_rew_mean      | -140     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2708     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2825     |\n",
      "|    total_timesteps  | 705852   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.8      |\n",
      "|    n_updates        | 705848   |\n",
      "----------------------------------\n",
      "Num timesteps: 706000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -140.16\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 252      |\n",
      "|    ep_rew_mean      | -140     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2712     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2828     |\n",
      "|    total_timesteps  | 706683   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.77     |\n",
      "|    n_updates        | 706680   |\n",
      "----------------------------------\n",
      "Num timesteps: 707000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -138.80\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 253      |\n",
      "|    ep_rew_mean      | -139     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2716     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2830     |\n",
      "|    total_timesteps  | 707210   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.08     |\n",
      "|    n_updates        | 707208   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 255      |\n",
      "|    ep_rew_mean      | -140     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2720     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2832     |\n",
      "|    total_timesteps  | 707828   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.26     |\n",
      "|    n_updates        | 707824   |\n",
      "----------------------------------\n",
      "Num timesteps: 708000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -137.98\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 258      |\n",
      "|    ep_rew_mean      | -139     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2724     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2836     |\n",
      "|    total_timesteps  | 708750   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.85     |\n",
      "|    n_updates        | 708748   |\n",
      "----------------------------------\n",
      "Num timesteps: 709000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -137.79\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 255      |\n",
      "|    ep_rew_mean      | -141     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2728     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2839     |\n",
      "|    total_timesteps  | 709512   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.93     |\n",
      "|    n_updates        | 709508   |\n",
      "----------------------------------\n",
      "Num timesteps: 710000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -143.97\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 254      |\n",
      "|    ep_rew_mean      | -144     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2732     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2842     |\n",
      "|    total_timesteps  | 710250   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.25     |\n",
      "|    n_updates        | 710248   |\n",
      "----------------------------------\n",
      "Num timesteps: 711000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -145.92\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 253      |\n",
      "|    ep_rew_mean      | -146     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2736     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2845     |\n",
      "|    total_timesteps  | 711082   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 4.01     |\n",
      "|    n_updates        | 711080   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 248      |\n",
      "|    ep_rew_mean      | -146     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2740     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2848     |\n",
      "|    total_timesteps  | 711815   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 4.1      |\n",
      "|    n_updates        | 711812   |\n",
      "----------------------------------\n",
      "Num timesteps: 712000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -145.79\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 246      |\n",
      "|    ep_rew_mean      | -147     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2744     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2851     |\n",
      "|    total_timesteps  | 712641   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 4.97     |\n",
      "|    n_updates        | 712640   |\n",
      "----------------------------------\n",
      "Num timesteps: 713000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -147.12\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 249      |\n",
      "|    ep_rew_mean      | -153     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2748     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2854     |\n",
      "|    total_timesteps  | 713430   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.3      |\n",
      "|    n_updates        | 713428   |\n",
      "----------------------------------\n",
      "Num timesteps: 714000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -169.66\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 240      |\n",
      "|    ep_rew_mean      | -173     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2752     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2856     |\n",
      "|    total_timesteps  | 714051   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.32     |\n",
      "|    n_updates        | 714048   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 240      |\n",
      "|    ep_rew_mean      | -178     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2756     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2858     |\n",
      "|    total_timesteps  | 714564   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 5.44     |\n",
      "|    n_updates        | 714560   |\n",
      "----------------------------------\n",
      "Num timesteps: 715000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -178.43\n",
      "Num timesteps: 716000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -178.24\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 247      |\n",
      "|    ep_rew_mean      | -178     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2760     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2864     |\n",
      "|    total_timesteps  | 716080   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.59     |\n",
      "|    n_updates        | 716076   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 243      |\n",
      "|    ep_rew_mean      | -182     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2764     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2868     |\n",
      "|    total_timesteps  | 716968   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 4.2      |\n",
      "|    n_updates        | 716964   |\n",
      "----------------------------------\n",
      "Num timesteps: 717000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -182.45\n",
      "Num timesteps: 718000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -180.88\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 248      |\n",
      "|    ep_rew_mean      | -179     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2768     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2877     |\n",
      "|    total_timesteps  | 718948   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.39     |\n",
      "|    n_updates        | 718944   |\n",
      "----------------------------------\n",
      "Num timesteps: 719000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -179.34\n",
      "Num timesteps: 720000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -178.32\n",
      "Num timesteps: 721000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -177.01\n",
      "Num timesteps: 722000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -177.49\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 270      |\n",
      "|    ep_rew_mean      | -178     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2772     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2892     |\n",
      "|    total_timesteps  | 722099   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.83     |\n",
      "|    n_updates        | 722096   |\n",
      "----------------------------------\n",
      "Num timesteps: 723000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -177.71\n",
      "Num timesteps: 724000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -173.16\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 285      |\n",
      "|    ep_rew_mean      | -169     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2776     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2903     |\n",
      "|    total_timesteps  | 724668   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.97     |\n",
      "|    n_updates        | 724664   |\n",
      "----------------------------------\n",
      "Num timesteps: 725000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -169.43\n",
      "Num timesteps: 726000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -165.12\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 293      |\n",
      "|    ep_rew_mean      | -161     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2780     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2911     |\n",
      "|    total_timesteps  | 726537   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 6.38     |\n",
      "|    n_updates        | 726536   |\n",
      "----------------------------------\n",
      "Num timesteps: 727000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -164.79\n",
      "Num timesteps: 728000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -168.33\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 303      |\n",
      "|    ep_rew_mean      | -166     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2784     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2921     |\n",
      "|    total_timesteps  | 728791   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.39     |\n",
      "|    n_updates        | 728788   |\n",
      "----------------------------------\n",
      "Num timesteps: 729000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -166.16\n",
      "Num timesteps: 730000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -163.31\n",
      "Num timesteps: 731000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -160.42\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 320      |\n",
      "|    ep_rew_mean      | -159     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2788     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2932     |\n",
      "|    total_timesteps  | 731359   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 3.59     |\n",
      "|    n_updates        | 731356   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 316      |\n",
      "|    ep_rew_mean      | -162     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2792     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2934     |\n",
      "|    total_timesteps  | 731732   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.54     |\n",
      "|    n_updates        | 731728   |\n",
      "----------------------------------\n",
      "Num timesteps: 732000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -164.68\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 303      |\n",
      "|    ep_rew_mean      | -163     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2796     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2936     |\n",
      "|    total_timesteps  | 732230   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.06     |\n",
      "|    n_updates        | 732228   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 294      |\n",
      "|    ep_rew_mean      | -168     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2800     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2938     |\n",
      "|    total_timesteps  | 732867   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.05     |\n",
      "|    n_updates        | 732864   |\n",
      "----------------------------------\n",
      "Num timesteps: 733000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -167.92\n",
      "Num timesteps: 734000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -166.22\n",
      "Num timesteps: 735000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -163.36\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 304      |\n",
      "|    ep_rew_mean      | -158     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2804     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2948     |\n",
      "|    total_timesteps  | 735152   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.41     |\n",
      "|    n_updates        | 735148   |\n",
      "----------------------------------\n",
      "Num timesteps: 736000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -152.56\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 306      |\n",
      "|    ep_rew_mean      | -149     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2808     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2953     |\n",
      "|    total_timesteps  | 736417   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.44     |\n",
      "|    n_updates        | 736416   |\n",
      "----------------------------------\n",
      "Num timesteps: 737000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -145.06\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 313      |\n",
      "|    ep_rew_mean      | -142     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2812     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2960     |\n",
      "|    total_timesteps  | 737997   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2        |\n",
      "|    n_updates        | 737996   |\n",
      "----------------------------------\n",
      "Num timesteps: 738000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -141.84\n",
      "Num timesteps: 739000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -136.90\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 323      |\n",
      "|    ep_rew_mean      | -131     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2816     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2966     |\n",
      "|    total_timesteps  | 739545   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.91     |\n",
      "|    n_updates        | 739544   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 321      |\n",
      "|    ep_rew_mean      | -123     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2820     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2967     |\n",
      "|    total_timesteps  | 739894   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.51     |\n",
      "|    n_updates        | 739892   |\n",
      "----------------------------------\n",
      "Num timesteps: 740000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -121.06\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 316      |\n",
      "|    ep_rew_mean      | -115     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2824     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2969     |\n",
      "|    total_timesteps  | 740324   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.38     |\n",
      "|    n_updates        | 740320   |\n",
      "----------------------------------\n",
      "Num timesteps: 741000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -115.13\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 315      |\n",
      "|    ep_rew_mean      | -112     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2828     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2972     |\n",
      "|    total_timesteps  | 741039   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.46     |\n",
      "|    n_updates        | 741036   |\n",
      "----------------------------------\n",
      "Num timesteps: 742000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -111.24\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 318      |\n",
      "|    ep_rew_mean      | -108     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2832     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2976     |\n",
      "|    total_timesteps  | 742064   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.42     |\n",
      "|    n_updates        | 742060   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 315      |\n",
      "|    ep_rew_mean      | -98.7    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2836     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2977     |\n",
      "|    total_timesteps  | 742547   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.75     |\n",
      "|    n_updates        | 742544   |\n",
      "----------------------------------\n",
      "Num timesteps: 743000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -97.10\n",
      "Num timesteps: 744000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -93.01\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 325      |\n",
      "|    ep_rew_mean      | -89.4    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2840     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2984     |\n",
      "|    total_timesteps  | 744292   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.96     |\n",
      "|    n_updates        | 744288   |\n",
      "----------------------------------\n",
      "Num timesteps: 745000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -89.04\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 332      |\n",
      "|    ep_rew_mean      | -79.3    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2844     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2991     |\n",
      "|    total_timesteps  | 745817   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.61     |\n",
      "|    n_updates        | 745816   |\n",
      "----------------------------------\n",
      "Num timesteps: 746000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -79.25\n",
      "Num timesteps: 747000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -76.56\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 340      |\n",
      "|    ep_rew_mean      | -69      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2848     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 2997     |\n",
      "|    total_timesteps  | 747393   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.69     |\n",
      "|    n_updates        | 747392   |\n",
      "----------------------------------\n",
      "Num timesteps: 748000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -68.96\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 346      |\n",
      "|    ep_rew_mean      | -51.6    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2852     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 3003     |\n",
      "|    total_timesteps  | 748666   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.55     |\n",
      "|    n_updates        | 748664   |\n",
      "----------------------------------\n",
      "Num timesteps: 749000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -51.61\n",
      "Num timesteps: 750000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -44.72\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 362      |\n",
      "|    ep_rew_mean      | -38.5    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2856     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 3012     |\n",
      "|    total_timesteps  | 750780   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.92     |\n",
      "|    n_updates        | 750776   |\n",
      "----------------------------------\n",
      "Num timesteps: 751000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -37.00\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 354      |\n",
      "|    ep_rew_mean      | -34.9    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2860     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 3015     |\n",
      "|    total_timesteps  | 751511   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.54     |\n",
      "|    n_updates        | 751508   |\n",
      "----------------------------------\n",
      "Num timesteps: 752000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -36.45\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 359      |\n",
      "|    ep_rew_mean      | -21.1    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2864     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 3020     |\n",
      "|    total_timesteps  | 752832   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.61     |\n",
      "|    n_updates        | 752828   |\n",
      "----------------------------------\n",
      "Num timesteps: 753000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -21.07\n",
      "Num timesteps: 754000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -18.61\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 357      |\n",
      "|    ep_rew_mean      | -17.6    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2868     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 3028     |\n",
      "|    total_timesteps  | 754642   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.73     |\n",
      "|    n_updates        | 754640   |\n",
      "----------------------------------\n",
      "Num timesteps: 755000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -17.62\n",
      "Num timesteps: 756000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -15.95\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 349      |\n",
      "|    ep_rew_mean      | -12.8    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2872     |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 3038     |\n",
      "|    total_timesteps  | 756953   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.84     |\n",
      "|    n_updates        | 756952   |\n",
      "----------------------------------\n",
      "Num timesteps: 757000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -12.82\n",
      "Num timesteps: 758000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -13.08\n",
      "Num timesteps: 759000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -14.51\n",
      "Num timesteps: 760000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -14.73\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 359      |\n",
      "|    ep_rew_mean      | -13.9    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2876     |\n",
      "|    fps              | 248      |\n",
      "|    time_elapsed     | 3056     |\n",
      "|    total_timesteps  | 760528   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.63     |\n",
      "|    n_updates        | 760524   |\n",
      "----------------------------------\n",
      "Num timesteps: 761000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -13.89\n",
      "Num timesteps: 762000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -23.07\n",
      "Num timesteps: 763000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -22.14\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 369      |\n",
      "|    ep_rew_mean      | -23.1    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2880     |\n",
      "|    fps              | 248      |\n",
      "|    time_elapsed     | 3068     |\n",
      "|    total_timesteps  | 763388   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.65     |\n",
      "|    n_updates        | 763384   |\n",
      "----------------------------------\n",
      "Num timesteps: 764000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -23.07\n",
      "Num timesteps: 765000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -17.67\n",
      "Num timesteps: 766000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -15.42\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 377      |\n",
      "|    ep_rew_mean      | -15.8    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2884     |\n",
      "|    fps              | 248      |\n",
      "|    time_elapsed     | 3083     |\n",
      "|    total_timesteps  | 766504   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.988    |\n",
      "|    n_updates        | 766500   |\n",
      "----------------------------------\n",
      "Num timesteps: 767000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -20.49\n",
      "Num timesteps: 768000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -18.76\n",
      "Num timesteps: 769000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -21.88\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 380      |\n",
      "|    ep_rew_mean      | -23.2    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2888     |\n",
      "|    fps              | 248      |\n",
      "|    time_elapsed     | 3096     |\n",
      "|    total_timesteps  | 769384   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.13     |\n",
      "|    n_updates        | 769380   |\n",
      "----------------------------------\n",
      "Num timesteps: 770000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -23.16\n",
      "Num timesteps: 771000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -24.97\n",
      "Num timesteps: 772000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -23.84\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 405      |\n",
      "|    ep_rew_mean      | -24.7    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2892     |\n",
      "|    fps              | 248      |\n",
      "|    time_elapsed     | 3110     |\n",
      "|    total_timesteps  | 772254   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.09     |\n",
      "|    n_updates        | 772252   |\n",
      "----------------------------------\n",
      "Num timesteps: 773000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -24.71\n",
      "Num timesteps: 774000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -21.72\n",
      "Num timesteps: 775000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -22.01\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 433      |\n",
      "|    ep_rew_mean      | -23      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2896     |\n",
      "|    fps              | 248      |\n",
      "|    time_elapsed     | 3125     |\n",
      "|    total_timesteps  | 775495   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.34     |\n",
      "|    n_updates        | 775492   |\n",
      "----------------------------------\n",
      "Num timesteps: 776000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -17.95\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 440      |\n",
      "|    ep_rew_mean      | -5.37    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2900     |\n",
      "|    fps              | 248      |\n",
      "|    time_elapsed     | 3131     |\n",
      "|    total_timesteps  | 776870   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.12     |\n",
      "|    n_updates        | 776868   |\n",
      "----------------------------------\n",
      "Num timesteps: 777000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -5.37\n",
      "Num timesteps: 778000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -6.25\n",
      "Num timesteps: 779000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -7.13\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 444      |\n",
      "|    ep_rew_mean      | -4.6     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2904     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3143     |\n",
      "|    total_timesteps  | 779591   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.16     |\n",
      "|    n_updates        | 779588   |\n",
      "----------------------------------\n",
      "Num timesteps: 780000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -4.60\n",
      "Num timesteps: 781000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -3.87\n",
      "Num timesteps: 782000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: -0.99\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 457      |\n",
      "|    ep_rew_mean      | 1.5      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2908     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3154     |\n",
      "|    total_timesteps  | 782078   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.849    |\n",
      "|    n_updates        | 782076   |\n",
      "----------------------------------\n",
      "Num timesteps: 783000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 4.40\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 457      |\n",
      "|    ep_rew_mean      | 6.18     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2912     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3161     |\n",
      "|    total_timesteps  | 783709   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.06     |\n",
      "|    n_updates        | 783708   |\n",
      "----------------------------------\n",
      "Num timesteps: 784000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 6.51\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 454      |\n",
      "|    ep_rew_mean      | 8.6      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2916     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3166     |\n",
      "|    total_timesteps  | 784904   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.02     |\n",
      "|    n_updates        | 784900   |\n",
      "----------------------------------\n",
      "Num timesteps: 785000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 8.60\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 461      |\n",
      "|    ep_rew_mean      | 12       |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2920     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3170     |\n",
      "|    total_timesteps  | 785952   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.856    |\n",
      "|    n_updates        | 785948   |\n",
      "----------------------------------\n",
      "Num timesteps: 786000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 12.03\n",
      "Num timesteps: 787000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 16.09\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 471      |\n",
      "|    ep_rew_mean      | 23.8     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2924     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3176     |\n",
      "|    total_timesteps  | 787439   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.803    |\n",
      "|    n_updates        | 787436   |\n",
      "----------------------------------\n",
      "Num timesteps: 788000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 25.37\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 477      |\n",
      "|    ep_rew_mean      | 35.5     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2928     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3181     |\n",
      "|    total_timesteps  | 788704   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.743    |\n",
      "|    n_updates        | 788700   |\n",
      "----------------------------------\n",
      "Num timesteps: 789000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 35.46\n",
      "Num timesteps: 790000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 46.09\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 483      |\n",
      "|    ep_rew_mean      | 48.8     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2932     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3188     |\n",
      "|    total_timesteps  | 790321   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.953    |\n",
      "|    n_updates        | 790320   |\n",
      "----------------------------------\n",
      "Num timesteps: 791000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 51.55\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 492      |\n",
      "|    ep_rew_mean      | 58       |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2936     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3194     |\n",
      "|    total_timesteps  | 791782   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.873    |\n",
      "|    n_updates        | 791780   |\n",
      "----------------------------------\n",
      "Num timesteps: 792000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 57.96\n",
      "Num timesteps: 793000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 61.04\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 494      |\n",
      "|    ep_rew_mean      | 63       |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2940     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3201     |\n",
      "|    total_timesteps  | 793647   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.12     |\n",
      "|    n_updates        | 793644   |\n",
      "----------------------------------\n",
      "Num timesteps: 794000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 62.99\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 491      |\n",
      "|    ep_rew_mean      | 64.4     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2944     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3206     |\n",
      "|    total_timesteps  | 794921   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.01     |\n",
      "|    n_updates        | 794920   |\n",
      "----------------------------------\n",
      "Num timesteps: 795000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 64.39\n",
      "Num timesteps: 796000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 68.70\n",
      "Num timesteps: 797000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 71.52\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 505      |\n",
      "|    ep_rew_mean      | 74.3     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2948     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3219     |\n",
      "|    total_timesteps  | 797848   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.07     |\n",
      "|    n_updates        | 797844   |\n",
      "----------------------------------\n",
      "Num timesteps: 798000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 74.34\n",
      "Num timesteps: 799000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 73.89\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 513      |\n",
      "|    ep_rew_mean      | 81.7     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2952     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3228     |\n",
      "|    total_timesteps  | 799984   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.622    |\n",
      "|    n_updates        | 799980   |\n",
      "----------------------------------\n",
      "Num timesteps: 800000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 81.69\n",
      "Num timesteps: 801000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 81.76\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 511      |\n",
      "|    ep_rew_mean      | 85.9     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2956     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3236     |\n",
      "|    total_timesteps  | 801878   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.897    |\n",
      "|    n_updates        | 801876   |\n",
      "----------------------------------\n",
      "Num timesteps: 802000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 85.89\n",
      "Num timesteps: 803000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 92.57\n",
      "Num timesteps: 804000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 96.06\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 527      |\n",
      "|    ep_rew_mean      | 95       |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2960     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3245     |\n",
      "|    total_timesteps  | 804182   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.816    |\n",
      "|    n_updates        | 804180   |\n",
      "----------------------------------\n",
      "Num timesteps: 805000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 97.18\n",
      "Num timesteps: 806000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 96.42\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 533      |\n",
      "|    ep_rew_mean      | 96.9     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2964     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3254     |\n",
      "|    total_timesteps  | 806138   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.931    |\n",
      "|    n_updates        | 806136   |\n",
      "----------------------------------\n",
      "Num timesteps: 807000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 96.82\n",
      "Num timesteps: 808000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 103.16\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 536      |\n",
      "|    ep_rew_mean      | 98.7     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2968     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3263     |\n",
      "|    total_timesteps  | 808231   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.18     |\n",
      "|    n_updates        | 808228   |\n",
      "----------------------------------\n",
      "Num timesteps: 809000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 98.73\n",
      "Num timesteps: 810000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 99.76\n",
      "Num timesteps: 811000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 101.68\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 544      |\n",
      "|    ep_rew_mean      | 101      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2972     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3276     |\n",
      "|    total_timesteps  | 811351   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.04     |\n",
      "|    n_updates        | 811348   |\n",
      "----------------------------------\n",
      "Num timesteps: 812000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 100.86\n",
      "Num timesteps: 813000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 98.26\n",
      "Num timesteps: 814000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 98.89\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 538      |\n",
      "|    ep_rew_mean      | 101      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2976     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3289     |\n",
      "|    total_timesteps  | 814363   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.04     |\n",
      "|    n_updates        | 814360   |\n",
      "----------------------------------\n",
      "Num timesteps: 815000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 100.79\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 525      |\n",
      "|    ep_rew_mean      | 110      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2980     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3295     |\n",
      "|    total_timesteps  | 815910   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.688    |\n",
      "|    n_updates        | 815908   |\n",
      "----------------------------------\n",
      "Num timesteps: 816000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 109.69\n",
      "Num timesteps: 817000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 103.83\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 505      |\n",
      "|    ep_rew_mean      | 102      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2984     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3300     |\n",
      "|    total_timesteps  | 817014   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.826    |\n",
      "|    n_updates        | 817012   |\n",
      "----------------------------------\n",
      "Num timesteps: 818000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 106.30\n",
      "Num timesteps: 819000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 107.38\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 499      |\n",
      "|    ep_rew_mean      | 111      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2988     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3310     |\n",
      "|    total_timesteps  | 819246   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.47     |\n",
      "|    n_updates        | 819244   |\n",
      "----------------------------------\n",
      "Num timesteps: 820000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 110.51\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 487      |\n",
      "|    ep_rew_mean      | 121      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2992     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3318     |\n",
      "|    total_timesteps  | 820934   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.449    |\n",
      "|    n_updates        | 820932   |\n",
      "----------------------------------\n",
      "Num timesteps: 821000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 121.11\n",
      "Num timesteps: 822000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 130.02\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 474      |\n",
      "|    ep_rew_mean      | 133      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 2996     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3326     |\n",
      "|    total_timesteps  | 822877   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.656    |\n",
      "|    n_updates        | 822876   |\n",
      "----------------------------------\n",
      "Num timesteps: 823000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 132.72\n",
      "Num timesteps: 824000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 132.13\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 477      |\n",
      "|    ep_rew_mean      | 132      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3000     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3333     |\n",
      "|    total_timesteps  | 824554   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.05     |\n",
      "|    n_updates        | 824552   |\n",
      "----------------------------------\n",
      "Num timesteps: 825000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 136.31\n",
      "Num timesteps: 826000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 134.84\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 467      |\n",
      "|    ep_rew_mean      | 134      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3004     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3340     |\n",
      "|    total_timesteps  | 826311   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.15     |\n",
      "|    n_updates        | 826308   |\n",
      "----------------------------------\n",
      "Num timesteps: 827000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 132.59\n",
      "Num timesteps: 828000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 135.33\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 466      |\n",
      "|    ep_rew_mean      | 136      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3008     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3350     |\n",
      "|    total_timesteps  | 828709   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.675    |\n",
      "|    n_updates        | 828708   |\n",
      "----------------------------------\n",
      "Num timesteps: 829000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 135.90\n",
      "Num timesteps: 830000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 138.48\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 463      |\n",
      "|    ep_rew_mean      | 141      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3012     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3355     |\n",
      "|    total_timesteps  | 830032   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.645    |\n",
      "|    n_updates        | 830028   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 460      |\n",
      "|    ep_rew_mean      | 142      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3016     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3359     |\n",
      "|    total_timesteps  | 830914   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.619    |\n",
      "|    n_updates        | 830912   |\n",
      "----------------------------------\n",
      "Num timesteps: 831000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 141.63\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 458      |\n",
      "|    ep_rew_mean      | 144      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3020     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3362     |\n",
      "|    total_timesteps  | 831724   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.522    |\n",
      "|    n_updates        | 831720   |\n",
      "----------------------------------\n",
      "Num timesteps: 832000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 142.09\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 452      |\n",
      "|    ep_rew_mean      | 140      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3024     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3365     |\n",
      "|    total_timesteps  | 832593   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.639    |\n",
      "|    n_updates        | 832592   |\n",
      "----------------------------------\n",
      "Num timesteps: 833000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 137.13\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 444      |\n",
      "|    ep_rew_mean      | 136      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3028     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3367     |\n",
      "|    total_timesteps  | 833113   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.855    |\n",
      "|    n_updates        | 833112   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 436      |\n",
      "|    ep_rew_mean      | 133      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3032     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3370     |\n",
      "|    total_timesteps  | 833883   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.585    |\n",
      "|    n_updates        | 833880   |\n",
      "----------------------------------\n",
      "Num timesteps: 834000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 133.48\n",
      "Num timesteps: 835000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 136.68\n",
      "Num timesteps: 836000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 136.46\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 444      |\n",
      "|    ep_rew_mean      | 135      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3036     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3380     |\n",
      "|    total_timesteps  | 836221   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.378    |\n",
      "|    n_updates        | 836220   |\n",
      "----------------------------------\n",
      "Num timesteps: 837000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 135.35\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 437      |\n",
      "|    ep_rew_mean      | 128      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3040     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3385     |\n",
      "|    total_timesteps  | 837352   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.397    |\n",
      "|    n_updates        | 837348   |\n",
      "----------------------------------\n",
      "Num timesteps: 838000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 124.16\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 436      |\n",
      "|    ep_rew_mean      | 121      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3044     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3389     |\n",
      "|    total_timesteps  | 838484   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.588    |\n",
      "|    n_updates        | 838480   |\n",
      "----------------------------------\n",
      "Num timesteps: 839000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 117.22\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 416      |\n",
      "|    ep_rew_mean      | 107      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3048     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3393     |\n",
      "|    total_timesteps  | 839484   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.91     |\n",
      "|    n_updates        | 839480   |\n",
      "----------------------------------\n",
      "Num timesteps: 840000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 107.22\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 400      |\n",
      "|    ep_rew_mean      | 105      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3052     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3395     |\n",
      "|    total_timesteps  | 840015   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.629    |\n",
      "|    n_updates        | 840012   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 390      |\n",
      "|    ep_rew_mean      | 99       |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3056     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3399     |\n",
      "|    total_timesteps  | 840852   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.693    |\n",
      "|    n_updates        | 840848   |\n",
      "----------------------------------\n",
      "Num timesteps: 841000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 98.97\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 376      |\n",
      "|    ep_rew_mean      | 86.2     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3060     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3403     |\n",
      "|    total_timesteps  | 841805   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.45     |\n",
      "|    n_updates        | 841804   |\n",
      "----------------------------------\n",
      "Num timesteps: 842000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 86.24\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 366      |\n",
      "|    ep_rew_mean      | 78.3     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3064     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3407     |\n",
      "|    total_timesteps  | 842779   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.72     |\n",
      "|    n_updates        | 842776   |\n",
      "----------------------------------\n",
      "Num timesteps: 843000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 75.95\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 351      |\n",
      "|    ep_rew_mean      | 73.7     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3068     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3409     |\n",
      "|    total_timesteps  | 843366   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.504    |\n",
      "|    n_updates        | 843364   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 326      |\n",
      "|    ep_rew_mean      | 68.3     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3072     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3411     |\n",
      "|    total_timesteps  | 843951   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.496    |\n",
      "|    n_updates        | 843948   |\n",
      "----------------------------------\n",
      "Num timesteps: 844000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 68.32\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 304      |\n",
      "|    ep_rew_mean      | 68.3     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3076     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3415     |\n",
      "|    total_timesteps  | 844795   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.414    |\n",
      "|    n_updates        | 844792   |\n",
      "----------------------------------\n",
      "Num timesteps: 845000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 66.05\n",
      "Num timesteps: 846000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 66.79\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 301      |\n",
      "|    ep_rew_mean      | 67.6     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3080     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3420     |\n",
      "|    total_timesteps  | 846018   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.481    |\n",
      "|    n_updates        | 846016   |\n",
      "----------------------------------\n",
      "Num timesteps: 847000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 70.47\n",
      "Num timesteps: 848000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 73.73\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 316      |\n",
      "|    ep_rew_mean      | 85.3     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3084     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3431     |\n",
      "|    total_timesteps  | 848662   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.968    |\n",
      "|    n_updates        | 848660   |\n",
      "----------------------------------\n",
      "Num timesteps: 849000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 83.64\n",
      "Num timesteps: 850000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 85.78\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 314      |\n",
      "|    ep_rew_mean      | 93       |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3088     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3440     |\n",
      "|    total_timesteps  | 850688   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.751    |\n",
      "|    n_updates        | 850684   |\n",
      "----------------------------------\n",
      "Num timesteps: 851000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 92.98\n",
      "Num timesteps: 852000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 93.42\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 319      |\n",
      "|    ep_rew_mean      | 99.9     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3092     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3449     |\n",
      "|    total_timesteps  | 852839   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.454    |\n",
      "|    n_updates        | 852836   |\n",
      "----------------------------------\n",
      "Num timesteps: 853000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 97.86\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 309      |\n",
      "|    ep_rew_mean      | 97.5     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3096     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3453     |\n",
      "|    total_timesteps  | 853804   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.518    |\n",
      "|    n_updates        | 853800   |\n",
      "----------------------------------\n",
      "Num timesteps: 854000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 97.46\n",
      "Num timesteps: 855000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 95.55\n",
      "Num timesteps: 856000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 95.28\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 319      |\n",
      "|    ep_rew_mean      | 95.1     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3100     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3464     |\n",
      "|    total_timesteps  | 856442   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.802    |\n",
      "|    n_updates        | 856440   |\n",
      "----------------------------------\n",
      "Num timesteps: 857000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 94.77\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 315      |\n",
      "|    ep_rew_mean      | 98       |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3104     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3470     |\n",
      "|    total_timesteps  | 857846   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.639    |\n",
      "|    n_updates        | 857844   |\n",
      "----------------------------------\n",
      "Num timesteps: 858000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 97.76\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 299      |\n",
      "|    ep_rew_mean      | 96.9     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3108     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3473     |\n",
      "|    total_timesteps  | 858595   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.658    |\n",
      "|    n_updates        | 858592   |\n",
      "----------------------------------\n",
      "Num timesteps: 859000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 96.58\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 297      |\n",
      "|    ep_rew_mean      | 96.2     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3112     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3477     |\n",
      "|    total_timesteps  | 859733   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.777    |\n",
      "|    n_updates        | 859732   |\n",
      "----------------------------------\n",
      "Num timesteps: 860000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 96.19\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 297      |\n",
      "|    ep_rew_mean      | 97.9     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3116     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3481     |\n",
      "|    total_timesteps  | 860608   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.15     |\n",
      "|    n_updates        | 860604   |\n",
      "----------------------------------\n",
      "Num timesteps: 861000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 99.68\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 302      |\n",
      "|    ep_rew_mean      | 97.9     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3120     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3486     |\n",
      "|    total_timesteps  | 861880   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.551    |\n",
      "|    n_updates        | 861876   |\n",
      "----------------------------------\n",
      "Num timesteps: 862000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 97.91\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 302      |\n",
      "|    ep_rew_mean      | 99.5     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3124     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3490     |\n",
      "|    total_timesteps  | 862809   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.873    |\n",
      "|    n_updates        | 862808   |\n",
      "----------------------------------\n",
      "Num timesteps: 863000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 97.97\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 303      |\n",
      "|    ep_rew_mean      | 101      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3128     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3492     |\n",
      "|    total_timesteps  | 863386   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.08     |\n",
      "|    n_updates        | 863384   |\n",
      "----------------------------------\n",
      "Num timesteps: 864000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 103.33\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 307      |\n",
      "|    ep_rew_mean      | 103      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3132     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3497     |\n",
      "|    total_timesteps  | 864599   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.381    |\n",
      "|    n_updates        | 864596   |\n",
      "----------------------------------\n",
      "Num timesteps: 865000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 96.22\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 288      |\n",
      "|    ep_rew_mean      | 94.7     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3136     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3498     |\n",
      "|    total_timesteps  | 865071   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.853    |\n",
      "|    n_updates        | 865068   |\n",
      "----------------------------------\n",
      "Num timesteps: 866000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 94.01\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 292      |\n",
      "|    ep_rew_mean      | 97.9     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3140     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3505     |\n",
      "|    total_timesteps  | 866585   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.574    |\n",
      "|    n_updates        | 866584   |\n",
      "----------------------------------\n",
      "Num timesteps: 867000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 103.75\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 288      |\n",
      "|    ep_rew_mean      | 106      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3144     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3508     |\n",
      "|    total_timesteps  | 867316   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.367    |\n",
      "|    n_updates        | 867312   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 282      |\n",
      "|    ep_rew_mean      | 111      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3148     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3509     |\n",
      "|    total_timesteps  | 867729   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.14     |\n",
      "|    n_updates        | 867728   |\n",
      "----------------------------------\n",
      "Num timesteps: 868000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 114.12\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 283      |\n",
      "|    ep_rew_mean      | 114      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3152     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3512     |\n",
      "|    total_timesteps  | 868347   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.854    |\n",
      "|    n_updates        | 868344   |\n",
      "----------------------------------\n",
      "Num timesteps: 869000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 112.62\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 286      |\n",
      "|    ep_rew_mean      | 118      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3156     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3516     |\n",
      "|    total_timesteps  | 869449   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.52     |\n",
      "|    n_updates        | 869448   |\n",
      "----------------------------------\n",
      "Num timesteps: 870000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 122.94\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 289      |\n",
      "|    ep_rew_mean      | 130      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3160     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3521     |\n",
      "|    total_timesteps  | 870702   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.873    |\n",
      "|    n_updates        | 870700   |\n",
      "----------------------------------\n",
      "Num timesteps: 871000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 131.06\n",
      "Num timesteps: 872000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 134.16\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 294      |\n",
      "|    ep_rew_mean      | 136      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3164     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3528     |\n",
      "|    total_timesteps  | 872209   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.84     |\n",
      "|    n_updates        | 872208   |\n",
      "----------------------------------\n",
      "Num timesteps: 873000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 139.52\n",
      "Num timesteps: 874000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 141.18\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 314      |\n",
      "|    ep_rew_mean      | 143      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3168     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3539     |\n",
      "|    total_timesteps  | 874798   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.641    |\n",
      "|    n_updates        | 874796   |\n",
      "----------------------------------\n",
      "Num timesteps: 875000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 142.53\n",
      "Num timesteps: 876000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 144.20\n",
      "Num timesteps: 877000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 145.21\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 332      |\n",
      "|    ep_rew_mean      | 138      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3172     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3549     |\n",
      "|    total_timesteps  | 877116   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.706    |\n",
      "|    n_updates        | 877112   |\n",
      "----------------------------------\n",
      "Num timesteps: 878000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 139.96\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 340      |\n",
      "|    ep_rew_mean      | 142      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3176     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3556     |\n",
      "|    total_timesteps  | 878832   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.745    |\n",
      "|    n_updates        | 878828   |\n",
      "----------------------------------\n",
      "Num timesteps: 879000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 142.40\n",
      "Num timesteps: 880000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 142.57\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 344      |\n",
      "|    ep_rew_mean      | 144      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3180     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3563     |\n",
      "|    total_timesteps  | 880429   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.587    |\n",
      "|    n_updates        | 880428   |\n",
      "----------------------------------\n",
      "Num timesteps: 881000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 143.94\n",
      "Num timesteps: 882000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 142.06\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 343      |\n",
      "|    ep_rew_mean      | 138      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3184     |\n",
      "|    fps              | 247      |\n",
      "|    time_elapsed     | 3574     |\n",
      "|    total_timesteps  | 882928   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.04     |\n",
      "|    n_updates        | 882924   |\n",
      "----------------------------------\n",
      "Num timesteps: 883000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 138.10\n",
      "Num timesteps: 884000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 140.76\n",
      "Num timesteps: 885000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 136.75\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 344      |\n",
      "|    ep_rew_mean      | 137      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3188     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3583     |\n",
      "|    total_timesteps  | 885122   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.306    |\n",
      "|    n_updates        | 885120   |\n",
      "----------------------------------\n",
      "Num timesteps: 886000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 138.27\n",
      "Num timesteps: 887000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 137.67\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 347      |\n",
      "|    ep_rew_mean      | 138      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3192     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3594     |\n",
      "|    total_timesteps  | 887563   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.577    |\n",
      "|    n_updates        | 887560   |\n",
      "----------------------------------\n",
      "Num timesteps: 888000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 135.07\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 348      |\n",
      "|    ep_rew_mean      | 137      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3196     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3598     |\n",
      "|    total_timesteps  | 888569   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.38     |\n",
      "|    n_updates        | 888568   |\n",
      "----------------------------------\n",
      "Num timesteps: 889000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 137.41\n",
      "Num timesteps: 890000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 139.11\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 339      |\n",
      "|    ep_rew_mean      | 139      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3200     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3605     |\n",
      "|    total_timesteps  | 890334   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.374    |\n",
      "|    n_updates        | 890332   |\n",
      "----------------------------------\n",
      "Num timesteps: 891000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 141.67\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 335      |\n",
      "|    ep_rew_mean      | 140      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3204     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3609     |\n",
      "|    total_timesteps  | 891315   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.925    |\n",
      "|    n_updates        | 891312   |\n",
      "----------------------------------\n",
      "Num timesteps: 892000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 139.96\n",
      "Num timesteps: 893000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 139.42\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 347      |\n",
      "|    ep_rew_mean      | 139      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3208     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3618     |\n",
      "|    total_timesteps  | 893331   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.475    |\n",
      "|    n_updates        | 893328   |\n",
      "----------------------------------\n",
      "Num timesteps: 894000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 135.51\n",
      "Num timesteps: 895000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 139.85\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 353      |\n",
      "|    ep_rew_mean      | 140      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3212     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3625     |\n",
      "|    total_timesteps  | 895000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.925    |\n",
      "|    n_updates        | 894996   |\n",
      "----------------------------------\n",
      "Num timesteps: 896000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 139.92\n",
      "Num timesteps: 897000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 142.61\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 365      |\n",
      "|    ep_rew_mean      | 145      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3216     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3634     |\n",
      "|    total_timesteps  | 897129   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.717    |\n",
      "|    n_updates        | 897128   |\n",
      "----------------------------------\n",
      "Num timesteps: 898000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 146.97\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 363      |\n",
      "|    ep_rew_mean      | 152      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3220     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3639     |\n",
      "|    total_timesteps  | 898160   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.628    |\n",
      "|    n_updates        | 898156   |\n",
      "----------------------------------\n",
      "Num timesteps: 899000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 153.94\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 371      |\n",
      "|    ep_rew_mean      | 157      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3224     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3646     |\n",
      "|    total_timesteps  | 899862   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.72     |\n",
      "|    n_updates        | 899860   |\n",
      "----------------------------------\n",
      "Num timesteps: 900000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 156.56\n",
      "Num timesteps: 901000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 163.10\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 383      |\n",
      "|    ep_rew_mean      | 166      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3228     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3653     |\n",
      "|    total_timesteps  | 901716   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.538    |\n",
      "|    n_updates        | 901712   |\n",
      "----------------------------------\n",
      "Num timesteps: 902000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 166.24\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 381      |\n",
      "|    ep_rew_mean      | 171      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3232     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3657     |\n",
      "|    total_timesteps  | 902714   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.483    |\n",
      "|    n_updates        | 902712   |\n",
      "----------------------------------\n",
      "Num timesteps: 903000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 171.17\n",
      "Num timesteps: 904000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 173.24\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 397      |\n",
      "|    ep_rew_mean      | 178      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3236     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3666     |\n",
      "|    total_timesteps  | 904737   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.953    |\n",
      "|    n_updates        | 904736   |\n",
      "----------------------------------\n",
      "Num timesteps: 905000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 173.06\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 388      |\n",
      "|    ep_rew_mean      | 167      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3240     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3669     |\n",
      "|    total_timesteps  | 905432   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.569    |\n",
      "|    n_updates        | 905428   |\n",
      "----------------------------------\n",
      "Num timesteps: 906000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 169.68\n",
      "Num timesteps: 907000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 168.61\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 406      |\n",
      "|    ep_rew_mean      | 170      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3244     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3679     |\n",
      "|    total_timesteps  | 907915   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.409    |\n",
      "|    n_updates        | 907912   |\n",
      "----------------------------------\n",
      "Num timesteps: 908000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 170.33\n",
      "Num timesteps: 909000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 174.40\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 419      |\n",
      "|    ep_rew_mean      | 174      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3248     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3686     |\n",
      "|    total_timesteps  | 909612   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.19     |\n",
      "|    n_updates        | 909608   |\n",
      "----------------------------------\n",
      "Num timesteps: 910000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 174.24\n",
      "Num timesteps: 911000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 177.16\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 427      |\n",
      "|    ep_rew_mean      | 177      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3252     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3692     |\n",
      "|    total_timesteps  | 911016   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.05     |\n",
      "|    n_updates        | 911012   |\n",
      "----------------------------------\n",
      "Num timesteps: 912000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 182.34\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 429      |\n",
      "|    ep_rew_mean      | 180      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3256     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3697     |\n",
      "|    total_timesteps  | 912327   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.523    |\n",
      "|    n_updates        | 912324   |\n",
      "----------------------------------\n",
      "Num timesteps: 913000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 179.80\n",
      "Num timesteps: 914000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 179.44\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 440      |\n",
      "|    ep_rew_mean      | 182      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3260     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3707     |\n",
      "|    total_timesteps  | 914692   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.498    |\n",
      "|    n_updates        | 914688   |\n",
      "----------------------------------\n",
      "Num timesteps: 915000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 181.51\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 434      |\n",
      "|    ep_rew_mean      | 180      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3264     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3711     |\n",
      "|    total_timesteps  | 915639   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.569    |\n",
      "|    n_updates        | 915636   |\n",
      "----------------------------------\n",
      "Num timesteps: 916000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 179.98\n",
      "Num timesteps: 917000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 181.88\n",
      "Num timesteps: 918000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 181.67\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 437      |\n",
      "|    ep_rew_mean      | 181      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3268     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3724     |\n",
      "|    total_timesteps  | 918476   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.594    |\n",
      "|    n_updates        | 918472   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 419      |\n",
      "|    ep_rew_mean      | 183      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3272     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3726     |\n",
      "|    total_timesteps  | 918990   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.651    |\n",
      "|    n_updates        | 918988   |\n",
      "----------------------------------\n",
      "Num timesteps: 919000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 182.90\n",
      "Num timesteps: 920000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 182.63\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 414      |\n",
      "|    ep_rew_mean      | 185      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3276     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3731     |\n",
      "|    total_timesteps  | 920188   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.4      |\n",
      "|    n_updates        | 920184   |\n",
      "----------------------------------\n",
      "Num timesteps: 921000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 181.61\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 406      |\n",
      "|    ep_rew_mean      | 184      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3280     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3734     |\n",
      "|    total_timesteps  | 921022   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.689    |\n",
      "|    n_updates        | 921020   |\n",
      "----------------------------------\n",
      "Num timesteps: 922000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 184.47\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 396      |\n",
      "|    ep_rew_mean      | 186      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3284     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3741     |\n",
      "|    total_timesteps  | 922563   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.341    |\n",
      "|    n_updates        | 922560   |\n",
      "----------------------------------\n",
      "Num timesteps: 923000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 185.85\n",
      "Num timesteps: 924000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 184.16\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 389      |\n",
      "|    ep_rew_mean      | 184      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3288     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3747     |\n",
      "|    total_timesteps  | 924028   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.482    |\n",
      "|    n_updates        | 924024   |\n",
      "----------------------------------\n",
      "Num timesteps: 925000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 181.75\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 376      |\n",
      "|    ep_rew_mean      | 182      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3292     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3751     |\n",
      "|    total_timesteps  | 925150   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.433    |\n",
      "|    n_updates        | 925148   |\n",
      "----------------------------------\n",
      "Num timesteps: 926000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 181.85\n",
      "Num timesteps: 927000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 185.30\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 393      |\n",
      "|    ep_rew_mean      | 185      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3296     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3763     |\n",
      "|    total_timesteps  | 927887   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.414    |\n",
      "|    n_updates        | 927884   |\n",
      "----------------------------------\n",
      "Num timesteps: 928000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 185.39\n",
      "Num timesteps: 929000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 185.80\n",
      "Num timesteps: 930000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 185.12\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 401      |\n",
      "|    ep_rew_mean      | 186      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3300     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3774     |\n",
      "|    total_timesteps  | 930475   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.372    |\n",
      "|    n_updates        | 930472   |\n",
      "----------------------------------\n",
      "Num timesteps: 931000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 180.94\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 400      |\n",
      "|    ep_rew_mean      | 183      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3304     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3778     |\n",
      "|    total_timesteps  | 931285   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.433    |\n",
      "|    n_updates        | 931284   |\n",
      "----------------------------------\n",
      "Num timesteps: 932000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 185.68\n",
      "Num timesteps: 933000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 185.41\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 401      |\n",
      "|    ep_rew_mean      | 187      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3308     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3787     |\n",
      "|    total_timesteps  | 933443   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.993    |\n",
      "|    n_updates        | 933440   |\n",
      "----------------------------------\n",
      "Num timesteps: 934000\n",
      "Best mean reward: 187.36 - Last mean reward per episode: 191.32\n",
      "Saving new best model to log_dir_DQN/best_model.zip\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 397      |\n",
      "|    ep_rew_mean      | 189      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3312     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3792     |\n",
      "|    total_timesteps  | 934721   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.268    |\n",
      "|    n_updates        | 934720   |\n",
      "----------------------------------\n",
      "Num timesteps: 935000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: 187.23\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 387      |\n",
      "|    ep_rew_mean      | 180      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3316     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3797     |\n",
      "|    total_timesteps  | 935821   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.68     |\n",
      "|    n_updates        | 935820   |\n",
      "----------------------------------\n",
      "Num timesteps: 936000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: 179.96\n",
      "Num timesteps: 937000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: 180.40\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 394      |\n",
      "|    ep_rew_mean      | 180      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3320     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3804     |\n",
      "|    total_timesteps  | 937577   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.619    |\n",
      "|    n_updates        | 937576   |\n",
      "----------------------------------\n",
      "Num timesteps: 938000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: 178.01\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 385      |\n",
      "|    ep_rew_mean      | 176      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3324     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3808     |\n",
      "|    total_timesteps  | 938388   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.45     |\n",
      "|    n_updates        | 938384   |\n",
      "----------------------------------\n",
      "Num timesteps: 939000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: 173.96\n",
      "Num timesteps: 940000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: 174.16\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 385      |\n",
      "|    ep_rew_mean      | 174      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3328     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3815     |\n",
      "|    total_timesteps  | 940174   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.919    |\n",
      "|    n_updates        | 940172   |\n",
      "----------------------------------\n",
      "Num timesteps: 941000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: 172.91\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 388      |\n",
      "|    ep_rew_mean      | 170      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3332     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3821     |\n",
      "|    total_timesteps  | 941509   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.74     |\n",
      "|    n_updates        | 941508   |\n",
      "----------------------------------\n",
      "Num timesteps: 942000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: 170.35\n",
      "Num timesteps: 943000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: 170.24\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 391      |\n",
      "|    ep_rew_mean      | 173      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3336     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3830     |\n",
      "|    total_timesteps  | 943788   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.352    |\n",
      "|    n_updates        | 943784   |\n",
      "----------------------------------\n",
      "Num timesteps: 944000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: 178.71\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 390      |\n",
      "|    ep_rew_mean      | 183      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3340     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3833     |\n",
      "|    total_timesteps  | 944383   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.589    |\n",
      "|    n_updates        | 944380   |\n",
      "----------------------------------\n",
      "Num timesteps: 945000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: 181.87\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 373      |\n",
      "|    ep_rew_mean      | 181      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3344     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3836     |\n",
      "|    total_timesteps  | 945262   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.888    |\n",
      "|    n_updates        | 945260   |\n",
      "----------------------------------\n",
      "Num timesteps: 946000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: 180.58\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 371      |\n",
      "|    ep_rew_mean      | 184      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3348     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3842     |\n",
      "|    total_timesteps  | 946695   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.423    |\n",
      "|    n_updates        | 946692   |\n",
      "----------------------------------\n",
      "Num timesteps: 947000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: 167.17\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 369      |\n",
      "|    ep_rew_mean      | 167      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3352     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3847     |\n",
      "|    total_timesteps  | 947894   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.454    |\n",
      "|    n_updates        | 947892   |\n",
      "----------------------------------\n",
      "Num timesteps: 948000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: 167.31\n",
      "Num timesteps: 949000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: 167.26\n",
      "Num timesteps: 950000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: 166.12\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 381      |\n",
      "|    ep_rew_mean      | 165      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3356     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3858     |\n",
      "|    total_timesteps  | 950400   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.334    |\n",
      "|    n_updates        | 950396   |\n",
      "----------------------------------\n",
      "Num timesteps: 951000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: 162.62\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 366      |\n",
      "|    ep_rew_mean      | 160      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3360     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3862     |\n",
      "|    total_timesteps  | 951306   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.435    |\n",
      "|    n_updates        | 951304   |\n",
      "----------------------------------\n",
      "Num timesteps: 952000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: 154.24\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 365      |\n",
      "|    ep_rew_mean      | 155      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3364     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3865     |\n",
      "|    total_timesteps  | 952090   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.398    |\n",
      "|    n_updates        | 952088   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 344      |\n",
      "|    ep_rew_mean      | 147      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3368     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3868     |\n",
      "|    total_timesteps  | 952864   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.488    |\n",
      "|    n_updates        | 952860   |\n",
      "----------------------------------\n",
      "Num timesteps: 953000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: 147.51\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 349      |\n",
      "|    ep_rew_mean      | 149      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3372     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3873     |\n",
      "|    total_timesteps  | 953935   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.04     |\n",
      "|    n_updates        | 953932   |\n",
      "----------------------------------\n",
      "Num timesteps: 954000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: 149.23\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 348      |\n",
      "|    ep_rew_mean      | 133      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3376     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3877     |\n",
      "|    total_timesteps  | 954986   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.658    |\n",
      "|    n_updates        | 954984   |\n",
      "----------------------------------\n",
      "Num timesteps: 955000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: 133.24\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 347      |\n",
      "|    ep_rew_mean      | 122      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3380     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3880     |\n",
      "|    total_timesteps  | 955759   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.52     |\n",
      "|    n_updates        | 955756   |\n",
      "----------------------------------\n",
      "Num timesteps: 956000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: 120.93\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 336      |\n",
      "|    ep_rew_mean      | 118      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3384     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3882     |\n",
      "|    total_timesteps  | 956209   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.49     |\n",
      "|    n_updates        | 956208   |\n",
      "----------------------------------\n",
      "Num timesteps: 957000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: 117.47\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 336      |\n",
      "|    ep_rew_mean      | 120      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3388     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3887     |\n",
      "|    total_timesteps  | 957652   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.606    |\n",
      "|    n_updates        | 957648   |\n",
      "----------------------------------\n",
      "Num timesteps: 958000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: 111.86\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 332      |\n",
      "|    ep_rew_mean      | 105      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3392     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3890     |\n",
      "|    total_timesteps  | 958345   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.93     |\n",
      "|    n_updates        | 958344   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 309      |\n",
      "|    ep_rew_mean      | 92.9     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3396     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3892     |\n",
      "|    total_timesteps  | 958811   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.585    |\n",
      "|    n_updates        | 958808   |\n",
      "----------------------------------\n",
      "Num timesteps: 959000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: 92.89\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 291      |\n",
      "|    ep_rew_mean      | 81.2     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3400     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3895     |\n",
      "|    total_timesteps  | 959556   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.592    |\n",
      "|    n_updates        | 959552   |\n",
      "----------------------------------\n",
      "Num timesteps: 960000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: 75.40\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 287      |\n",
      "|    ep_rew_mean      | 73.2     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3404     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3897     |\n",
      "|    total_timesteps  | 960010   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.714    |\n",
      "|    n_updates        | 960008   |\n",
      "----------------------------------\n",
      "Num timesteps: 961000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: 64.45\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 282      |\n",
      "|    ep_rew_mean      | 65.9     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3408     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3903     |\n",
      "|    total_timesteps  | 961610   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.735    |\n",
      "|    n_updates        | 961608   |\n",
      "----------------------------------\n",
      "Num timesteps: 962000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: 63.26\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 275      |\n",
      "|    ep_rew_mean      | 57.7     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3412     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3906     |\n",
      "|    total_timesteps  | 962198   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.97     |\n",
      "|    n_updates        | 962196   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 269      |\n",
      "|    ep_rew_mean      | 49.2     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3416     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3908     |\n",
      "|    total_timesteps  | 962762   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.847    |\n",
      "|    n_updates        | 962760   |\n",
      "----------------------------------\n",
      "Num timesteps: 963000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: 41.69\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 256      |\n",
      "|    ep_rew_mean      | 36.7     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3420     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3909     |\n",
      "|    total_timesteps  | 963132   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.987    |\n",
      "|    n_updates        | 963128   |\n",
      "----------------------------------\n",
      "Num timesteps: 964000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: 25.68\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 257      |\n",
      "|    ep_rew_mean      | 28       |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3424     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3913     |\n",
      "|    total_timesteps  | 964083   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.32     |\n",
      "|    n_updates        | 964080   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 244      |\n",
      "|    ep_rew_mean      | 17.9     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3428     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3915     |\n",
      "|    total_timesteps  | 964583   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.813    |\n",
      "|    n_updates        | 964580   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 235      |\n",
      "|    ep_rew_mean      | 2.63     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3432     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3916     |\n",
      "|    total_timesteps  | 964988   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.716    |\n",
      "|    n_updates        | 964984   |\n",
      "----------------------------------\n",
      "Num timesteps: 965000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: 2.63\n",
      "Num timesteps: 966000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: -4.29\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 224      |\n",
      "|    ep_rew_mean      | -19.3    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3436     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3921     |\n",
      "|    total_timesteps  | 966204   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.66     |\n",
      "|    n_updates        | 966200   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 223      |\n",
      "|    ep_rew_mean      | -33.5    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3440     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3923     |\n",
      "|    total_timesteps  | 966655   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.994    |\n",
      "|    n_updates        | 966652   |\n",
      "----------------------------------\n",
      "Num timesteps: 967000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: -38.80\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 226      |\n",
      "|    ep_rew_mean      | -33.9    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3444     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3928     |\n",
      "|    total_timesteps  | 967884   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.606    |\n",
      "|    n_updates        | 967880   |\n",
      "----------------------------------\n",
      "Num timesteps: 968000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: -39.77\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 217      |\n",
      "|    ep_rew_mean      | -51.7    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3448     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3930     |\n",
      "|    total_timesteps  | 968388   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.737    |\n",
      "|    n_updates        | 968384   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 210      |\n",
      "|    ep_rew_mean      | -58.9    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3452     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3932     |\n",
      "|    total_timesteps  | 968845   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.15     |\n",
      "|    n_updates        | 968844   |\n",
      "----------------------------------\n",
      "Num timesteps: 969000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: -68.15\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 190      |\n",
      "|    ep_rew_mean      | -74.7    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3456     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3934     |\n",
      "|    total_timesteps  | 969405   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.33     |\n",
      "|    n_updates        | 969404   |\n",
      "----------------------------------\n",
      "Num timesteps: 970000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: -83.14\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 189      |\n",
      "|    ep_rew_mean      | -92.4    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3460     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3937     |\n",
      "|    total_timesteps  | 970215   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.72     |\n",
      "|    n_updates        | 970212   |\n",
      "----------------------------------\n",
      "Num timesteps: 971000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: -94.03\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 194      |\n",
      "|    ep_rew_mean      | -99.9    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3464     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3943     |\n",
      "|    total_timesteps  | 971526   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.95     |\n",
      "|    n_updates        | 971524   |\n",
      "----------------------------------\n",
      "Num timesteps: 972000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: -104.07\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 198      |\n",
      "|    ep_rew_mean      | -107     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3468     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3948     |\n",
      "|    total_timesteps  | 972713   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.08     |\n",
      "|    n_updates        | 972712   |\n",
      "----------------------------------\n",
      "Num timesteps: 973000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: -106.81\n",
      "Num timesteps: 974000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: -105.74\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 204      |\n",
      "|    ep_rew_mean      | -107     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3472     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3954     |\n",
      "|    total_timesteps  | 974299   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.12     |\n",
      "|    n_updates        | 974296   |\n",
      "----------------------------------\n",
      "Num timesteps: 975000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: -115.91\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 203      |\n",
      "|    ep_rew_mean      | -109     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3476     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3958     |\n",
      "|    total_timesteps  | 975318   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.39     |\n",
      "|    n_updates        | 975316   |\n",
      "----------------------------------\n",
      "Num timesteps: 976000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: -111.71\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 203      |\n",
      "|    ep_rew_mean      | -106     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3480     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3961     |\n",
      "|    total_timesteps  | 976063   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.22     |\n",
      "|    n_updates        | 976060   |\n",
      "----------------------------------\n",
      "Num timesteps: 977000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: -100.28\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 208      |\n",
      "|    ep_rew_mean      | -100     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3484     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3965     |\n",
      "|    total_timesteps  | 977055   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.46     |\n",
      "|    n_updates        | 977052   |\n",
      "----------------------------------\n",
      "Num timesteps: 978000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: -107.34\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 206      |\n",
      "|    ep_rew_mean      | -107     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3488     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3970     |\n",
      "|    total_timesteps  | 978266   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.979    |\n",
      "|    n_updates        | 978264   |\n",
      "----------------------------------\n",
      "Num timesteps: 979000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: -101.04\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 210      |\n",
      "|    ep_rew_mean      | -93.4    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3492     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3975     |\n",
      "|    total_timesteps  | 979349   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.14     |\n",
      "|    n_updates        | 979348   |\n",
      "----------------------------------\n",
      "Num timesteps: 980000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: -91.82\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 219      |\n",
      "|    ep_rew_mean      | -86.2    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3496     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3980     |\n",
      "|    total_timesteps  | 980738   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.58     |\n",
      "|    n_updates        | 980736   |\n",
      "----------------------------------\n",
      "Num timesteps: 981000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: -86.16\n",
      "Num timesteps: 982000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: -81.92\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 229      |\n",
      "|    ep_rew_mean      | -79      |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3500     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3988     |\n",
      "|    total_timesteps  | 982457   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.22     |\n",
      "|    n_updates        | 982456   |\n",
      "----------------------------------\n",
      "Num timesteps: 983000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: -76.73\n",
      "Num timesteps: 984000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: -70.61\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 244      |\n",
      "|    ep_rew_mean      | -69.4    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3504     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 3996     |\n",
      "|    total_timesteps  | 984373   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.41     |\n",
      "|    n_updates        | 984372   |\n",
      "----------------------------------\n",
      "Num timesteps: 985000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: -64.41\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 237      |\n",
      "|    ep_rew_mean      | -64.4    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3508     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 4000     |\n",
      "|    total_timesteps  | 985311   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.69     |\n",
      "|    n_updates        | 985308   |\n",
      "----------------------------------\n",
      "Num timesteps: 986000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: -62.40\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 246      |\n",
      "|    ep_rew_mean      | -57.3    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3512     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 4006     |\n",
      "|    total_timesteps  | 986840   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.92     |\n",
      "|    n_updates        | 986836   |\n",
      "----------------------------------\n",
      "Num timesteps: 987000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: -55.76\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 249      |\n",
      "|    ep_rew_mean      | -44.2    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3516     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 4010     |\n",
      "|    total_timesteps  | 987706   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.25     |\n",
      "|    n_updates        | 987704   |\n",
      "----------------------------------\n",
      "Num timesteps: 988000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: -44.19\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 255      |\n",
      "|    ep_rew_mean      | -36.8    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3520     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 4014     |\n",
      "|    total_timesteps  | 988609   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.786    |\n",
      "|    n_updates        | 988608   |\n",
      "----------------------------------\n",
      "Num timesteps: 989000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: -36.76\n",
      "Num timesteps: 990000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: -29.02\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 266      |\n",
      "|    ep_rew_mean      | -24.9    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3524     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 4023     |\n",
      "|    total_timesteps  | 990720   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 2.44     |\n",
      "|    n_updates        | 990716   |\n",
      "----------------------------------\n",
      "Num timesteps: 991000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: -24.86\n",
      "Num timesteps: 992000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: -15.12\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 277      |\n",
      "|    ep_rew_mean      | -12.2    |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3528     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 4029     |\n",
      "|    total_timesteps  | 992237   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.2      |\n",
      "|    n_updates        | 992236   |\n",
      "----------------------------------\n",
      "Num timesteps: 993000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: -9.45\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 286      |\n",
      "|    ep_rew_mean      | 6.39     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3532     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 4034     |\n",
      "|    total_timesteps  | 993562   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.688    |\n",
      "|    n_updates        | 993560   |\n",
      "----------------------------------\n",
      "Num timesteps: 994000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: 13.24\n",
      "Num timesteps: 995000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: 13.56\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 292      |\n",
      "|    ep_rew_mean      | 27.9     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3536     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 4042     |\n",
      "|    total_timesteps  | 995408   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 0.874    |\n",
      "|    n_updates        | 995404   |\n",
      "----------------------------------\n",
      "Num timesteps: 996000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: 34.08\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 298      |\n",
      "|    ep_rew_mean      | 46.9     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3540     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 4046     |\n",
      "|    total_timesteps  | 996429   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.64     |\n",
      "|    n_updates        | 996428   |\n",
      "----------------------------------\n",
      "Num timesteps: 997000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: 49.69\n",
      "Num timesteps: 998000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: 49.60\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 306      |\n",
      "|    ep_rew_mean      | 48.5     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3544     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 4055     |\n",
      "|    total_timesteps  | 998511   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.31     |\n",
      "|    n_updates        | 998508   |\n",
      "----------------------------------\n",
      "Num timesteps: 999000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: 54.02\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 314      |\n",
      "|    ep_rew_mean      | 66       |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 3548     |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 4060     |\n",
      "|    total_timesteps  | 999788   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.00063  |\n",
      "|    loss             | 1.47     |\n",
      "|    n_updates        | 999784   |\n",
      "----------------------------------\n",
      "Num timesteps: 1000000\n",
      "Best mean reward: 191.32 - Last mean reward per episode: 70.40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x7f02e81a9370>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_from_file = False\n",
    "# Hyperparameters are from RL_Zoo\n",
    "# https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/dqn.yml\n",
    "\n",
    "\n",
    "n_timesteps = 1e6\n",
    "policy =  'MlpPolicy'\n",
    "learning_rate = 6.3e-4\n",
    "batch_size = 128\n",
    "buffer_size = 50000\n",
    "learning_starts = 0\n",
    "gamma = 0.99\n",
    "target_update_interval = 250\n",
    "train_freq = 4\n",
    "gradient_steps = -1\n",
    "exploration_fraction = 0.12\n",
    "exploration_final_eps = 0.1\n",
    "policy_kwargs = \"dict(net_arch=[256, 256])\"\n",
    "\n",
    "n_envs = 1\n",
    "\n",
    "\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=\"log_dir_DQN/\")\n",
    "\n",
    "# env\n",
    "env = make_vec_env(\"LunarLander-v2\", n_envs=n_envs, monitor_dir=\"log_dir_DQN/\")\n",
    "\n",
    "# instantiate the agent\n",
    "if train_from_file:\n",
    "  model = DQN.load(path=\"log_dir_DQN/best_model.zip\", env=env)\n",
    "else:\n",
    "  model = DQN(\n",
    "      policy,\n",
    "      env,\n",
    "      learning_rate = learning_rate,\n",
    "      batch_size = batch_size,\n",
    "      buffer_size = buffer_size,\n",
    "      learning_starts = learning_starts,\n",
    "      gamma = gamma,\n",
    "      target_update_interval = target_update_interval,\n",
    "      train_freq = train_freq,\n",
    "      gradient_steps = gradient_steps,\n",
    "      exploration_fraction = exploration_fraction,\n",
    "      exploration_final_eps = exploration_final_eps,\n",
    "      policy_kwargs = dict(net_arch=[256, 256]),\n",
    "      tensorboard_log=\"./TensorBoardLog/\", verbose=1)\n",
    "\n",
    "# train the agent\n",
    "model.learn(total_timesteps=n_timesteps, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1dc8fa",
   "metadata": {
    "id": "5b1dc8fa"
   },
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "366b80a8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "366b80a8",
    "outputId": "ef52cb57-c46f-4a7b-a2df-467e257da243"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def moving_average(values, window):\n",
    "    \"\"\"\n",
    "    Smooth values by doing a moving average\n",
    "    :param values: (numpy array)\n",
    "    :param window: (int)\n",
    "    :return: (numpy array)\n",
    "    \"\"\"\n",
    "    weights = np.repeat(1.0, window) / window\n",
    "    return np.convolve(values, weights, 'valid')\n",
    "\n",
    "def plot_results(log_folder, title='Learning Curve'):\n",
    "    \"\"\"\n",
    "    plot the results\n",
    "\n",
    "    :param log_folder: (str) the save location of the results to plot\n",
    "    :param title: (str) the title of the task to plot\n",
    "    \"\"\"\n",
    "\n",
    "    x, y = ts2xy(load_results(log_folder), 'timesteps')\n",
    "    y = moving_average(y, window=100)\n",
    "    # Truncate x\n",
    "    x = x[len(x) - len(y):]\n",
    "    fig = plt.figure(title, figsize=(12,5))\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel('Number of Timesteps')\n",
    "    plt.ylabel('Rewards')\n",
    "    plt.title(title + \" Smoothed DQN\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "plot_results(\"log_dir_DQN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00f2a81",
   "metadata": {
    "id": "b00f2a81"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "815393a0",
   "metadata": {
    "id": "815393a0"
   },
   "outputs": [],
   "source": [
    "env = make_vec_env(\"LunarLander-v2\", n_envs=1, monitor_dir=\"evaluate_log_dir_DQN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63611e6e",
   "metadata": {
    "id": "63611e6e"
   },
   "outputs": [],
   "source": [
    "model = DQN.load(path=\"log_dir_DQN/best_model.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b06e1a3",
   "metadata": {
    "id": "3b06e1a3"
   },
   "source": [
    "#### Stable Baseline 3 Evaluation Function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d4fd326",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9d4fd326",
    "outputId": "350cc4bc-803a-464e-fa02-524191bb612f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean & Std Reward after 10 max run is 138.94286449999998 & 195.6787426956158\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, env,n_eval_episodes=10, render=True, deterministic=True)\n",
    "print(\"Mean & Std Reward after {} max run is {} & {}\".format(10,mean_reward, std_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49c5168",
   "metadata": {
    "id": "e49c5168"
   },
   "source": [
    "# GIF of a Train Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60cc63dc",
   "metadata": {
    "id": "60cc63dc"
   },
   "outputs": [],
   "source": [
    "env = make_vec_env(\"LunarLander-v2\", n_envs=1)\n",
    "model = DQN.load(path=\"log_dir_DQN/best_model.zip\")\n",
    "\n",
    "images = []\n",
    "obs = env.reset()\n",
    "img = env.render(mode=\"rgb_array\")\n",
    "for i in range(1000):\n",
    "    images.append(img)\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, _, _ ,_ = env.step(action)\n",
    "    img = env.render(mode=\"rgb_array\")\n",
    "\n",
    "imageio.mimsave(\"lunar lander_DQN.gif\", [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afc060b",
   "metadata": {
    "id": "3afc060b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "857970f990130bbcaee778cf1846f7875676d945310dca1379fe4b5ef3d258a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
