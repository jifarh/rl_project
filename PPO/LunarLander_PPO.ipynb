{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "57601915",
      "metadata": {
        "id": "57601915",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f13b230-139a-482d-bff6-186e955d592a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting shap\n",
            "  Downloading shap-0.45.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (538 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m538.2/538.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (1.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.66.2)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (24.0)\n",
            "Collecting slicer==0.0.7 (from shap)\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.58.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.41.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2023.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (3.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->shap) (1.16.0)\n",
            "Installing collected packages: slicer, shap\n",
            "Successfully installed shap-0.45.0 slicer-0.0.7\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.25.2)\n",
            "Collecting swig\n",
            "  Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.2.1\n",
            "Collecting Box2D\n",
            "  Downloading Box2D-2.3.2.tar.gz (427 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.9/427.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: Box2D\n",
            "  Building wheel for Box2D (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Box2D: filename=Box2D-2.3.2-cp310-cp310-linux_x86_64.whl size=2394337 sha256=7aac11b30a86db18d6a5a46176da3a3d3fc8bf13be670080b7e89ef33116e300\n",
            "  Stored in directory: /root/.cache/pip/wheels/eb/cb/be/e663f3ce9aba6580611c0febaf7cd3cf7603f87047de2a52f9\n",
            "Successfully built Box2D\n",
            "Installing collected packages: Box2D\n",
            "Successfully installed Box2D-2.3.2\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
            "Collecting pyglet==1.5.27\n",
            "  Downloading pyglet-1.5.27-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyglet\n",
            "Successfully installed pyglet-1.5.27\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement stable-baseline3 (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for stable-baseline3\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting gymnasium[all]\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (4.10.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[all])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting shimmy[atari]<1.0,>=0.1.0 (from gymnasium[all])\n",
            "  Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[all])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (2.5.2)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (4.2.1)\n",
            "Collecting mujoco-py<2.2,>=2.1 (from gymnasium[all])\n",
            "  Downloading mujoco_py-2.1.2.14-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cython<3 (from gymnasium[all])\n",
            "  Downloading Cython-0.29.37-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mujoco>=2.3.3 (from gymnasium[all])\n",
            "  Downloading mujoco-3.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (2.31.6)\n",
            "Requirement already satisfied: jax>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (0.4.23)\n",
            "Requirement already satisfied: jaxlib>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (0.4.23+cuda12.cudnn89)\n",
            "Collecting lz4>=3.1.0 (from gymnasium[all])\n",
            "  Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python>=3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (4.8.0.76)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (3.7.1)\n",
            "Requirement already satisfied: moviepy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (1.0.3)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (2.2.1+cu121)\n",
            "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio>=2.14.1->gymnasium[all]) (9.4.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.0->gymnasium[all]) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.0->gymnasium[all]) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.0->gymnasium[all]) (1.11.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gymnasium[all]) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gymnasium[all]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gymnasium[all]) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gymnasium[all]) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gymnasium[all]) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gymnasium[all]) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gymnasium[all]) (2.8.2)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy>=1.0.0->gymnasium[all]) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy>=1.0.0->gymnasium[all]) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy>=1.0.0->gymnasium[all]) (2.31.0)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy>=1.0.0->gymnasium[all]) (0.1.10)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy>=1.0.0->gymnasium[all]) (0.4.9)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.3.3->gymnasium[all]) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.3.3->gymnasium[all]) (1.7.0)\n",
            "Collecting glfw (from mujoco>=2.3.3->gymnasium[all])\n",
            "  Downloading glfw-2.7.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (211 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.8/211.8 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyopengl in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.3.3->gymnasium[all]) (3.1.7)\n",
            "Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.10/dist-packages (from mujoco-py<2.2,>=2.1->gymnasium[all]) (1.16.0)\n",
            "Collecting fasteners~=0.15 (from mujoco-py<2.2,>=2.1->gymnasium[all])\n",
            "  Downloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0->gymnasium[all])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->gymnasium[all]) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->gymnasium[all]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->gymnasium[all]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->gymnasium[all]) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->gymnasium[all]) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.0.0->gymnasium[all])\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.0.0->gymnasium[all])\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.0.0->gymnasium[all])\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.0.0->gymnasium[all])\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.0.0->gymnasium[all])\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.0.0->gymnasium[all])\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.0.0->gymnasium[all])\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.0.0->gymnasium[all])\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.0.0->gymnasium[all])\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.0.0->gymnasium[all])\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.0.0->gymnasium[all])\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->gymnasium[all]) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->gymnasium[all])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0->gymnasium[all]) (6.3.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.10->mujoco-py<2.2,>=2.1->gymnasium[all]) (2.21)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy>=1.0.0->gymnasium[all]) (67.7.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->gymnasium[all]) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[all]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[all]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[all]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[all]) (2024.2.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[all]) (3.18.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->gymnasium[all]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->gymnasium[all]) (1.3.0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2376096 sha256=e0f843e8fdfd4d0cc4df3db992e37f30b828d50f8c27acdd94b54488825face2\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: glfw, farama-notifications, box2d-py, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lz4, gymnasium, fasteners, cython, ale-py, shimmy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, mujoco-py, nvidia-cusolver-cu12, mujoco\n",
            "  Attempting uninstall: cython\n",
            "    Found existing installation: Cython 3.0.9\n",
            "    Uninstalling Cython-3.0.9:\n",
            "      Successfully uninstalled Cython-3.0.9\n",
            "Successfully installed ale-py-0.8.1 box2d-py-2.3.5 cython-0.29.37 farama-notifications-0.0.4 fasteners-0.19 glfw-2.7.0 gymnasium-0.29.1 lz4-4.3.3 mujoco-3.1.3 mujoco-py-2.1.2.14 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 shimmy-0.2.1\n",
            "Collecting stable_baselines3\n",
            "  Downloading stable_baselines3-2.2.1-py3-none-any.whl (181 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.7/181.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.25.2)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.2.1+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (4.10.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->stable_baselines3) (12.4.99)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable_baselines3) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable_baselines3) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable_baselines3) (1.3.0)\n",
            "Installing collected packages: stable_baselines3\n",
            "Successfully installed stable_baselines3-2.2.1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install shap\n",
        "!pip install opencv-python\n",
        "!pip install swig\n",
        "!pip install Box2D\n",
        "\n",
        "\n",
        "# !pip install box2d pygame\n",
        "\n",
        "\n",
        "!pip install gym\n",
        "!pip install pyglet==1.5.27\n",
        "!pip install stable-baseline3\n",
        "!pip install \"gymnasium[all]\"\n",
        "\n",
        "!pip install stable_baselines3\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "PtTyQewr3gxt"
      },
      "id": "PtTyQewr3gxt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b00a128f",
      "metadata": {
        "id": "b00a128f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import imageio\n",
        "import os\n",
        "from stable_baselines3 import PPO, A2C\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack, SubprocVecEnv, VecNormalize\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common import results_plotter\n",
        "import gymnasium  as gym\n",
        "import matplotlib.pyplot as plt\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "import scipy.stats as stats\n",
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "import tensorflow as tf\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# seeds\n",
        "# Set seed for numpy\n",
        "np.random.seed(100)\n",
        "\n",
        "# Set seed for Python random module\n",
        "import random\n",
        "random.seed(100)\n",
        "\n",
        "# Set seed for TensorFlow\n",
        "tf.random.set_seed(100)\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "if tf.test.gpu_device_name():\n",
        "    print('Default GPU Device:', tf.test.gpu_device_name())\n",
        "else:\n",
        "    print(\"GPU not found. Please ensure that GPU is enabled in Colab.\")"
      ],
      "metadata": {
        "id": "YtZN-eC7NwuS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e49a85c-8359-45d8-e667-d7a4980aaa72"
      },
      "id": "YtZN-eC7NwuS",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n",
            "GPU not found. Please ensure that GPU is enabled in Colab.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "780afb92",
      "metadata": {
        "id": "780afb92"
      },
      "source": [
        "<h1> Important Libraries To Install </h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2826cd85",
      "metadata": {
        "id": "2826cd85"
      },
      "source": [
        "<h1> Parameter & Environment Information </h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87ef75ca",
      "metadata": {
        "id": "87ef75ca"
      },
      "source": [
        "<p>\n",
        "    This environment is part of the Box2D environments.\n",
        "</p>\n",
        "\n",
        "<ul>\n",
        "    <li> Action Space Discrete(4) </li>\n",
        "    <li> Observation Shape (8,) </li>\n",
        "    <li> Observation High [1.5 1.5 5. 5. 3.14 5. 1. 1. ] </li>\n",
        "    <li> Observation Low [-1.5 -1.5 -5. -5. -3.14 -5. -0. -0. ] </li>\n",
        "    <li> Import gymnasium.make(\"LunarLander-v2\") </li>\n",
        "</ul>\n",
        "\n",
        "<h3> Description </h3>\n",
        "<p>This environment is a classic rocket trajectory optimization problem. According to Pontryagin’s maximum principle, it is optimal to fire the engine at full throttle or turn it off. This is the reason why this environment has discrete actions: engine on or off.\n",
        "\n",
        "There are two environment versions: discrete or continuous. The landing pad is always at coordinates (0,0). The coordinates are the first two numbers in the state vector. Landing outside of the landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt.</p>\n",
        "\n",
        "<h3> Action Space </h3>\n",
        "<p>\n",
        "There are four discrete actions available:\n",
        "\n",
        "* 0: do nothing\n",
        "* 1: fire left orientation engine\n",
        "* 2: fire main engine\n",
        "* 3: fire right orientation engine\n",
        "\n",
        "</p>\n",
        "\n",
        "<h3> Observation Space </h3>\n",
        "<p>\n",
        "The state is an 8-dimensional vector: the coordinates of the lander in x & y, its linear velocities in x & y, its angle, its angular velocity, and two booleans that represent whether each leg is in contact with the ground or not.\n",
        "</p>\n",
        "\n",
        "<h3> Reward </h3>\n",
        "<p>\n",
        "After every step a reward is granted. The total reward of an episode is the sum of the rewards for all the steps within that episode.\n",
        "\n",
        "For each step, the reward:\n",
        "\n",
        "* is increased/decreased the closer/further the lander is to the landing pad.\n",
        "* is increased/decreased the slower/faster the lander is moving.\n",
        "* is decreased the more the lander is tilted (angle not horizontal).\n",
        "* is increased by 10 points for each leg that is in contact with the ground.\n",
        "* is decreased by 0.03 points each frame a side engine is firing.\n",
        "* is decreased by 0.3 points each frame the main engine is firing.\n",
        "\n",
        "The episode receive an additional reward of -100 or +100 points for crashing or landing safely respectively.\n",
        "\n",
        "An episode is considered a solution if it scores at least 200 points.\n",
        "</p>\n",
        "\n",
        "<h3> Starting State </h3>\n",
        "\n",
        "<p>The lander starts at the top center of the viewport with a random initial force applied to its center of mass.</p>\n",
        "\n",
        "<h3> Episode Termination </h3>\n",
        "<p> The episode finishes if:<br>\n",
        "    \n",
        "1. the lander crashes (the lander body gets in contact with the moon);<br>\n",
        "2. the lander gets outside of the viewport (x coordinate is greater than 1);<br>\n",
        "3. the lander is not awake. From the Box2D docs, a body which is not awake is a body which doesn’t move and doesn’t collide with any other body:<br>\n",
        "\n",
        "When Box2D determines that a body (or group of bodies) has come to rest, the body enters a sleep state which has very little CPU overhead. If a body is awake and collides with a sleeping body, then the sleeping body wakes up. Bodies will also wake up if a joint or contact attached to them is destroyed.\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3d0543a5",
      "metadata": {
        "scrolled": true,
        "id": "3d0543a5"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b3fb45b2",
      "metadata": {
        "id": "b3fb45b2",
        "outputId": "f81eb084-19b4-4f51-f4f4-c82607cc6cb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Action inter is descrete 4\n",
            "Shape of Observation is (8,)\n"
          ]
        }
      ],
      "source": [
        "print(\"The Action inter is descrete {}\".format(env.action_space.n))\n",
        "print(\"Shape of Observation is {}\".format(env.observation_space.sample().shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b9ff9c9",
      "metadata": {
        "id": "8b9ff9c9"
      },
      "source": [
        "<h1> Baseline Model. </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d35101af",
      "metadata": {
        "scrolled": true,
        "id": "d35101af",
        "outputId": "94c9ecca-7b72-44d5-de3b-2c535bf0a5ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Reward after 10 max run is -3.3459560991625956\n"
          ]
        }
      ],
      "source": [
        "rewards = []\n",
        "obs = env.reset()\n",
        "done = False\n",
        "MAX_RUN = 10\n",
        "\n",
        "for i in range(MAX_RUN):\n",
        "    while not done:\n",
        "        env.render()\n",
        "        action_sample = env.action_space.sample()\n",
        "        # let's take a step in the environment\n",
        "        obs, rwd, done, info ,_  = env.step(action_sample)\n",
        "        rewards.append(rwd)\n",
        "env.close()\n",
        "print(\"Mean Reward after {} max run is {}\".format(MAX_RUN, np.mean(np.array(rewards))))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac737551",
      "metadata": {
        "id": "ac737551"
      },
      "source": [
        "<h1> Reinforcement Learning For Training The Model </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "bdaa0e55",
      "metadata": {
        "id": "bdaa0e55"
      },
      "outputs": [],
      "source": [
        "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
        "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
        "\n",
        "    :param check_freq: (int)\n",
        "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
        "      It must contains the file created by the ``Monitor`` wrapper.\n",
        "    :param verbose: (int)\n",
        "    \"\"\"\n",
        "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
        "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
        "        self.check_freq = check_freq\n",
        "        self.log_dir = log_dir\n",
        "        self.save_path = os.path.join(log_dir, 'best_model')\n",
        "        self.best_mean_reward = -np.inf\n",
        "\n",
        "    def _init_callback(self) -> None:\n",
        "        # Create folder if needed\n",
        "        if self.save_path is not None:\n",
        "            os.makedirs(self.save_path, exist_ok=True)\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.n_calls % self.check_freq == 0:\n",
        "\n",
        "          # Retrieve training reward\n",
        "          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
        "          if len(x) > 0:\n",
        "              # Mean training reward over the last 100 episodes\n",
        "              mean_reward = np.mean(y[-100:])\n",
        "              if self.verbose > 0:\n",
        "                print(f\"Num timesteps: {self.num_timesteps}\")\n",
        "                print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n",
        "\n",
        "              # New best model, you could save the agent here\n",
        "              if mean_reward > self.best_mean_reward:\n",
        "                  self.best_mean_reward = mean_reward\n",
        "                  # Example for saving best model\n",
        "                  if self.verbose > 0:\n",
        "                    print(f\"Saving new best model to {self.save_path}.zip\")\n",
        "                  self.model.save(self.save_path)\n",
        "\n",
        "        return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "2d311d6a",
      "metadata": {
        "scrolled": true,
        "id": "2d311d6a",
        "outputId": "3c960c90-bbc7-4506-a186-3928d014cd7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "Logging to ./TensorBoardLog/PPO_1\n",
            "Num timesteps: 16000\n",
            "Best mean reward: -inf - Last mean reward per episode: -185.35\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 91       |\n",
            "|    ep_rew_mean     | -183     |\n",
            "| time/              |          |\n",
            "|    fps             | 3000     |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 5        |\n",
            "|    total_timesteps | 16384    |\n",
            "---------------------------------\n",
            "Num timesteps: 32000\n",
            "Best mean reward: -185.35 - Last mean reward per episode: -159.05\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 93           |\n",
            "|    ep_rew_mean          | -157         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 2052         |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 15           |\n",
            "|    total_timesteps      | 32768        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0057414332 |\n",
            "|    clip_fraction        | 0.0265       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | -0.000165    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.08e+03     |\n",
            "|    n_updates            | 4            |\n",
            "|    policy_gradient_loss | -0.00478     |\n",
            "|    value_loss           | 4.28e+03     |\n",
            "------------------------------------------\n",
            "Num timesteps: 48000\n",
            "Best mean reward: -159.05 - Last mean reward per episode: -136.80\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 95.6        |\n",
            "|    ep_rew_mean          | -131        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1939        |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 25          |\n",
            "|    total_timesteps      | 49152       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008045603 |\n",
            "|    clip_fraction        | 0.049       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.37       |\n",
            "|    explained_variance   | -0.00388    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 793         |\n",
            "|    n_updates            | 8           |\n",
            "|    policy_gradient_loss | -0.00483    |\n",
            "|    value_loss           | 2.94e+03    |\n",
            "-----------------------------------------\n",
            "Num timesteps: 64000\n",
            "Best mean reward: -136.80 - Last mean reward per episode: -133.19\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 103         |\n",
            "|    ep_rew_mean          | -132        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1860        |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 35          |\n",
            "|    total_timesteps      | 65536       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008622939 |\n",
            "|    clip_fraction        | 0.0605      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.36       |\n",
            "|    explained_variance   | -0.00168    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 441         |\n",
            "|    n_updates            | 12          |\n",
            "|    policy_gradient_loss | -0.00671    |\n",
            "|    value_loss           | 1.39e+03    |\n",
            "-----------------------------------------\n",
            "Num timesteps: 80000\n",
            "Best mean reward: -133.19 - Last mean reward per episode: -110.17\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 97.2         |\n",
            "|    ep_rew_mean          | -111         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1777         |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 46           |\n",
            "|    total_timesteps      | 81920        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0069988733 |\n",
            "|    clip_fraction        | 0.0805       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.35        |\n",
            "|    explained_variance   | -0.000479    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 499          |\n",
            "|    n_updates            | 16           |\n",
            "|    policy_gradient_loss | -0.00485     |\n",
            "|    value_loss           | 893          |\n",
            "------------------------------------------\n",
            "Num timesteps: 96000\n",
            "Best mean reward: -110.17 - Last mean reward per episode: -95.56\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 99           |\n",
            "|    ep_rew_mean          | -94.7        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1721         |\n",
            "|    iterations           | 6            |\n",
            "|    time_elapsed         | 57           |\n",
            "|    total_timesteps      | 98304        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0047659827 |\n",
            "|    clip_fraction        | 0.063        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.35        |\n",
            "|    explained_variance   | -0.000411    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 226          |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00397     |\n",
            "|    value_loss           | 643          |\n",
            "------------------------------------------\n",
            "Num timesteps: 112000\n",
            "Best mean reward: -95.56 - Last mean reward per episode: -83.15\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 109         |\n",
            "|    ep_rew_mean          | -82.8       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1667        |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 68          |\n",
            "|    total_timesteps      | 114688      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008788282 |\n",
            "|    clip_fraction        | 0.0487      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.34       |\n",
            "|    explained_variance   | -8.7e-06    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 134         |\n",
            "|    n_updates            | 24          |\n",
            "|    policy_gradient_loss | -0.00587    |\n",
            "|    value_loss           | 328         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 128000\n",
            "Best mean reward: -83.15 - Last mean reward per episode: -59.35\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 117         |\n",
            "|    ep_rew_mean          | -60.3       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1651        |\n",
            "|    iterations           | 8           |\n",
            "|    time_elapsed         | 79          |\n",
            "|    total_timesteps      | 131072      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008327958 |\n",
            "|    clip_fraction        | 0.0546      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.3        |\n",
            "|    explained_variance   | -1.91e-06   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 191         |\n",
            "|    n_updates            | 28          |\n",
            "|    policy_gradient_loss | -0.00469    |\n",
            "|    value_loss           | 313         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 144000\n",
            "Best mean reward: -59.35 - Last mean reward per episode: -35.56\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 122         |\n",
            "|    ep_rew_mean          | -35.7       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1630        |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 90          |\n",
            "|    total_timesteps      | 147456      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010724522 |\n",
            "|    clip_fraction        | 0.0885      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.27       |\n",
            "|    explained_variance   | -4.77e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 193         |\n",
            "|    n_updates            | 32          |\n",
            "|    policy_gradient_loss | -0.00567    |\n",
            "|    value_loss           | 291         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 160000\n",
            "Best mean reward: -35.56 - Last mean reward per episode: -29.20\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 166         |\n",
            "|    ep_rew_mean          | -22.2       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1565        |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 104         |\n",
            "|    total_timesteps      | 163840      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009866448 |\n",
            "|    clip_fraction        | 0.0464      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.24       |\n",
            "|    explained_variance   | -1.07e-06   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 137         |\n",
            "|    n_updates            | 36          |\n",
            "|    policy_gradient_loss | -0.00325    |\n",
            "|    value_loss           | 371         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 176000\n",
            "Best mean reward: -29.20 - Last mean reward per episode: -21.99\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 166         |\n",
            "|    ep_rew_mean          | -22.5       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1503        |\n",
            "|    iterations           | 11          |\n",
            "|    time_elapsed         | 119         |\n",
            "|    total_timesteps      | 180224      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010152387 |\n",
            "|    clip_fraction        | 0.0678      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.2        |\n",
            "|    explained_variance   | 1.7e-05     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 156         |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00423    |\n",
            "|    value_loss           | 365         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 192000\n",
            "Best mean reward: -21.99 - Last mean reward per episode: -13.66\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 172         |\n",
            "|    ep_rew_mean          | -15.6       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1462        |\n",
            "|    iterations           | 12          |\n",
            "|    time_elapsed         | 134         |\n",
            "|    total_timesteps      | 196608      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005100731 |\n",
            "|    clip_fraction        | 0.0301      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.17       |\n",
            "|    explained_variance   | -0.00221    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 263         |\n",
            "|    n_updates            | 44          |\n",
            "|    policy_gradient_loss | -0.00192    |\n",
            "|    value_loss           | 512         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 208000\n",
            "Best mean reward: -13.66 - Last mean reward per episode: -15.31\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 179         |\n",
            "|    ep_rew_mean          | -10.3       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1412        |\n",
            "|    iterations           | 13          |\n",
            "|    time_elapsed         | 150         |\n",
            "|    total_timesteps      | 212992      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006432354 |\n",
            "|    clip_fraction        | 0.0379      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.14       |\n",
            "|    explained_variance   | 0.000858    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 377         |\n",
            "|    n_updates            | 48          |\n",
            "|    policy_gradient_loss | -0.0012     |\n",
            "|    value_loss           | 619         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 224000\n",
            "Best mean reward: -13.66 - Last mean reward per episode: -1.84\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 205         |\n",
            "|    ep_rew_mean          | 0.199       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1371        |\n",
            "|    iterations           | 14          |\n",
            "|    time_elapsed         | 167         |\n",
            "|    total_timesteps      | 229376      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004470762 |\n",
            "|    clip_fraction        | 0.0154      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.16       |\n",
            "|    explained_variance   | -0.000195   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 372         |\n",
            "|    n_updates            | 52          |\n",
            "|    policy_gradient_loss | -0.00111    |\n",
            "|    value_loss           | 690         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 240000\n",
            "Best mean reward: -1.84 - Last mean reward per episode: 3.77\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 268         |\n",
            "|    ep_rew_mean          | 5.56        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1290        |\n",
            "|    iterations           | 15          |\n",
            "|    time_elapsed         | 190         |\n",
            "|    total_timesteps      | 245760      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006343517 |\n",
            "|    clip_fraction        | 0.0154      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.15       |\n",
            "|    explained_variance   | 0.0556      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 235         |\n",
            "|    n_updates            | 56          |\n",
            "|    policy_gradient_loss | -0.00293    |\n",
            "|    value_loss           | 618         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 256000\n",
            "Best mean reward: 3.77 - Last mean reward per episode: 11.42\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 348         |\n",
            "|    ep_rew_mean          | 14.8        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1228        |\n",
            "|    iterations           | 16          |\n",
            "|    time_elapsed         | 213         |\n",
            "|    total_timesteps      | 262144      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005860704 |\n",
            "|    clip_fraction        | 0.037       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.2        |\n",
            "|    explained_variance   | 0.224       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 203         |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.00263    |\n",
            "|    value_loss           | 392         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 272000\n",
            "Best mean reward: 11.42 - Last mean reward per episode: 21.95\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 431         |\n",
            "|    ep_rew_mean          | 26.5        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1148        |\n",
            "|    iterations           | 17          |\n",
            "|    time_elapsed         | 242         |\n",
            "|    total_timesteps      | 278528      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009714868 |\n",
            "|    clip_fraction        | 0.0755      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.18       |\n",
            "|    explained_variance   | 0.383       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 122         |\n",
            "|    n_updates            | 64          |\n",
            "|    policy_gradient_loss | -0.00343    |\n",
            "|    value_loss           | 344         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 288000\n",
            "Best mean reward: 21.95 - Last mean reward per episode: 34.65\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 537          |\n",
            "|    ep_rew_mean          | 34.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1093         |\n",
            "|    iterations           | 18           |\n",
            "|    time_elapsed         | 269          |\n",
            "|    total_timesteps      | 294912       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0060022604 |\n",
            "|    clip_fraction        | 0.0291       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.2         |\n",
            "|    explained_variance   | 0.543        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 119          |\n",
            "|    n_updates            | 68           |\n",
            "|    policy_gradient_loss | -0.00145     |\n",
            "|    value_loss           | 195          |\n",
            "------------------------------------------\n",
            "Num timesteps: 304000\n",
            "Best mean reward: 34.65 - Last mean reward per episode: 37.18\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 624          |\n",
            "|    ep_rew_mean          | 43.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1035         |\n",
            "|    iterations           | 19           |\n",
            "|    time_elapsed         | 300          |\n",
            "|    total_timesteps      | 311296       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0054399697 |\n",
            "|    clip_fraction        | 0.0374       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.17        |\n",
            "|    explained_variance   | 0.712        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 82.6         |\n",
            "|    n_updates            | 72           |\n",
            "|    policy_gradient_loss | -0.00209     |\n",
            "|    value_loss           | 164          |\n",
            "------------------------------------------\n",
            "Num timesteps: 320000\n",
            "Best mean reward: 37.18 - Last mean reward per episode: 50.03\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 723         |\n",
            "|    ep_rew_mean          | 55.8        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 988         |\n",
            "|    iterations           | 20          |\n",
            "|    time_elapsed         | 331         |\n",
            "|    total_timesteps      | 327680      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007872209 |\n",
            "|    clip_fraction        | 0.0671      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.18       |\n",
            "|    explained_variance   | 0.895       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.92        |\n",
            "|    n_updates            | 76          |\n",
            "|    policy_gradient_loss | -0.00399    |\n",
            "|    value_loss           | 63.2        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 336000\n",
            "Best mean reward: 50.03 - Last mean reward per episode: 57.74\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 810         |\n",
            "|    ep_rew_mean          | 62.6        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 944         |\n",
            "|    iterations           | 21          |\n",
            "|    time_elapsed         | 364         |\n",
            "|    total_timesteps      | 344064      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005826464 |\n",
            "|    clip_fraction        | 0.0397      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.15       |\n",
            "|    explained_variance   | 0.911       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 34.9        |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.00136    |\n",
            "|    value_loss           | 60.4        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 352000\n",
            "Best mean reward: 57.74 - Last mean reward per episode: 60.29\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 879          |\n",
            "|    ep_rew_mean          | 64.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 906          |\n",
            "|    iterations           | 22           |\n",
            "|    time_elapsed         | 397          |\n",
            "|    total_timesteps      | 360448       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0052797487 |\n",
            "|    clip_fraction        | 0.0457       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.15        |\n",
            "|    explained_variance   | 0.897        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 29           |\n",
            "|    n_updates            | 84           |\n",
            "|    policy_gradient_loss | -0.00201     |\n",
            "|    value_loss           | 79.3         |\n",
            "------------------------------------------\n",
            "Num timesteps: 368000\n",
            "Best mean reward: 60.29 - Last mean reward per episode: 63.24\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 874         |\n",
            "|    ep_rew_mean          | 62.9        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 876         |\n",
            "|    iterations           | 23          |\n",
            "|    time_elapsed         | 429         |\n",
            "|    total_timesteps      | 376832      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007251812 |\n",
            "|    clip_fraction        | 0.0405      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.13       |\n",
            "|    explained_variance   | 0.885       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 121         |\n",
            "|    n_updates            | 88          |\n",
            "|    policy_gradient_loss | -0.00201    |\n",
            "|    value_loss           | 96.9        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 384000\n",
            "Best mean reward: 63.24 - Last mean reward per episode: 67.05\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 913         |\n",
            "|    ep_rew_mean          | 76.6        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 856         |\n",
            "|    iterations           | 24          |\n",
            "|    time_elapsed         | 458         |\n",
            "|    total_timesteps      | 393216      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006050801 |\n",
            "|    clip_fraction        | 0.03        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.1        |\n",
            "|    explained_variance   | 0.853       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 197         |\n",
            "|    n_updates            | 92          |\n",
            "|    policy_gradient_loss | -0.00173    |\n",
            "|    value_loss           | 124         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 400000\n",
            "Best mean reward: 67.05 - Last mean reward per episode: 78.06\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 912         |\n",
            "|    ep_rew_mean          | 77.1        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 832         |\n",
            "|    iterations           | 25          |\n",
            "|    time_elapsed         | 491         |\n",
            "|    total_timesteps      | 409600      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006605743 |\n",
            "|    clip_fraction        | 0.0649      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.09       |\n",
            "|    explained_variance   | 0.915       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 113         |\n",
            "|    n_updates            | 96          |\n",
            "|    policy_gradient_loss | -0.00272    |\n",
            "|    value_loss           | 61.3        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 416000\n",
            "Best mean reward: 78.06 - Last mean reward per episode: 77.57\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 890         |\n",
            "|    ep_rew_mean          | 75.2        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 823         |\n",
            "|    iterations           | 26          |\n",
            "|    time_elapsed         | 517         |\n",
            "|    total_timesteps      | 425984      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004824112 |\n",
            "|    clip_fraction        | 0.0329      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.1        |\n",
            "|    explained_variance   | 0.946       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 14.5        |\n",
            "|    n_updates            | 100         |\n",
            "|    policy_gradient_loss | -0.00161    |\n",
            "|    value_loss           | 40.7        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 432000\n",
            "Best mean reward: 78.06 - Last mean reward per episode: 74.29\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 855         |\n",
            "|    ep_rew_mean          | 77.4        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 813         |\n",
            "|    iterations           | 27          |\n",
            "|    time_elapsed         | 543         |\n",
            "|    total_timesteps      | 442368      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007827729 |\n",
            "|    clip_fraction        | 0.0511      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.05       |\n",
            "|    explained_variance   | 0.8         |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 53.4        |\n",
            "|    n_updates            | 104         |\n",
            "|    policy_gradient_loss | -0.00124    |\n",
            "|    value_loss           | 198         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 448000\n",
            "Best mean reward: 78.06 - Last mean reward per episode: 78.26\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 861          |\n",
            "|    ep_rew_mean          | 82.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 799          |\n",
            "|    iterations           | 28           |\n",
            "|    time_elapsed         | 574          |\n",
            "|    total_timesteps      | 458752       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041907327 |\n",
            "|    clip_fraction        | 0.024        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.05        |\n",
            "|    explained_variance   | 0.868        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 127          |\n",
            "|    n_updates            | 108          |\n",
            "|    policy_gradient_loss | -0.000379    |\n",
            "|    value_loss           | 121          |\n",
            "------------------------------------------\n",
            "Num timesteps: 464000\n",
            "Best mean reward: 78.26 - Last mean reward per episode: 84.10\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 870          |\n",
            "|    ep_rew_mean          | 87.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 788          |\n",
            "|    iterations           | 29           |\n",
            "|    time_elapsed         | 602          |\n",
            "|    total_timesteps      | 475136       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0059407107 |\n",
            "|    clip_fraction        | 0.0352       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.08        |\n",
            "|    explained_variance   | 0.947        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 11.4         |\n",
            "|    n_updates            | 112          |\n",
            "|    policy_gradient_loss | -0.00145     |\n",
            "|    value_loss           | 38.6         |\n",
            "------------------------------------------\n",
            "Num timesteps: 480000\n",
            "Best mean reward: 84.10 - Last mean reward per episode: 86.11\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 857          |\n",
            "|    ep_rew_mean          | 87.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 780          |\n",
            "|    iterations           | 30           |\n",
            "|    time_elapsed         | 629          |\n",
            "|    total_timesteps      | 491520       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041234344 |\n",
            "|    clip_fraction        | 0.0475       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.07        |\n",
            "|    explained_variance   | 0.973        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.67         |\n",
            "|    n_updates            | 116          |\n",
            "|    policy_gradient_loss | -0.00101     |\n",
            "|    value_loss           | 12.3         |\n",
            "------------------------------------------\n",
            "Num timesteps: 496000\n",
            "Best mean reward: 86.11 - Last mean reward per episode: 87.07\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 859         |\n",
            "|    ep_rew_mean          | 95.8        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 771         |\n",
            "|    iterations           | 31          |\n",
            "|    time_elapsed         | 658         |\n",
            "|    total_timesteps      | 507904      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004934959 |\n",
            "|    clip_fraction        | 0.0263      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.03       |\n",
            "|    explained_variance   | 0.945       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 35.1        |\n",
            "|    n_updates            | 120         |\n",
            "|    policy_gradient_loss | -0.000943   |\n",
            "|    value_loss           | 44.3        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 512000\n",
            "Best mean reward: 87.07 - Last mean reward per episode: 98.16\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 901         |\n",
            "|    ep_rew_mean          | 106         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 761         |\n",
            "|    iterations           | 32          |\n",
            "|    time_elapsed         | 688         |\n",
            "|    total_timesteps      | 524288      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005286401 |\n",
            "|    clip_fraction        | 0.0454      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.04       |\n",
            "|    explained_variance   | 0.927       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.25        |\n",
            "|    n_updates            | 124         |\n",
            "|    policy_gradient_loss | -0.000999   |\n",
            "|    value_loss           | 49.6        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 528000\n",
            "Best mean reward: 98.16 - Last mean reward per episode: 111.50\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 932        |\n",
            "|    ep_rew_mean          | 116        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 753        |\n",
            "|    iterations           | 33         |\n",
            "|    time_elapsed         | 717        |\n",
            "|    total_timesteps      | 540672     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00654734 |\n",
            "|    clip_fraction        | 0.0549     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.05      |\n",
            "|    explained_variance   | 0.971      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 2.24       |\n",
            "|    n_updates            | 128        |\n",
            "|    policy_gradient_loss | -0.000866  |\n",
            "|    value_loss           | 21.7       |\n",
            "----------------------------------------\n",
            "Num timesteps: 544000\n",
            "Best mean reward: 111.50 - Last mean reward per episode: 116.66\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 953          |\n",
            "|    ep_rew_mean          | 119          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 745          |\n",
            "|    iterations           | 34           |\n",
            "|    time_elapsed         | 746          |\n",
            "|    total_timesteps      | 557056       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0042085843 |\n",
            "|    clip_fraction        | 0.0379       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.04        |\n",
            "|    explained_variance   | 0.958        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.46         |\n",
            "|    n_updates            | 132          |\n",
            "|    policy_gradient_loss | -0.000726    |\n",
            "|    value_loss           | 31.3         |\n",
            "------------------------------------------\n",
            "Num timesteps: 560000\n",
            "Best mean reward: 116.66 - Last mean reward per episode: 120.53\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 953         |\n",
            "|    ep_rew_mean          | 122         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 739         |\n",
            "|    iterations           | 35          |\n",
            "|    time_elapsed         | 775         |\n",
            "|    total_timesteps      | 573440      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004999685 |\n",
            "|    clip_fraction        | 0.0414      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.03       |\n",
            "|    explained_variance   | 0.986       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.82        |\n",
            "|    n_updates            | 136         |\n",
            "|    policy_gradient_loss | -0.000643   |\n",
            "|    value_loss           | 8.41        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 576000\n",
            "Best mean reward: 120.53 - Last mean reward per episode: 123.87\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 966         |\n",
            "|    ep_rew_mean          | 128         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 734         |\n",
            "|    iterations           | 36          |\n",
            "|    time_elapsed         | 802         |\n",
            "|    total_timesteps      | 589824      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005947454 |\n",
            "|    clip_fraction        | 0.0396      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.03       |\n",
            "|    explained_variance   | 0.989       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.41        |\n",
            "|    n_updates            | 140         |\n",
            "|    policy_gradient_loss | 6.93e-05    |\n",
            "|    value_loss           | 7.75        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 592000\n",
            "Best mean reward: 123.87 - Last mean reward per episode: 128.76\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 948          |\n",
            "|    ep_rew_mean          | 126          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 724          |\n",
            "|    iterations           | 37           |\n",
            "|    time_elapsed         | 836          |\n",
            "|    total_timesteps      | 606208       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0072713117 |\n",
            "|    clip_fraction        | 0.0361       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.03        |\n",
            "|    explained_variance   | 0.982        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.63         |\n",
            "|    n_updates            | 144          |\n",
            "|    policy_gradient_loss | -0.0013      |\n",
            "|    value_loss           | 18.2         |\n",
            "------------------------------------------\n",
            "Num timesteps: 608000\n",
            "Best mean reward: 128.76 - Last mean reward per episode: 123.92\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 955         |\n",
            "|    ep_rew_mean          | 127         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 721         |\n",
            "|    iterations           | 38          |\n",
            "|    time_elapsed         | 863         |\n",
            "|    total_timesteps      | 622592      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003536899 |\n",
            "|    clip_fraction        | 0.0211      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.984      |\n",
            "|    explained_variance   | 0.962       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.64        |\n",
            "|    n_updates            | 148         |\n",
            "|    policy_gradient_loss | -0.00113    |\n",
            "|    value_loss           | 43.7        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 624000\n",
            "Best mean reward: 128.76 - Last mean reward per episode: 127.40\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 958        |\n",
            "|    ep_rew_mean          | 127        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 717        |\n",
            "|    iterations           | 39         |\n",
            "|    time_elapsed         | 890        |\n",
            "|    total_timesteps      | 638976     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00455683 |\n",
            "|    clip_fraction        | 0.0452     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.994     |\n",
            "|    explained_variance   | 0.985      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 2.95       |\n",
            "|    n_updates            | 152        |\n",
            "|    policy_gradient_loss | -0.000753  |\n",
            "|    value_loss           | 16.1       |\n",
            "----------------------------------------\n",
            "Num timesteps: 640000\n",
            "Best mean reward: 128.76 - Last mean reward per episode: 127.00\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 938          |\n",
            "|    ep_rew_mean          | 122          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 710          |\n",
            "|    iterations           | 40           |\n",
            "|    time_elapsed         | 921          |\n",
            "|    total_timesteps      | 655360       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041631274 |\n",
            "|    clip_fraction        | 0.0458       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.965       |\n",
            "|    explained_variance   | 0.995        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.09         |\n",
            "|    n_updates            | 156          |\n",
            "|    policy_gradient_loss | -0.00127     |\n",
            "|    value_loss           | 4.91         |\n",
            "------------------------------------------\n",
            "Num timesteps: 656000\n",
            "Best mean reward: 128.76 - Last mean reward per episode: 122.12\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 923          |\n",
            "|    ep_rew_mean          | 121          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 706          |\n",
            "|    iterations           | 41           |\n",
            "|    time_elapsed         | 950          |\n",
            "|    total_timesteps      | 671744       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0026576952 |\n",
            "|    clip_fraction        | 0.0238       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.97        |\n",
            "|    explained_variance   | 0.974        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 43.5         |\n",
            "|    n_updates            | 160          |\n",
            "|    policy_gradient_loss | -0.000489    |\n",
            "|    value_loss           | 33.9         |\n",
            "------------------------------------------\n",
            "Num timesteps: 672000\n",
            "Best mean reward: 128.76 - Last mean reward per episode: 121.25\n",
            "Num timesteps: 688000\n",
            "Best mean reward: 128.76 - Last mean reward per episode: 122.76\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 916          |\n",
            "|    ep_rew_mean          | 123          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 704          |\n",
            "|    iterations           | 42           |\n",
            "|    time_elapsed         | 976          |\n",
            "|    total_timesteps      | 688128       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0046689482 |\n",
            "|    clip_fraction        | 0.0371       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.952       |\n",
            "|    explained_variance   | 0.964        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.2          |\n",
            "|    n_updates            | 164          |\n",
            "|    policy_gradient_loss | -8.14e-06    |\n",
            "|    value_loss           | 50.9         |\n",
            "------------------------------------------\n",
            "Num timesteps: 704000\n",
            "Best mean reward: 128.76 - Last mean reward per episode: 128.15\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 936        |\n",
            "|    ep_rew_mean          | 128        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 703        |\n",
            "|    iterations           | 43         |\n",
            "|    time_elapsed         | 1002       |\n",
            "|    total_timesteps      | 704512     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00322677 |\n",
            "|    clip_fraction        | 0.0388     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.94      |\n",
            "|    explained_variance   | 0.98       |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 2.67       |\n",
            "|    n_updates            | 168        |\n",
            "|    policy_gradient_loss | 0.000305   |\n",
            "|    value_loss           | 24.9       |\n",
            "----------------------------------------\n",
            "Num timesteps: 720000\n",
            "Best mean reward: 128.76 - Last mean reward per episode: 127.64\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 921          |\n",
            "|    ep_rew_mean          | 128          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 699          |\n",
            "|    iterations           | 44           |\n",
            "|    time_elapsed         | 1029         |\n",
            "|    total_timesteps      | 720896       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0042474074 |\n",
            "|    clip_fraction        | 0.0352       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.913       |\n",
            "|    explained_variance   | 0.977        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 99.6         |\n",
            "|    n_updates            | 172          |\n",
            "|    policy_gradient_loss | -3.83e-05    |\n",
            "|    value_loss           | 31.7         |\n",
            "------------------------------------------\n",
            "Num timesteps: 736000\n",
            "Best mean reward: 128.76 - Last mean reward per episode: 130.86\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 912         |\n",
            "|    ep_rew_mean          | 131         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 700         |\n",
            "|    iterations           | 45          |\n",
            "|    time_elapsed         | 1052        |\n",
            "|    total_timesteps      | 737280      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004122862 |\n",
            "|    clip_fraction        | 0.0256      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.907      |\n",
            "|    explained_variance   | 0.981       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.23        |\n",
            "|    n_updates            | 176         |\n",
            "|    policy_gradient_loss | -9.12e-05   |\n",
            "|    value_loss           | 22          |\n",
            "-----------------------------------------\n",
            "Num timesteps: 752000\n",
            "Best mean reward: 130.86 - Last mean reward per episode: 135.89\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 911          |\n",
            "|    ep_rew_mean          | 136          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 699          |\n",
            "|    iterations           | 46           |\n",
            "|    time_elapsed         | 1078         |\n",
            "|    total_timesteps      | 753664       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0027199315 |\n",
            "|    clip_fraction        | 0.0292       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.877       |\n",
            "|    explained_variance   | 0.971        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 19.4         |\n",
            "|    n_updates            | 180          |\n",
            "|    policy_gradient_loss | -0.00114     |\n",
            "|    value_loss           | 39.2         |\n",
            "------------------------------------------\n",
            "Num timesteps: 768000\n",
            "Best mean reward: 135.89 - Last mean reward per episode: 136.87\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 903          |\n",
            "|    ep_rew_mean          | 137          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 697          |\n",
            "|    iterations           | 47           |\n",
            "|    time_elapsed         | 1103         |\n",
            "|    total_timesteps      | 770048       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038504298 |\n",
            "|    clip_fraction        | 0.0394       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.861       |\n",
            "|    explained_variance   | 0.984        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 11           |\n",
            "|    n_updates            | 184          |\n",
            "|    policy_gradient_loss | -0.000614    |\n",
            "|    value_loss           | 22.9         |\n",
            "------------------------------------------\n",
            "Num timesteps: 784000\n",
            "Best mean reward: 136.87 - Last mean reward per episode: 138.21\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 910         |\n",
            "|    ep_rew_mean          | 139         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 694         |\n",
            "|    iterations           | 48          |\n",
            "|    time_elapsed         | 1131        |\n",
            "|    total_timesteps      | 786432      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004233444 |\n",
            "|    clip_fraction        | 0.0391      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.858      |\n",
            "|    explained_variance   | 0.961       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 32.1        |\n",
            "|    n_updates            | 188         |\n",
            "|    policy_gradient_loss | -0.000377   |\n",
            "|    value_loss           | 50.5        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 800000\n",
            "Best mean reward: 138.21 - Last mean reward per episode: 142.02\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 925         |\n",
            "|    ep_rew_mean          | 142         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 693         |\n",
            "|    iterations           | 49          |\n",
            "|    time_elapsed         | 1158        |\n",
            "|    total_timesteps      | 802816      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004530482 |\n",
            "|    clip_fraction        | 0.0455      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.873      |\n",
            "|    explained_variance   | 0.988       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.28        |\n",
            "|    n_updates            | 192         |\n",
            "|    policy_gradient_loss | -0.000304   |\n",
            "|    value_loss           | 17.6        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 816000\n",
            "Best mean reward: 142.02 - Last mean reward per episode: 143.16\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 918          |\n",
            "|    ep_rew_mean          | 144          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 692          |\n",
            "|    iterations           | 50           |\n",
            "|    time_elapsed         | 1183         |\n",
            "|    total_timesteps      | 819200       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0036434329 |\n",
            "|    clip_fraction        | 0.0339       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.9         |\n",
            "|    explained_variance   | 0.989        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.84         |\n",
            "|    n_updates            | 196          |\n",
            "|    policy_gradient_loss | -0.000161    |\n",
            "|    value_loss           | 16.8         |\n",
            "------------------------------------------\n",
            "Num timesteps: 832000\n",
            "Best mean reward: 143.16 - Last mean reward per episode: 147.33\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 940          |\n",
            "|    ep_rew_mean          | 147          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 690          |\n",
            "|    iterations           | 51           |\n",
            "|    time_elapsed         | 1210         |\n",
            "|    total_timesteps      | 835584       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0043721152 |\n",
            "|    clip_fraction        | 0.0443       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.916       |\n",
            "|    explained_variance   | 0.985        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.76         |\n",
            "|    n_updates            | 200          |\n",
            "|    policy_gradient_loss | -0.000911    |\n",
            "|    value_loss           | 23.3         |\n",
            "------------------------------------------\n",
            "Num timesteps: 848000\n",
            "Best mean reward: 147.33 - Last mean reward per episode: 149.23\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 934          |\n",
            "|    ep_rew_mean          | 147          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 689          |\n",
            "|    iterations           | 52           |\n",
            "|    time_elapsed         | 1235         |\n",
            "|    total_timesteps      | 851968       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0045087487 |\n",
            "|    clip_fraction        | 0.0475       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.886       |\n",
            "|    explained_variance   | 0.996        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.707        |\n",
            "|    n_updates            | 204          |\n",
            "|    policy_gradient_loss | -0.000104    |\n",
            "|    value_loss           | 2.51         |\n",
            "------------------------------------------\n",
            "Num timesteps: 864000\n",
            "Best mean reward: 149.23 - Last mean reward per episode: 147.16\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 943          |\n",
            "|    ep_rew_mean          | 147          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 687          |\n",
            "|    iterations           | 53           |\n",
            "|    time_elapsed         | 1262         |\n",
            "|    total_timesteps      | 868352       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034027556 |\n",
            "|    clip_fraction        | 0.0296       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.901       |\n",
            "|    explained_variance   | 0.965        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 44           |\n",
            "|    n_updates            | 208          |\n",
            "|    policy_gradient_loss | -0.000394    |\n",
            "|    value_loss           | 51.7         |\n",
            "------------------------------------------\n",
            "Num timesteps: 880000\n",
            "Best mean reward: 149.23 - Last mean reward per episode: 146.61\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 936          |\n",
            "|    ep_rew_mean          | 146          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 686          |\n",
            "|    iterations           | 54           |\n",
            "|    time_elapsed         | 1288         |\n",
            "|    total_timesteps      | 884736       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044566933 |\n",
            "|    clip_fraction        | 0.0536       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.892       |\n",
            "|    explained_variance   | 0.994        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.81         |\n",
            "|    n_updates            | 212          |\n",
            "|    policy_gradient_loss | 0.000201     |\n",
            "|    value_loss           | 3.27         |\n",
            "------------------------------------------\n",
            "Num timesteps: 896000\n",
            "Best mean reward: 149.23 - Last mean reward per episode: 143.51\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 927          |\n",
            "|    ep_rew_mean          | 144          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 685          |\n",
            "|    iterations           | 55           |\n",
            "|    time_elapsed         | 1314         |\n",
            "|    total_timesteps      | 901120       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034654003 |\n",
            "|    clip_fraction        | 0.0222       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.905       |\n",
            "|    explained_variance   | 0.979        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1            |\n",
            "|    n_updates            | 216          |\n",
            "|    policy_gradient_loss | -0.000241    |\n",
            "|    value_loss           | 32.3         |\n",
            "------------------------------------------\n",
            "Num timesteps: 912000\n",
            "Best mean reward: 149.23 - Last mean reward per episode: 141.51\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 912         |\n",
            "|    ep_rew_mean          | 138         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 685         |\n",
            "|    iterations           | 56          |\n",
            "|    time_elapsed         | 1339        |\n",
            "|    total_timesteps      | 917504      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005360668 |\n",
            "|    clip_fraction        | 0.0464      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.919      |\n",
            "|    explained_variance   | 0.972       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 24.3        |\n",
            "|    n_updates            | 220         |\n",
            "|    policy_gradient_loss | -0.000237   |\n",
            "|    value_loss           | 38.1        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 928000\n",
            "Best mean reward: 149.23 - Last mean reward per episode: 137.43\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 926         |\n",
            "|    ep_rew_mean          | 138         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 684         |\n",
            "|    iterations           | 57          |\n",
            "|    time_elapsed         | 1364        |\n",
            "|    total_timesteps      | 933888      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005237466 |\n",
            "|    clip_fraction        | 0.0424      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.889      |\n",
            "|    explained_variance   | 0.972       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 44.8        |\n",
            "|    n_updates            | 224         |\n",
            "|    policy_gradient_loss | -0.00184    |\n",
            "|    value_loss           | 38.2        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 944000\n",
            "Best mean reward: 149.23 - Last mean reward per episode: 138.15\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 933        |\n",
            "|    ep_rew_mean          | 138        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 683        |\n",
            "|    iterations           | 58         |\n",
            "|    time_elapsed         | 1390       |\n",
            "|    total_timesteps      | 950272     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00406028 |\n",
            "|    clip_fraction        | 0.0658     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.884     |\n",
            "|    explained_variance   | 0.993      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.659      |\n",
            "|    n_updates            | 228        |\n",
            "|    policy_gradient_loss | -7.08e-05  |\n",
            "|    value_loss           | 4.44       |\n",
            "----------------------------------------\n",
            "Num timesteps: 960000\n",
            "Best mean reward: 149.23 - Last mean reward per episode: 135.72\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 907          |\n",
            "|    ep_rew_mean          | 134          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 683          |\n",
            "|    iterations           | 59           |\n",
            "|    time_elapsed         | 1414         |\n",
            "|    total_timesteps      | 966656       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0054394146 |\n",
            "|    clip_fraction        | 0.0437       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.886       |\n",
            "|    explained_variance   | 0.989        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.01         |\n",
            "|    n_updates            | 232          |\n",
            "|    policy_gradient_loss | -0.00106     |\n",
            "|    value_loss           | 14.5         |\n",
            "------------------------------------------\n",
            "Num timesteps: 976000\n",
            "Best mean reward: 149.23 - Last mean reward per episode: 137.65\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 921          |\n",
            "|    ep_rew_mean          | 138          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 682          |\n",
            "|    iterations           | 60           |\n",
            "|    time_elapsed         | 1440         |\n",
            "|    total_timesteps      | 983040       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0052690282 |\n",
            "|    clip_fraction        | 0.0261       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.849       |\n",
            "|    explained_variance   | 0.951        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 15.8         |\n",
            "|    n_updates            | 236          |\n",
            "|    policy_gradient_loss | -0.00174     |\n",
            "|    value_loss           | 67.8         |\n",
            "------------------------------------------\n",
            "Num timesteps: 992000\n",
            "Best mean reward: 149.23 - Last mean reward per episode: 139.12\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 921          |\n",
            "|    ep_rew_mean          | 139          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 681          |\n",
            "|    iterations           | 61           |\n",
            "|    time_elapsed         | 1465         |\n",
            "|    total_timesteps      | 999424       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034523094 |\n",
            "|    clip_fraction        | 0.0522       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.831       |\n",
            "|    explained_variance   | 0.995        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.66         |\n",
            "|    n_updates            | 240          |\n",
            "|    policy_gradient_loss | -4.36e-05    |\n",
            "|    value_loss           | 2.93         |\n",
            "------------------------------------------\n",
            "Num timesteps: 1008000\n",
            "Best mean reward: 149.23 - Last mean reward per episode: 142.26\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 935          |\n",
            "|    ep_rew_mean          | 143          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 681          |\n",
            "|    iterations           | 62           |\n",
            "|    time_elapsed         | 1491         |\n",
            "|    total_timesteps      | 1015808      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0039373664 |\n",
            "|    clip_fraction        | 0.0458       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.853       |\n",
            "|    explained_variance   | 0.977        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 20.5         |\n",
            "|    n_updates            | 244          |\n",
            "|    policy_gradient_loss | 0.000196     |\n",
            "|    value_loss           | 22.4         |\n",
            "------------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.ppo.ppo.PPO at 0x7e0c583cfb50>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "train_from_file = False\n",
        "# Hyperparameters are from RL_Zoo\n",
        "# https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/ppo.yml\n",
        "\n",
        "policy = \"MlpPolicy\"\n",
        "n_steps = 1024\n",
        "batch_size = 64\n",
        "n_epochs = 4\n",
        "n_envs = 16\n",
        "n_timesteps = 1e6\n",
        "gamma = 0.999\n",
        "gae_lambda = 0.98\n",
        "ent_coef = 0.01\n",
        "\n",
        "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=\"log_dir_PPO/\")\n",
        "\n",
        "# env\n",
        "env = make_vec_env(\"LunarLander-v2\", n_envs=n_envs, monitor_dir=\"log_dir_PPO/\")\n",
        "\n",
        "# instantiate the agent\n",
        "if train_from_file:\n",
        "  model = PPO.load(path=\"log_dir_PPO/best_model.zip\", env=env)\n",
        "else:\n",
        "  model = PPO(\n",
        "      policy,\n",
        "      env,\n",
        "      n_steps = n_steps,\n",
        "      ent_coef= ent_coef,\n",
        "      batch_size=batch_size,\n",
        "      n_epochs=n_epochs,\n",
        "      gamma=gamma,\n",
        "      gae_lambda=gae_lambda,\n",
        "      tensorboard_log=\"./TensorBoardLog/\", verbose=1)\n",
        "\n",
        "# train the agent\n",
        "model.learn(total_timesteps=n_timesteps, callback=callback)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b1dc8fa",
      "metadata": {
        "id": "5b1dc8fa"
      },
      "source": [
        "# Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "366b80a8",
      "metadata": {
        "id": "366b80a8",
        "outputId": "7de4adc0-0ca5-4e7a-f70f-c18a4c1c2fd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "def moving_average(values, window):\n",
        "    \"\"\"\n",
        "    Smooth values by doing a moving average\n",
        "    :param values: (numpy array)\n",
        "    :param window: (int)\n",
        "    :return: (numpy array)\n",
        "    \"\"\"\n",
        "    weights = np.repeat(1.0, window) / window\n",
        "    return np.convolve(values, weights, 'valid')\n",
        "\n",
        "def plot_results(log_folder, title='Learning Curve'):\n",
        "    \"\"\"\n",
        "    plot the results\n",
        "\n",
        "    :param log_folder: (str) the save location of the results to plot\n",
        "    :param title: (str) the title of the task to plot\n",
        "    \"\"\"\n",
        "\n",
        "    x, y = ts2xy(load_results(log_folder), 'timesteps')\n",
        "    y = moving_average(y, window=100)\n",
        "    # Truncate x\n",
        "    x = x[len(x) - len(y):]\n",
        "    fig = plt.figure(title, figsize=(12,5))\n",
        "    plt.plot(x, y)\n",
        "    plt.xlabel('Number of Timesteps')\n",
        "    plt.ylabel('Rewards')\n",
        "    plt.title(title + \" Smoothed PPO\")\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "plot_results(\"log_dir_PPO\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b00f2a81",
      "metadata": {
        "id": "b00f2a81"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "815393a0",
      "metadata": {
        "id": "815393a0"
      },
      "outputs": [],
      "source": [
        "env = make_vec_env(\"LunarLander-v2\", n_envs=1,monitor_dir=\"log_dir_PPO\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "63611e6e",
      "metadata": {
        "id": "63611e6e"
      },
      "outputs": [],
      "source": [
        "model = PPO.load(path=\"log_dir_PPO/best_model.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b06e1a3",
      "metadata": {
        "id": "3b06e1a3"
      },
      "source": [
        "#### Stable Baseline 3 Evaluation Function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "9d4fd326",
      "metadata": {
        "id": "9d4fd326",
        "outputId": "24d33fe8-2815-4805-9e01-899576a77037",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean & Std Reward after 10 max run is 222.209796 & 18.228417978644533\n"
          ]
        }
      ],
      "source": [
        "mean_reward, std_reward = evaluate_policy(model, env,n_eval_episodes=10, render=True, deterministic=True)\n",
        "print(\"Mean & Std Reward after {} max run is {} & {}\".format(10,mean_reward, std_reward))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e49c5168",
      "metadata": {
        "id": "e49c5168"
      },
      "source": [
        "# GIF of a Train Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "60cc63dc",
      "metadata": {
        "id": "60cc63dc"
      },
      "outputs": [],
      "source": [
        "env = make_vec_env(\"LunarLander-v2\", n_envs=1)\n",
        "model = PPO.load(path=\"log_dir_PPO/best_model.zip\")\n",
        "\n",
        "images = []\n",
        "obs = env.reset()\n",
        "img = env.render(mode=\"rgb_array\")\n",
        "for i in range(1000):\n",
        "    images.append(img)\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, _, _ ,_ = env.step(action)\n",
        "    img = env.render(mode=\"rgb_array\")\n",
        "\n",
        "imageio.mimsave(\"lunar lander_PPO.gif\", [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3afc060b",
      "metadata": {
        "id": "3afc060b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "857970f990130bbcaee778cf1846f7875676d945310dca1379fe4b5ef3d258a5"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}