{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "57601915",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57601915",
        "outputId": "2f13b230-139a-482d-bff6-186e955d592a"
      },
      "outputs": [],
      "source": [
        "\n",
        "# !pip install shap\n",
        "# !pip install opencv-python\n",
        "# !pip install swig\n",
        "# !pip install Box2D\n",
        "\n",
        "\n",
        "# # !pip install box2d pygame\n",
        "\n",
        "\n",
        "# !pip install gym\n",
        "# !pip install pyglet==1.5.27\n",
        "# !pip install stable-baseline3\n",
        "# !pip install \"gymnasium[all]\"\n",
        "\n",
        "# !pip install stable_baselines3\n",
        "\n",
        "## FOR LOCAL Jupyter notebook (no need for google colab)\n",
        "# !pip install tensorflow\n",
        "# !pip install torch\n",
        "# !pip install pygame\n",
        "# !pip install tensorboard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "PtTyQewr3gxt",
      "metadata": {
        "id": "PtTyQewr3gxt"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b00a128f",
      "metadata": {
        "id": "b00a128f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-26 13:00:09.589753: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-03-26 13:00:10.249042: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import imageio\n",
        "import os\n",
        "from stable_baselines3 import PPO, A2C\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack, SubprocVecEnv, VecNormalize\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common import results_plotter\n",
        "import gymnasium  as gym\n",
        "import matplotlib.pyplot as plt\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "import scipy.stats as stats\n",
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "import tensorflow as tf\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "YtZN-eC7NwuS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtZN-eC7NwuS",
        "outputId": "4e49a85c-8359-45d8-e667-d7a4980aaa72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n",
            "GPU not found. Please ensure that GPU is enabled in Colab.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-26 13:00:12.199233: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
          ]
        }
      ],
      "source": [
        "# seeds\n",
        "# Set seed for numpy\n",
        "np.random.seed(100)\n",
        "\n",
        "# Set seed for Python random module\n",
        "import random\n",
        "random.seed(100)\n",
        "\n",
        "# Set seed for TensorFlow\n",
        "tf.random.set_seed(100)\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "if tf.test.gpu_device_name():\n",
        "    print('Default GPU Device:', tf.test.gpu_device_name())\n",
        "else:\n",
        "    print(\"GPU not found. Please ensure that GPU is enabled in Colab.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "780afb92",
      "metadata": {
        "id": "780afb92"
      },
      "source": [
        "<h1> Important Libraries To Install </h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2826cd85",
      "metadata": {
        "id": "2826cd85"
      },
      "source": [
        "<h1> Parameter & Environment Information </h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87ef75ca",
      "metadata": {
        "id": "87ef75ca"
      },
      "source": [
        "<p>\n",
        "    This environment is part of the Box2D environments.\n",
        "</p>\n",
        "\n",
        "<ul>\n",
        "    <li> Action Space Discrete(4) </li>\n",
        "    <li> Observation Shape (8,) </li>\n",
        "    <li> Observation High [1.5 1.5 5. 5. 3.14 5. 1. 1. ] </li>\n",
        "    <li> Observation Low [-1.5 -1.5 -5. -5. -3.14 -5. -0. -0. ] </li>\n",
        "    <li> Import gymnasium.make(\"LunarLander-v2\") </li>\n",
        "</ul>\n",
        "\n",
        "<h3> Description </h3>\n",
        "<p>This environment is a classic rocket trajectory optimization problem. According to Pontryagin’s maximum principle, it is optimal to fire the engine at full throttle or turn it off. This is the reason why this environment has discrete actions: engine on or off.\n",
        "\n",
        "There are two environment versions: discrete or continuous. The landing pad is always at coordinates (0,0). The coordinates are the first two numbers in the state vector. Landing outside of the landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt.</p>\n",
        "\n",
        "<h3> Action Space </h3>\n",
        "<p>\n",
        "There are four discrete actions available:\n",
        "\n",
        "* 0: do nothing\n",
        "* 1: fire left orientation engine\n",
        "* 2: fire main engine\n",
        "* 3: fire right orientation engine\n",
        "\n",
        "</p>\n",
        "\n",
        "<h3> Observation Space </h3>\n",
        "<p>\n",
        "The state is an 8-dimensional vector: the coordinates of the lander in x & y, its linear velocities in x & y, its angle, its angular velocity, and two booleans that represent whether each leg is in contact with the ground or not.\n",
        "</p>\n",
        "\n",
        "<h3> Reward </h3>\n",
        "<p>\n",
        "After every step a reward is granted. The total reward of an episode is the sum of the rewards for all the steps within that episode.\n",
        "\n",
        "For each step, the reward:\n",
        "\n",
        "* is increased/decreased the closer/further the lander is to the landing pad.\n",
        "* is increased/decreased the slower/faster the lander is moving.\n",
        "* is decreased the more the lander is tilted (angle not horizontal).\n",
        "* is increased by 10 points for each leg that is in contact with the ground.\n",
        "* is decreased by 0.03 points each frame a side engine is firing.\n",
        "* is decreased by 0.3 points each frame the main engine is firing.\n",
        "\n",
        "The episode receive an additional reward of -100 or +100 points for crashing or landing safely respectively.\n",
        "\n",
        "An episode is considered a solution if it scores at least 200 points.\n",
        "</p>\n",
        "\n",
        "<h3> Starting State </h3>\n",
        "\n",
        "<p>The lander starts at the top center of the viewport with a random initial force applied to its center of mass.</p>\n",
        "\n",
        "<h3> Episode Termination </h3>\n",
        "<p> The episode finishes if:<br>\n",
        "    \n",
        "1. the lander crashes (the lander body gets in contact with the moon);<br>\n",
        "2. the lander gets outside of the viewport (x coordinate is greater than 1);<br>\n",
        "3. the lander is not awake. From the Box2D docs, a body which is not awake is a body which doesn’t move and doesn’t collide with any other body:<br>\n",
        "\n",
        "When Box2D determines that a body (or group of bodies) has come to rest, the body enters a sleep state which has very little CPU overhead. If a body is awake and collides with a sleeping body, then the sleeping body wakes up. Bodies will also wake up if a joint or contact attached to them is destroyed.\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3d0543a5",
      "metadata": {
        "id": "3d0543a5",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b3fb45b2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3fb45b2",
        "outputId": "f81eb084-19b4-4f51-f4f4-c82607cc6cb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Action inter is descrete 4\n",
            "Shape of Observation is (8,)\n"
          ]
        }
      ],
      "source": [
        "print(\"The Action inter is descrete {}\".format(env.action_space.n))\n",
        "print(\"Shape of Observation is {}\".format(env.observation_space.sample().shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b9ff9c9",
      "metadata": {
        "id": "8b9ff9c9"
      },
      "source": [
        "<h1> Baseline Model. </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d35101af",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d35101af",
        "outputId": "94c9ecca-7b72-44d5-de3b-2c535bf0a5ea",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Reward after 10 max run is -1.722505557121256\n"
          ]
        }
      ],
      "source": [
        "rewards = []\n",
        "obs = env.reset()\n",
        "done = False\n",
        "MAX_RUN = 10\n",
        "\n",
        "for i in range(MAX_RUN):\n",
        "    while not done:\n",
        "        env.render()\n",
        "        action_sample = env.action_space.sample()\n",
        "        # let's take a step in the environment\n",
        "        obs, rwd, done, info ,_  = env.step(action_sample)\n",
        "        rewards.append(rwd)\n",
        "env.close()\n",
        "print(\"Mean Reward after {} max run is {}\".format(MAX_RUN, np.mean(np.array(rewards))))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac737551",
      "metadata": {
        "id": "ac737551"
      },
      "source": [
        "<h1> Reinforcement Learning For Training The Model </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "bdaa0e55",
      "metadata": {
        "id": "bdaa0e55"
      },
      "outputs": [],
      "source": [
        "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
        "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
        "\n",
        "    :param check_freq: (int)\n",
        "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
        "      It must contains the file created by the ``Monitor`` wrapper.\n",
        "    :param verbose: (int)\n",
        "    \"\"\"\n",
        "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
        "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
        "        self.check_freq = check_freq\n",
        "        self.log_dir = log_dir\n",
        "        self.save_path = os.path.join(log_dir, 'best_model')\n",
        "        self.best_mean_reward = -np.inf\n",
        "\n",
        "    def _init_callback(self) -> None:\n",
        "        # Create folder if needed\n",
        "        if self.save_path is not None:\n",
        "            os.makedirs(self.save_path, exist_ok=True)\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.n_calls % self.check_freq == 0:\n",
        "\n",
        "          # Retrieve training reward\n",
        "          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
        "          if len(x) > 0:\n",
        "              # Mean training reward over the last 100 episodes\n",
        "              mean_reward = np.mean(y[-100:])\n",
        "              if self.verbose > 0:\n",
        "                print(f\"Num timesteps: {self.num_timesteps}\")\n",
        "                print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n",
        "\n",
        "              # New best model, you could save the agent here\n",
        "              if mean_reward > self.best_mean_reward:\n",
        "                  self.best_mean_reward = mean_reward\n",
        "                  # Example for saving best model\n",
        "                  if self.verbose > 0:\n",
        "                    print(f\"Saving new best model to {self.save_path}.zip\")\n",
        "                  self.model.save(self.save_path)\n",
        "\n",
        "        return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2d311d6a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d311d6a",
        "outputId": "3c960c90-bbc7-4506-a186-3928d014cd7f",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "Logging to ./TensorBoardLog/PPO_3\n",
            "Num timesteps: 16000\n",
            "Best mean reward: -inf - Last mean reward per episode: -187.28\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 95       |\n",
            "|    ep_rew_mean     | -187     |\n",
            "| time/              |          |\n",
            "|    fps             | 4979     |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 3        |\n",
            "|    total_timesteps | 16384    |\n",
            "---------------------------------\n",
            "Num timesteps: 32000\n",
            "Best mean reward: -187.28 - Last mean reward per episode: -153.69\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 95.2         |\n",
            "|    ep_rew_mean          | -157         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 3344         |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 9            |\n",
            "|    total_timesteps      | 32768        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0062795193 |\n",
            "|    clip_fraction        | 0.0331       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | -0.00231     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.3e+03      |\n",
            "|    n_updates            | 4            |\n",
            "|    policy_gradient_loss | -0.00607     |\n",
            "|    value_loss           | 4.52e+03     |\n",
            "------------------------------------------\n",
            "Num timesteps: 48000\n",
            "Best mean reward: -153.69 - Last mean reward per episode: -137.85\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 95.2        |\n",
            "|    ep_rew_mean          | -143        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3005        |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 16          |\n",
            "|    total_timesteps      | 49152       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007154992 |\n",
            "|    clip_fraction        | 0.028       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.37       |\n",
            "|    explained_variance   | -0.00474    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.34e+03    |\n",
            "|    n_updates            | 8           |\n",
            "|    policy_gradient_loss | -0.00526    |\n",
            "|    value_loss           | 2.61e+03    |\n",
            "-----------------------------------------\n",
            "Num timesteps: 64000\n",
            "Best mean reward: -137.85 - Last mean reward per episode: -113.39\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 97.8         |\n",
            "|    ep_rew_mean          | -112         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 2853         |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 22           |\n",
            "|    total_timesteps      | 65536        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0062901215 |\n",
            "|    clip_fraction        | 0.0349       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.36        |\n",
            "|    explained_variance   | -0.000636    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 693          |\n",
            "|    n_updates            | 12           |\n",
            "|    policy_gradient_loss | -0.00472     |\n",
            "|    value_loss           | 1.43e+03     |\n",
            "------------------------------------------\n",
            "Num timesteps: 80000\n",
            "Best mean reward: -113.39 - Last mean reward per episode: -101.84\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 94.7        |\n",
            "|    ep_rew_mean          | -100        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 2770        |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 29          |\n",
            "|    total_timesteps      | 81920       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005545976 |\n",
            "|    clip_fraction        | 0.0652      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.35       |\n",
            "|    explained_variance   | -0.000651   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 328         |\n",
            "|    n_updates            | 16          |\n",
            "|    policy_gradient_loss | -0.00383    |\n",
            "|    value_loss           | 872         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 96000\n",
            "Best mean reward: -101.84 - Last mean reward per episode: -102.58\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 98.6         |\n",
            "|    ep_rew_mean          | -104         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 2715         |\n",
            "|    iterations           | 6            |\n",
            "|    time_elapsed         | 36           |\n",
            "|    total_timesteps      | 98304        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041898405 |\n",
            "|    clip_fraction        | 0.023        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.36        |\n",
            "|    explained_variance   | -5.21e-05    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 197          |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00326     |\n",
            "|    value_loss           | 790          |\n",
            "------------------------------------------\n",
            "Num timesteps: 112000\n",
            "Best mean reward: -101.84 - Last mean reward per episode: -88.08\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 95.5        |\n",
            "|    ep_rew_mean          | -88.9       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 2678        |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 42          |\n",
            "|    total_timesteps      | 114688      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006298837 |\n",
            "|    clip_fraction        | 0.0377      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.35       |\n",
            "|    explained_variance   | -0.000713   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 138         |\n",
            "|    n_updates            | 24          |\n",
            "|    policy_gradient_loss | -0.00252    |\n",
            "|    value_loss           | 600         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 128000\n",
            "Best mean reward: -88.08 - Last mean reward per episode: -65.47\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 96.4        |\n",
            "|    ep_rew_mean          | -67.7       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 2649        |\n",
            "|    iterations           | 8           |\n",
            "|    time_elapsed         | 49          |\n",
            "|    total_timesteps      | 131072      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010492071 |\n",
            "|    clip_fraction        | 0.0909      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.33       |\n",
            "|    explained_variance   | -2.81e-05   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 159         |\n",
            "|    n_updates            | 28          |\n",
            "|    policy_gradient_loss | -0.00737    |\n",
            "|    value_loss           | 420         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 144000\n",
            "Best mean reward: -65.47 - Last mean reward per episode: -57.46\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 102         |\n",
            "|    ep_rew_mean          | -59.3       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 2622        |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 56          |\n",
            "|    total_timesteps      | 147456      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010299187 |\n",
            "|    clip_fraction        | 0.0799      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.3        |\n",
            "|    explained_variance   | 0.000299    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 287         |\n",
            "|    n_updates            | 32          |\n",
            "|    policy_gradient_loss | -0.00711    |\n",
            "|    value_loss           | 304         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 160000\n",
            "Best mean reward: -57.46 - Last mean reward per episode: -39.26\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 112         |\n",
            "|    ep_rew_mean          | -36.7       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 2592        |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 63          |\n",
            "|    total_timesteps      | 163840      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011075339 |\n",
            "|    clip_fraction        | 0.0888      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.25       |\n",
            "|    explained_variance   | -0.000238   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 87.1        |\n",
            "|    n_updates            | 36          |\n",
            "|    policy_gradient_loss | -0.00647    |\n",
            "|    value_loss           | 275         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 176000\n",
            "Best mean reward: -39.26 - Last mean reward per episode: -29.20\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 124         |\n",
            "|    ep_rew_mean          | -23.8       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 2554        |\n",
            "|    iterations           | 11          |\n",
            "|    time_elapsed         | 70          |\n",
            "|    total_timesteps      | 180224      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010017821 |\n",
            "|    clip_fraction        | 0.0371      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.21       |\n",
            "|    explained_variance   | -9.2e-05    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 138         |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00356    |\n",
            "|    value_loss           | 386         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 192000\n",
            "Best mean reward: -29.20 - Last mean reward per episode: -28.12\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 145         |\n",
            "|    ep_rew_mean          | -27.3       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 2501        |\n",
            "|    iterations           | 12          |\n",
            "|    time_elapsed         | 78          |\n",
            "|    total_timesteps      | 196608      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007892482 |\n",
            "|    clip_fraction        | 0.0531      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.18       |\n",
            "|    explained_variance   | -0.00023    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 205         |\n",
            "|    n_updates            | 44          |\n",
            "|    policy_gradient_loss | -0.00337    |\n",
            "|    value_loss           | 500         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 208000\n",
            "Best mean reward: -28.12 - Last mean reward per episode: -28.48\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 156         |\n",
            "|    ep_rew_mean          | -29.3       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 2446        |\n",
            "|    iterations           | 13          |\n",
            "|    time_elapsed         | 87          |\n",
            "|    total_timesteps      | 212992      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007136364 |\n",
            "|    clip_fraction        | 0.0642      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.17       |\n",
            "|    explained_variance   | -2.15e-06   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 283         |\n",
            "|    n_updates            | 48          |\n",
            "|    policy_gradient_loss | -0.00297    |\n",
            "|    value_loss           | 555         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 224000\n",
            "Best mean reward: -28.12 - Last mean reward per episode: -25.91\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 192          |\n",
            "|    ep_rew_mean          | -20.2        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 2343         |\n",
            "|    iterations           | 14           |\n",
            "|    time_elapsed         | 97           |\n",
            "|    total_timesteps      | 229376       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0068149213 |\n",
            "|    clip_fraction        | 0.0475       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.17        |\n",
            "|    explained_variance   | 1.61e-06     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 257          |\n",
            "|    n_updates            | 52           |\n",
            "|    policy_gradient_loss | -0.0026      |\n",
            "|    value_loss           | 692          |\n",
            "------------------------------------------\n",
            "Num timesteps: 240000\n",
            "Best mean reward: -25.91 - Last mean reward per episode: -22.11\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 245         |\n",
            "|    ep_rew_mean          | -21.5       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 2226        |\n",
            "|    iterations           | 15          |\n",
            "|    time_elapsed         | 110         |\n",
            "|    total_timesteps      | 245760      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004210507 |\n",
            "|    clip_fraction        | 0.0275      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.15       |\n",
            "|    explained_variance   | 4.77e-06    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 278         |\n",
            "|    n_updates            | 56          |\n",
            "|    policy_gradient_loss | -0.00259    |\n",
            "|    value_loss           | 628         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 256000\n",
            "Best mean reward: -22.11 - Last mean reward per episode: -20.35\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 300        |\n",
            "|    ep_rew_mean          | -20.7      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 2084       |\n",
            "|    iterations           | 16         |\n",
            "|    time_elapsed         | 125        |\n",
            "|    total_timesteps      | 262144     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00618641 |\n",
            "|    clip_fraction        | 0.0352     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.13      |\n",
            "|    explained_variance   | -0.000252  |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 229        |\n",
            "|    n_updates            | 60         |\n",
            "|    policy_gradient_loss | -0.0024    |\n",
            "|    value_loss           | 576        |\n",
            "----------------------------------------\n",
            "Num timesteps: 272000\n",
            "Best mean reward: -20.35 - Last mean reward per episode: -14.69\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 342          |\n",
            "|    ep_rew_mean          | -15          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1981         |\n",
            "|    iterations           | 17           |\n",
            "|    time_elapsed         | 140          |\n",
            "|    total_timesteps      | 278528       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034102849 |\n",
            "|    clip_fraction        | 0.0144       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.14        |\n",
            "|    explained_variance   | 0.0209       |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 71.6         |\n",
            "|    n_updates            | 64           |\n",
            "|    policy_gradient_loss | -0.00164     |\n",
            "|    value_loss           | 395          |\n",
            "------------------------------------------\n",
            "Num timesteps: 288000\n",
            "Best mean reward: -14.69 - Last mean reward per episode: -8.57\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 454          |\n",
            "|    ep_rew_mean          | -9.46        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1858         |\n",
            "|    iterations           | 18           |\n",
            "|    time_elapsed         | 158          |\n",
            "|    total_timesteps      | 294912       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0036193188 |\n",
            "|    clip_fraction        | 0.00659      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.13        |\n",
            "|    explained_variance   | 0.139        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 132          |\n",
            "|    n_updates            | 68           |\n",
            "|    policy_gradient_loss | -0.0013      |\n",
            "|    value_loss           | 416          |\n",
            "------------------------------------------\n",
            "Num timesteps: 304000\n",
            "Best mean reward: -8.57 - Last mean reward per episode: -9.38\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 494          |\n",
            "|    ep_rew_mean          | -11.6        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1769         |\n",
            "|    iterations           | 19           |\n",
            "|    time_elapsed         | 175          |\n",
            "|    total_timesteps      | 311296       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044417474 |\n",
            "|    clip_fraction        | 0.0158       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.13        |\n",
            "|    explained_variance   | 0.391        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 137          |\n",
            "|    n_updates            | 72           |\n",
            "|    policy_gradient_loss | -0.00151     |\n",
            "|    value_loss           | 362          |\n",
            "------------------------------------------\n",
            "Num timesteps: 320000\n",
            "Best mean reward: -8.57 - Last mean reward per episode: -13.01\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 537          |\n",
            "|    ep_rew_mean          | -5.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1700         |\n",
            "|    iterations           | 20           |\n",
            "|    time_elapsed         | 192          |\n",
            "|    total_timesteps      | 327680       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0069971792 |\n",
            "|    clip_fraction        | 0.0581       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.15        |\n",
            "|    explained_variance   | 0.398        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 196          |\n",
            "|    n_updates            | 76           |\n",
            "|    policy_gradient_loss | -0.00198     |\n",
            "|    value_loss           | 428          |\n",
            "------------------------------------------\n",
            "Num timesteps: 336000\n",
            "Best mean reward: -8.57 - Last mean reward per episode: -3.83\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 566         |\n",
            "|    ep_rew_mean          | -1.44       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1638        |\n",
            "|    iterations           | 21          |\n",
            "|    time_elapsed         | 210         |\n",
            "|    total_timesteps      | 344064      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005743598 |\n",
            "|    clip_fraction        | 0.0488      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.16       |\n",
            "|    explained_variance   | 0.607       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 117         |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.00233    |\n",
            "|    value_loss           | 260         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 352000\n",
            "Best mean reward: -3.83 - Last mean reward per episode: 3.01\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 603          |\n",
            "|    ep_rew_mean          | 13.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1582         |\n",
            "|    iterations           | 22           |\n",
            "|    time_elapsed         | 227          |\n",
            "|    total_timesteps      | 360448       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0053524724 |\n",
            "|    clip_fraction        | 0.0369       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.14        |\n",
            "|    explained_variance   | 0.743        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 57.2         |\n",
            "|    n_updates            | 84           |\n",
            "|    policy_gradient_loss | -0.00141     |\n",
            "|    value_loss           | 137          |\n",
            "------------------------------------------\n",
            "Num timesteps: 368000\n",
            "Best mean reward: 3.01 - Last mean reward per episode: 20.82\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 661          |\n",
            "|    ep_rew_mean          | 31.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1529         |\n",
            "|    iterations           | 23           |\n",
            "|    time_elapsed         | 246          |\n",
            "|    total_timesteps      | 376832       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0062162774 |\n",
            "|    clip_fraction        | 0.0314       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.14        |\n",
            "|    explained_variance   | 0.77         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 68.6         |\n",
            "|    n_updates            | 88           |\n",
            "|    policy_gradient_loss | -0.00102     |\n",
            "|    value_loss           | 163          |\n",
            "------------------------------------------\n",
            "Num timesteps: 384000\n",
            "Best mean reward: 20.82 - Last mean reward per episode: 36.40\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 704          |\n",
            "|    ep_rew_mean          | 39.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1494         |\n",
            "|    iterations           | 24           |\n",
            "|    time_elapsed         | 263          |\n",
            "|    total_timesteps      | 393216       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0064968904 |\n",
            "|    clip_fraction        | 0.0621       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.13        |\n",
            "|    explained_variance   | 0.823        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 38.5         |\n",
            "|    n_updates            | 92           |\n",
            "|    policy_gradient_loss | -0.00369     |\n",
            "|    value_loss           | 117          |\n",
            "------------------------------------------\n",
            "Num timesteps: 400000\n",
            "Best mean reward: 36.40 - Last mean reward per episode: 46.38\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 771         |\n",
            "|    ep_rew_mean          | 53.5        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1457        |\n",
            "|    iterations           | 25          |\n",
            "|    time_elapsed         | 280         |\n",
            "|    total_timesteps      | 409600      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006722347 |\n",
            "|    clip_fraction        | 0.0543      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.09       |\n",
            "|    explained_variance   | 0.81        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 71.5        |\n",
            "|    n_updates            | 96          |\n",
            "|    policy_gradient_loss | -0.00302    |\n",
            "|    value_loss           | 137         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 416000\n",
            "Best mean reward: 46.38 - Last mean reward per episode: 60.13\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 803         |\n",
            "|    ep_rew_mean          | 64.6        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1413        |\n",
            "|    iterations           | 26          |\n",
            "|    time_elapsed         | 301         |\n",
            "|    total_timesteps      | 425984      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004345316 |\n",
            "|    clip_fraction        | 0.0242      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.1        |\n",
            "|    explained_variance   | 0.91        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 19.2        |\n",
            "|    n_updates            | 100         |\n",
            "|    policy_gradient_loss | -0.000364   |\n",
            "|    value_loss           | 62.3        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 432000\n",
            "Best mean reward: 60.13 - Last mean reward per episode: 70.27\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 847         |\n",
            "|    ep_rew_mean          | 71.8        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1380        |\n",
            "|    iterations           | 27          |\n",
            "|    time_elapsed         | 320         |\n",
            "|    total_timesteps      | 442368      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007349261 |\n",
            "|    clip_fraction        | 0.0725      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.1        |\n",
            "|    explained_variance   | 0.924       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 18.2        |\n",
            "|    n_updates            | 104         |\n",
            "|    policy_gradient_loss | -0.00217    |\n",
            "|    value_loss           | 56.8        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 448000\n",
            "Best mean reward: 70.27 - Last mean reward per episode: 75.41\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 888         |\n",
            "|    ep_rew_mean          | 79.6        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1355        |\n",
            "|    iterations           | 28          |\n",
            "|    time_elapsed         | 338         |\n",
            "|    total_timesteps      | 458752      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005756779 |\n",
            "|    clip_fraction        | 0.0306      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.08       |\n",
            "|    explained_variance   | 0.935       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 33.2        |\n",
            "|    n_updates            | 108         |\n",
            "|    policy_gradient_loss | -0.000994   |\n",
            "|    value_loss           | 50.3        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 464000\n",
            "Best mean reward: 75.41 - Last mean reward per episode: 78.67\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 919         |\n",
            "|    ep_rew_mean          | 83.1        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1327        |\n",
            "|    iterations           | 29          |\n",
            "|    time_elapsed         | 357         |\n",
            "|    total_timesteps      | 475136      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004142296 |\n",
            "|    clip_fraction        | 0.0248      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.05       |\n",
            "|    explained_variance   | 0.937       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 30.5        |\n",
            "|    n_updates            | 112         |\n",
            "|    policy_gradient_loss | -0.00148    |\n",
            "|    value_loss           | 46.2        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 480000\n",
            "Best mean reward: 78.67 - Last mean reward per episode: 87.20\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 948          |\n",
            "|    ep_rew_mean          | 92.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1301         |\n",
            "|    iterations           | 30           |\n",
            "|    time_elapsed         | 377          |\n",
            "|    total_timesteps      | 491520       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0051631955 |\n",
            "|    clip_fraction        | 0.0268       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.05        |\n",
            "|    explained_variance   | 0.925        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 30.5         |\n",
            "|    n_updates            | 116          |\n",
            "|    policy_gradient_loss | -0.000893    |\n",
            "|    value_loss           | 45.1         |\n",
            "------------------------------------------\n",
            "Num timesteps: 496000\n",
            "Best mean reward: 87.20 - Last mean reward per episode: 94.96\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 969         |\n",
            "|    ep_rew_mean          | 97.4        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1281        |\n",
            "|    iterations           | 31          |\n",
            "|    time_elapsed         | 396         |\n",
            "|    total_timesteps      | 507904      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005258837 |\n",
            "|    clip_fraction        | 0.0377      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.04       |\n",
            "|    explained_variance   | 0.954       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.41        |\n",
            "|    n_updates            | 120         |\n",
            "|    policy_gradient_loss | -0.00142    |\n",
            "|    value_loss           | 36.1        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 512000\n",
            "Best mean reward: 94.96 - Last mean reward per episode: 96.18\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 956          |\n",
            "|    ep_rew_mean          | 93.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1265         |\n",
            "|    iterations           | 32           |\n",
            "|    time_elapsed         | 414          |\n",
            "|    total_timesteps      | 524288       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0077110445 |\n",
            "|    clip_fraction        | 0.043        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.04        |\n",
            "|    explained_variance   | 0.972        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 11.2         |\n",
            "|    n_updates            | 124          |\n",
            "|    policy_gradient_loss | -0.00217     |\n",
            "|    value_loss           | 20.9         |\n",
            "------------------------------------------\n",
            "Num timesteps: 528000\n",
            "Best mean reward: 96.18 - Last mean reward per episode: 91.97\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 949          |\n",
            "|    ep_rew_mean          | 95.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1246         |\n",
            "|    iterations           | 33           |\n",
            "|    time_elapsed         | 433          |\n",
            "|    total_timesteps      | 540672       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0033167014 |\n",
            "|    clip_fraction        | 0.0232       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.03        |\n",
            "|    explained_variance   | 0.932        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.8          |\n",
            "|    n_updates            | 128          |\n",
            "|    policy_gradient_loss | -0.000934    |\n",
            "|    value_loss           | 59.1         |\n",
            "------------------------------------------\n",
            "Num timesteps: 544000\n",
            "Best mean reward: 96.18 - Last mean reward per episode: 96.62\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 956         |\n",
            "|    ep_rew_mean          | 99.4        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1230        |\n",
            "|    iterations           | 34          |\n",
            "|    time_elapsed         | 452         |\n",
            "|    total_timesteps      | 557056      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004641342 |\n",
            "|    clip_fraction        | 0.0374      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.02       |\n",
            "|    explained_variance   | 0.957       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 13.8        |\n",
            "|    n_updates            | 132         |\n",
            "|    policy_gradient_loss | -0.00163    |\n",
            "|    value_loss           | 35.7        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 560000\n",
            "Best mean reward: 96.62 - Last mean reward per episode: 100.92\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 955         |\n",
            "|    ep_rew_mean          | 101         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1217        |\n",
            "|    iterations           | 35          |\n",
            "|    time_elapsed         | 470         |\n",
            "|    total_timesteps      | 573440      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004906873 |\n",
            "|    clip_fraction        | 0.0386      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.02       |\n",
            "|    explained_variance   | 0.983       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.96        |\n",
            "|    n_updates            | 136         |\n",
            "|    policy_gradient_loss | -0.000185   |\n",
            "|    value_loss           | 13.9        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 576000\n",
            "Best mean reward: 100.92 - Last mean reward per episode: 102.13\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 956          |\n",
            "|    ep_rew_mean          | 102          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1203         |\n",
            "|    iterations           | 36           |\n",
            "|    time_elapsed         | 490          |\n",
            "|    total_timesteps      | 589824       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0058169584 |\n",
            "|    clip_fraction        | 0.0426       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.01        |\n",
            "|    explained_variance   | 0.968        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.62         |\n",
            "|    n_updates            | 140          |\n",
            "|    policy_gradient_loss | -0.000777    |\n",
            "|    value_loss           | 25.2         |\n",
            "------------------------------------------\n",
            "Num timesteps: 592000\n",
            "Best mean reward: 102.13 - Last mean reward per episode: 101.90\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 956          |\n",
            "|    ep_rew_mean          | 104          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1192         |\n",
            "|    iterations           | 37           |\n",
            "|    time_elapsed         | 508          |\n",
            "|    total_timesteps      | 606208       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044346624 |\n",
            "|    clip_fraction        | 0.0231       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.977       |\n",
            "|    explained_variance   | 0.968        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.78         |\n",
            "|    n_updates            | 144          |\n",
            "|    policy_gradient_loss | -0.000395    |\n",
            "|    value_loss           | 24.4         |\n",
            "------------------------------------------\n",
            "Num timesteps: 608000\n",
            "Best mean reward: 102.13 - Last mean reward per episode: 104.99\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 975          |\n",
            "|    ep_rew_mean          | 112          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1181         |\n",
            "|    iterations           | 38           |\n",
            "|    time_elapsed         | 527          |\n",
            "|    total_timesteps      | 622592       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0077472194 |\n",
            "|    clip_fraction        | 0.0503       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.96        |\n",
            "|    explained_variance   | 0.984        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 15.9         |\n",
            "|    n_updates            | 148          |\n",
            "|    policy_gradient_loss | -0.00147     |\n",
            "|    value_loss           | 12.1         |\n",
            "------------------------------------------\n",
            "Num timesteps: 624000\n",
            "Best mean reward: 104.99 - Last mean reward per episode: 113.51\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 981          |\n",
            "|    ep_rew_mean          | 114          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1170         |\n",
            "|    iterations           | 39           |\n",
            "|    time_elapsed         | 546          |\n",
            "|    total_timesteps      | 638976       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0047908425 |\n",
            "|    clip_fraction        | 0.0464       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.946       |\n",
            "|    explained_variance   | 0.987        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.49         |\n",
            "|    n_updates            | 152          |\n",
            "|    policy_gradient_loss | -0.0015      |\n",
            "|    value_loss           | 10.5         |\n",
            "------------------------------------------\n",
            "Num timesteps: 640000\n",
            "Best mean reward: 113.51 - Last mean reward per episode: 113.82\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 969          |\n",
            "|    ep_rew_mean          | 114          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1162         |\n",
            "|    iterations           | 40           |\n",
            "|    time_elapsed         | 563          |\n",
            "|    total_timesteps      | 655360       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034648743 |\n",
            "|    clip_fraction        | 0.0247       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.903       |\n",
            "|    explained_variance   | 0.97         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.47         |\n",
            "|    n_updates            | 156          |\n",
            "|    policy_gradient_loss | -0.000565    |\n",
            "|    value_loss           | 23.4         |\n",
            "------------------------------------------\n",
            "Num timesteps: 656000\n",
            "Best mean reward: 113.82 - Last mean reward per episode: 114.08\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 966          |\n",
            "|    ep_rew_mean          | 116          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1156         |\n",
            "|    iterations           | 41           |\n",
            "|    time_elapsed         | 580          |\n",
            "|    total_timesteps      | 671744       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038722078 |\n",
            "|    clip_fraction        | 0.0338       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.924       |\n",
            "|    explained_variance   | 0.961        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 15.1         |\n",
            "|    n_updates            | 160          |\n",
            "|    policy_gradient_loss | -0.000847    |\n",
            "|    value_loss           | 33           |\n",
            "------------------------------------------\n",
            "Num timesteps: 672000\n",
            "Best mean reward: 114.08 - Last mean reward per episode: 116.40\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "Num timesteps: 688000\n",
            "Best mean reward: 116.40 - Last mean reward per episode: 118.70\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 945          |\n",
            "|    ep_rew_mean          | 119          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1151         |\n",
            "|    iterations           | 42           |\n",
            "|    time_elapsed         | 597          |\n",
            "|    total_timesteps      | 688128       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0054307417 |\n",
            "|    clip_fraction        | 0.0491       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.864       |\n",
            "|    explained_variance   | 0.957        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 12.8         |\n",
            "|    n_updates            | 164          |\n",
            "|    policy_gradient_loss | -0.00201     |\n",
            "|    value_loss           | 42.5         |\n",
            "------------------------------------------\n",
            "Num timesteps: 704000\n",
            "Best mean reward: 118.70 - Last mean reward per episode: 118.80\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 932         |\n",
            "|    ep_rew_mean          | 119         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1146        |\n",
            "|    iterations           | 43          |\n",
            "|    time_elapsed         | 614         |\n",
            "|    total_timesteps      | 704512      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004579535 |\n",
            "|    clip_fraction        | 0.043       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.858      |\n",
            "|    explained_variance   | 0.89        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 8.62        |\n",
            "|    n_updates            | 168         |\n",
            "|    policy_gradient_loss | -0.000567   |\n",
            "|    value_loss           | 117         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 720000\n",
            "Best mean reward: 118.80 - Last mean reward per episode: 116.02\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 905         |\n",
            "|    ep_rew_mean          | 116         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1142        |\n",
            "|    iterations           | 44          |\n",
            "|    time_elapsed         | 630         |\n",
            "|    total_timesteps      | 720896      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003232403 |\n",
            "|    clip_fraction        | 0.0392      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.865      |\n",
            "|    explained_variance   | 0.968       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.55        |\n",
            "|    n_updates            | 172         |\n",
            "|    policy_gradient_loss | -0.000954   |\n",
            "|    value_loss           | 26.7        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 736000\n",
            "Best mean reward: 118.80 - Last mean reward per episode: 121.40\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 912        |\n",
            "|    ep_rew_mean          | 121        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 1138       |\n",
            "|    iterations           | 45         |\n",
            "|    time_elapsed         | 647        |\n",
            "|    total_timesteps      | 737280     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00389268 |\n",
            "|    clip_fraction        | 0.0312     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.846     |\n",
            "|    explained_variance   | 0.94       |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 128        |\n",
            "|    n_updates            | 176        |\n",
            "|    policy_gradient_loss | 0.000402   |\n",
            "|    value_loss           | 56.2       |\n",
            "----------------------------------------\n",
            "Num timesteps: 752000\n",
            "Best mean reward: 121.40 - Last mean reward per episode: 123.73\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 912        |\n",
            "|    ep_rew_mean          | 124        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 1133       |\n",
            "|    iterations           | 46         |\n",
            "|    time_elapsed         | 665        |\n",
            "|    total_timesteps      | 753664     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00460082 |\n",
            "|    clip_fraction        | 0.0388     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.863     |\n",
            "|    explained_variance   | 0.97       |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 4.5        |\n",
            "|    n_updates            | 180        |\n",
            "|    policy_gradient_loss | 4.18e-05   |\n",
            "|    value_loss           | 32.3       |\n",
            "----------------------------------------\n",
            "Num timesteps: 768000\n",
            "Best mean reward: 123.73 - Last mean reward per episode: 127.22\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 916          |\n",
            "|    ep_rew_mean          | 127          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1127         |\n",
            "|    iterations           | 47           |\n",
            "|    time_elapsed         | 682          |\n",
            "|    total_timesteps      | 770048       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0047354596 |\n",
            "|    clip_fraction        | 0.0364       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.852       |\n",
            "|    explained_variance   | 0.984        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 32.1         |\n",
            "|    n_updates            | 184          |\n",
            "|    policy_gradient_loss | -0.000293    |\n",
            "|    value_loss           | 14.9         |\n",
            "------------------------------------------\n",
            "Num timesteps: 784000\n",
            "Best mean reward: 127.22 - Last mean reward per episode: 128.77\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 920          |\n",
            "|    ep_rew_mean          | 128          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1122         |\n",
            "|    iterations           | 48           |\n",
            "|    time_elapsed         | 700          |\n",
            "|    total_timesteps      | 786432       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0026423335 |\n",
            "|    clip_fraction        | 0.0307       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.807       |\n",
            "|    explained_variance   | 0.937        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 60.1         |\n",
            "|    n_updates            | 188          |\n",
            "|    policy_gradient_loss | -0.000107    |\n",
            "|    value_loss           | 61.8         |\n",
            "------------------------------------------\n",
            "Num timesteps: 800000\n",
            "Best mean reward: 128.77 - Last mean reward per episode: 132.70\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 924        |\n",
            "|    ep_rew_mean          | 131        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 1118       |\n",
            "|    iterations           | 49         |\n",
            "|    time_elapsed         | 717        |\n",
            "|    total_timesteps      | 802816     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00456684 |\n",
            "|    clip_fraction        | 0.0424     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.828     |\n",
            "|    explained_variance   | 0.937      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 13.3       |\n",
            "|    n_updates            | 192        |\n",
            "|    policy_gradient_loss | -0.000239  |\n",
            "|    value_loss           | 64.4       |\n",
            "----------------------------------------\n",
            "Num timesteps: 816000\n",
            "Best mean reward: 132.70 - Last mean reward per episode: 136.72\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 900         |\n",
            "|    ep_rew_mean          | 137         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1116        |\n",
            "|    iterations           | 50          |\n",
            "|    time_elapsed         | 733         |\n",
            "|    total_timesteps      | 819200      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005121425 |\n",
            "|    clip_fraction        | 0.0491      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.796      |\n",
            "|    explained_variance   | 0.931       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.05        |\n",
            "|    n_updates            | 196         |\n",
            "|    policy_gradient_loss | -0.00183    |\n",
            "|    value_loss           | 66.6        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 832000\n",
            "Best mean reward: 136.72 - Last mean reward per episode: 145.13\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 879          |\n",
            "|    ep_rew_mean          | 146          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1111         |\n",
            "|    iterations           | 51           |\n",
            "|    time_elapsed         | 751          |\n",
            "|    total_timesteps      | 835584       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040690983 |\n",
            "|    clip_fraction        | 0.028        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.763       |\n",
            "|    explained_variance   | 0.823        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 94.5         |\n",
            "|    n_updates            | 200          |\n",
            "|    policy_gradient_loss | -0.00119     |\n",
            "|    value_loss           | 197          |\n",
            "------------------------------------------\n",
            "Num timesteps: 848000\n",
            "Best mean reward: 145.13 - Last mean reward per episode: 157.51\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 823          |\n",
            "|    ep_rew_mean          | 157          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1111         |\n",
            "|    iterations           | 52           |\n",
            "|    time_elapsed         | 766          |\n",
            "|    total_timesteps      | 851968       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0056731375 |\n",
            "|    clip_fraction        | 0.0688       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.728       |\n",
            "|    explained_variance   | 0.809        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 93           |\n",
            "|    n_updates            | 204          |\n",
            "|    policy_gradient_loss | -0.00223     |\n",
            "|    value_loss           | 170          |\n",
            "------------------------------------------\n",
            "Num timesteps: 864000\n",
            "Best mean reward: 157.51 - Last mean reward per episode: 176.87\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 713         |\n",
            "|    ep_rew_mean          | 185         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1111        |\n",
            "|    iterations           | 53          |\n",
            "|    time_elapsed         | 781         |\n",
            "|    total_timesteps      | 868352      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007872232 |\n",
            "|    clip_fraction        | 0.0921      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.78       |\n",
            "|    explained_variance   | 0.726       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 90.1        |\n",
            "|    n_updates            | 208         |\n",
            "|    policy_gradient_loss | -0.00278    |\n",
            "|    value_loss           | 233         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 880000\n",
            "Best mean reward: 176.87 - Last mean reward per episode: 202.02\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 558         |\n",
            "|    ep_rew_mean          | 209         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1115        |\n",
            "|    iterations           | 54          |\n",
            "|    time_elapsed         | 793         |\n",
            "|    total_timesteps      | 884736      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006102496 |\n",
            "|    clip_fraction        | 0.0578      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.79       |\n",
            "|    explained_variance   | 0.646       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 120         |\n",
            "|    n_updates            | 212         |\n",
            "|    policy_gradient_loss | -0.00198    |\n",
            "|    value_loss           | 263         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 896000\n",
            "Best mean reward: 202.02 - Last mean reward per episode: 229.08\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 446         |\n",
            "|    ep_rew_mean          | 233         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1118        |\n",
            "|    iterations           | 55          |\n",
            "|    time_elapsed         | 805         |\n",
            "|    total_timesteps      | 901120      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006867604 |\n",
            "|    clip_fraction        | 0.0654      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.845      |\n",
            "|    explained_variance   | 0.502       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 193         |\n",
            "|    n_updates            | 216         |\n",
            "|    policy_gradient_loss | -0.00165    |\n",
            "|    value_loss           | 338         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 912000\n",
            "Best mean reward: 229.08 - Last mean reward per episode: 229.97\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 424          |\n",
            "|    ep_rew_mean          | 233          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1122         |\n",
            "|    iterations           | 56           |\n",
            "|    time_elapsed         | 817          |\n",
            "|    total_timesteps      | 917504       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0047241254 |\n",
            "|    clip_fraction        | 0.0473       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.81        |\n",
            "|    explained_variance   | 0.704        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 58           |\n",
            "|    n_updates            | 220          |\n",
            "|    policy_gradient_loss | -0.000537    |\n",
            "|    value_loss           | 149          |\n",
            "------------------------------------------\n",
            "Num timesteps: 928000\n",
            "Best mean reward: 229.97 - Last mean reward per episode: 235.86\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 417         |\n",
            "|    ep_rew_mean          | 235         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1126        |\n",
            "|    iterations           | 57          |\n",
            "|    time_elapsed         | 828         |\n",
            "|    total_timesteps      | 933888      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004641008 |\n",
            "|    clip_fraction        | 0.0474      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.82       |\n",
            "|    explained_variance   | 0.74        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 65.4        |\n",
            "|    n_updates            | 224         |\n",
            "|    policy_gradient_loss | -0.000214   |\n",
            "|    value_loss           | 154         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 944000\n",
            "Best mean reward: 235.86 - Last mean reward per episode: 233.92\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 389          |\n",
            "|    ep_rew_mean          | 238          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1131         |\n",
            "|    iterations           | 58           |\n",
            "|    time_elapsed         | 840          |\n",
            "|    total_timesteps      | 950272       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041692313 |\n",
            "|    clip_fraction        | 0.0464       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.823       |\n",
            "|    explained_variance   | 0.758        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 11.3         |\n",
            "|    n_updates            | 228          |\n",
            "|    policy_gradient_loss | -0.000477    |\n",
            "|    value_loss           | 100          |\n",
            "------------------------------------------\n",
            "Num timesteps: 960000\n",
            "Best mean reward: 235.86 - Last mean reward per episode: 229.44\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 370         |\n",
            "|    ep_rew_mean          | 230         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1135        |\n",
            "|    iterations           | 59          |\n",
            "|    time_elapsed         | 851         |\n",
            "|    total_timesteps      | 966656      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004799627 |\n",
            "|    clip_fraction        | 0.0456      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.812      |\n",
            "|    explained_variance   | 0.641       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 26.5        |\n",
            "|    n_updates            | 232         |\n",
            "|    policy_gradient_loss | -0.00127    |\n",
            "|    value_loss           | 138         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 976000\n",
            "Best mean reward: 235.86 - Last mean reward per episode: 226.91\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 355          |\n",
            "|    ep_rew_mean          | 230          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1140         |\n",
            "|    iterations           | 60           |\n",
            "|    time_elapsed         | 862          |\n",
            "|    total_timesteps      | 983040       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0039198957 |\n",
            "|    clip_fraction        | 0.0322       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.797       |\n",
            "|    explained_variance   | 0.322        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 47.2         |\n",
            "|    n_updates            | 236          |\n",
            "|    policy_gradient_loss | -0.00206     |\n",
            "|    value_loss           | 399          |\n",
            "------------------------------------------\n",
            "Num timesteps: 992000\n",
            "Best mean reward: 235.86 - Last mean reward per episode: 234.84\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 350          |\n",
            "|    ep_rew_mean          | 237          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1144         |\n",
            "|    iterations           | 61           |\n",
            "|    time_elapsed         | 873          |\n",
            "|    total_timesteps      | 999424       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040123044 |\n",
            "|    clip_fraction        | 0.0427       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.803       |\n",
            "|    explained_variance   | 0.672        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 14.2         |\n",
            "|    n_updates            | 240          |\n",
            "|    policy_gradient_loss | -0.000279    |\n",
            "|    value_loss           | 149          |\n",
            "------------------------------------------\n",
            "Num timesteps: 1008000\n",
            "Best mean reward: 235.86 - Last mean reward per episode: 239.13\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 359          |\n",
            "|    ep_rew_mean          | 243          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1149         |\n",
            "|    iterations           | 62           |\n",
            "|    time_elapsed         | 883          |\n",
            "|    total_timesteps      | 1015808      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0039177546 |\n",
            "|    clip_fraction        | 0.0376       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.815       |\n",
            "|    explained_variance   | 0.656        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 165          |\n",
            "|    n_updates            | 244          |\n",
            "|    policy_gradient_loss | -0.000454    |\n",
            "|    value_loss           | 247          |\n",
            "------------------------------------------\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<stable_baselines3.ppo.ppo.PPO at 0x7f69fca35400>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_from_file = False\n",
        "# Hyperparameters are from RL_Zoo\n",
        "# https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/ppo.yml\n",
        "\n",
        "policy = \"MlpPolicy\"\n",
        "n_steps = 1024\n",
        "batch_size = 64\n",
        "n_epochs = 4\n",
        "n_envs = 16\n",
        "n_timesteps = 1e6\n",
        "gamma = 0.999\n",
        "gae_lambda = 0.98\n",
        "ent_coef = 0.01\n",
        "\n",
        "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=\"log_dir_PPO/\")\n",
        "\n",
        "# env\n",
        "env = make_vec_env(\"LunarLander-v2\", n_envs=n_envs, monitor_dir=\"log_dir_PPO/\")\n",
        "\n",
        "# instantiate the agent\n",
        "if train_from_file:\n",
        "  model = PPO.load(path=\"log_dir_PPO/best_model.zip\", env=env)\n",
        "else:\n",
        "  model = PPO(\n",
        "      policy,\n",
        "      env,\n",
        "      n_steps = n_steps,\n",
        "      ent_coef= ent_coef,\n",
        "      batch_size=batch_size,\n",
        "      n_epochs=n_epochs,\n",
        "      gamma=gamma,\n",
        "      gae_lambda=gae_lambda,\n",
        "      tensorboard_log=\"./TensorBoardLog/\", verbose=1)\n",
        "\n",
        "# train the agent\n",
        "model.learn(total_timesteps=n_timesteps, callback=callback)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b1dc8fa",
      "metadata": {
        "id": "5b1dc8fa"
      },
      "source": [
        "# Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "366b80a8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "366b80a8",
        "outputId": "7de4adc0-0ca5-4e7a-f70f-c18a4c1c2fd7"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def moving_average(values, window):\n",
        "    \"\"\"\n",
        "    Smooth values by doing a moving average\n",
        "    :param values: (numpy array)\n",
        "    :param window: (int)\n",
        "    :return: (numpy array)\n",
        "    \"\"\"\n",
        "    weights = np.repeat(1.0, window) / window\n",
        "    return np.convolve(values, weights, 'valid')\n",
        "\n",
        "def plot_results(log_folder, title='Learning Curve'):\n",
        "    \"\"\"\n",
        "    plot the results\n",
        "\n",
        "    :param log_folder: (str) the save location of the results to plot\n",
        "    :param title: (str) the title of the task to plot\n",
        "    \"\"\"\n",
        "\n",
        "    x, y = ts2xy(load_results(log_folder), 'timesteps')\n",
        "    y = moving_average(y, window=100)\n",
        "    # Truncate x\n",
        "    x = x[len(x) - len(y):]\n",
        "    fig = plt.figure(title, figsize=(12,5))\n",
        "    plt.plot(x, y)\n",
        "    plt.xlabel('Number of Timesteps')\n",
        "    plt.ylabel('Rewards')\n",
        "    plt.title(title + \" Smoothed PPO\")\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "plot_results(\"log_dir_PPO\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b00f2a81",
      "metadata": {
        "id": "b00f2a81"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "815393a0",
      "metadata": {
        "id": "815393a0"
      },
      "outputs": [],
      "source": [
        "env = make_vec_env(\"LunarLander-v2\", n_envs=1,monitor_dir=\"evaluate_log_dir_PPO\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "63611e6e",
      "metadata": {
        "id": "63611e6e"
      },
      "outputs": [],
      "source": [
        "model = PPO.load(path=\"log_dir_PPO/best_model.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b06e1a3",
      "metadata": {
        "id": "3b06e1a3"
      },
      "source": [
        "#### Stable Baseline 3 Evaluation Function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "9d4fd326",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d4fd326",
        "outputId": "24d33fe8-2815-4805-9e01-899576a77037"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean & Std Reward after 10 max run is 217.3019382 & 35.82558092694991\n"
          ]
        }
      ],
      "source": [
        "mean_reward, std_reward = evaluate_policy(model, env,n_eval_episodes=10, render=True, deterministic=True)\n",
        "print(\"Mean & Std Reward after {} max run is {} & {}\".format(10,mean_reward, std_reward))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e49c5168",
      "metadata": {
        "id": "e49c5168"
      },
      "source": [
        "# GIF of a Train Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "60cc63dc",
      "metadata": {
        "id": "60cc63dc"
      },
      "outputs": [],
      "source": [
        "env = make_vec_env(\"LunarLander-v2\", n_envs=1)\n",
        "model = PPO.load(path=\"log_dir_PPO/best_model.zip\")\n",
        "\n",
        "images = []\n",
        "obs = env.reset()\n",
        "img = env.render(mode=\"rgb_array\")\n",
        "for i in range(1000):\n",
        "    images.append(img)\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, _, _ ,_ = env.step(action)\n",
        "    img = env.render(mode=\"rgb_array\")\n",
        "\n",
        "imageio.mimsave(\"lunar lander_PPO.gif\", [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3afc060b",
      "metadata": {
        "id": "3afc060b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "857970f990130bbcaee778cf1846f7875676d945310dca1379fe4b5ef3d258a5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
