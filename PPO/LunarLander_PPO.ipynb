{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57601915",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57601915",
        "outputId": "2f13b230-139a-482d-bff6-186e955d592a"
      },
      "outputs": [],
      "source": [
        "\n",
        "# !pip install shap\n",
        "# !pip install opencv-python\n",
        "# !pip install swig\n",
        "# !pip install Box2D\n",
        "\n",
        "\n",
        "# # !pip install box2d pygame\n",
        "\n",
        "\n",
        "# !pip install gym\n",
        "# !pip install pyglet==1.5.27\n",
        "# !pip install stable-baseline3\n",
        "# !pip install \"gymnasium[all]\"\n",
        "\n",
        "# !pip install stable_baselines3\n",
        "\n",
        "## FOR LOCAL Jupyter notebook (no need for google colab)\n",
        "# !pip install tensorflow\n",
        "# !pip install torch\n",
        "# !pip install pygame\n",
        "# !pip install tensorboard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PtTyQewr3gxt",
      "metadata": {
        "id": "PtTyQewr3gxt"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b00a128f",
      "metadata": {
        "id": "b00a128f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import imageio\n",
        "import os\n",
        "from stable_baselines3 import PPO, A2C\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack, SubprocVecEnv, VecNormalize\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common import results_plotter\n",
        "import gymnasium  as gym\n",
        "import matplotlib.pyplot as plt\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "import scipy.stats as stats\n",
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "import tensorflow as tf\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "YtZN-eC7NwuS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtZN-eC7NwuS",
        "outputId": "4e49a85c-8359-45d8-e667-d7a4980aaa72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n",
            "GPU not found. Please ensure that GPU is enabled in Colab.\n"
          ]
        }
      ],
      "source": [
        "# seeds\n",
        "# Set seed for numpy\n",
        "np.random.seed(100)\n",
        "\n",
        "# Set seed for Python random module\n",
        "import random\n",
        "random.seed(100)\n",
        "\n",
        "# Set seed for TensorFlow\n",
        "tf.random.set_seed(100)\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "if tf.test.gpu_device_name():\n",
        "    print('Default GPU Device:', tf.test.gpu_device_name())\n",
        "else:\n",
        "    print(\"GPU not found. Please ensure that GPU is enabled in Colab.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "780afb92",
      "metadata": {
        "id": "780afb92"
      },
      "source": [
        "<h1> Important Libraries To Install </h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2826cd85",
      "metadata": {
        "id": "2826cd85"
      },
      "source": [
        "<h1> Parameter & Environment Information </h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87ef75ca",
      "metadata": {
        "id": "87ef75ca"
      },
      "source": [
        "<p>\n",
        "    This environment is part of the Box2D environments.\n",
        "</p>\n",
        "\n",
        "<ul>\n",
        "    <li> Action Space Discrete(4) </li>\n",
        "    <li> Observation Shape (8,) </li>\n",
        "    <li> Observation High [1.5 1.5 5. 5. 3.14 5. 1. 1. ] </li>\n",
        "    <li> Observation Low [-1.5 -1.5 -5. -5. -3.14 -5. -0. -0. ] </li>\n",
        "    <li> Import gymnasium.make(\"LunarLander-v2\") </li>\n",
        "</ul>\n",
        "\n",
        "<h3> Description </h3>\n",
        "<p>This environment is a classic rocket trajectory optimization problem. According to Pontryagin’s maximum principle, it is optimal to fire the engine at full throttle or turn it off. This is the reason why this environment has discrete actions: engine on or off.\n",
        "\n",
        "There are two environment versions: discrete or continuous. The landing pad is always at coordinates (0,0). The coordinates are the first two numbers in the state vector. Landing outside of the landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt.</p>\n",
        "\n",
        "<h3> Action Space </h3>\n",
        "<p>\n",
        "There are four discrete actions available:\n",
        "\n",
        "* 0: do nothing\n",
        "* 1: fire left orientation engine\n",
        "* 2: fire main engine\n",
        "* 3: fire right orientation engine\n",
        "\n",
        "</p>\n",
        "\n",
        "<h3> Observation Space </h3>\n",
        "<p>\n",
        "The state is an 8-dimensional vector: the coordinates of the lander in x & y, its linear velocities in x & y, its angle, its angular velocity, and two booleans that represent whether each leg is in contact with the ground or not.\n",
        "</p>\n",
        "\n",
        "<h3> Reward </h3>\n",
        "<p>\n",
        "After every step a reward is granted. The total reward of an episode is the sum of the rewards for all the steps within that episode.\n",
        "\n",
        "For each step, the reward:\n",
        "\n",
        "* is increased/decreased the closer/further the lander is to the landing pad.\n",
        "* is increased/decreased the slower/faster the lander is moving.\n",
        "* is decreased the more the lander is tilted (angle not horizontal).\n",
        "* is increased by 10 points for each leg that is in contact with the ground.\n",
        "* is decreased by 0.03 points each frame a side engine is firing.\n",
        "* is decreased by 0.3 points each frame the main engine is firing.\n",
        "\n",
        "The episode receive an additional reward of -100 or +100 points for crashing or landing safely respectively.\n",
        "\n",
        "An episode is considered a solution if it scores at least 200 points.\n",
        "</p>\n",
        "\n",
        "<h3> Starting State </h3>\n",
        "\n",
        "<p>The lander starts at the top center of the viewport with a random initial force applied to its center of mass.</p>\n",
        "\n",
        "<h3> Episode Termination </h3>\n",
        "<p> The episode finishes if:<br>\n",
        "    \n",
        "1. the lander crashes (the lander body gets in contact with the moon);<br>\n",
        "2. the lander gets outside of the viewport (x coordinate is greater than 1);<br>\n",
        "3. the lander is not awake. From the Box2D docs, a body which is not awake is a body which doesn’t move and doesn’t collide with any other body:<br>\n",
        "\n",
        "When Box2D determines that a body (or group of bodies) has come to rest, the body enters a sleep state which has very little CPU overhead. If a body is awake and collides with a sleeping body, then the sleeping body wakes up. Bodies will also wake up if a joint or contact attached to them is destroyed.\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3d0543a5",
      "metadata": {
        "id": "3d0543a5",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b3fb45b2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3fb45b2",
        "outputId": "f81eb084-19b4-4f51-f4f4-c82607cc6cb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Action inter is descrete 4\n",
            "Shape of Observation is (8,)\n"
          ]
        }
      ],
      "source": [
        "print(\"The Action inter is descrete {}\".format(env.action_space.n))\n",
        "print(\"Shape of Observation is {}\".format(env.observation_space.sample().shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b9ff9c9",
      "metadata": {
        "id": "8b9ff9c9"
      },
      "source": [
        "<h1> Baseline Model. </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d35101af",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d35101af",
        "outputId": "94c9ecca-7b72-44d5-de3b-2c535bf0a5ea",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Reward after 10 max run is -0.9202175383771041\n"
          ]
        }
      ],
      "source": [
        "rewards = []\n",
        "obs = env.reset()\n",
        "done = False\n",
        "MAX_RUN = 10\n",
        "\n",
        "for i in range(MAX_RUN):\n",
        "    while not done:\n",
        "        env.render()\n",
        "        action_sample = env.action_space.sample()\n",
        "        # let's take a step in the environment\n",
        "        obs, rwd, done, info ,_  = env.step(action_sample)\n",
        "        rewards.append(rwd)\n",
        "env.close()\n",
        "print(\"Mean Reward after {} max run is {}\".format(MAX_RUN, np.mean(np.array(rewards))))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac737551",
      "metadata": {
        "id": "ac737551"
      },
      "source": [
        "<h1> Reinforcement Learning For Training The Model </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "bdaa0e55",
      "metadata": {
        "id": "bdaa0e55"
      },
      "outputs": [],
      "source": [
        "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
        "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
        "\n",
        "    :param check_freq: (int)\n",
        "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
        "      It must contains the file created by the ``Monitor`` wrapper.\n",
        "    :param verbose: (int)\n",
        "    \"\"\"\n",
        "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
        "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
        "        self.check_freq = check_freq\n",
        "        self.log_dir = log_dir\n",
        "        self.save_path = os.path.join(log_dir, 'best_model')\n",
        "        self.best_mean_reward = -np.inf\n",
        "\n",
        "    def _init_callback(self) -> None:\n",
        "        # Create folder if needed\n",
        "        if self.save_path is not None:\n",
        "            os.makedirs(self.save_path, exist_ok=True)\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.n_calls % self.check_freq == 0:\n",
        "\n",
        "          # Retrieve training reward\n",
        "          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
        "          if len(x) > 0:\n",
        "              # Mean training reward over the last 100 episodes\n",
        "              mean_reward = np.mean(y[-100:])\n",
        "              if self.verbose > 0:\n",
        "                print(f\"Num timesteps: {self.num_timesteps}\")\n",
        "                print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n",
        "\n",
        "              # New best model, you could save the agent here\n",
        "              if mean_reward > self.best_mean_reward:\n",
        "                  self.best_mean_reward = mean_reward\n",
        "                  # Example for saving best model\n",
        "                  if self.verbose > 0:\n",
        "                    print(f\"Saving new best model to {self.save_path}.zip\")\n",
        "                  self.model.save(self.save_path)\n",
        "\n",
        "        return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "2d311d6a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d311d6a",
        "outputId": "3c960c90-bbc7-4506-a186-3928d014cd7f",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "Logging to ./TensorBoardLog/PPO_2\n",
            "Num timesteps: 16000\n",
            "Best mean reward: -inf - Last mean reward per episode: -169.03\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 89.5     |\n",
            "|    ep_rew_mean     | -174     |\n",
            "| time/              |          |\n",
            "|    fps             | 4331     |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 3        |\n",
            "|    total_timesteps | 16384    |\n",
            "---------------------------------\n",
            "Num timesteps: 32000\n",
            "Best mean reward: -169.03 - Last mean reward per episode: -171.60\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 99.7        |\n",
            "|    ep_rew_mean          | -172        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 2644        |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 12          |\n",
            "|    total_timesteps      | 32768       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009979017 |\n",
            "|    clip_fraction        | 0.0428      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.38       |\n",
            "|    explained_variance   | 0.000611    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.46e+03    |\n",
            "|    n_updates            | 4           |\n",
            "|    policy_gradient_loss | -0.00444    |\n",
            "|    value_loss           | 4.57e+03    |\n",
            "-----------------------------------------\n",
            "Num timesteps: 48000\n",
            "Best mean reward: -169.03 - Last mean reward per episode: -117.05\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 97.3        |\n",
            "|    ep_rew_mean          | -120        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 2321        |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 21          |\n",
            "|    total_timesteps      | 49152       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010995761 |\n",
            "|    clip_fraction        | 0.056       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.37       |\n",
            "|    explained_variance   | 0.000896    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.11e+03    |\n",
            "|    n_updates            | 8           |\n",
            "|    policy_gradient_loss | -0.00616    |\n",
            "|    value_loss           | 3.11e+03    |\n",
            "-----------------------------------------\n",
            "Num timesteps: 64000\n",
            "Best mean reward: -117.05 - Last mean reward per episode: -121.20\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 94.7        |\n",
            "|    ep_rew_mean          | -120        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 2236        |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 29          |\n",
            "|    total_timesteps      | 65536       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006195801 |\n",
            "|    clip_fraction        | 0.0373      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.37       |\n",
            "|    explained_variance   | 0.00142     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 384         |\n",
            "|    n_updates            | 12          |\n",
            "|    policy_gradient_loss | -0.00421    |\n",
            "|    value_loss           | 1.09e+03    |\n",
            "-----------------------------------------\n",
            "Num timesteps: 80000\n",
            "Best mean reward: -117.05 - Last mean reward per episode: -111.84\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 103          |\n",
            "|    ep_rew_mean          | -117         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 2153         |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 38           |\n",
            "|    total_timesteps      | 81920        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0074710594 |\n",
            "|    clip_fraction        | 0.0545       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.36        |\n",
            "|    explained_variance   | -0.0114      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 347          |\n",
            "|    n_updates            | 16           |\n",
            "|    policy_gradient_loss | -0.0015      |\n",
            "|    value_loss           | 1.05e+03     |\n",
            "------------------------------------------\n",
            "Num timesteps: 96000\n",
            "Best mean reward: -111.84 - Last mean reward per episode: -101.02\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 98.2        |\n",
            "|    ep_rew_mean          | -99.1       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 2109        |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 46          |\n",
            "|    total_timesteps      | 98304       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005391952 |\n",
            "|    clip_fraction        | 0.013       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.36       |\n",
            "|    explained_variance   | 0.0502      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 369         |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00269    |\n",
            "|    value_loss           | 713         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 112000\n",
            "Best mean reward: -101.02 - Last mean reward per episode: -90.88\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 111         |\n",
            "|    ep_rew_mean          | -94.6       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 2071        |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 55          |\n",
            "|    total_timesteps      | 114688      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010190461 |\n",
            "|    clip_fraction        | 0.081       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.34       |\n",
            "|    explained_variance   | 0.144       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 193         |\n",
            "|    n_updates            | 24          |\n",
            "|    policy_gradient_loss | -0.00545    |\n",
            "|    value_loss           | 540         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 128000\n",
            "Best mean reward: -90.88 - Last mean reward per episode: -65.23\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 110         |\n",
            "|    ep_rew_mean          | -65         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 2051        |\n",
            "|    iterations           | 8           |\n",
            "|    time_elapsed         | 63          |\n",
            "|    total_timesteps      | 131072      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010422713 |\n",
            "|    clip_fraction        | 0.0944      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.3        |\n",
            "|    explained_variance   | 0.0883      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 201         |\n",
            "|    n_updates            | 28          |\n",
            "|    policy_gradient_loss | -0.00719    |\n",
            "|    value_loss           | 493         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 144000\n",
            "Best mean reward: -65.23 - Last mean reward per episode: -65.96\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 110         |\n",
            "|    ep_rew_mean          | -65.2       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 2031        |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 72          |\n",
            "|    total_timesteps      | 147456      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009125222 |\n",
            "|    clip_fraction        | 0.0865      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.27       |\n",
            "|    explained_variance   | 0.0216      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 194         |\n",
            "|    n_updates            | 32          |\n",
            "|    policy_gradient_loss | -0.00392    |\n",
            "|    value_loss           | 364         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 160000\n",
            "Best mean reward: -65.23 - Last mean reward per episode: -35.49\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 121         |\n",
            "|    ep_rew_mean          | -34.1       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 2019        |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 81          |\n",
            "|    total_timesteps      | 163840      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011101779 |\n",
            "|    clip_fraction        | 0.0948      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.22       |\n",
            "|    explained_variance   | -0.0209     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 252         |\n",
            "|    n_updates            | 36          |\n",
            "|    policy_gradient_loss | -0.00654    |\n",
            "|    value_loss           | 403         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 176000\n",
            "Best mean reward: -35.49 - Last mean reward per episode: -25.83\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 141         |\n",
            "|    ep_rew_mean          | -22.5       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1977        |\n",
            "|    iterations           | 11          |\n",
            "|    time_elapsed         | 91          |\n",
            "|    total_timesteps      | 180224      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007113008 |\n",
            "|    clip_fraction        | 0.031       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.21       |\n",
            "|    explained_variance   | -0.00237    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 139         |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00274    |\n",
            "|    value_loss           | 411         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 192000\n",
            "Best mean reward: -25.83 - Last mean reward per episode: -14.51\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 178         |\n",
            "|    ep_rew_mean          | -16         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1915        |\n",
            "|    iterations           | 12          |\n",
            "|    time_elapsed         | 102         |\n",
            "|    total_timesteps      | 196608      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010257053 |\n",
            "|    clip_fraction        | 0.0185      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.19       |\n",
            "|    explained_variance   | 0.0219      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 160         |\n",
            "|    n_updates            | 44          |\n",
            "|    policy_gradient_loss | -0.00398    |\n",
            "|    value_loss           | 430         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 208000\n",
            "Best mean reward: -14.51 - Last mean reward per episode: -16.75\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 207          |\n",
            "|    ep_rew_mean          | -18.6        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1845         |\n",
            "|    iterations           | 13           |\n",
            "|    time_elapsed         | 115          |\n",
            "|    total_timesteps      | 212992       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0090572145 |\n",
            "|    clip_fraction        | 0.048        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.16        |\n",
            "|    explained_variance   | 0.00226      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 232          |\n",
            "|    n_updates            | 48           |\n",
            "|    policy_gradient_loss | -0.00311     |\n",
            "|    value_loss           | 447          |\n",
            "------------------------------------------\n",
            "Num timesteps: 224000\n",
            "Best mean reward: -14.51 - Last mean reward per episode: -18.22\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 277          |\n",
            "|    ep_rew_mean          | -16.5        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1734         |\n",
            "|    iterations           | 14           |\n",
            "|    time_elapsed         | 132          |\n",
            "|    total_timesteps      | 229376       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0065485016 |\n",
            "|    clip_fraction        | 0.0574       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.16        |\n",
            "|    explained_variance   | -0.00811     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 380          |\n",
            "|    n_updates            | 52           |\n",
            "|    policy_gradient_loss | -0.0021      |\n",
            "|    value_loss           | 630          |\n",
            "------------------------------------------\n",
            "Num timesteps: 240000\n",
            "Best mean reward: -14.51 - Last mean reward per episode: -17.70\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 349         |\n",
            "|    ep_rew_mean          | -22.2       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1662        |\n",
            "|    iterations           | 15          |\n",
            "|    time_elapsed         | 147         |\n",
            "|    total_timesteps      | 245760      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007220728 |\n",
            "|    clip_fraction        | 0.0845      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.19       |\n",
            "|    explained_variance   | 0.00429     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 205         |\n",
            "|    n_updates            | 56          |\n",
            "|    policy_gradient_loss | -0.00289    |\n",
            "|    value_loss           | 541         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 256000\n",
            "Best mean reward: -14.51 - Last mean reward per episode: -21.52\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 404          |\n",
            "|    ep_rew_mean          | -17.1        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1582         |\n",
            "|    iterations           | 16           |\n",
            "|    time_elapsed         | 165          |\n",
            "|    total_timesteps      | 262144       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0042085024 |\n",
            "|    clip_fraction        | 0.0137       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.15        |\n",
            "|    explained_variance   | 0.0471       |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 216          |\n",
            "|    n_updates            | 60           |\n",
            "|    policy_gradient_loss | -0.00112     |\n",
            "|    value_loss           | 478          |\n",
            "------------------------------------------\n",
            "Num timesteps: 272000\n",
            "Best mean reward: -14.51 - Last mean reward per episode: -6.84\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 443          |\n",
            "|    ep_rew_mean          | -7.44        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1512         |\n",
            "|    iterations           | 17           |\n",
            "|    time_elapsed         | 184          |\n",
            "|    total_timesteps      | 278528       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0053147855 |\n",
            "|    clip_fraction        | 0.021        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.17        |\n",
            "|    explained_variance   | 0.347        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 89.8         |\n",
            "|    n_updates            | 64           |\n",
            "|    policy_gradient_loss | -0.00233     |\n",
            "|    value_loss           | 323          |\n",
            "------------------------------------------\n",
            "Num timesteps: 288000\n",
            "Best mean reward: -6.84 - Last mean reward per episode: -6.33\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 472         |\n",
            "|    ep_rew_mean          | -2.02       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1474        |\n",
            "|    iterations           | 18          |\n",
            "|    time_elapsed         | 199         |\n",
            "|    total_timesteps      | 294912      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004267752 |\n",
            "|    clip_fraction        | 0.0288      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.17       |\n",
            "|    explained_variance   | 0.47        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 165         |\n",
            "|    n_updates            | 68          |\n",
            "|    policy_gradient_loss | -0.00219    |\n",
            "|    value_loss           | 260         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 304000\n",
            "Best mean reward: -6.33 - Last mean reward per episode: 5.05\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 538         |\n",
            "|    ep_rew_mean          | 13.6        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1422        |\n",
            "|    iterations           | 19          |\n",
            "|    time_elapsed         | 218         |\n",
            "|    total_timesteps      | 311296      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003527724 |\n",
            "|    clip_fraction        | 0.0278      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.15       |\n",
            "|    explained_variance   | 0.544       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 129         |\n",
            "|    n_updates            | 72          |\n",
            "|    policy_gradient_loss | -0.00189    |\n",
            "|    value_loss           | 288         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 320000\n",
            "Best mean reward: 5.05 - Last mean reward per episode: 16.76\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 534          |\n",
            "|    ep_rew_mean          | 21           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1380         |\n",
            "|    iterations           | 20           |\n",
            "|    time_elapsed         | 237          |\n",
            "|    total_timesteps      | 327680       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0037055404 |\n",
            "|    clip_fraction        | 0.0277       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.16        |\n",
            "|    explained_variance   | 0.704        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 65           |\n",
            "|    n_updates            | 76           |\n",
            "|    policy_gradient_loss | -0.00201     |\n",
            "|    value_loss           | 145          |\n",
            "------------------------------------------\n",
            "Num timesteps: 336000\n",
            "Best mean reward: 16.76 - Last mean reward per episode: 26.66\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 591          |\n",
            "|    ep_rew_mean          | 27.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1342         |\n",
            "|    iterations           | 21           |\n",
            "|    time_elapsed         | 256          |\n",
            "|    total_timesteps      | 344064       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0054808683 |\n",
            "|    clip_fraction        | 0.0423       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.13        |\n",
            "|    explained_variance   | 0.762        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 74.3         |\n",
            "|    n_updates            | 80           |\n",
            "|    policy_gradient_loss | -0.00241     |\n",
            "|    value_loss           | 145          |\n",
            "------------------------------------------\n",
            "Num timesteps: 352000\n",
            "Best mean reward: 26.66 - Last mean reward per episode: 36.83\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 684         |\n",
            "|    ep_rew_mean          | 44.5        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1310        |\n",
            "|    iterations           | 22          |\n",
            "|    time_elapsed         | 275         |\n",
            "|    total_timesteps      | 360448      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004405304 |\n",
            "|    clip_fraction        | 0.0351      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.13       |\n",
            "|    explained_variance   | 0.639       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 68.3        |\n",
            "|    n_updates            | 84          |\n",
            "|    policy_gradient_loss | -0.00254    |\n",
            "|    value_loss           | 223         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 368000\n",
            "Best mean reward: 36.83 - Last mean reward per episode: 47.25\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 718          |\n",
            "|    ep_rew_mean          | 50           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1281         |\n",
            "|    iterations           | 23           |\n",
            "|    time_elapsed         | 294          |\n",
            "|    total_timesteps      | 376832       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0053087734 |\n",
            "|    clip_fraction        | 0.0455       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.08        |\n",
            "|    explained_variance   | 0.776        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 188          |\n",
            "|    n_updates            | 88           |\n",
            "|    policy_gradient_loss | -0.00194     |\n",
            "|    value_loss           | 148          |\n",
            "------------------------------------------\n",
            "Num timesteps: 384000\n",
            "Best mean reward: 47.25 - Last mean reward per episode: 54.84\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 751          |\n",
            "|    ep_rew_mean          | 59.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1256         |\n",
            "|    iterations           | 24           |\n",
            "|    time_elapsed         | 312          |\n",
            "|    total_timesteps      | 393216       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0066373795 |\n",
            "|    clip_fraction        | 0.0596       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.09        |\n",
            "|    explained_variance   | 0.866        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 36.3         |\n",
            "|    n_updates            | 92           |\n",
            "|    policy_gradient_loss | -0.00195     |\n",
            "|    value_loss           | 88.6         |\n",
            "------------------------------------------\n",
            "Num timesteps: 400000\n",
            "Best mean reward: 54.84 - Last mean reward per episode: 65.35\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 790         |\n",
            "|    ep_rew_mean          | 68.7        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1231        |\n",
            "|    iterations           | 25          |\n",
            "|    time_elapsed         | 332         |\n",
            "|    total_timesteps      | 409600      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003427125 |\n",
            "|    clip_fraction        | 0.0234      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.09       |\n",
            "|    explained_variance   | 0.885       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 62          |\n",
            "|    n_updates            | 96          |\n",
            "|    policy_gradient_loss | -0.000871   |\n",
            "|    value_loss           | 86          |\n",
            "-----------------------------------------\n",
            "Num timesteps: 416000\n",
            "Best mean reward: 65.35 - Last mean reward per episode: 73.60\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 831          |\n",
            "|    ep_rew_mean          | 85.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1208         |\n",
            "|    iterations           | 26           |\n",
            "|    time_elapsed         | 352          |\n",
            "|    total_timesteps      | 425984       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035413506 |\n",
            "|    clip_fraction        | 0.029        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.11        |\n",
            "|    explained_variance   | 0.852        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 23.1         |\n",
            "|    n_updates            | 100          |\n",
            "|    policy_gradient_loss | -0.000489    |\n",
            "|    value_loss           | 108          |\n",
            "------------------------------------------\n",
            "Num timesteps: 432000\n",
            "Best mean reward: 73.60 - Last mean reward per episode: 84.86\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 846         |\n",
            "|    ep_rew_mean          | 92.1        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1189        |\n",
            "|    iterations           | 27          |\n",
            "|    time_elapsed         | 371         |\n",
            "|    total_timesteps      | 442368      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006150531 |\n",
            "|    clip_fraction        | 0.0495      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.1        |\n",
            "|    explained_variance   | 0.938       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 15.4        |\n",
            "|    n_updates            | 104         |\n",
            "|    policy_gradient_loss | -0.00148    |\n",
            "|    value_loss           | 38.9        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 448000\n",
            "Best mean reward: 84.86 - Last mean reward per episode: 95.82\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 869          |\n",
            "|    ep_rew_mean          | 100          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1172         |\n",
            "|    iterations           | 28           |\n",
            "|    time_elapsed         | 391          |\n",
            "|    total_timesteps      | 458752       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0061052795 |\n",
            "|    clip_fraction        | 0.0511       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.07        |\n",
            "|    explained_variance   | 0.932        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 16.2         |\n",
            "|    n_updates            | 108          |\n",
            "|    policy_gradient_loss | -0.00147     |\n",
            "|    value_loss           | 52.4         |\n",
            "------------------------------------------\n",
            "Num timesteps: 464000\n",
            "Best mean reward: 95.82 - Last mean reward per episode: 101.96\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 886          |\n",
            "|    ep_rew_mean          | 106          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1153         |\n",
            "|    iterations           | 29           |\n",
            "|    time_elapsed         | 411          |\n",
            "|    total_timesteps      | 475136       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0061788787 |\n",
            "|    clip_fraction        | 0.022        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.09        |\n",
            "|    explained_variance   | 0.946        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 17.1         |\n",
            "|    n_updates            | 112          |\n",
            "|    policy_gradient_loss | -0.00128     |\n",
            "|    value_loss           | 49.8         |\n",
            "------------------------------------------\n",
            "Num timesteps: 480000\n",
            "Best mean reward: 101.96 - Last mean reward per episode: 107.86\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 921          |\n",
            "|    ep_rew_mean          | 108          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1136         |\n",
            "|    iterations           | 30           |\n",
            "|    time_elapsed         | 432          |\n",
            "|    total_timesteps      | 491520       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0032074773 |\n",
            "|    clip_fraction        | 0.0311       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.07        |\n",
            "|    explained_variance   | 0.93         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 25.4         |\n",
            "|    n_updates            | 116          |\n",
            "|    policy_gradient_loss | -0.00121     |\n",
            "|    value_loss           | 71           |\n",
            "------------------------------------------\n",
            "Num timesteps: 496000\n",
            "Best mean reward: 107.86 - Last mean reward per episode: 109.40\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 921          |\n",
            "|    ep_rew_mean          | 112          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1122         |\n",
            "|    iterations           | 31           |\n",
            "|    time_elapsed         | 452          |\n",
            "|    total_timesteps      | 507904       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044546183 |\n",
            "|    clip_fraction        | 0.03         |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.06        |\n",
            "|    explained_variance   | 0.967        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 16.9         |\n",
            "|    n_updates            | 120          |\n",
            "|    policy_gradient_loss | -0.000303    |\n",
            "|    value_loss           | 29.9         |\n",
            "------------------------------------------\n",
            "Num timesteps: 512000\n",
            "Best mean reward: 109.40 - Last mean reward per episode: 113.15\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 929         |\n",
            "|    ep_rew_mean          | 114         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1111        |\n",
            "|    iterations           | 32          |\n",
            "|    time_elapsed         | 471         |\n",
            "|    total_timesteps      | 524288      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004689337 |\n",
            "|    clip_fraction        | 0.0361      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.05       |\n",
            "|    explained_variance   | 0.971       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 33.8        |\n",
            "|    n_updates            | 124         |\n",
            "|    policy_gradient_loss | -0.00167    |\n",
            "|    value_loss           | 26.4        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 528000\n",
            "Best mean reward: 113.15 - Last mean reward per episode: 112.25\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 901          |\n",
            "|    ep_rew_mean          | 112          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1099         |\n",
            "|    iterations           | 33           |\n",
            "|    time_elapsed         | 491          |\n",
            "|    total_timesteps      | 540672       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0052047158 |\n",
            "|    clip_fraction        | 0.0296       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.04        |\n",
            "|    explained_variance   | 0.964        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 59           |\n",
            "|    n_updates            | 128          |\n",
            "|    policy_gradient_loss | -0.000588    |\n",
            "|    value_loss           | 37.1         |\n",
            "------------------------------------------\n",
            "Num timesteps: 544000\n",
            "Best mean reward: 113.15 - Last mean reward per episode: 113.35\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 906          |\n",
            "|    ep_rew_mean          | 112          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1089         |\n",
            "|    iterations           | 34           |\n",
            "|    time_elapsed         | 511          |\n",
            "|    total_timesteps      | 557056       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0046848934 |\n",
            "|    clip_fraction        | 0.0207       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.06        |\n",
            "|    explained_variance   | 0.945        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 14.6         |\n",
            "|    n_updates            | 132          |\n",
            "|    policy_gradient_loss | -0.000409    |\n",
            "|    value_loss           | 62.1         |\n",
            "------------------------------------------\n",
            "Num timesteps: 560000\n",
            "Best mean reward: 113.35 - Last mean reward per episode: 113.74\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 888         |\n",
            "|    ep_rew_mean          | 111         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1082        |\n",
            "|    iterations           | 35          |\n",
            "|    time_elapsed         | 529         |\n",
            "|    total_timesteps      | 573440      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004727387 |\n",
            "|    clip_fraction        | 0.0342      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.03       |\n",
            "|    explained_variance   | 0.945       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.28        |\n",
            "|    n_updates            | 136         |\n",
            "|    policy_gradient_loss | -0.000122   |\n",
            "|    value_loss           | 58.6        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 576000\n",
            "Best mean reward: 113.74 - Last mean reward per episode: 110.24\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 879         |\n",
            "|    ep_rew_mean          | 114         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1074        |\n",
            "|    iterations           | 36          |\n",
            "|    time_elapsed         | 549         |\n",
            "|    total_timesteps      | 589824      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004601469 |\n",
            "|    clip_fraction        | 0.0216      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.04       |\n",
            "|    explained_variance   | 0.929       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 16.1        |\n",
            "|    n_updates            | 140         |\n",
            "|    policy_gradient_loss | 0.000182    |\n",
            "|    value_loss           | 89.7        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 592000\n",
            "Best mean reward: 113.74 - Last mean reward per episode: 112.40\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 833         |\n",
            "|    ep_rew_mean          | 109         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1064        |\n",
            "|    iterations           | 37          |\n",
            "|    time_elapsed         | 569         |\n",
            "|    total_timesteps      | 606208      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003949446 |\n",
            "|    clip_fraction        | 0.0275      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.948      |\n",
            "|    explained_variance   | 0.924       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 13.7        |\n",
            "|    n_updates            | 144         |\n",
            "|    policy_gradient_loss | -0.000166   |\n",
            "|    value_loss           | 71.4        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 608000\n",
            "Best mean reward: 113.74 - Last mean reward per episode: 109.66\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 827          |\n",
            "|    ep_rew_mean          | 113          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1057         |\n",
            "|    iterations           | 38           |\n",
            "|    time_elapsed         | 588          |\n",
            "|    total_timesteps      | 622592       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0039750854 |\n",
            "|    clip_fraction        | 0.0301       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.985       |\n",
            "|    explained_variance   | 0.887        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 85.6         |\n",
            "|    n_updates            | 148          |\n",
            "|    policy_gradient_loss | -0.00109     |\n",
            "|    value_loss           | 119          |\n",
            "------------------------------------------\n",
            "Num timesteps: 624000\n",
            "Best mean reward: 113.74 - Last mean reward per episode: 111.99\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 822         |\n",
            "|    ep_rew_mean          | 111         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1048        |\n",
            "|    iterations           | 39          |\n",
            "|    time_elapsed         | 609         |\n",
            "|    total_timesteps      | 638976      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004806943 |\n",
            "|    clip_fraction        | 0.044       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.979      |\n",
            "|    explained_variance   | 0.934       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 26.7        |\n",
            "|    n_updates            | 152         |\n",
            "|    policy_gradient_loss | -0.000172   |\n",
            "|    value_loss           | 69.6        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 640000\n",
            "Best mean reward: 113.74 - Last mean reward per episode: 111.22\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 830         |\n",
            "|    ep_rew_mean          | 114         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1039        |\n",
            "|    iterations           | 40          |\n",
            "|    time_elapsed         | 630         |\n",
            "|    total_timesteps      | 655360      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004771688 |\n",
            "|    clip_fraction        | 0.0396      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.999      |\n",
            "|    explained_variance   | 0.953       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.29        |\n",
            "|    n_updates            | 156         |\n",
            "|    policy_gradient_loss | -0.000627   |\n",
            "|    value_loss           | 45.8        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 656000\n",
            "Best mean reward: 113.74 - Last mean reward per episode: 114.12\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 819          |\n",
            "|    ep_rew_mean          | 107          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1033         |\n",
            "|    iterations           | 41           |\n",
            "|    time_elapsed         | 650          |\n",
            "|    total_timesteps      | 671744       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0059849285 |\n",
            "|    clip_fraction        | 0.0542       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.01        |\n",
            "|    explained_variance   | 0.965        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 10.8         |\n",
            "|    n_updates            | 160          |\n",
            "|    policy_gradient_loss | -0.00134     |\n",
            "|    value_loss           | 37.2         |\n",
            "------------------------------------------\n",
            "Num timesteps: 672000\n",
            "Best mean reward: 114.12 - Last mean reward per episode: 105.66\n",
            "Num timesteps: 688000\n",
            "Best mean reward: 114.12 - Last mean reward per episode: 109.51\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 849          |\n",
            "|    ep_rew_mean          | 110          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1025         |\n",
            "|    iterations           | 42           |\n",
            "|    time_elapsed         | 671          |\n",
            "|    total_timesteps      | 688128       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0037358163 |\n",
            "|    clip_fraction        | 0.0219       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.985       |\n",
            "|    explained_variance   | 0.928        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 39.4         |\n",
            "|    n_updates            | 164          |\n",
            "|    policy_gradient_loss | -0.00068     |\n",
            "|    value_loss           | 71.9         |\n",
            "------------------------------------------\n",
            "Num timesteps: 704000\n",
            "Best mean reward: 114.12 - Last mean reward per episode: 110.94\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 863          |\n",
            "|    ep_rew_mean          | 111          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1019         |\n",
            "|    iterations           | 43           |\n",
            "|    time_elapsed         | 691          |\n",
            "|    total_timesteps      | 704512       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040172786 |\n",
            "|    clip_fraction        | 0.0327       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.972       |\n",
            "|    explained_variance   | 0.936        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 26.7         |\n",
            "|    n_updates            | 168          |\n",
            "|    policy_gradient_loss | -0.000321    |\n",
            "|    value_loss           | 43.9         |\n",
            "------------------------------------------\n",
            "Num timesteps: 720000\n",
            "Best mean reward: 114.12 - Last mean reward per episode: 113.43\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 898         |\n",
            "|    ep_rew_mean          | 114         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1012        |\n",
            "|    iterations           | 44          |\n",
            "|    time_elapsed         | 711         |\n",
            "|    total_timesteps      | 720896      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005698392 |\n",
            "|    clip_fraction        | 0.0476      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.01       |\n",
            "|    explained_variance   | 0.98        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.42        |\n",
            "|    n_updates            | 172         |\n",
            "|    policy_gradient_loss | 0.000299    |\n",
            "|    value_loss           | 19          |\n",
            "-----------------------------------------\n",
            "Num timesteps: 736000\n",
            "Best mean reward: 114.12 - Last mean reward per episode: 116.47\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 932          |\n",
            "|    ep_rew_mean          | 118          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1004         |\n",
            "|    iterations           | 45           |\n",
            "|    time_elapsed         | 733          |\n",
            "|    total_timesteps      | 737280       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0051298635 |\n",
            "|    clip_fraction        | 0.048        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.02        |\n",
            "|    explained_variance   | 0.991        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.7          |\n",
            "|    n_updates            | 176          |\n",
            "|    policy_gradient_loss | -0.00105     |\n",
            "|    value_loss           | 6.56         |\n",
            "------------------------------------------\n",
            "Num timesteps: 752000\n",
            "Best mean reward: 116.47 - Last mean reward per episode: 119.86\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 932          |\n",
            "|    ep_rew_mean          | 120          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 998          |\n",
            "|    iterations           | 46           |\n",
            "|    time_elapsed         | 754          |\n",
            "|    total_timesteps      | 753664       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0043129553 |\n",
            "|    clip_fraction        | 0.0339       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.987       |\n",
            "|    explained_variance   | 0.983        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.79         |\n",
            "|    n_updates            | 180          |\n",
            "|    policy_gradient_loss | -0.00062     |\n",
            "|    value_loss           | 8.49         |\n",
            "------------------------------------------\n",
            "Num timesteps: 768000\n",
            "Best mean reward: 119.86 - Last mean reward per episode: 127.42\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 978          |\n",
            "|    ep_rew_mean          | 129          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 993          |\n",
            "|    iterations           | 47           |\n",
            "|    time_elapsed         | 775          |\n",
            "|    total_timesteps      | 770048       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0058631324 |\n",
            "|    clip_fraction        | 0.0555       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.987       |\n",
            "|    explained_variance   | 0.994        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1            |\n",
            "|    n_updates            | 184          |\n",
            "|    policy_gradient_loss | -0.00141     |\n",
            "|    value_loss           | 4.31         |\n",
            "------------------------------------------\n",
            "Num timesteps: 784000\n",
            "Best mean reward: 127.42 - Last mean reward per episode: 131.19\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 987         |\n",
            "|    ep_rew_mean          | 132         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 987         |\n",
            "|    iterations           | 48          |\n",
            "|    time_elapsed         | 796         |\n",
            "|    total_timesteps      | 786432      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006737025 |\n",
            "|    clip_fraction        | 0.0404      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1          |\n",
            "|    explained_variance   | 0.993       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.56        |\n",
            "|    n_updates            | 188         |\n",
            "|    policy_gradient_loss | -0.00121    |\n",
            "|    value_loss           | 4.01        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 800000\n",
            "Best mean reward: 131.19 - Last mean reward per episode: 132.60\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 985         |\n",
            "|    ep_rew_mean          | 132         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 982         |\n",
            "|    iterations           | 49          |\n",
            "|    time_elapsed         | 817         |\n",
            "|    total_timesteps      | 802816      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006106767 |\n",
            "|    clip_fraction        | 0.0368      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.994      |\n",
            "|    explained_variance   | 0.996       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.31        |\n",
            "|    n_updates            | 192         |\n",
            "|    policy_gradient_loss | -0.000318   |\n",
            "|    value_loss           | 3.08        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 816000\n",
            "Best mean reward: 132.60 - Last mean reward per episode: 128.27\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 964          |\n",
            "|    ep_rew_mean          | 129          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 979          |\n",
            "|    iterations           | 50           |\n",
            "|    time_elapsed         | 836          |\n",
            "|    total_timesteps      | 819200       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0049598515 |\n",
            "|    clip_fraction        | 0.0236       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.962       |\n",
            "|    explained_variance   | 0.968        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.47         |\n",
            "|    n_updates            | 196          |\n",
            "|    policy_gradient_loss | -0.000254    |\n",
            "|    value_loss           | 34.6         |\n",
            "------------------------------------------\n",
            "Num timesteps: 832000\n",
            "Best mean reward: 132.60 - Last mean reward per episode: 128.05\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 957          |\n",
            "|    ep_rew_mean          | 128          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 976          |\n",
            "|    iterations           | 51           |\n",
            "|    time_elapsed         | 855          |\n",
            "|    total_timesteps      | 835584       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0045895707 |\n",
            "|    clip_fraction        | 0.0457       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.967       |\n",
            "|    explained_variance   | 0.94         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.24         |\n",
            "|    n_updates            | 200          |\n",
            "|    policy_gradient_loss | -0.000459    |\n",
            "|    value_loss           | 56.2         |\n",
            "------------------------------------------\n",
            "Num timesteps: 848000\n",
            "Best mean reward: 132.60 - Last mean reward per episode: 127.86\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 944          |\n",
            "|    ep_rew_mean          | 127          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 972          |\n",
            "|    iterations           | 52           |\n",
            "|    time_elapsed         | 876          |\n",
            "|    total_timesteps      | 851968       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0051617688 |\n",
            "|    clip_fraction        | 0.0598       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.97        |\n",
            "|    explained_variance   | 0.984        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.73         |\n",
            "|    n_updates            | 204          |\n",
            "|    policy_gradient_loss | -0.00097     |\n",
            "|    value_loss           | 18.4         |\n",
            "------------------------------------------\n",
            "Num timesteps: 864000\n",
            "Best mean reward: 132.60 - Last mean reward per episode: 124.63\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 930         |\n",
            "|    ep_rew_mean          | 124         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 969         |\n",
            "|    iterations           | 53          |\n",
            "|    time_elapsed         | 895         |\n",
            "|    total_timesteps      | 868352      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006625493 |\n",
            "|    clip_fraction        | 0.0537      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.982      |\n",
            "|    explained_variance   | 0.971       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 57.1        |\n",
            "|    n_updates            | 208         |\n",
            "|    policy_gradient_loss | -0.000297   |\n",
            "|    value_loss           | 30.8        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 880000\n",
            "Best mean reward: 132.60 - Last mean reward per episode: 119.74\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 902         |\n",
            "|    ep_rew_mean          | 120         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 967         |\n",
            "|    iterations           | 54          |\n",
            "|    time_elapsed         | 914         |\n",
            "|    total_timesteps      | 884736      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006208048 |\n",
            "|    clip_fraction        | 0.0424      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.957      |\n",
            "|    explained_variance   | 0.979       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.56        |\n",
            "|    n_updates            | 212         |\n",
            "|    policy_gradient_loss | -9.57e-05   |\n",
            "|    value_loss           | 27.5        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 896000\n",
            "Best mean reward: 132.60 - Last mean reward per episode: 121.36\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 901          |\n",
            "|    ep_rew_mean          | 123          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 966          |\n",
            "|    iterations           | 55           |\n",
            "|    time_elapsed         | 932          |\n",
            "|    total_timesteps      | 901120       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0028842757 |\n",
            "|    clip_fraction        | 0.0183       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.947       |\n",
            "|    explained_variance   | 0.957        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 17           |\n",
            "|    n_updates            | 216          |\n",
            "|    policy_gradient_loss | 4.57e-05     |\n",
            "|    value_loss           | 53.7         |\n",
            "------------------------------------------\n",
            "Num timesteps: 912000\n",
            "Best mean reward: 132.60 - Last mean reward per episode: 121.60\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 880          |\n",
            "|    ep_rew_mean          | 123          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 964          |\n",
            "|    iterations           | 56           |\n",
            "|    time_elapsed         | 950          |\n",
            "|    total_timesteps      | 917504       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035255994 |\n",
            "|    clip_fraction        | 0.0235       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.912       |\n",
            "|    explained_variance   | 0.959        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.67         |\n",
            "|    n_updates            | 220          |\n",
            "|    policy_gradient_loss | -0.00126     |\n",
            "|    value_loss           | 51.1         |\n",
            "------------------------------------------\n",
            "Num timesteps: 928000\n",
            "Best mean reward: 132.60 - Last mean reward per episode: 122.60\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 894          |\n",
            "|    ep_rew_mean          | 126          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 963          |\n",
            "|    iterations           | 57           |\n",
            "|    time_elapsed         | 969          |\n",
            "|    total_timesteps      | 933888       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0043993113 |\n",
            "|    clip_fraction        | 0.0352       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.902       |\n",
            "|    explained_variance   | 0.967        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.16         |\n",
            "|    n_updates            | 224          |\n",
            "|    policy_gradient_loss | -0.0013      |\n",
            "|    value_loss           | 37           |\n",
            "------------------------------------------\n",
            "Num timesteps: 944000\n",
            "Best mean reward: 132.60 - Last mean reward per episode: 126.57\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 907         |\n",
            "|    ep_rew_mean          | 131         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 961         |\n",
            "|    iterations           | 58          |\n",
            "|    time_elapsed         | 988         |\n",
            "|    total_timesteps      | 950272      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003691328 |\n",
            "|    clip_fraction        | 0.051       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.913      |\n",
            "|    explained_variance   | 0.994       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.32        |\n",
            "|    n_updates            | 228         |\n",
            "|    policy_gradient_loss | 0.000637    |\n",
            "|    value_loss           | 3.75        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 960000\n",
            "Best mean reward: 132.60 - Last mean reward per episode: 130.05\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 928          |\n",
            "|    ep_rew_mean          | 134          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 959          |\n",
            "|    iterations           | 59           |\n",
            "|    time_elapsed         | 1007         |\n",
            "|    total_timesteps      | 966656       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0048235273 |\n",
            "|    clip_fraction        | 0.0552       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.901       |\n",
            "|    explained_variance   | 0.995        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.477        |\n",
            "|    n_updates            | 232          |\n",
            "|    policy_gradient_loss | -0.000936    |\n",
            "|    value_loss           | 3.11         |\n",
            "------------------------------------------\n",
            "Num timesteps: 976000\n",
            "Best mean reward: 132.60 - Last mean reward per episode: 133.74\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 922          |\n",
            "|    ep_rew_mean          | 135          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 958          |\n",
            "|    iterations           | 60           |\n",
            "|    time_elapsed         | 1025         |\n",
            "|    total_timesteps      | 983040       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041438118 |\n",
            "|    clip_fraction        | 0.0448       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.891       |\n",
            "|    explained_variance   | 0.997        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.939        |\n",
            "|    n_updates            | 236          |\n",
            "|    policy_gradient_loss | 0.000337     |\n",
            "|    value_loss           | 2.82         |\n",
            "------------------------------------------\n",
            "Num timesteps: 992000\n",
            "Best mean reward: 133.74 - Last mean reward per episode: 136.71\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 937          |\n",
            "|    ep_rew_mean          | 142          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 956          |\n",
            "|    iterations           | 61           |\n",
            "|    time_elapsed         | 1044         |\n",
            "|    total_timesteps      | 999424       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0049488046 |\n",
            "|    clip_fraction        | 0.0238       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.819       |\n",
            "|    explained_variance   | 0.969        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.3          |\n",
            "|    n_updates            | 240          |\n",
            "|    policy_gradient_loss | -0.00047     |\n",
            "|    value_loss           | 35.6         |\n",
            "------------------------------------------\n",
            "Num timesteps: 1008000\n",
            "Best mean reward: 136.71 - Last mean reward per episode: 144.42\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 932          |\n",
            "|    ep_rew_mean          | 150          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 955          |\n",
            "|    iterations           | 62           |\n",
            "|    time_elapsed         | 1063         |\n",
            "|    total_timesteps      | 1015808      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035879253 |\n",
            "|    clip_fraction        | 0.0399       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.788       |\n",
            "|    explained_variance   | 0.95         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 25           |\n",
            "|    n_updates            | 244          |\n",
            "|    policy_gradient_loss | -0.000887    |\n",
            "|    value_loss           | 82.6         |\n",
            "------------------------------------------\n",
            "Num timesteps: 1024000\n",
            "Best mean reward: 144.42 - Last mean reward per episode: 155.65\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 853          |\n",
            "|    ep_rew_mean          | 163          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 954          |\n",
            "|    iterations           | 63           |\n",
            "|    time_elapsed         | 1081         |\n",
            "|    total_timesteps      | 1032192      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044382988 |\n",
            "|    clip_fraction        | 0.0403       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.74        |\n",
            "|    explained_variance   | 0.929        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 70.6         |\n",
            "|    n_updates            | 248          |\n",
            "|    policy_gradient_loss | -0.00106     |\n",
            "|    value_loss           | 112          |\n",
            "------------------------------------------\n",
            "Num timesteps: 1040000\n",
            "Best mean reward: 155.65 - Last mean reward per episode: 175.76\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 756         |\n",
            "|    ep_rew_mean          | 185         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 955         |\n",
            "|    iterations           | 64          |\n",
            "|    time_elapsed         | 1097        |\n",
            "|    total_timesteps      | 1048576     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004554011 |\n",
            "|    clip_fraction        | 0.053       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.706      |\n",
            "|    explained_variance   | 0.817       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 124         |\n",
            "|    n_updates            | 252         |\n",
            "|    policy_gradient_loss | -0.00293    |\n",
            "|    value_loss           | 282         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 1056000\n",
            "Best mean reward: 175.76 - Last mean reward per episode: 197.37\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 663          |\n",
            "|    ep_rew_mean          | 206          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 955          |\n",
            "|    iterations           | 65           |\n",
            "|    time_elapsed         | 1114         |\n",
            "|    total_timesteps      | 1064960      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0077698505 |\n",
            "|    clip_fraction        | 0.0769       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.721       |\n",
            "|    explained_variance   | 0.74         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 105          |\n",
            "|    n_updates            | 256          |\n",
            "|    policy_gradient_loss | -0.00255     |\n",
            "|    value_loss           | 300          |\n",
            "------------------------------------------\n",
            "Num timesteps: 1072000\n",
            "Best mean reward: 197.37 - Last mean reward per episode: 210.77\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 544         |\n",
            "|    ep_rew_mean          | 217         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 958         |\n",
            "|    iterations           | 66          |\n",
            "|    time_elapsed         | 1128        |\n",
            "|    total_timesteps      | 1081344     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006447036 |\n",
            "|    clip_fraction        | 0.0394      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.734      |\n",
            "|    explained_variance   | 0.824       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 129         |\n",
            "|    n_updates            | 260         |\n",
            "|    policy_gradient_loss | -0.00307    |\n",
            "|    value_loss           | 211         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 1088000\n",
            "Best mean reward: 210.77 - Last mean reward per episode: 213.85\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 465          |\n",
            "|    ep_rew_mean          | 221          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 960          |\n",
            "|    iterations           | 67           |\n",
            "|    time_elapsed         | 1143         |\n",
            "|    total_timesteps      | 1097728      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0037985507 |\n",
            "|    clip_fraction        | 0.0476       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.816       |\n",
            "|    explained_variance   | 0.777        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 71           |\n",
            "|    n_updates            | 264          |\n",
            "|    policy_gradient_loss | -0.00109     |\n",
            "|    value_loss           | 221          |\n",
            "------------------------------------------\n",
            "Num timesteps: 1104000\n",
            "Best mean reward: 213.85 - Last mean reward per episode: 220.38\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 409         |\n",
            "|    ep_rew_mean          | 229         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 962         |\n",
            "|    iterations           | 68          |\n",
            "|    time_elapsed         | 1157        |\n",
            "|    total_timesteps      | 1114112     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004118935 |\n",
            "|    clip_fraction        | 0.0441      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.826      |\n",
            "|    explained_variance   | 0.806       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 51.1        |\n",
            "|    n_updates            | 268         |\n",
            "|    policy_gradient_loss | 1.71e-05    |\n",
            "|    value_loss           | 204         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 1120000\n",
            "Best mean reward: 220.38 - Last mean reward per episode: 231.47\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 425          |\n",
            "|    ep_rew_mean          | 234          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 964          |\n",
            "|    iterations           | 69           |\n",
            "|    time_elapsed         | 1172         |\n",
            "|    total_timesteps      | 1130496      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041777515 |\n",
            "|    clip_fraction        | 0.0578       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.84        |\n",
            "|    explained_variance   | 0.801        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 67.1         |\n",
            "|    n_updates            | 272          |\n",
            "|    policy_gradient_loss | -0.00126     |\n",
            "|    value_loss           | 246          |\n",
            "------------------------------------------\n",
            "Num timesteps: 1136000\n",
            "Best mean reward: 231.47 - Last mean reward per episode: 232.68\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 441          |\n",
            "|    ep_rew_mean          | 235          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 966          |\n",
            "|    iterations           | 70           |\n",
            "|    time_elapsed         | 1187         |\n",
            "|    total_timesteps      | 1146880      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0048443144 |\n",
            "|    clip_fraction        | 0.0401       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.816       |\n",
            "|    explained_variance   | 0.913        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 37.3         |\n",
            "|    n_updates            | 276          |\n",
            "|    policy_gradient_loss | -0.000328    |\n",
            "|    value_loss           | 98.4         |\n",
            "------------------------------------------\n",
            "Num timesteps: 1152000\n",
            "Best mean reward: 232.68 - Last mean reward per episode: 236.22\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 452          |\n",
            "|    ep_rew_mean          | 236          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 969          |\n",
            "|    iterations           | 71           |\n",
            "|    time_elapsed         | 1200         |\n",
            "|    total_timesteps      | 1163264      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0037153675 |\n",
            "|    clip_fraction        | 0.0249       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.816       |\n",
            "|    explained_variance   | 0.916        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 15.2         |\n",
            "|    n_updates            | 280          |\n",
            "|    policy_gradient_loss | -0.00101     |\n",
            "|    value_loss           | 88           |\n",
            "------------------------------------------\n",
            "Num timesteps: 1168000\n",
            "Best mean reward: 236.22 - Last mean reward per episode: 242.22\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 412         |\n",
            "|    ep_rew_mean          | 247         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 971         |\n",
            "|    iterations           | 72          |\n",
            "|    time_elapsed         | 1214        |\n",
            "|    total_timesteps      | 1179648     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003905866 |\n",
            "|    clip_fraction        | 0.0429      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.778      |\n",
            "|    explained_variance   | 0.909       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 113         |\n",
            "|    n_updates            | 284         |\n",
            "|    policy_gradient_loss | -0.000198   |\n",
            "|    value_loss           | 116         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 1184000\n",
            "Best mean reward: 242.22 - Last mean reward per episode: 247.41\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 401          |\n",
            "|    ep_rew_mean          | 251          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 974          |\n",
            "|    iterations           | 73           |\n",
            "|    time_elapsed         | 1227         |\n",
            "|    total_timesteps      | 1196032      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034686143 |\n",
            "|    clip_fraction        | 0.0373       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.794       |\n",
            "|    explained_variance   | 0.93         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 27           |\n",
            "|    n_updates            | 288          |\n",
            "|    policy_gradient_loss | 0.000148     |\n",
            "|    value_loss           | 109          |\n",
            "------------------------------------------\n",
            "Num timesteps: 1200000\n",
            "Best mean reward: 247.41 - Last mean reward per episode: 251.09\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 397         |\n",
            "|    ep_rew_mean          | 248         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 977         |\n",
            "|    iterations           | 74          |\n",
            "|    time_elapsed         | 1240        |\n",
            "|    total_timesteps      | 1212416     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004247684 |\n",
            "|    clip_fraction        | 0.0455      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.81       |\n",
            "|    explained_variance   | 0.904       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.47        |\n",
            "|    n_updates            | 292         |\n",
            "|    policy_gradient_loss | 0.000336    |\n",
            "|    value_loss           | 103         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 1216000\n",
            "Best mean reward: 251.09 - Last mean reward per episode: 245.83\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 394          |\n",
            "|    ep_rew_mean          | 245          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 979          |\n",
            "|    iterations           | 75           |\n",
            "|    time_elapsed         | 1254         |\n",
            "|    total_timesteps      | 1228800      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0036245612 |\n",
            "|    clip_fraction        | 0.0225       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.767       |\n",
            "|    explained_variance   | 0.856        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 43.6         |\n",
            "|    n_updates            | 296          |\n",
            "|    policy_gradient_loss | -0.0011      |\n",
            "|    value_loss           | 262          |\n",
            "------------------------------------------\n",
            "Num timesteps: 1232000\n",
            "Best mean reward: 251.09 - Last mean reward per episode: 245.27\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 390         |\n",
            "|    ep_rew_mean          | 247         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 982         |\n",
            "|    iterations           | 76          |\n",
            "|    time_elapsed         | 1267        |\n",
            "|    total_timesteps      | 1245184     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004802634 |\n",
            "|    clip_fraction        | 0.0522      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.799      |\n",
            "|    explained_variance   | 0.918       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 16.6        |\n",
            "|    n_updates            | 300         |\n",
            "|    policy_gradient_loss | -0.000891   |\n",
            "|    value_loss           | 114         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 1248000\n",
            "Best mean reward: 251.09 - Last mean reward per episode: 248.66\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 396          |\n",
            "|    ep_rew_mean          | 239          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 985          |\n",
            "|    iterations           | 77           |\n",
            "|    time_elapsed         | 1280         |\n",
            "|    total_timesteps      | 1261568      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044217827 |\n",
            "|    clip_fraction        | 0.0482       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.78        |\n",
            "|    explained_variance   | 0.851        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 184          |\n",
            "|    n_updates            | 304          |\n",
            "|    policy_gradient_loss | 0.000431     |\n",
            "|    value_loss           | 171          |\n",
            "------------------------------------------\n",
            "Num timesteps: 1264000\n",
            "Best mean reward: 251.09 - Last mean reward per episode: 238.42\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 400          |\n",
            "|    ep_rew_mean          | 232          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 988          |\n",
            "|    iterations           | 78           |\n",
            "|    time_elapsed         | 1293         |\n",
            "|    total_timesteps      | 1277952      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0042230217 |\n",
            "|    clip_fraction        | 0.0417       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.785       |\n",
            "|    explained_variance   | 0.917        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 18.4         |\n",
            "|    n_updates            | 308          |\n",
            "|    policy_gradient_loss | -0.000251    |\n",
            "|    value_loss           | 176          |\n",
            "------------------------------------------\n",
            "Num timesteps: 1280000\n",
            "Best mean reward: 251.09 - Last mean reward per episode: 230.10\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 395          |\n",
            "|    ep_rew_mean          | 233          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 990          |\n",
            "|    iterations           | 79           |\n",
            "|    time_elapsed         | 1307         |\n",
            "|    total_timesteps      | 1294336      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0029176218 |\n",
            "|    clip_fraction        | 0.0284       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.777       |\n",
            "|    explained_variance   | 0.876        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 21           |\n",
            "|    n_updates            | 312          |\n",
            "|    policy_gradient_loss | -0.00169     |\n",
            "|    value_loss           | 223          |\n",
            "------------------------------------------\n",
            "Num timesteps: 1296000\n",
            "Best mean reward: 251.09 - Last mean reward per episode: 232.50\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 405         |\n",
            "|    ep_rew_mean          | 234         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 992         |\n",
            "|    iterations           | 80          |\n",
            "|    time_elapsed         | 1320        |\n",
            "|    total_timesteps      | 1310720     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005223081 |\n",
            "|    clip_fraction        | 0.0559      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.78       |\n",
            "|    explained_variance   | 0.921       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 187         |\n",
            "|    n_updates            | 316         |\n",
            "|    policy_gradient_loss | -0.00129    |\n",
            "|    value_loss           | 147         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 1312000\n",
            "Best mean reward: 251.09 - Last mean reward per episode: 236.43\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 393          |\n",
            "|    ep_rew_mean          | 243          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 995          |\n",
            "|    iterations           | 81           |\n",
            "|    time_elapsed         | 1333         |\n",
            "|    total_timesteps      | 1327104      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0052409703 |\n",
            "|    clip_fraction        | 0.0583       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.793       |\n",
            "|    explained_variance   | 0.897        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 168          |\n",
            "|    n_updates            | 320          |\n",
            "|    policy_gradient_loss | -0.0012      |\n",
            "|    value_loss           | 164          |\n",
            "------------------------------------------\n",
            "Num timesteps: 1328000\n",
            "Best mean reward: 251.09 - Last mean reward per episode: 243.89\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 374         |\n",
            "|    ep_rew_mean          | 244         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 998         |\n",
            "|    iterations           | 82          |\n",
            "|    time_elapsed         | 1345        |\n",
            "|    total_timesteps      | 1343488     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003868597 |\n",
            "|    clip_fraction        | 0.0382      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.783      |\n",
            "|    explained_variance   | 0.904       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 56.8        |\n",
            "|    n_updates            | 324         |\n",
            "|    policy_gradient_loss | -0.00054    |\n",
            "|    value_loss           | 185         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 1344000\n",
            "Best mean reward: 251.09 - Last mean reward per episode: 247.95\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 359         |\n",
            "|    ep_rew_mean          | 254         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1001        |\n",
            "|    iterations           | 83          |\n",
            "|    time_elapsed         | 1357        |\n",
            "|    total_timesteps      | 1359872     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004024526 |\n",
            "|    clip_fraction        | 0.046       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.765      |\n",
            "|    explained_variance   | 0.916       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.89        |\n",
            "|    n_updates            | 328         |\n",
            "|    policy_gradient_loss | -5.44e-05   |\n",
            "|    value_loss           | 139         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 1360000\n",
            "Best mean reward: 251.09 - Last mean reward per episode: 254.31\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "Num timesteps: 1376000\n",
            "Best mean reward: 254.31 - Last mean reward per episode: 257.17\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 351          |\n",
            "|    ep_rew_mean          | 258          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1004         |\n",
            "|    iterations           | 84           |\n",
            "|    time_elapsed         | 1369         |\n",
            "|    total_timesteps      | 1376256      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0043014456 |\n",
            "|    clip_fraction        | 0.0371       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.759       |\n",
            "|    explained_variance   | 0.95         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.21         |\n",
            "|    n_updates            | 332          |\n",
            "|    policy_gradient_loss | 0.00097      |\n",
            "|    value_loss           | 72.2         |\n",
            "------------------------------------------\n",
            "Num timesteps: 1392000\n",
            "Best mean reward: 257.17 - Last mean reward per episode: 256.56\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 346         |\n",
            "|    ep_rew_mean          | 256         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1008        |\n",
            "|    iterations           | 85          |\n",
            "|    time_elapsed         | 1381        |\n",
            "|    total_timesteps      | 1392640     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004132809 |\n",
            "|    clip_fraction        | 0.047       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.755      |\n",
            "|    explained_variance   | 0.958       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 13.1        |\n",
            "|    n_updates            | 336         |\n",
            "|    policy_gradient_loss | -0.000279   |\n",
            "|    value_loss           | 77.5        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 1408000\n",
            "Best mean reward: 257.17 - Last mean reward per episode: 260.75\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 314          |\n",
            "|    ep_rew_mean          | 257          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1011         |\n",
            "|    iterations           | 86           |\n",
            "|    time_elapsed         | 1392         |\n",
            "|    total_timesteps      | 1409024      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038665782 |\n",
            "|    clip_fraction        | 0.0295       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.764       |\n",
            "|    explained_variance   | 0.922        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.94         |\n",
            "|    n_updates            | 340          |\n",
            "|    policy_gradient_loss | 0.000189     |\n",
            "|    value_loss           | 124          |\n",
            "------------------------------------------\n",
            "Num timesteps: 1424000\n",
            "Best mean reward: 260.75 - Last mean reward per episode: 251.59\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 323          |\n",
            "|    ep_rew_mean          | 253          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1015         |\n",
            "|    iterations           | 87           |\n",
            "|    time_elapsed         | 1404         |\n",
            "|    total_timesteps      | 1425408      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0033673756 |\n",
            "|    clip_fraction        | 0.0335       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.747       |\n",
            "|    explained_variance   | 0.902        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 21.8         |\n",
            "|    n_updates            | 344          |\n",
            "|    policy_gradient_loss | -0.000117    |\n",
            "|    value_loss           | 158          |\n",
            "------------------------------------------\n",
            "Num timesteps: 1440000\n",
            "Best mean reward: 260.75 - Last mean reward per episode: 248.84\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 334          |\n",
            "|    ep_rew_mean          | 251          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1017         |\n",
            "|    iterations           | 88           |\n",
            "|    time_elapsed         | 1416         |\n",
            "|    total_timesteps      | 1441792      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040802457 |\n",
            "|    clip_fraction        | 0.0527       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.779       |\n",
            "|    explained_variance   | 0.965        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.28         |\n",
            "|    n_updates            | 348          |\n",
            "|    policy_gradient_loss | -0.00021     |\n",
            "|    value_loss           | 75.5         |\n",
            "------------------------------------------\n",
            "Num timesteps: 1456000\n",
            "Best mean reward: 260.75 - Last mean reward per episode: 255.82\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 321         |\n",
            "|    ep_rew_mean          | 256         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1021        |\n",
            "|    iterations           | 89          |\n",
            "|    time_elapsed         | 1427        |\n",
            "|    total_timesteps      | 1458176     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003772067 |\n",
            "|    clip_fraction        | 0.0312      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.751      |\n",
            "|    explained_variance   | 0.896       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 21.3        |\n",
            "|    n_updates            | 352         |\n",
            "|    policy_gradient_loss | -0.0002     |\n",
            "|    value_loss           | 189         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 1472000\n",
            "Best mean reward: 260.75 - Last mean reward per episode: 258.61\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 317          |\n",
            "|    ep_rew_mean          | 256          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1024         |\n",
            "|    iterations           | 90           |\n",
            "|    time_elapsed         | 1438         |\n",
            "|    total_timesteps      | 1474560      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0028885659 |\n",
            "|    clip_fraction        | 0.0471       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.735       |\n",
            "|    explained_variance   | 0.938        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 240          |\n",
            "|    n_updates            | 356          |\n",
            "|    policy_gradient_loss | 0.000437     |\n",
            "|    value_loss           | 125          |\n",
            "------------------------------------------\n",
            "Num timesteps: 1488000\n",
            "Best mean reward: 260.75 - Last mean reward per episode: 259.05\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 309         |\n",
            "|    ep_rew_mean          | 259         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1028        |\n",
            "|    iterations           | 91          |\n",
            "|    time_elapsed         | 1449        |\n",
            "|    total_timesteps      | 1490944     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004857309 |\n",
            "|    clip_fraction        | 0.0464      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.751      |\n",
            "|    explained_variance   | 0.895       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 130         |\n",
            "|    n_updates            | 360         |\n",
            "|    policy_gradient_loss | -0.000638   |\n",
            "|    value_loss           | 199         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 1504000\n",
            "Best mean reward: 260.75 - Last mean reward per episode: 262.56\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 305          |\n",
            "|    ep_rew_mean          | 260          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1031         |\n",
            "|    iterations           | 92           |\n",
            "|    time_elapsed         | 1461         |\n",
            "|    total_timesteps      | 1507328      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038574329 |\n",
            "|    clip_fraction        | 0.0529       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.761       |\n",
            "|    explained_variance   | 0.927        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 278          |\n",
            "|    n_updates            | 364          |\n",
            "|    policy_gradient_loss | 0.000687     |\n",
            "|    value_loss           | 136          |\n",
            "------------------------------------------\n",
            "Num timesteps: 1520000\n",
            "Best mean reward: 262.56 - Last mean reward per episode: 259.75\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 302         |\n",
            "|    ep_rew_mean          | 260         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1035        |\n",
            "|    iterations           | 93          |\n",
            "|    time_elapsed         | 1471        |\n",
            "|    total_timesteps      | 1523712     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004211344 |\n",
            "|    clip_fraction        | 0.0521      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.753      |\n",
            "|    explained_variance   | 0.925       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.76        |\n",
            "|    n_updates            | 368         |\n",
            "|    policy_gradient_loss | -0.000148   |\n",
            "|    value_loss           | 124         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 1536000\n",
            "Best mean reward: 262.56 - Last mean reward per episode: 263.21\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 295          |\n",
            "|    ep_rew_mean          | 263          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1038         |\n",
            "|    iterations           | 94           |\n",
            "|    time_elapsed         | 1483         |\n",
            "|    total_timesteps      | 1540096      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035689643 |\n",
            "|    clip_fraction        | 0.0386       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.72        |\n",
            "|    explained_variance   | 0.905        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 292          |\n",
            "|    n_updates            | 372          |\n",
            "|    policy_gradient_loss | 0.000101     |\n",
            "|    value_loss           | 185          |\n",
            "------------------------------------------\n",
            "Num timesteps: 1552000\n",
            "Best mean reward: 263.21 - Last mean reward per episode: 254.82\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 295          |\n",
            "|    ep_rew_mean          | 256          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1041         |\n",
            "|    iterations           | 95           |\n",
            "|    time_elapsed         | 1494         |\n",
            "|    total_timesteps      | 1556480      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0056531923 |\n",
            "|    clip_fraction        | 0.0554       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.756       |\n",
            "|    explained_variance   | 0.931        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 69.9         |\n",
            "|    n_updates            | 376          |\n",
            "|    policy_gradient_loss | -0.000695    |\n",
            "|    value_loss           | 141          |\n",
            "------------------------------------------\n",
            "Num timesteps: 1568000\n",
            "Best mean reward: 263.21 - Last mean reward per episode: 254.20\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 295          |\n",
            "|    ep_rew_mean          | 257          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1044         |\n",
            "|    iterations           | 96           |\n",
            "|    time_elapsed         | 1505         |\n",
            "|    total_timesteps      | 1572864      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038834931 |\n",
            "|    clip_fraction        | 0.0415       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.771       |\n",
            "|    explained_variance   | 0.864        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 42.7         |\n",
            "|    n_updates            | 380          |\n",
            "|    policy_gradient_loss | -0.000246    |\n",
            "|    value_loss           | 247          |\n",
            "------------------------------------------\n",
            "Num timesteps: 1584000\n",
            "Best mean reward: 263.21 - Last mean reward per episode: 259.51\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 298          |\n",
            "|    ep_rew_mean          | 265          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1047         |\n",
            "|    iterations           | 97           |\n",
            "|    time_elapsed         | 1516         |\n",
            "|    total_timesteps      | 1589248      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0045806495 |\n",
            "|    clip_fraction        | 0.0595       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.787       |\n",
            "|    explained_variance   | 0.932        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 19.2         |\n",
            "|    n_updates            | 384          |\n",
            "|    policy_gradient_loss | 3.45e-05     |\n",
            "|    value_loss           | 130          |\n",
            "------------------------------------------\n",
            "Num timesteps: 1600000\n",
            "Best mean reward: 263.21 - Last mean reward per episode: 261.69\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 299          |\n",
            "|    ep_rew_mean          | 264          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1050         |\n",
            "|    iterations           | 98           |\n",
            "|    time_elapsed         | 1528         |\n",
            "|    total_timesteps      | 1605632      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0043273387 |\n",
            "|    clip_fraction        | 0.0617       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.741       |\n",
            "|    explained_variance   | 0.962        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.5          |\n",
            "|    n_updates            | 388          |\n",
            "|    policy_gradient_loss | -0.00106     |\n",
            "|    value_loss           | 73.9         |\n",
            "------------------------------------------\n",
            "Num timesteps: 1616000\n",
            "Best mean reward: 263.21 - Last mean reward per episode: 263.80\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 303          |\n",
            "|    ep_rew_mean          | 267          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1053         |\n",
            "|    iterations           | 99           |\n",
            "|    time_elapsed         | 1539         |\n",
            "|    total_timesteps      | 1622016      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0042835306 |\n",
            "|    clip_fraction        | 0.0551       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.757       |\n",
            "|    explained_variance   | 0.938        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 11.3         |\n",
            "|    n_updates            | 392          |\n",
            "|    policy_gradient_loss | -0.000967    |\n",
            "|    value_loss           | 112          |\n",
            "------------------------------------------\n",
            "Num timesteps: 1632000\n",
            "Best mean reward: 263.80 - Last mean reward per episode: 264.46\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 291          |\n",
            "|    ep_rew_mean          | 258          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1056         |\n",
            "|    iterations           | 100          |\n",
            "|    time_elapsed         | 1550         |\n",
            "|    total_timesteps      | 1638400      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0036637012 |\n",
            "|    clip_fraction        | 0.0498       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.736       |\n",
            "|    explained_variance   | 0.991        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.79         |\n",
            "|    n_updates            | 396          |\n",
            "|    policy_gradient_loss | 0.000296     |\n",
            "|    value_loss           | 7.93         |\n",
            "------------------------------------------\n",
            "Num timesteps: 1648000\n",
            "Best mean reward: 264.46 - Last mean reward per episode: 246.53\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 277          |\n",
            "|    ep_rew_mean          | 240          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1060         |\n",
            "|    iterations           | 101          |\n",
            "|    time_elapsed         | 1560         |\n",
            "|    total_timesteps      | 1654784      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0031454994 |\n",
            "|    clip_fraction        | 0.0148       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.765       |\n",
            "|    explained_variance   | 0.833        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 182          |\n",
            "|    n_updates            | 400          |\n",
            "|    policy_gradient_loss | 0.000208     |\n",
            "|    value_loss           | 315          |\n",
            "------------------------------------------\n",
            "Num timesteps: 1664000\n",
            "Best mean reward: 264.46 - Last mean reward per episode: 246.11\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 279         |\n",
            "|    ep_rew_mean          | 244         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1062        |\n",
            "|    iterations           | 102         |\n",
            "|    time_elapsed         | 1572        |\n",
            "|    total_timesteps      | 1671168     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004012879 |\n",
            "|    clip_fraction        | 0.0328      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.765      |\n",
            "|    explained_variance   | 0.856       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 197         |\n",
            "|    n_updates            | 404         |\n",
            "|    policy_gradient_loss | -0.000836   |\n",
            "|    value_loss           | 342         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 1680000\n",
            "Best mean reward: 264.46 - Last mean reward per episode: 247.68\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 291          |\n",
            "|    ep_rew_mean          | 246          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1066         |\n",
            "|    iterations           | 103          |\n",
            "|    time_elapsed         | 1582         |\n",
            "|    total_timesteps      | 1687552      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0023362257 |\n",
            "|    clip_fraction        | 0.0209       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.745       |\n",
            "|    explained_variance   | 0.876        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 67.4         |\n",
            "|    n_updates            | 408          |\n",
            "|    policy_gradient_loss | -3.47e-05    |\n",
            "|    value_loss           | 313          |\n",
            "------------------------------------------\n",
            "Num timesteps: 1696000\n",
            "Best mean reward: 264.46 - Last mean reward per episode: 252.42\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 279          |\n",
            "|    ep_rew_mean          | 254          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1069         |\n",
            "|    iterations           | 104          |\n",
            "|    time_elapsed         | 1593         |\n",
            "|    total_timesteps      | 1703936      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034392187 |\n",
            "|    clip_fraction        | 0.0286       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.728       |\n",
            "|    explained_variance   | 0.884        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 18           |\n",
            "|    n_updates            | 412          |\n",
            "|    policy_gradient_loss | -0.000135    |\n",
            "|    value_loss           | 269          |\n",
            "------------------------------------------\n",
            "Num timesteps: 1712000\n",
            "Best mean reward: 264.46 - Last mean reward per episode: 261.66\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 286         |\n",
            "|    ep_rew_mean          | 260         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1071        |\n",
            "|    iterations           | 105         |\n",
            "|    time_elapsed         | 1604        |\n",
            "|    total_timesteps      | 1720320     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004915826 |\n",
            "|    clip_fraction        | 0.0417      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.713      |\n",
            "|    explained_variance   | 0.887       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 238         |\n",
            "|    n_updates            | 416         |\n",
            "|    policy_gradient_loss | 9.08e-05    |\n",
            "|    value_loss           | 262         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 1728000\n",
            "Best mean reward: 264.46 - Last mean reward per episode: 263.92\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 282          |\n",
            "|    ep_rew_mean          | 268          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1074         |\n",
            "|    iterations           | 106          |\n",
            "|    time_elapsed         | 1615         |\n",
            "|    total_timesteps      | 1736704      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040465505 |\n",
            "|    clip_fraction        | 0.0544       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.72        |\n",
            "|    explained_variance   | 0.939        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 196          |\n",
            "|    n_updates            | 420          |\n",
            "|    policy_gradient_loss | 0.00026      |\n",
            "|    value_loss           | 151          |\n",
            "------------------------------------------\n",
            "Num timesteps: 1744000\n",
            "Best mean reward: 264.46 - Last mean reward per episode: 260.57\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 274         |\n",
            "|    ep_rew_mean          | 263         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1077        |\n",
            "|    iterations           | 107         |\n",
            "|    time_elapsed         | 1626        |\n",
            "|    total_timesteps      | 1753088     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004196667 |\n",
            "|    clip_fraction        | 0.0558      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.729      |\n",
            "|    explained_variance   | 0.938       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 20.6        |\n",
            "|    n_updates            | 424         |\n",
            "|    policy_gradient_loss | 0.000257    |\n",
            "|    value_loss           | 85.8        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 1760000\n",
            "Best mean reward: 264.46 - Last mean reward per episode: 260.75\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 272          |\n",
            "|    ep_rew_mean          | 258          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1080         |\n",
            "|    iterations           | 108          |\n",
            "|    time_elapsed         | 1637         |\n",
            "|    total_timesteps      | 1769472      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0030233313 |\n",
            "|    clip_fraction        | 0.0265       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.726       |\n",
            "|    explained_variance   | 0.887        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 367          |\n",
            "|    n_updates            | 428          |\n",
            "|    policy_gradient_loss | 0.000363     |\n",
            "|    value_loss           | 225          |\n",
            "------------------------------------------\n",
            "Num timesteps: 1776000\n",
            "Best mean reward: 264.46 - Last mean reward per episode: 260.06\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 274         |\n",
            "|    ep_rew_mean          | 265         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1084        |\n",
            "|    iterations           | 109         |\n",
            "|    time_elapsed         | 1647        |\n",
            "|    total_timesteps      | 1785856     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005285562 |\n",
            "|    clip_fraction        | 0.0554      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.731      |\n",
            "|    explained_variance   | 0.892       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 241         |\n",
            "|    n_updates            | 432         |\n",
            "|    policy_gradient_loss | 7.55e-05    |\n",
            "|    value_loss           | 220         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 1792000\n",
            "Best mean reward: 264.46 - Last mean reward per episode: 262.57\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 271         |\n",
            "|    ep_rew_mean          | 261         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1087        |\n",
            "|    iterations           | 110         |\n",
            "|    time_elapsed         | 1657        |\n",
            "|    total_timesteps      | 1802240     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004170643 |\n",
            "|    clip_fraction        | 0.0544      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.721      |\n",
            "|    explained_variance   | 0.957       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 12.4        |\n",
            "|    n_updates            | 436         |\n",
            "|    policy_gradient_loss | 0.000419    |\n",
            "|    value_loss           | 73.8        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 1808000\n",
            "Best mean reward: 264.46 - Last mean reward per episode: 262.56\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 267          |\n",
            "|    ep_rew_mean          | 262          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1090         |\n",
            "|    iterations           | 111          |\n",
            "|    time_elapsed         | 1668         |\n",
            "|    total_timesteps      | 1818624      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019933658 |\n",
            "|    clip_fraction        | 0.0259       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.741       |\n",
            "|    explained_variance   | 0.872        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 171          |\n",
            "|    n_updates            | 440          |\n",
            "|    policy_gradient_loss | 8.71e-05     |\n",
            "|    value_loss           | 239          |\n",
            "------------------------------------------\n",
            "Num timesteps: 1824000\n",
            "Best mean reward: 264.46 - Last mean reward per episode: 266.76\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 270          |\n",
            "|    ep_rew_mean          | 268          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1092         |\n",
            "|    iterations           | 112          |\n",
            "|    time_elapsed         | 1679         |\n",
            "|    total_timesteps      | 1835008      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0045854943 |\n",
            "|    clip_fraction        | 0.0445       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.693       |\n",
            "|    explained_variance   | 0.956        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 19           |\n",
            "|    n_updates            | 444          |\n",
            "|    policy_gradient_loss | 0.000346     |\n",
            "|    value_loss           | 50.9         |\n",
            "------------------------------------------\n",
            "Num timesteps: 1840000\n",
            "Best mean reward: 266.76 - Last mean reward per episode: 268.68\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 273          |\n",
            "|    ep_rew_mean          | 270          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1095         |\n",
            "|    iterations           | 113          |\n",
            "|    time_elapsed         | 1689         |\n",
            "|    total_timesteps      | 1851392      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0057728468 |\n",
            "|    clip_fraction        | 0.0561       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.7         |\n",
            "|    explained_variance   | 0.971        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.17         |\n",
            "|    n_updates            | 448          |\n",
            "|    policy_gradient_loss | 0.000502     |\n",
            "|    value_loss           | 48.1         |\n",
            "------------------------------------------\n",
            "Num timesteps: 1856000\n",
            "Best mean reward: 268.68 - Last mean reward per episode: 275.64\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 266          |\n",
            "|    ep_rew_mean          | 271          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1098         |\n",
            "|    iterations           | 114          |\n",
            "|    time_elapsed         | 1699         |\n",
            "|    total_timesteps      | 1867776      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034437075 |\n",
            "|    clip_fraction        | 0.0408       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.707       |\n",
            "|    explained_variance   | 0.996        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.21         |\n",
            "|    n_updates            | 452          |\n",
            "|    policy_gradient_loss | 0.000986     |\n",
            "|    value_loss           | 5.17         |\n",
            "------------------------------------------\n",
            "Num timesteps: 1872000\n",
            "Best mean reward: 275.64 - Last mean reward per episode: 269.97\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 269          |\n",
            "|    ep_rew_mean          | 267          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1101         |\n",
            "|    iterations           | 115          |\n",
            "|    time_elapsed         | 1710         |\n",
            "|    total_timesteps      | 1884160      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0048450185 |\n",
            "|    clip_fraction        | 0.0406       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.71        |\n",
            "|    explained_variance   | 0.936        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.76         |\n",
            "|    n_updates            | 456          |\n",
            "|    policy_gradient_loss | -0.000441    |\n",
            "|    value_loss           | 143          |\n",
            "------------------------------------------\n",
            "Num timesteps: 1888000\n",
            "Best mean reward: 275.64 - Last mean reward per episode: 262.57\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 272         |\n",
            "|    ep_rew_mean          | 270         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1104        |\n",
            "|    iterations           | 116         |\n",
            "|    time_elapsed         | 1720        |\n",
            "|    total_timesteps      | 1900544     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003889022 |\n",
            "|    clip_fraction        | 0.0484      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.679      |\n",
            "|    explained_variance   | 0.963       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 34.9        |\n",
            "|    n_updates            | 460         |\n",
            "|    policy_gradient_loss | -0.0006     |\n",
            "|    value_loss           | 73.6        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 1904000\n",
            "Best mean reward: 275.64 - Last mean reward per episode: 269.65\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 283          |\n",
            "|    ep_rew_mean          | 273          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1107         |\n",
            "|    iterations           | 117          |\n",
            "|    time_elapsed         | 1731         |\n",
            "|    total_timesteps      | 1916928      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038643775 |\n",
            "|    clip_fraction        | 0.0394       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.678       |\n",
            "|    explained_variance   | 0.985        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.57         |\n",
            "|    n_updates            | 464          |\n",
            "|    policy_gradient_loss | 0.000672     |\n",
            "|    value_loss           | 21.5         |\n",
            "------------------------------------------\n",
            "Num timesteps: 1920000\n",
            "Best mean reward: 275.64 - Last mean reward per episode: 270.34\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 275        |\n",
            "|    ep_rew_mean          | 258        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 1110       |\n",
            "|    iterations           | 118        |\n",
            "|    time_elapsed         | 1741       |\n",
            "|    total_timesteps      | 1933312    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00519864 |\n",
            "|    clip_fraction        | 0.0474     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.667     |\n",
            "|    explained_variance   | 0.994      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.94       |\n",
            "|    n_updates            | 468        |\n",
            "|    policy_gradient_loss | -1.96e-05  |\n",
            "|    value_loss           | 10.1       |\n",
            "----------------------------------------\n",
            "Num timesteps: 1936000\n",
            "Best mean reward: 275.64 - Last mean reward per episode: 260.16\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 264          |\n",
            "|    ep_rew_mean          | 266          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1113         |\n",
            "|    iterations           | 119          |\n",
            "|    time_elapsed         | 1751         |\n",
            "|    total_timesteps      | 1949696      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034389915 |\n",
            "|    clip_fraction        | 0.0181       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.685       |\n",
            "|    explained_variance   | 0.873        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 337          |\n",
            "|    n_updates            | 472          |\n",
            "|    policy_gradient_loss | 0.000227     |\n",
            "|    value_loss           | 258          |\n",
            "------------------------------------------\n",
            "Num timesteps: 1952000\n",
            "Best mean reward: 275.64 - Last mean reward per episode: 264.12\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 266         |\n",
            "|    ep_rew_mean          | 268         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1116        |\n",
            "|    iterations           | 120         |\n",
            "|    time_elapsed         | 1761        |\n",
            "|    total_timesteps      | 1966080     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003961308 |\n",
            "|    clip_fraction        | 0.0479      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.671      |\n",
            "|    explained_variance   | 0.982       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.01        |\n",
            "|    n_updates            | 476         |\n",
            "|    policy_gradient_loss | 0.000637    |\n",
            "|    value_loss           | 20.2        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 1968000\n",
            "Best mean reward: 275.64 - Last mean reward per episode: 268.95\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 261          |\n",
            "|    ep_rew_mean          | 265          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1119         |\n",
            "|    iterations           | 121          |\n",
            "|    time_elapsed         | 1771         |\n",
            "|    total_timesteps      | 1982464      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0045012464 |\n",
            "|    clip_fraction        | 0.0479       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.68        |\n",
            "|    explained_variance   | 0.941        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.09         |\n",
            "|    n_updates            | 480          |\n",
            "|    policy_gradient_loss | -0.000561    |\n",
            "|    value_loss           | 136          |\n",
            "------------------------------------------\n",
            "Num timesteps: 1984000\n",
            "Best mean reward: 275.64 - Last mean reward per episode: 265.62\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 259         |\n",
            "|    ep_rew_mean          | 263         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1121        |\n",
            "|    iterations           | 122         |\n",
            "|    time_elapsed         | 1781        |\n",
            "|    total_timesteps      | 1998848     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004129472 |\n",
            "|    clip_fraction        | 0.047       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.659      |\n",
            "|    explained_variance   | 0.925       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 328         |\n",
            "|    n_updates            | 484         |\n",
            "|    policy_gradient_loss | -0.000366   |\n",
            "|    value_loss           | 172         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 2000000\n",
            "Best mean reward: 275.64 - Last mean reward per episode: 262.62\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 258          |\n",
            "|    ep_rew_mean          | 265          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1124         |\n",
            "|    iterations           | 123          |\n",
            "|    time_elapsed         | 1792         |\n",
            "|    total_timesteps      | 2015232      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0033365332 |\n",
            "|    clip_fraction        | 0.0324       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.653       |\n",
            "|    explained_variance   | 0.893        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.43         |\n",
            "|    n_updates            | 488          |\n",
            "|    policy_gradient_loss | -0.00024     |\n",
            "|    value_loss           | 268          |\n",
            "------------------------------------------\n",
            "Num timesteps: 2016000\n",
            "Best mean reward: 275.64 - Last mean reward per episode: 265.49\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 255         |\n",
            "|    ep_rew_mean          | 264         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1127        |\n",
            "|    iterations           | 124         |\n",
            "|    time_elapsed         | 1802        |\n",
            "|    total_timesteps      | 2031616     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004816095 |\n",
            "|    clip_fraction        | 0.0477      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.663      |\n",
            "|    explained_variance   | 0.919       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 12.4        |\n",
            "|    n_updates            | 492         |\n",
            "|    policy_gradient_loss | -0.00141    |\n",
            "|    value_loss           | 180         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 2032000\n",
            "Best mean reward: 275.64 - Last mean reward per episode: 263.33\n",
            "Num timesteps: 2048000\n",
            "Best mean reward: 275.64 - Last mean reward per episode: 257.04\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 257        |\n",
            "|    ep_rew_mean          | 257        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 1130       |\n",
            "|    iterations           | 125        |\n",
            "|    time_elapsed         | 1812       |\n",
            "|    total_timesteps      | 2048000    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00392453 |\n",
            "|    clip_fraction        | 0.0423     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.662     |\n",
            "|    explained_variance   | 0.933      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 36         |\n",
            "|    n_updates            | 496        |\n",
            "|    policy_gradient_loss | 0.000303   |\n",
            "|    value_loss           | 148        |\n",
            "----------------------------------------\n",
            "Num timesteps: 2064000\n",
            "Best mean reward: 275.64 - Last mean reward per episode: 261.10\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 259          |\n",
            "|    ep_rew_mean          | 263          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1132         |\n",
            "|    iterations           | 126          |\n",
            "|    time_elapsed         | 1822         |\n",
            "|    total_timesteps      | 2064384      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0037668045 |\n",
            "|    clip_fraction        | 0.0338       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.697       |\n",
            "|    explained_variance   | 0.893        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 87.5         |\n",
            "|    n_updates            | 500          |\n",
            "|    policy_gradient_loss | -5.39e-06    |\n",
            "|    value_loss           | 267          |\n",
            "------------------------------------------\n",
            "Num timesteps: 2080000\n",
            "Best mean reward: 275.64 - Last mean reward per episode: 265.05\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 264          |\n",
            "|    ep_rew_mean          | 267          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1135         |\n",
            "|    iterations           | 127          |\n",
            "|    time_elapsed         | 1832         |\n",
            "|    total_timesteps      | 2080768      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0047414275 |\n",
            "|    clip_fraction        | 0.0414       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.662       |\n",
            "|    explained_variance   | 0.893        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 506          |\n",
            "|    n_updates            | 504          |\n",
            "|    policy_gradient_loss | 3.06e-06     |\n",
            "|    value_loss           | 340          |\n",
            "------------------------------------------\n",
            "Num timesteps: 2096000\n",
            "Best mean reward: 275.64 - Last mean reward per episode: 271.48\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 263          |\n",
            "|    ep_rew_mean          | 271          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1137         |\n",
            "|    iterations           | 128          |\n",
            "|    time_elapsed         | 1842         |\n",
            "|    total_timesteps      | 2097152      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0037686161 |\n",
            "|    clip_fraction        | 0.0556       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.655       |\n",
            "|    explained_variance   | 0.957        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 66.2         |\n",
            "|    n_updates            | 508          |\n",
            "|    policy_gradient_loss | 0.000442     |\n",
            "|    value_loss           | 96.8         |\n",
            "------------------------------------------\n",
            "Num timesteps: 2112000\n",
            "Best mean reward: 275.64 - Last mean reward per episode: 270.01\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 255          |\n",
            "|    ep_rew_mean          | 272          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1140         |\n",
            "|    iterations           | 129          |\n",
            "|    time_elapsed         | 1853         |\n",
            "|    total_timesteps      | 2113536      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0046058907 |\n",
            "|    clip_fraction        | 0.0615       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.671       |\n",
            "|    explained_variance   | 0.968        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 248          |\n",
            "|    n_updates            | 512          |\n",
            "|    policy_gradient_loss | 0.000681     |\n",
            "|    value_loss           | 73.5         |\n",
            "------------------------------------------\n",
            "Num timesteps: 2128000\n",
            "Best mean reward: 275.64 - Last mean reward per episode: 266.54\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 246         |\n",
            "|    ep_rew_mean          | 266         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1142        |\n",
            "|    iterations           | 130         |\n",
            "|    time_elapsed         | 1863        |\n",
            "|    total_timesteps      | 2129920     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003945565 |\n",
            "|    clip_fraction        | 0.0347      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.651      |\n",
            "|    explained_variance   | 0.938       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 27.3        |\n",
            "|    n_updates            | 516         |\n",
            "|    policy_gradient_loss | 0.00045     |\n",
            "|    value_loss           | 153         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 2144000\n",
            "Best mean reward: 275.64 - Last mean reward per episode: 256.79\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 245          |\n",
            "|    ep_rew_mean          | 254          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1145         |\n",
            "|    iterations           | 131          |\n",
            "|    time_elapsed         | 1874         |\n",
            "|    total_timesteps      | 2146304      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0029962272 |\n",
            "|    clip_fraction        | 0.0278       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.669       |\n",
            "|    explained_variance   | 0.881        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.68         |\n",
            "|    n_updates            | 520          |\n",
            "|    policy_gradient_loss | -0.000416    |\n",
            "|    value_loss           | 260          |\n",
            "------------------------------------------\n",
            "Num timesteps: 2160000\n",
            "Best mean reward: 275.64 - Last mean reward per episode: 256.93\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 251          |\n",
            "|    ep_rew_mean          | 259          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1147         |\n",
            "|    iterations           | 132          |\n",
            "|    time_elapsed         | 1883         |\n",
            "|    total_timesteps      | 2162688      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034999535 |\n",
            "|    clip_fraction        | 0.0328       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.665       |\n",
            "|    explained_variance   | 0.902        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 195          |\n",
            "|    n_updates            | 524          |\n",
            "|    policy_gradient_loss | -0.000731    |\n",
            "|    value_loss           | 299          |\n",
            "------------------------------------------\n",
            "Num timesteps: 2176000\n",
            "Best mean reward: 275.64 - Last mean reward per episode: 268.85\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 249          |\n",
            "|    ep_rew_mean          | 272          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1150         |\n",
            "|    iterations           | 133          |\n",
            "|    time_elapsed         | 1894         |\n",
            "|    total_timesteps      | 2179072      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0030548107 |\n",
            "|    clip_fraction        | 0.0327       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.644       |\n",
            "|    explained_variance   | 0.934        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 40.5         |\n",
            "|    n_updates            | 528          |\n",
            "|    policy_gradient_loss | -7.59e-05    |\n",
            "|    value_loss           | 167          |\n",
            "------------------------------------------\n",
            "Num timesteps: 2192000\n",
            "Best mean reward: 275.64 - Last mean reward per episode: 264.43\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 257          |\n",
            "|    ep_rew_mean          | 262          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1152         |\n",
            "|    iterations           | 134          |\n",
            "|    time_elapsed         | 1904         |\n",
            "|    total_timesteps      | 2195456      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035167024 |\n",
            "|    clip_fraction        | 0.035        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.651       |\n",
            "|    explained_variance   | 0.952        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.51         |\n",
            "|    n_updates            | 532          |\n",
            "|    policy_gradient_loss | 0.00197      |\n",
            "|    value_loss           | 114          |\n",
            "------------------------------------------\n",
            "Num timesteps: 2208000\n",
            "Best mean reward: 275.64 - Last mean reward per episode: 260.02\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 264          |\n",
            "|    ep_rew_mean          | 264          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1155         |\n",
            "|    iterations           | 135          |\n",
            "|    time_elapsed         | 1914         |\n",
            "|    total_timesteps      | 2211840      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034858852 |\n",
            "|    clip_fraction        | 0.0412       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.659       |\n",
            "|    explained_variance   | 0.906        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 17.6         |\n",
            "|    n_updates            | 536          |\n",
            "|    policy_gradient_loss | -0.000262    |\n",
            "|    value_loss           | 290          |\n",
            "------------------------------------------\n",
            "Num timesteps: 2224000\n",
            "Best mean reward: 275.64 - Last mean reward per episode: 270.37\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 264         |\n",
            "|    ep_rew_mean          | 269         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1157        |\n",
            "|    iterations           | 136         |\n",
            "|    time_elapsed         | 1924        |\n",
            "|    total_timesteps      | 2228224     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004354301 |\n",
            "|    clip_fraction        | 0.0577      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.658      |\n",
            "|    explained_variance   | 0.934       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 8.56        |\n",
            "|    n_updates            | 540         |\n",
            "|    policy_gradient_loss | 0.00108     |\n",
            "|    value_loss           | 149         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 2240000\n",
            "Best mean reward: 275.64 - Last mean reward per episode: 271.02\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 267         |\n",
            "|    ep_rew_mean          | 271         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1159        |\n",
            "|    iterations           | 137         |\n",
            "|    time_elapsed         | 1935        |\n",
            "|    total_timesteps      | 2244608     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004131206 |\n",
            "|    clip_fraction        | 0.0538      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.678      |\n",
            "|    explained_variance   | 0.94        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 15.4        |\n",
            "|    n_updates            | 544         |\n",
            "|    policy_gradient_loss | -0.000113   |\n",
            "|    value_loss           | 142         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 2256000\n",
            "Best mean reward: 275.64 - Last mean reward per episode: 271.01\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 255         |\n",
            "|    ep_rew_mean          | 270         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1162        |\n",
            "|    iterations           | 138         |\n",
            "|    time_elapsed         | 1945        |\n",
            "|    total_timesteps      | 2260992     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004231347 |\n",
            "|    clip_fraction        | 0.0542      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.653      |\n",
            "|    explained_variance   | 0.983       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.83        |\n",
            "|    n_updates            | 548         |\n",
            "|    policy_gradient_loss | 0.00102     |\n",
            "|    value_loss           | 19.8        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 2272000\n",
            "Best mean reward: 275.64 - Last mean reward per episode: 262.90\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 264          |\n",
            "|    ep_rew_mean          | 269          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1164         |\n",
            "|    iterations           | 139          |\n",
            "|    time_elapsed         | 1955         |\n",
            "|    total_timesteps      | 2277376      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0031160691 |\n",
            "|    clip_fraction        | 0.0292       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.675       |\n",
            "|    explained_variance   | 0.928        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 111          |\n",
            "|    n_updates            | 552          |\n",
            "|    policy_gradient_loss | 9.08e-05     |\n",
            "|    value_loss           | 216          |\n",
            "------------------------------------------\n",
            "Num timesteps: 2288000\n",
            "Best mean reward: 275.64 - Last mean reward per episode: 270.90\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 254          |\n",
            "|    ep_rew_mean          | 272          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1166         |\n",
            "|    iterations           | 140          |\n",
            "|    time_elapsed         | 1966         |\n",
            "|    total_timesteps      | 2293760      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035156207 |\n",
            "|    clip_fraction        | 0.0415       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.675       |\n",
            "|    explained_variance   | 0.951        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.26         |\n",
            "|    n_updates            | 556          |\n",
            "|    policy_gradient_loss | -9.48e-05    |\n",
            "|    value_loss           | 125          |\n",
            "------------------------------------------\n",
            "Num timesteps: 2304000\n",
            "Best mean reward: 275.64 - Last mean reward per episode: 271.15\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 249          |\n",
            "|    ep_rew_mean          | 264          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1168         |\n",
            "|    iterations           | 141          |\n",
            "|    time_elapsed         | 1976         |\n",
            "|    total_timesteps      | 2310144      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0056049842 |\n",
            "|    clip_fraction        | 0.0627       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.675       |\n",
            "|    explained_variance   | 0.958        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.87         |\n",
            "|    n_updates            | 560          |\n",
            "|    policy_gradient_loss | 0.000555     |\n",
            "|    value_loss           | 85.4         |\n",
            "------------------------------------------\n",
            "Num timesteps: 2320000\n",
            "Best mean reward: 275.64 - Last mean reward per episode: 256.23\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 249         |\n",
            "|    ep_rew_mean          | 256         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1171        |\n",
            "|    iterations           | 142         |\n",
            "|    time_elapsed         | 1986        |\n",
            "|    total_timesteps      | 2326528     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003359213 |\n",
            "|    clip_fraction        | 0.0417      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.642      |\n",
            "|    explained_variance   | 0.924       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.05        |\n",
            "|    n_updates            | 564         |\n",
            "|    policy_gradient_loss | 0.000201    |\n",
            "|    value_loss           | 155         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 2336000\n",
            "Best mean reward: 275.64 - Last mean reward per episode: 264.74\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 254          |\n",
            "|    ep_rew_mean          | 271          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1173         |\n",
            "|    iterations           | 143          |\n",
            "|    time_elapsed         | 1995         |\n",
            "|    total_timesteps      | 2342912      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044202525 |\n",
            "|    clip_fraction        | 0.0354       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.688       |\n",
            "|    explained_variance   | 0.914        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 148          |\n",
            "|    n_updates            | 568          |\n",
            "|    policy_gradient_loss | 0.000735     |\n",
            "|    value_loss           | 171          |\n",
            "------------------------------------------\n",
            "Num timesteps: 2352000\n",
            "Best mean reward: 275.64 - Last mean reward per episode: 276.14\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 246         |\n",
            "|    ep_rew_mean          | 274         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1176        |\n",
            "|    iterations           | 144         |\n",
            "|    time_elapsed         | 2005        |\n",
            "|    total_timesteps      | 2359296     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002919137 |\n",
            "|    clip_fraction        | 0.0473      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.651      |\n",
            "|    explained_variance   | 0.972       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.18        |\n",
            "|    n_updates            | 572         |\n",
            "|    policy_gradient_loss | 0.00102     |\n",
            "|    value_loss           | 19.2        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 2368000\n",
            "Best mean reward: 276.14 - Last mean reward per episode: 274.35\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 250         |\n",
            "|    ep_rew_mean          | 272         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1178        |\n",
            "|    iterations           | 145         |\n",
            "|    time_elapsed         | 2015        |\n",
            "|    total_timesteps      | 2375680     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003946961 |\n",
            "|    clip_fraction        | 0.0403      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.657      |\n",
            "|    explained_variance   | 0.96        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 161         |\n",
            "|    n_updates            | 576         |\n",
            "|    policy_gradient_loss | 0.000254    |\n",
            "|    value_loss           | 99          |\n",
            "-----------------------------------------\n",
            "Num timesteps: 2384000\n",
            "Best mean reward: 276.14 - Last mean reward per episode: 272.53\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 248         |\n",
            "|    ep_rew_mean          | 274         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1180        |\n",
            "|    iterations           | 146         |\n",
            "|    time_elapsed         | 2025        |\n",
            "|    total_timesteps      | 2392064     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004358188 |\n",
            "|    clip_fraction        | 0.0353      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.629      |\n",
            "|    explained_variance   | 0.912       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 121         |\n",
            "|    n_updates            | 580         |\n",
            "|    policy_gradient_loss | -0.000172   |\n",
            "|    value_loss           | 221         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 2400000\n",
            "Best mean reward: 276.14 - Last mean reward per episode: 273.41\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 254         |\n",
            "|    ep_rew_mean          | 272         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1182        |\n",
            "|    iterations           | 147         |\n",
            "|    time_elapsed         | 2036        |\n",
            "|    total_timesteps      | 2408448     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003920763 |\n",
            "|    clip_fraction        | 0.0517      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.658      |\n",
            "|    explained_variance   | 0.97        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 365         |\n",
            "|    n_updates            | 584         |\n",
            "|    policy_gradient_loss | 0.000465    |\n",
            "|    value_loss           | 77.4        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 2416000\n",
            "Best mean reward: 276.14 - Last mean reward per episode: 272.03\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 257          |\n",
            "|    ep_rew_mean          | 269          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1184         |\n",
            "|    iterations           | 148          |\n",
            "|    time_elapsed         | 2046         |\n",
            "|    total_timesteps      | 2424832      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0032682219 |\n",
            "|    clip_fraction        | 0.0389       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.673       |\n",
            "|    explained_variance   | 0.953        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.98         |\n",
            "|    n_updates            | 588          |\n",
            "|    policy_gradient_loss | 0.000295     |\n",
            "|    value_loss           | 123          |\n",
            "------------------------------------------\n",
            "Num timesteps: 2432000\n",
            "Best mean reward: 276.14 - Last mean reward per episode: 271.77\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 256        |\n",
            "|    ep_rew_mean          | 268        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 1186       |\n",
            "|    iterations           | 149        |\n",
            "|    time_elapsed         | 2057       |\n",
            "|    total_timesteps      | 2441216    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00553523 |\n",
            "|    clip_fraction        | 0.0621     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.678     |\n",
            "|    explained_variance   | 0.97       |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 6.49       |\n",
            "|    n_updates            | 592        |\n",
            "|    policy_gradient_loss | 3.82e-05   |\n",
            "|    value_loss           | 77.1       |\n",
            "----------------------------------------\n",
            "Num timesteps: 2448000\n",
            "Best mean reward: 276.14 - Last mean reward per episode: 268.90\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 274         |\n",
            "|    ep_rew_mean          | 266         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1188        |\n",
            "|    iterations           | 150         |\n",
            "|    time_elapsed         | 2068        |\n",
            "|    total_timesteps      | 2457600     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005860287 |\n",
            "|    clip_fraction        | 0.0493      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.667      |\n",
            "|    explained_variance   | 0.967       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.86        |\n",
            "|    n_updates            | 596         |\n",
            "|    policy_gradient_loss | 0.000136    |\n",
            "|    value_loss           | 62.5        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 2464000\n",
            "Best mean reward: 276.14 - Last mean reward per episode: 268.44\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 264         |\n",
            "|    ep_rew_mean          | 264         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1190        |\n",
            "|    iterations           | 151         |\n",
            "|    time_elapsed         | 2078        |\n",
            "|    total_timesteps      | 2473984     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004568197 |\n",
            "|    clip_fraction        | 0.0409      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.666      |\n",
            "|    explained_variance   | 0.964       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 17.1        |\n",
            "|    n_updates            | 600         |\n",
            "|    policy_gradient_loss | -0.000206   |\n",
            "|    value_loss           | 111         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 2480000\n",
            "Best mean reward: 276.14 - Last mean reward per episode: 269.05\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 252          |\n",
            "|    ep_rew_mean          | 272          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1192         |\n",
            "|    iterations           | 152          |\n",
            "|    time_elapsed         | 2087         |\n",
            "|    total_timesteps      | 2490368      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0027777203 |\n",
            "|    clip_fraction        | 0.0235       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.631       |\n",
            "|    explained_variance   | 0.941        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 16.4         |\n",
            "|    n_updates            | 604          |\n",
            "|    policy_gradient_loss | -0.000225    |\n",
            "|    value_loss           | 209          |\n",
            "------------------------------------------\n",
            "Num timesteps: 2496000\n",
            "Best mean reward: 276.14 - Last mean reward per episode: 274.38\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 239          |\n",
            "|    ep_rew_mean          | 280          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1195         |\n",
            "|    iterations           | 153          |\n",
            "|    time_elapsed         | 2097         |\n",
            "|    total_timesteps      | 2506752      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041626743 |\n",
            "|    clip_fraction        | 0.0589       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.624       |\n",
            "|    explained_variance   | 0.987        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.77         |\n",
            "|    n_updates            | 608          |\n",
            "|    policy_gradient_loss | 0.000403     |\n",
            "|    value_loss           | 10.5         |\n",
            "------------------------------------------\n",
            "Num timesteps: 2512000\n",
            "Best mean reward: 276.14 - Last mean reward per episode: 280.05\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 238        |\n",
            "|    ep_rew_mean          | 261        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 1197       |\n",
            "|    iterations           | 154        |\n",
            "|    time_elapsed         | 2107       |\n",
            "|    total_timesteps      | 2523136    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00425212 |\n",
            "|    clip_fraction        | 0.0513     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.617     |\n",
            "|    explained_variance   | 0.981      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 112        |\n",
            "|    n_updates            | 612        |\n",
            "|    policy_gradient_loss | -9.85e-05  |\n",
            "|    value_loss           | 37.3       |\n",
            "----------------------------------------\n",
            "Num timesteps: 2528000\n",
            "Best mean reward: 280.05 - Last mean reward per episode: 257.98\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 228          |\n",
            "|    ep_rew_mean          | 257          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1199         |\n",
            "|    iterations           | 155          |\n",
            "|    time_elapsed         | 2117         |\n",
            "|    total_timesteps      | 2539520      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015826374 |\n",
            "|    clip_fraction        | 0.00641      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.636       |\n",
            "|    explained_variance   | 0.863        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 181          |\n",
            "|    n_updates            | 616          |\n",
            "|    policy_gradient_loss | -0.000257    |\n",
            "|    value_loss           | 382          |\n",
            "------------------------------------------\n",
            "Num timesteps: 2544000\n",
            "Best mean reward: 280.05 - Last mean reward per episode: 260.85\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 242         |\n",
            "|    ep_rew_mean          | 272         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1201        |\n",
            "|    iterations           | 156         |\n",
            "|    time_elapsed         | 2126        |\n",
            "|    total_timesteps      | 2555904     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004536943 |\n",
            "|    clip_fraction        | 0.0262      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.633      |\n",
            "|    explained_variance   | 0.943       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 13.2        |\n",
            "|    n_updates            | 620         |\n",
            "|    policy_gradient_loss | -0.00039    |\n",
            "|    value_loss           | 214         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 2560000\n",
            "Best mean reward: 280.05 - Last mean reward per episode: 271.56\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 237          |\n",
            "|    ep_rew_mean          | 275          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1203         |\n",
            "|    iterations           | 157          |\n",
            "|    time_elapsed         | 2136         |\n",
            "|    total_timesteps      | 2572288      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0050407574 |\n",
            "|    clip_fraction        | 0.0531       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.632       |\n",
            "|    explained_variance   | 0.947        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 30.6         |\n",
            "|    n_updates            | 624          |\n",
            "|    policy_gradient_loss | 0.000756     |\n",
            "|    value_loss           | 123          |\n",
            "------------------------------------------\n",
            "Num timesteps: 2576000\n",
            "Best mean reward: 280.05 - Last mean reward per episode: 275.38\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 231          |\n",
            "|    ep_rew_mean          | 276          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1206         |\n",
            "|    iterations           | 158          |\n",
            "|    time_elapsed         | 2145         |\n",
            "|    total_timesteps      | 2588672      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041733207 |\n",
            "|    clip_fraction        | 0.0546       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.626       |\n",
            "|    explained_variance   | 0.993        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.53         |\n",
            "|    n_updates            | 628          |\n",
            "|    policy_gradient_loss | 0.00202      |\n",
            "|    value_loss           | 7            |\n",
            "------------------------------------------\n",
            "Num timesteps: 2592000\n",
            "Best mean reward: 280.05 - Last mean reward per episode: 276.47\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 231         |\n",
            "|    ep_rew_mean          | 272         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1208        |\n",
            "|    iterations           | 159         |\n",
            "|    time_elapsed         | 2155        |\n",
            "|    total_timesteps      | 2605056     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004082483 |\n",
            "|    clip_fraction        | 0.045       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.622      |\n",
            "|    explained_variance   | 0.996       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.49        |\n",
            "|    n_updates            | 632         |\n",
            "|    policy_gradient_loss | 0.000857    |\n",
            "|    value_loss           | 3.75        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 2608000\n",
            "Best mean reward: 280.05 - Last mean reward per episode: 271.02\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 232         |\n",
            "|    ep_rew_mean          | 267         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1210        |\n",
            "|    iterations           | 160         |\n",
            "|    time_elapsed         | 2165        |\n",
            "|    total_timesteps      | 2621440     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001796056 |\n",
            "|    clip_fraction        | 0.029       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.638      |\n",
            "|    explained_variance   | 0.925       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 15.5        |\n",
            "|    n_updates            | 636         |\n",
            "|    policy_gradient_loss | 0.000119    |\n",
            "|    value_loss           | 179         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 2624000\n",
            "Best mean reward: 280.05 - Last mean reward per episode: 266.53\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 240          |\n",
            "|    ep_rew_mean          | 266          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1212         |\n",
            "|    iterations           | 161          |\n",
            "|    time_elapsed         | 2175         |\n",
            "|    total_timesteps      | 2637824      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0028953715 |\n",
            "|    clip_fraction        | 0.0354       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.637       |\n",
            "|    explained_variance   | 0.978        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.32         |\n",
            "|    n_updates            | 640          |\n",
            "|    policy_gradient_loss | 0.000276     |\n",
            "|    value_loss           | 32.2         |\n",
            "------------------------------------------\n",
            "Num timesteps: 2640000\n",
            "Best mean reward: 280.05 - Last mean reward per episode: 268.35\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 233          |\n",
            "|    ep_rew_mean          | 274          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1214         |\n",
            "|    iterations           | 162          |\n",
            "|    time_elapsed         | 2184         |\n",
            "|    total_timesteps      | 2654208      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041075516 |\n",
            "|    clip_fraction        | 0.0335       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.64        |\n",
            "|    explained_variance   | 0.944        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.08         |\n",
            "|    n_updates            | 644          |\n",
            "|    policy_gradient_loss | 4.2e-05      |\n",
            "|    value_loss           | 138          |\n",
            "------------------------------------------\n",
            "Num timesteps: 2656000\n",
            "Best mean reward: 280.05 - Last mean reward per episode: 275.13\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 238          |\n",
            "|    ep_rew_mean          | 272          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1216         |\n",
            "|    iterations           | 163          |\n",
            "|    time_elapsed         | 2194         |\n",
            "|    total_timesteps      | 2670592      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0052236994 |\n",
            "|    clip_fraction        | 0.0637       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.618       |\n",
            "|    explained_variance   | 0.989        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.79         |\n",
            "|    n_updates            | 648          |\n",
            "|    policy_gradient_loss | 0.00039      |\n",
            "|    value_loss           | 15           |\n",
            "------------------------------------------\n",
            "Num timesteps: 2672000\n",
            "Best mean reward: 280.05 - Last mean reward per episode: 272.70\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 242          |\n",
            "|    ep_rew_mean          | 274          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1218         |\n",
            "|    iterations           | 164          |\n",
            "|    time_elapsed         | 2204         |\n",
            "|    total_timesteps      | 2686976      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0031753683 |\n",
            "|    clip_fraction        | 0.0318       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.635       |\n",
            "|    explained_variance   | 0.954        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.31         |\n",
            "|    n_updates            | 652          |\n",
            "|    policy_gradient_loss | 0.000682     |\n",
            "|    value_loss           | 93.1         |\n",
            "------------------------------------------\n",
            "Num timesteps: 2688000\n",
            "Best mean reward: 280.05 - Last mean reward per episode: 274.68\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 240          |\n",
            "|    ep_rew_mean          | 277          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1220         |\n",
            "|    iterations           | 165          |\n",
            "|    time_elapsed         | 2215         |\n",
            "|    total_timesteps      | 2703360      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038066914 |\n",
            "|    clip_fraction        | 0.0479       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.602       |\n",
            "|    explained_variance   | 0.997        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.58         |\n",
            "|    n_updates            | 656          |\n",
            "|    policy_gradient_loss | 0.000993     |\n",
            "|    value_loss           | 4.11         |\n",
            "------------------------------------------\n",
            "Num timesteps: 2704000\n",
            "Best mean reward: 280.05 - Last mean reward per episode: 276.50\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 236          |\n",
            "|    ep_rew_mean          | 270          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1222         |\n",
            "|    iterations           | 166          |\n",
            "|    time_elapsed         | 2225         |\n",
            "|    total_timesteps      | 2719744      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038465117 |\n",
            "|    clip_fraction        | 0.0377       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.638       |\n",
            "|    explained_variance   | 0.983        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.36         |\n",
            "|    n_updates            | 660          |\n",
            "|    policy_gradient_loss | 0.000599     |\n",
            "|    value_loss           | 40.6         |\n",
            "------------------------------------------\n",
            "Num timesteps: 2720000\n",
            "Best mean reward: 280.05 - Last mean reward per episode: 269.59\n",
            "Num timesteps: 2736000\n",
            "Best mean reward: 280.05 - Last mean reward per episode: 273.41\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 226          |\n",
            "|    ep_rew_mean          | 273          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1224         |\n",
            "|    iterations           | 167          |\n",
            "|    time_elapsed         | 2234         |\n",
            "|    total_timesteps      | 2736128      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0037392096 |\n",
            "|    clip_fraction        | 0.0394       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.641       |\n",
            "|    explained_variance   | 0.962        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 95.9         |\n",
            "|    n_updates            | 664          |\n",
            "|    policy_gradient_loss | -0.000118    |\n",
            "|    value_loss           | 101          |\n",
            "------------------------------------------\n",
            "Num timesteps: 2752000\n",
            "Best mean reward: 280.05 - Last mean reward per episode: 276.21\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 231          |\n",
            "|    ep_rew_mean          | 276          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1226         |\n",
            "|    iterations           | 168          |\n",
            "|    time_elapsed         | 2244         |\n",
            "|    total_timesteps      | 2752512      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041301968 |\n",
            "|    clip_fraction        | 0.0538       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.625       |\n",
            "|    explained_variance   | 0.988        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2            |\n",
            "|    n_updates            | 668          |\n",
            "|    policy_gradient_loss | 4.57e-05     |\n",
            "|    value_loss           | 12.1         |\n",
            "------------------------------------------\n",
            "Num timesteps: 2768000\n",
            "Best mean reward: 280.05 - Last mean reward per episode: 277.87\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 239          |\n",
            "|    ep_rew_mean          | 279          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1228         |\n",
            "|    iterations           | 169          |\n",
            "|    time_elapsed         | 2254         |\n",
            "|    total_timesteps      | 2768896      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0033942338 |\n",
            "|    clip_fraction        | 0.0426       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.614       |\n",
            "|    explained_variance   | 0.997        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.03         |\n",
            "|    n_updates            | 672          |\n",
            "|    policy_gradient_loss | 0.000961     |\n",
            "|    value_loss           | 5.18         |\n",
            "------------------------------------------\n",
            "Num timesteps: 2784000\n",
            "Best mean reward: 280.05 - Last mean reward per episode: 276.02\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 236         |\n",
            "|    ep_rew_mean          | 277         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1229        |\n",
            "|    iterations           | 170         |\n",
            "|    time_elapsed         | 2265        |\n",
            "|    total_timesteps      | 2785280     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003864204 |\n",
            "|    clip_fraction        | 0.0447      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.616      |\n",
            "|    explained_variance   | 0.987       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.58        |\n",
            "|    n_updates            | 676         |\n",
            "|    policy_gradient_loss | -0.000317   |\n",
            "|    value_loss           | 31.7        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 2800000\n",
            "Best mean reward: 280.05 - Last mean reward per episode: 278.64\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 233          |\n",
            "|    ep_rew_mean          | 281          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1231         |\n",
            "|    iterations           | 171          |\n",
            "|    time_elapsed         | 2274         |\n",
            "|    total_timesteps      | 2801664      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0025167689 |\n",
            "|    clip_fraction        | 0.0352       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.614       |\n",
            "|    explained_variance   | 0.925        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.49         |\n",
            "|    n_updates            | 680          |\n",
            "|    policy_gradient_loss | -0.000464    |\n",
            "|    value_loss           | 216          |\n",
            "------------------------------------------\n",
            "Num timesteps: 2816000\n",
            "Best mean reward: 280.05 - Last mean reward per episode: 279.62\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 231          |\n",
            "|    ep_rew_mean          | 279          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1233         |\n",
            "|    iterations           | 172          |\n",
            "|    time_elapsed         | 2284         |\n",
            "|    total_timesteps      | 2818048      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041057067 |\n",
            "|    clip_fraction        | 0.0603       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.619       |\n",
            "|    explained_variance   | 0.996        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.35         |\n",
            "|    n_updates            | 684          |\n",
            "|    policy_gradient_loss | 0.000477     |\n",
            "|    value_loss           | 4.68         |\n",
            "------------------------------------------\n",
            "Num timesteps: 2832000\n",
            "Best mean reward: 280.05 - Last mean reward per episode: 272.83\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 225          |\n",
            "|    ep_rew_mean          | 272          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1235         |\n",
            "|    iterations           | 173          |\n",
            "|    time_elapsed         | 2293         |\n",
            "|    total_timesteps      | 2834432      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0036487025 |\n",
            "|    clip_fraction        | 0.0492       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.606       |\n",
            "|    explained_variance   | 0.98         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.31         |\n",
            "|    n_updates            | 688          |\n",
            "|    policy_gradient_loss | 2.2e-05      |\n",
            "|    value_loss           | 62.3         |\n",
            "------------------------------------------\n",
            "Num timesteps: 2848000\n",
            "Best mean reward: 280.05 - Last mean reward per episode: 278.58\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 229          |\n",
            "|    ep_rew_mean          | 280          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1237         |\n",
            "|    iterations           | 174          |\n",
            "|    time_elapsed         | 2303         |\n",
            "|    total_timesteps      | 2850816      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038934993 |\n",
            "|    clip_fraction        | 0.0318       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.609       |\n",
            "|    explained_variance   | 0.967        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.94         |\n",
            "|    n_updates            | 692          |\n",
            "|    policy_gradient_loss | 0.00125      |\n",
            "|    value_loss           | 80           |\n",
            "------------------------------------------\n",
            "Num timesteps: 2864000\n",
            "Best mean reward: 280.05 - Last mean reward per episode: 282.80\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 242          |\n",
            "|    ep_rew_mean          | 281          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1238         |\n",
            "|    iterations           | 175          |\n",
            "|    time_elapsed         | 2314         |\n",
            "|    total_timesteps      | 2867200      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034181704 |\n",
            "|    clip_fraction        | 0.0439       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.593       |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.03         |\n",
            "|    n_updates            | 696          |\n",
            "|    policy_gradient_loss | 0.00137      |\n",
            "|    value_loss           | 3.51         |\n",
            "------------------------------------------\n",
            "Num timesteps: 2880000\n",
            "Best mean reward: 282.80 - Last mean reward per episode: 276.03\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 239          |\n",
            "|    ep_rew_mean          | 276          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1240         |\n",
            "|    iterations           | 176          |\n",
            "|    time_elapsed         | 2324         |\n",
            "|    total_timesteps      | 2883584      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0039869547 |\n",
            "|    clip_fraction        | 0.0394       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.606       |\n",
            "|    explained_variance   | 0.996        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.01         |\n",
            "|    n_updates            | 700          |\n",
            "|    policy_gradient_loss | -0.000924    |\n",
            "|    value_loss           | 5.74         |\n",
            "------------------------------------------\n",
            "Num timesteps: 2896000\n",
            "Best mean reward: 282.80 - Last mean reward per episode: 270.19\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 230          |\n",
            "|    ep_rew_mean          | 270          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1242         |\n",
            "|    iterations           | 177          |\n",
            "|    time_elapsed         | 2333         |\n",
            "|    total_timesteps      | 2899968      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040828567 |\n",
            "|    clip_fraction        | 0.0312       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.597       |\n",
            "|    explained_variance   | 0.96         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 14.8         |\n",
            "|    n_updates            | 704          |\n",
            "|    policy_gradient_loss | -0.000143    |\n",
            "|    value_loss           | 101          |\n",
            "------------------------------------------\n",
            "Num timesteps: 2912000\n",
            "Best mean reward: 282.80 - Last mean reward per episode: 270.42\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 248          |\n",
            "|    ep_rew_mean          | 269          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1244         |\n",
            "|    iterations           | 178          |\n",
            "|    time_elapsed         | 2343         |\n",
            "|    total_timesteps      | 2916352      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0031954509 |\n",
            "|    clip_fraction        | 0.0357       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.583       |\n",
            "|    explained_variance   | 0.966        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 36.5         |\n",
            "|    n_updates            | 708          |\n",
            "|    policy_gradient_loss | 0.000373     |\n",
            "|    value_loss           | 108          |\n",
            "------------------------------------------\n",
            "Num timesteps: 2928000\n",
            "Best mean reward: 282.80 - Last mean reward per episode: 270.31\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 244         |\n",
            "|    ep_rew_mean          | 275         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1246        |\n",
            "|    iterations           | 179         |\n",
            "|    time_elapsed         | 2353        |\n",
            "|    total_timesteps      | 2932736     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005189897 |\n",
            "|    clip_fraction        | 0.0425      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.593      |\n",
            "|    explained_variance   | 0.961       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 47.3        |\n",
            "|    n_updates            | 712         |\n",
            "|    policy_gradient_loss | -0.000328   |\n",
            "|    value_loss           | 150         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 2944000\n",
            "Best mean reward: 282.80 - Last mean reward per episode: 274.61\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 238          |\n",
            "|    ep_rew_mean          | 275          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1247         |\n",
            "|    iterations           | 180          |\n",
            "|    time_elapsed         | 2363         |\n",
            "|    total_timesteps      | 2949120      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0046131685 |\n",
            "|    clip_fraction        | 0.0537       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.575       |\n",
            "|    explained_variance   | 0.973        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.28         |\n",
            "|    n_updates            | 716          |\n",
            "|    policy_gradient_loss | 0.000336     |\n",
            "|    value_loss           | 86.9         |\n",
            "------------------------------------------\n",
            "Num timesteps: 2960000\n",
            "Best mean reward: 282.80 - Last mean reward per episode: 270.57\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 224         |\n",
            "|    ep_rew_mean          | 274         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1249        |\n",
            "|    iterations           | 181         |\n",
            "|    time_elapsed         | 2373        |\n",
            "|    total_timesteps      | 2965504     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005830121 |\n",
            "|    clip_fraction        | 0.0633      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.6        |\n",
            "|    explained_variance   | 0.954       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.42        |\n",
            "|    n_updates            | 720         |\n",
            "|    policy_gradient_loss | -0.000483   |\n",
            "|    value_loss           | 88.1        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 2976000\n",
            "Best mean reward: 282.80 - Last mean reward per episode: 274.83\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 236         |\n",
            "|    ep_rew_mean          | 275         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1250        |\n",
            "|    iterations           | 182         |\n",
            "|    time_elapsed         | 2383        |\n",
            "|    total_timesteps      | 2981888     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004081308 |\n",
            "|    clip_fraction        | 0.0479      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.591      |\n",
            "|    explained_variance   | 0.985       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 18.4        |\n",
            "|    n_updates            | 724         |\n",
            "|    policy_gradient_loss | 0.000942    |\n",
            "|    value_loss           | 30.9        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 2992000\n",
            "Best mean reward: 282.80 - Last mean reward per episode: 276.83\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 229         |\n",
            "|    ep_rew_mean          | 278         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1252        |\n",
            "|    iterations           | 183         |\n",
            "|    time_elapsed         | 2393        |\n",
            "|    total_timesteps      | 2998272     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004043429 |\n",
            "|    clip_fraction        | 0.0434      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.596      |\n",
            "|    explained_variance   | 0.98        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.52        |\n",
            "|    n_updates            | 728         |\n",
            "|    policy_gradient_loss | 0.00013     |\n",
            "|    value_loss           | 66.3        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 3008000\n",
            "Best mean reward: 282.80 - Last mean reward per episode: 277.95\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 231         |\n",
            "|    ep_rew_mean          | 279         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1254        |\n",
            "|    iterations           | 184         |\n",
            "|    time_elapsed         | 2403        |\n",
            "|    total_timesteps      | 3014656     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004230682 |\n",
            "|    clip_fraction        | 0.0495      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.583      |\n",
            "|    explained_variance   | 0.991       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.84        |\n",
            "|    n_updates            | 732         |\n",
            "|    policy_gradient_loss | 0.00132     |\n",
            "|    value_loss           | 12.9        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 3024000\n",
            "Best mean reward: 282.80 - Last mean reward per episode: 271.37\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 240         |\n",
            "|    ep_rew_mean          | 271         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1255        |\n",
            "|    iterations           | 185         |\n",
            "|    time_elapsed         | 2414        |\n",
            "|    total_timesteps      | 3031040     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003871038 |\n",
            "|    clip_fraction        | 0.0397      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.586      |\n",
            "|    explained_variance   | 0.982       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 19.2        |\n",
            "|    n_updates            | 736         |\n",
            "|    policy_gradient_loss | 0.000354    |\n",
            "|    value_loss           | 44.4        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 3040000\n",
            "Best mean reward: 282.80 - Last mean reward per episode: 270.16\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 235          |\n",
            "|    ep_rew_mean          | 278          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1257         |\n",
            "|    iterations           | 186          |\n",
            "|    time_elapsed         | 2423         |\n",
            "|    total_timesteps      | 3047424      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0030041817 |\n",
            "|    clip_fraction        | 0.0276       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.588       |\n",
            "|    explained_variance   | 0.967        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 49.8         |\n",
            "|    n_updates            | 740          |\n",
            "|    policy_gradient_loss | 0.000481     |\n",
            "|    value_loss           | 121          |\n",
            "------------------------------------------\n",
            "Num timesteps: 3056000\n",
            "Best mean reward: 282.80 - Last mean reward per episode: 274.49\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 224          |\n",
            "|    ep_rew_mean          | 280          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1259         |\n",
            "|    iterations           | 187          |\n",
            "|    time_elapsed         | 2433         |\n",
            "|    total_timesteps      | 3063808      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0045421375 |\n",
            "|    clip_fraction        | 0.055        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.589       |\n",
            "|    explained_variance   | 0.982        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.42         |\n",
            "|    n_updates            | 744          |\n",
            "|    policy_gradient_loss | 0.000391     |\n",
            "|    value_loss           | 43.3         |\n",
            "------------------------------------------\n",
            "Num timesteps: 3072000\n",
            "Best mean reward: 282.80 - Last mean reward per episode: 275.75\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 228          |\n",
            "|    ep_rew_mean          | 278          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1260         |\n",
            "|    iterations           | 188          |\n",
            "|    time_elapsed         | 2443         |\n",
            "|    total_timesteps      | 3080192      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0049217734 |\n",
            "|    clip_fraction        | 0.0657       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.573       |\n",
            "|    explained_variance   | 0.985        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.881        |\n",
            "|    n_updates            | 748          |\n",
            "|    policy_gradient_loss | 0.00117      |\n",
            "|    value_loss           | 34.7         |\n",
            "------------------------------------------\n",
            "Num timesteps: 3088000\n",
            "Best mean reward: 282.80 - Last mean reward per episode: 279.50\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 226          |\n",
            "|    ep_rew_mean          | 274          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1262         |\n",
            "|    iterations           | 189          |\n",
            "|    time_elapsed         | 2452         |\n",
            "|    total_timesteps      | 3096576      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0042583384 |\n",
            "|    clip_fraction        | 0.0468       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.58        |\n",
            "|    explained_variance   | 0.974        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 107          |\n",
            "|    n_updates            | 752          |\n",
            "|    policy_gradient_loss | 0.000328     |\n",
            "|    value_loss           | 79           |\n",
            "------------------------------------------\n",
            "Num timesteps: 3104000\n",
            "Best mean reward: 282.80 - Last mean reward per episode: 268.75\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 223          |\n",
            "|    ep_rew_mean          | 266          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1264         |\n",
            "|    iterations           | 190          |\n",
            "|    time_elapsed         | 2462         |\n",
            "|    total_timesteps      | 3112960      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044935304 |\n",
            "|    clip_fraction        | 0.0409       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.58        |\n",
            "|    explained_variance   | 0.958        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 37.3         |\n",
            "|    n_updates            | 756          |\n",
            "|    policy_gradient_loss | -7.23e-05    |\n",
            "|    value_loss           | 97.3         |\n",
            "------------------------------------------\n",
            "Num timesteps: 3120000\n",
            "Best mean reward: 282.80 - Last mean reward per episode: 263.12\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 219          |\n",
            "|    ep_rew_mean          | 266          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1266         |\n",
            "|    iterations           | 191          |\n",
            "|    time_elapsed         | 2471         |\n",
            "|    total_timesteps      | 3129344      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038349852 |\n",
            "|    clip_fraction        | 0.0573       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.605       |\n",
            "|    explained_variance   | 0.944        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 55.3         |\n",
            "|    n_updates            | 760          |\n",
            "|    policy_gradient_loss | -0.000449    |\n",
            "|    value_loss           | 166          |\n",
            "------------------------------------------\n",
            "Num timesteps: 3136000\n",
            "Best mean reward: 282.80 - Last mean reward per episode: 267.76\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 232          |\n",
            "|    ep_rew_mean          | 271          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1267         |\n",
            "|    iterations           | 192          |\n",
            "|    time_elapsed         | 2481         |\n",
            "|    total_timesteps      | 3145728      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040270234 |\n",
            "|    clip_fraction        | 0.0421       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.557       |\n",
            "|    explained_variance   | 0.901        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 25.9         |\n",
            "|    n_updates            | 764          |\n",
            "|    policy_gradient_loss | -7.24e-05    |\n",
            "|    value_loss           | 302          |\n",
            "------------------------------------------\n",
            "Num timesteps: 3152000\n",
            "Best mean reward: 282.80 - Last mean reward per episode: 266.42\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 231          |\n",
            "|    ep_rew_mean          | 264          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1269         |\n",
            "|    iterations           | 193          |\n",
            "|    time_elapsed         | 2490         |\n",
            "|    total_timesteps      | 3162112      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0047688065 |\n",
            "|    clip_fraction        | 0.054        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.566       |\n",
            "|    explained_variance   | 0.973        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 10.7         |\n",
            "|    n_updates            | 768          |\n",
            "|    policy_gradient_loss | 0.00137      |\n",
            "|    value_loss           | 82.4         |\n",
            "------------------------------------------\n",
            "Num timesteps: 3168000\n",
            "Best mean reward: 282.80 - Last mean reward per episode: 264.77\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 238          |\n",
            "|    ep_rew_mean          | 280          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1270         |\n",
            "|    iterations           | 194          |\n",
            "|    time_elapsed         | 2501         |\n",
            "|    total_timesteps      | 3178496      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0029327325 |\n",
            "|    clip_fraction        | 0.028        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.579       |\n",
            "|    explained_variance   | 0.921        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 87.1         |\n",
            "|    n_updates            | 772          |\n",
            "|    policy_gradient_loss | -0.000372    |\n",
            "|    value_loss           | 220          |\n",
            "------------------------------------------\n",
            "Num timesteps: 3184000\n",
            "Best mean reward: 282.80 - Last mean reward per episode: 278.96\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 232         |\n",
            "|    ep_rew_mean          | 279         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1272        |\n",
            "|    iterations           | 195         |\n",
            "|    time_elapsed         | 2510        |\n",
            "|    total_timesteps      | 3194880     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004655958 |\n",
            "|    clip_fraction        | 0.0699      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.583      |\n",
            "|    explained_variance   | 0.989       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.18        |\n",
            "|    n_updates            | 776         |\n",
            "|    policy_gradient_loss | 0.00222     |\n",
            "|    value_loss           | 11          |\n",
            "-----------------------------------------\n",
            "Num timesteps: 3200000\n",
            "Best mean reward: 282.80 - Last mean reward per episode: 278.29\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 232         |\n",
            "|    ep_rew_mean          | 276         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1273        |\n",
            "|    iterations           | 196         |\n",
            "|    time_elapsed         | 2522        |\n",
            "|    total_timesteps      | 3211264     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004418285 |\n",
            "|    clip_fraction        | 0.0463      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.583      |\n",
            "|    explained_variance   | 0.988       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.49        |\n",
            "|    n_updates            | 780         |\n",
            "|    policy_gradient_loss | 0.000189    |\n",
            "|    value_loss           | 18.2        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 3216000\n",
            "Best mean reward: 282.80 - Last mean reward per episode: 270.25\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 247         |\n",
            "|    ep_rew_mean          | 270         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1274        |\n",
            "|    iterations           | 197         |\n",
            "|    time_elapsed         | 2533        |\n",
            "|    total_timesteps      | 3227648     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002985286 |\n",
            "|    clip_fraction        | 0.0266      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.572      |\n",
            "|    explained_variance   | 0.981       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 18.6        |\n",
            "|    n_updates            | 784         |\n",
            "|    policy_gradient_loss | -0.000336   |\n",
            "|    value_loss           | 57.6        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 3232000\n",
            "Best mean reward: 282.80 - Last mean reward per episode: 270.03\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 229         |\n",
            "|    ep_rew_mean          | 279         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1275        |\n",
            "|    iterations           | 198         |\n",
            "|    time_elapsed         | 2542        |\n",
            "|    total_timesteps      | 3244032     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003902732 |\n",
            "|    clip_fraction        | 0.0467      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.587      |\n",
            "|    explained_variance   | 0.992       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.42        |\n",
            "|    n_updates            | 788         |\n",
            "|    policy_gradient_loss | 0.000973    |\n",
            "|    value_loss           | 15.4        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 3248000\n",
            "Best mean reward: 282.80 - Last mean reward per episode: 278.24\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 230          |\n",
            "|    ep_rew_mean          | 275          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1277         |\n",
            "|    iterations           | 199          |\n",
            "|    time_elapsed         | 2552         |\n",
            "|    total_timesteps      | 3260416      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0045389417 |\n",
            "|    clip_fraction        | 0.0489       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.604       |\n",
            "|    explained_variance   | 0.98         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.03         |\n",
            "|    n_updates            | 792          |\n",
            "|    policy_gradient_loss | 7.26e-05     |\n",
            "|    value_loss           | 49           |\n",
            "------------------------------------------\n",
            "Num timesteps: 3264000\n",
            "Best mean reward: 282.80 - Last mean reward per episode: 274.97\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 229         |\n",
            "|    ep_rew_mean          | 276         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1278        |\n",
            "|    iterations           | 200         |\n",
            "|    time_elapsed         | 2562        |\n",
            "|    total_timesteps      | 3276800     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004052789 |\n",
            "|    clip_fraction        | 0.0626      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.608      |\n",
            "|    explained_variance   | 0.973       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.3         |\n",
            "|    n_updates            | 796         |\n",
            "|    policy_gradient_loss | 0.000859    |\n",
            "|    value_loss           | 70          |\n",
            "-----------------------------------------\n",
            "Num timesteps: 3280000\n",
            "Best mean reward: 282.80 - Last mean reward per episode: 278.41\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 225         |\n",
            "|    ep_rew_mean          | 277         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1280        |\n",
            "|    iterations           | 201         |\n",
            "|    time_elapsed         | 2572        |\n",
            "|    total_timesteps      | 3293184     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006316553 |\n",
            "|    clip_fraction        | 0.0594      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.598      |\n",
            "|    explained_variance   | 0.975       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.25        |\n",
            "|    n_updates            | 800         |\n",
            "|    policy_gradient_loss | -0.000882   |\n",
            "|    value_loss           | 75.4        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 3296000\n",
            "Best mean reward: 282.80 - Last mean reward per episode: 279.62\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 238         |\n",
            "|    ep_rew_mean          | 278         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1281        |\n",
            "|    iterations           | 202         |\n",
            "|    time_elapsed         | 2582        |\n",
            "|    total_timesteps      | 3309568     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005150188 |\n",
            "|    clip_fraction        | 0.0442      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.578      |\n",
            "|    explained_variance   | 0.991       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.04        |\n",
            "|    n_updates            | 804         |\n",
            "|    policy_gradient_loss | 0.00131     |\n",
            "|    value_loss           | 19.6        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 3312000\n",
            "Best mean reward: 282.80 - Last mean reward per episode: 277.75\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 235          |\n",
            "|    ep_rew_mean          | 279          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1283         |\n",
            "|    iterations           | 203          |\n",
            "|    time_elapsed         | 2592         |\n",
            "|    total_timesteps      | 3325952      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0048857396 |\n",
            "|    clip_fraction        | 0.0423       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.606       |\n",
            "|    explained_variance   | 0.984        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.2          |\n",
            "|    n_updates            | 808          |\n",
            "|    policy_gradient_loss | 0.000606     |\n",
            "|    value_loss           | 16.8         |\n",
            "------------------------------------------\n",
            "Num timesteps: 3328000\n",
            "Best mean reward: 282.80 - Last mean reward per episode: 278.99\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 224          |\n",
            "|    ep_rew_mean          | 274          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1284         |\n",
            "|    iterations           | 204          |\n",
            "|    time_elapsed         | 2601         |\n",
            "|    total_timesteps      | 3342336      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0046838745 |\n",
            "|    clip_fraction        | 0.0554       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.589       |\n",
            "|    explained_variance   | 0.997        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.03         |\n",
            "|    n_updates            | 812          |\n",
            "|    policy_gradient_loss | -0.000185    |\n",
            "|    value_loss           | 5.2          |\n",
            "------------------------------------------\n",
            "Num timesteps: 3344000\n",
            "Best mean reward: 282.80 - Last mean reward per episode: 273.49\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 224          |\n",
            "|    ep_rew_mean          | 278          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1286         |\n",
            "|    iterations           | 205          |\n",
            "|    time_elapsed         | 2611         |\n",
            "|    total_timesteps      | 3358720      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0024907654 |\n",
            "|    clip_fraction        | 0.027        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.595       |\n",
            "|    explained_variance   | 0.968        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 132          |\n",
            "|    n_updates            | 816          |\n",
            "|    policy_gradient_loss | 0.00103      |\n",
            "|    value_loss           | 71           |\n",
            "------------------------------------------\n",
            "Num timesteps: 3360000\n",
            "Best mean reward: 282.80 - Last mean reward per episode: 280.58\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 234         |\n",
            "|    ep_rew_mean          | 280         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1287        |\n",
            "|    iterations           | 206         |\n",
            "|    time_elapsed         | 2621        |\n",
            "|    total_timesteps      | 3375104     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003979155 |\n",
            "|    clip_fraction        | 0.0514      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.597      |\n",
            "|    explained_variance   | 0.997       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.931       |\n",
            "|    n_updates            | 820         |\n",
            "|    policy_gradient_loss | 0.000778    |\n",
            "|    value_loss           | 3.77        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 3376000\n",
            "Best mean reward: 282.80 - Last mean reward per episode: 279.77\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 234          |\n",
            "|    ep_rew_mean          | 272          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1289         |\n",
            "|    iterations           | 207          |\n",
            "|    time_elapsed         | 2630         |\n",
            "|    total_timesteps      | 3391488      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0031133345 |\n",
            "|    clip_fraction        | 0.0378       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.572       |\n",
            "|    explained_variance   | 0.994        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.8          |\n",
            "|    n_updates            | 824          |\n",
            "|    policy_gradient_loss | 0.000561     |\n",
            "|    value_loss           | 6.92         |\n",
            "------------------------------------------\n",
            "Num timesteps: 3392000\n",
            "Best mean reward: 282.80 - Last mean reward per episode: 272.21\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 223          |\n",
            "|    ep_rew_mean          | 274          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1290         |\n",
            "|    iterations           | 208          |\n",
            "|    time_elapsed         | 2640         |\n",
            "|    total_timesteps      | 3407872      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0029660054 |\n",
            "|    clip_fraction        | 0.0204       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.586       |\n",
            "|    explained_variance   | 0.956        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 55.8         |\n",
            "|    n_updates            | 828          |\n",
            "|    policy_gradient_loss | -0.000603    |\n",
            "|    value_loss           | 84.1         |\n",
            "------------------------------------------\n",
            "Num timesteps: 3408000\n",
            "Best mean reward: 282.80 - Last mean reward per episode: 273.95\n",
            "Num timesteps: 3424000\n",
            "Best mean reward: 282.80 - Last mean reward per episode: 276.47\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 225         |\n",
            "|    ep_rew_mean          | 279         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1292        |\n",
            "|    iterations           | 209         |\n",
            "|    time_elapsed         | 2649        |\n",
            "|    total_timesteps      | 3424256     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004511335 |\n",
            "|    clip_fraction        | 0.0399      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.594      |\n",
            "|    explained_variance   | 0.962       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 10.7        |\n",
            "|    n_updates            | 832         |\n",
            "|    policy_gradient_loss | -0.000311   |\n",
            "|    value_loss           | 106         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 3440000\n",
            "Best mean reward: 282.80 - Last mean reward per episode: 283.56\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 230         |\n",
            "|    ep_rew_mean          | 284         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1293        |\n",
            "|    iterations           | 210         |\n",
            "|    time_elapsed         | 2658        |\n",
            "|    total_timesteps      | 3440640     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004185003 |\n",
            "|    clip_fraction        | 0.06        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.591      |\n",
            "|    explained_variance   | 0.997       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.48        |\n",
            "|    n_updates            | 836         |\n",
            "|    policy_gradient_loss | 0.00139     |\n",
            "|    value_loss           | 3.26        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 3456000\n",
            "Best mean reward: 283.56 - Last mean reward per episode: 278.84\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 228          |\n",
            "|    ep_rew_mean          | 279          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1295         |\n",
            "|    iterations           | 211          |\n",
            "|    time_elapsed         | 2668         |\n",
            "|    total_timesteps      | 3457024      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0045020804 |\n",
            "|    clip_fraction        | 0.0527       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.575       |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.928        |\n",
            "|    n_updates            | 840          |\n",
            "|    policy_gradient_loss | 0.000347     |\n",
            "|    value_loss           | 4.04         |\n",
            "------------------------------------------\n",
            "Num timesteps: 3472000\n",
            "Best mean reward: 283.56 - Last mean reward per episode: 277.25\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 231          |\n",
            "|    ep_rew_mean          | 277          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1296         |\n",
            "|    iterations           | 212          |\n",
            "|    time_elapsed         | 2678         |\n",
            "|    total_timesteps      | 3473408      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034072176 |\n",
            "|    clip_fraction        | 0.0341       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.578       |\n",
            "|    explained_variance   | 0.958        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.31         |\n",
            "|    n_updates            | 844          |\n",
            "|    policy_gradient_loss | -0.000139    |\n",
            "|    value_loss           | 119          |\n",
            "------------------------------------------\n",
            "Num timesteps: 3488000\n",
            "Best mean reward: 283.56 - Last mean reward per episode: 271.22\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 231          |\n",
            "|    ep_rew_mean          | 271          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1298         |\n",
            "|    iterations           | 213          |\n",
            "|    time_elapsed         | 2688         |\n",
            "|    total_timesteps      | 3489792      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035651168 |\n",
            "|    clip_fraction        | 0.0324       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.575       |\n",
            "|    explained_variance   | 0.966        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 23.2         |\n",
            "|    n_updates            | 848          |\n",
            "|    policy_gradient_loss | 0.000304     |\n",
            "|    value_loss           | 101          |\n",
            "------------------------------------------\n",
            "Num timesteps: 3504000\n",
            "Best mean reward: 283.56 - Last mean reward per episode: 275.06\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 232          |\n",
            "|    ep_rew_mean          | 273          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1299         |\n",
            "|    iterations           | 214          |\n",
            "|    time_elapsed         | 2697         |\n",
            "|    total_timesteps      | 3506176      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0056946464 |\n",
            "|    clip_fraction        | 0.0541       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.583       |\n",
            "|    explained_variance   | 0.959        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.16         |\n",
            "|    n_updates            | 852          |\n",
            "|    policy_gradient_loss | -0.000781    |\n",
            "|    value_loss           | 98.9         |\n",
            "------------------------------------------\n",
            "Num timesteps: 3520000\n",
            "Best mean reward: 283.56 - Last mean reward per episode: 276.88\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 239          |\n",
            "|    ep_rew_mean          | 276          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1301         |\n",
            "|    iterations           | 215          |\n",
            "|    time_elapsed         | 2707         |\n",
            "|    total_timesteps      | 3522560      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038334501 |\n",
            "|    clip_fraction        | 0.0388       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.59        |\n",
            "|    explained_variance   | 0.939        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 15.1         |\n",
            "|    n_updates            | 856          |\n",
            "|    policy_gradient_loss | 0.000276     |\n",
            "|    value_loss           | 199          |\n",
            "------------------------------------------\n",
            "Num timesteps: 3536000\n",
            "Best mean reward: 283.56 - Last mean reward per episode: 272.85\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 239         |\n",
            "|    ep_rew_mean          | 274         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1302        |\n",
            "|    iterations           | 216         |\n",
            "|    time_elapsed         | 2717        |\n",
            "|    total_timesteps      | 3538944     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004324174 |\n",
            "|    clip_fraction        | 0.0439      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.574      |\n",
            "|    explained_variance   | 0.968       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.75        |\n",
            "|    n_updates            | 860         |\n",
            "|    policy_gradient_loss | 0.000694    |\n",
            "|    value_loss           | 80.3        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 3552000\n",
            "Best mean reward: 283.56 - Last mean reward per episode: 282.05\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 229          |\n",
            "|    ep_rew_mean          | 282          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1303         |\n",
            "|    iterations           | 217          |\n",
            "|    time_elapsed         | 2727         |\n",
            "|    total_timesteps      | 3555328      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0045251613 |\n",
            "|    clip_fraction        | 0.0652       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.593       |\n",
            "|    explained_variance   | 0.983        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.98         |\n",
            "|    n_updates            | 864          |\n",
            "|    policy_gradient_loss | 0.00118      |\n",
            "|    value_loss           | 35.2         |\n",
            "------------------------------------------\n",
            "Num timesteps: 3568000\n",
            "Best mean reward: 283.56 - Last mean reward per episode: 272.32\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 237         |\n",
            "|    ep_rew_mean          | 273         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1304        |\n",
            "|    iterations           | 218         |\n",
            "|    time_elapsed         | 2737        |\n",
            "|    total_timesteps      | 3571712     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004443824 |\n",
            "|    clip_fraction        | 0.0622      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.593      |\n",
            "|    explained_variance   | 0.997       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.58        |\n",
            "|    n_updates            | 868         |\n",
            "|    policy_gradient_loss | 0.000962    |\n",
            "|    value_loss           | 5.96        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 3584000\n",
            "Best mean reward: 283.56 - Last mean reward per episode: 271.33\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 232          |\n",
            "|    ep_rew_mean          | 270          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1306         |\n",
            "|    iterations           | 219          |\n",
            "|    time_elapsed         | 2746         |\n",
            "|    total_timesteps      | 3588096      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0066656326 |\n",
            "|    clip_fraction        | 0.0433       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.596       |\n",
            "|    explained_variance   | 0.964        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.21         |\n",
            "|    n_updates            | 872          |\n",
            "|    policy_gradient_loss | 0.00105      |\n",
            "|    value_loss           | 92.1         |\n",
            "------------------------------------------\n",
            "Num timesteps: 3600000\n",
            "Best mean reward: 283.56 - Last mean reward per episode: 274.64\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 235        |\n",
            "|    ep_rew_mean          | 278        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 1307       |\n",
            "|    iterations           | 220        |\n",
            "|    time_elapsed         | 2756       |\n",
            "|    total_timesteps      | 3604480    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00353853 |\n",
            "|    clip_fraction        | 0.0399     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.589     |\n",
            "|    explained_variance   | 0.952      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 5.93       |\n",
            "|    n_updates            | 876        |\n",
            "|    policy_gradient_loss | -0.00023   |\n",
            "|    value_loss           | 128        |\n",
            "----------------------------------------\n",
            "Num timesteps: 3616000\n",
            "Best mean reward: 283.56 - Last mean reward per episode: 278.46\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 234          |\n",
            "|    ep_rew_mean          | 278          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1308         |\n",
            "|    iterations           | 221          |\n",
            "|    time_elapsed         | 2766         |\n",
            "|    total_timesteps      | 3620864      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0043210182 |\n",
            "|    clip_fraction        | 0.0477       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.571       |\n",
            "|    explained_variance   | 0.994        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.85         |\n",
            "|    n_updates            | 880          |\n",
            "|    policy_gradient_loss | 0.00105      |\n",
            "|    value_loss           | 8.78         |\n",
            "------------------------------------------\n",
            "Num timesteps: 3632000\n",
            "Best mean reward: 283.56 - Last mean reward per episode: 277.41\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 238          |\n",
            "|    ep_rew_mean          | 275          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1310         |\n",
            "|    iterations           | 222          |\n",
            "|    time_elapsed         | 2776         |\n",
            "|    total_timesteps      | 3637248      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0036833554 |\n",
            "|    clip_fraction        | 0.0356       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.575       |\n",
            "|    explained_variance   | 0.967        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.14         |\n",
            "|    n_updates            | 884          |\n",
            "|    policy_gradient_loss | -0.000166    |\n",
            "|    value_loss           | 88.9         |\n",
            "------------------------------------------\n",
            "Num timesteps: 3648000\n",
            "Best mean reward: 283.56 - Last mean reward per episode: 276.90\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 229          |\n",
            "|    ep_rew_mean          | 276          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1311         |\n",
            "|    iterations           | 223          |\n",
            "|    time_elapsed         | 2785         |\n",
            "|    total_timesteps      | 3653632      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0033618058 |\n",
            "|    clip_fraction        | 0.0386       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.56        |\n",
            "|    explained_variance   | 0.959        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.94         |\n",
            "|    n_updates            | 888          |\n",
            "|    policy_gradient_loss | -0.000147    |\n",
            "|    value_loss           | 139          |\n",
            "------------------------------------------\n",
            "Num timesteps: 3664000\n",
            "Best mean reward: 283.56 - Last mean reward per episode: 280.27\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 222         |\n",
            "|    ep_rew_mean          | 282         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1312        |\n",
            "|    iterations           | 224         |\n",
            "|    time_elapsed         | 2795        |\n",
            "|    total_timesteps      | 3670016     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004462474 |\n",
            "|    clip_fraction        | 0.0465      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.586      |\n",
            "|    explained_variance   | 0.969       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 262         |\n",
            "|    n_updates            | 892         |\n",
            "|    policy_gradient_loss | 0.000333    |\n",
            "|    value_loss           | 87.1        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 3680000\n",
            "Best mean reward: 283.56 - Last mean reward per episode: 282.95\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 226          |\n",
            "|    ep_rew_mean          | 282          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1313         |\n",
            "|    iterations           | 225          |\n",
            "|    time_elapsed         | 2805         |\n",
            "|    total_timesteps      | 3686400      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0042236676 |\n",
            "|    clip_fraction        | 0.055        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.583       |\n",
            "|    explained_variance   | 0.997        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.22         |\n",
            "|    n_updates            | 896          |\n",
            "|    policy_gradient_loss | 0.000611     |\n",
            "|    value_loss           | 4.62         |\n",
            "------------------------------------------\n",
            "Num timesteps: 3696000\n",
            "Best mean reward: 283.56 - Last mean reward per episode: 275.73\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 255         |\n",
            "|    ep_rew_mean          | 274         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1315        |\n",
            "|    iterations           | 226         |\n",
            "|    time_elapsed         | 2815        |\n",
            "|    total_timesteps      | 3702784     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004429392 |\n",
            "|    clip_fraction        | 0.0458      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.594      |\n",
            "|    explained_variance   | 0.99        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.61        |\n",
            "|    n_updates            | 900         |\n",
            "|    policy_gradient_loss | 0.00109     |\n",
            "|    value_loss           | 13.5        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 3712000\n",
            "Best mean reward: 283.56 - Last mean reward per episode: 276.92\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 249         |\n",
            "|    ep_rew_mean          | 278         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1316        |\n",
            "|    iterations           | 227         |\n",
            "|    time_elapsed         | 2825        |\n",
            "|    total_timesteps      | 3719168     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003998722 |\n",
            "|    clip_fraction        | 0.051       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.586      |\n",
            "|    explained_variance   | 0.998       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.85        |\n",
            "|    n_updates            | 904         |\n",
            "|    policy_gradient_loss | 0.000286    |\n",
            "|    value_loss           | 5.43        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 3728000\n",
            "Best mean reward: 283.56 - Last mean reward per episode: 277.15\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 252          |\n",
            "|    ep_rew_mean          | 277          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1317         |\n",
            "|    iterations           | 228          |\n",
            "|    time_elapsed         | 2835         |\n",
            "|    total_timesteps      | 3735552      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0031248978 |\n",
            "|    clip_fraction        | 0.0403       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.594       |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.11         |\n",
            "|    n_updates            | 908          |\n",
            "|    policy_gradient_loss | -0.000299    |\n",
            "|    value_loss           | 4.72         |\n",
            "------------------------------------------\n",
            "Num timesteps: 3744000\n",
            "Best mean reward: 283.56 - Last mean reward per episode: 280.64\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 228          |\n",
            "|    ep_rew_mean          | 280          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1318         |\n",
            "|    iterations           | 229          |\n",
            "|    time_elapsed         | 2845         |\n",
            "|    total_timesteps      | 3751936      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0037912806 |\n",
            "|    clip_fraction        | 0.0489       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.594       |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.778        |\n",
            "|    n_updates            | 912          |\n",
            "|    policy_gradient_loss | -0.00029     |\n",
            "|    value_loss           | 3.77         |\n",
            "------------------------------------------\n",
            "Num timesteps: 3760000\n",
            "Best mean reward: 283.56 - Last mean reward per episode: 281.96\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 231         |\n",
            "|    ep_rew_mean          | 281         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1319        |\n",
            "|    iterations           | 230         |\n",
            "|    time_elapsed         | 2855        |\n",
            "|    total_timesteps      | 3768320     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004272645 |\n",
            "|    clip_fraction        | 0.0384      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.593      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.76        |\n",
            "|    n_updates            | 916         |\n",
            "|    policy_gradient_loss | 0.000412    |\n",
            "|    value_loss           | 2.69        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 3776000\n",
            "Best mean reward: 283.56 - Last mean reward per episode: 277.26\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 236         |\n",
            "|    ep_rew_mean          | 276         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1321        |\n",
            "|    iterations           | 231         |\n",
            "|    time_elapsed         | 2864        |\n",
            "|    total_timesteps      | 3784704     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002779799 |\n",
            "|    clip_fraction        | 0.0315      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.563      |\n",
            "|    explained_variance   | 0.984       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 32.3        |\n",
            "|    n_updates            | 920         |\n",
            "|    policy_gradient_loss | 0.00057     |\n",
            "|    value_loss           | 44          |\n",
            "-----------------------------------------\n",
            "Num timesteps: 3792000\n",
            "Best mean reward: 283.56 - Last mean reward per episode: 273.41\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 229          |\n",
            "|    ep_rew_mean          | 279          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1322         |\n",
            "|    iterations           | 232          |\n",
            "|    time_elapsed         | 2874         |\n",
            "|    total_timesteps      | 3801088      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0049738786 |\n",
            "|    clip_fraction        | 0.0446       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.591       |\n",
            "|    explained_variance   | 0.977        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 194          |\n",
            "|    n_updates            | 924          |\n",
            "|    policy_gradient_loss | -0.000548    |\n",
            "|    value_loss           | 73.8         |\n",
            "------------------------------------------\n",
            "Num timesteps: 3808000\n",
            "Best mean reward: 283.56 - Last mean reward per episode: 282.53\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 228          |\n",
            "|    ep_rew_mean          | 284          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1323         |\n",
            "|    iterations           | 233          |\n",
            "|    time_elapsed         | 2883         |\n",
            "|    total_timesteps      | 3817472      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0039320723 |\n",
            "|    clip_fraction        | 0.0446       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.563       |\n",
            "|    explained_variance   | 0.994        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.02         |\n",
            "|    n_updates            | 928          |\n",
            "|    policy_gradient_loss | 0.000591     |\n",
            "|    value_loss           | 22.2         |\n",
            "------------------------------------------\n",
            "Num timesteps: 3824000\n",
            "Best mean reward: 283.56 - Last mean reward per episode: 280.88\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 219          |\n",
            "|    ep_rew_mean          | 282          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1324         |\n",
            "|    iterations           | 234          |\n",
            "|    time_elapsed         | 2893         |\n",
            "|    total_timesteps      | 3833856      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0046545817 |\n",
            "|    clip_fraction        | 0.0541       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.587       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.622        |\n",
            "|    n_updates            | 932          |\n",
            "|    policy_gradient_loss | -0.000147    |\n",
            "|    value_loss           | 2.22         |\n",
            "------------------------------------------\n",
            "Num timesteps: 3840000\n",
            "Best mean reward: 283.56 - Last mean reward per episode: 280.95\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 220          |\n",
            "|    ep_rew_mean          | 283          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1326         |\n",
            "|    iterations           | 235          |\n",
            "|    time_elapsed         | 2902         |\n",
            "|    total_timesteps      | 3850240      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0033682203 |\n",
            "|    clip_fraction        | 0.04         |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.583       |\n",
            "|    explained_variance   | 0.975        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.98         |\n",
            "|    n_updates            | 936          |\n",
            "|    policy_gradient_loss | 0.000117     |\n",
            "|    value_loss           | 57.5         |\n",
            "------------------------------------------\n",
            "Num timesteps: 3856000\n",
            "Best mean reward: 283.56 - Last mean reward per episode: 281.87\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 217          |\n",
            "|    ep_rew_mean          | 282          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1327         |\n",
            "|    iterations           | 236          |\n",
            "|    time_elapsed         | 2912         |\n",
            "|    total_timesteps      | 3866624      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0042156344 |\n",
            "|    clip_fraction        | 0.042        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.565       |\n",
            "|    explained_variance   | 0.972        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 90.2         |\n",
            "|    n_updates            | 940          |\n",
            "|    policy_gradient_loss | 0.000715     |\n",
            "|    value_loss           | 58.4         |\n",
            "------------------------------------------\n",
            "Num timesteps: 3872000\n",
            "Best mean reward: 283.56 - Last mean reward per episode: 275.67\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 228          |\n",
            "|    ep_rew_mean          | 278          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1328         |\n",
            "|    iterations           | 237          |\n",
            "|    time_elapsed         | 2922         |\n",
            "|    total_timesteps      | 3883008      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0028674086 |\n",
            "|    clip_fraction        | 0.0371       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.573       |\n",
            "|    explained_variance   | 0.991        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.45         |\n",
            "|    n_updates            | 944          |\n",
            "|    policy_gradient_loss | 0.000613     |\n",
            "|    value_loss           | 26.9         |\n",
            "------------------------------------------\n",
            "Num timesteps: 3888000\n",
            "Best mean reward: 283.56 - Last mean reward per episode: 277.54\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 222          |\n",
            "|    ep_rew_mean          | 281          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1329         |\n",
            "|    iterations           | 238          |\n",
            "|    time_elapsed         | 2932         |\n",
            "|    total_timesteps      | 3899392      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044583417 |\n",
            "|    clip_fraction        | 0.0435       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.566       |\n",
            "|    explained_variance   | 0.978        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.906        |\n",
            "|    n_updates            | 948          |\n",
            "|    policy_gradient_loss | 0.000413     |\n",
            "|    value_loss           | 68           |\n",
            "------------------------------------------\n",
            "Num timesteps: 3904000\n",
            "Best mean reward: 283.56 - Last mean reward per episode: 278.90\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 228          |\n",
            "|    ep_rew_mean          | 279          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1331         |\n",
            "|    iterations           | 239          |\n",
            "|    time_elapsed         | 2941         |\n",
            "|    total_timesteps      | 3915776      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040221894 |\n",
            "|    clip_fraction        | 0.0456       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.558       |\n",
            "|    explained_variance   | 0.98         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.76         |\n",
            "|    n_updates            | 952          |\n",
            "|    policy_gradient_loss | -5.19e-05    |\n",
            "|    value_loss           | 58.3         |\n",
            "------------------------------------------\n",
            "Num timesteps: 3920000\n",
            "Best mean reward: 283.56 - Last mean reward per episode: 282.34\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 219          |\n",
            "|    ep_rew_mean          | 285          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1332         |\n",
            "|    iterations           | 240          |\n",
            "|    time_elapsed         | 2951         |\n",
            "|    total_timesteps      | 3932160      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0048749964 |\n",
            "|    clip_fraction        | 0.054        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.556       |\n",
            "|    explained_variance   | 0.996        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.04         |\n",
            "|    n_updates            | 956          |\n",
            "|    policy_gradient_loss | 0.000778     |\n",
            "|    value_loss           | 8.81         |\n",
            "------------------------------------------\n",
            "Num timesteps: 3936000\n",
            "Best mean reward: 283.56 - Last mean reward per episode: 286.30\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 219         |\n",
            "|    ep_rew_mean          | 284         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1333        |\n",
            "|    iterations           | 241         |\n",
            "|    time_elapsed         | 2960        |\n",
            "|    total_timesteps      | 3948544     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004077983 |\n",
            "|    clip_fraction        | 0.0592      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.575      |\n",
            "|    explained_variance   | 0.997       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.34        |\n",
            "|    n_updates            | 960         |\n",
            "|    policy_gradient_loss | -0.00013    |\n",
            "|    value_loss           | 5.23        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 3952000\n",
            "Best mean reward: 286.30 - Last mean reward per episode: 284.33\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 226          |\n",
            "|    ep_rew_mean          | 282          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1334         |\n",
            "|    iterations           | 242          |\n",
            "|    time_elapsed         | 2971         |\n",
            "|    total_timesteps      | 3964928      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038347165 |\n",
            "|    clip_fraction        | 0.0532       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.551       |\n",
            "|    explained_variance   | 0.991        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.11         |\n",
            "|    n_updates            | 964          |\n",
            "|    policy_gradient_loss | 0.000221     |\n",
            "|    value_loss           | 23.2         |\n",
            "------------------------------------------\n",
            "Num timesteps: 3968000\n",
            "Best mean reward: 286.30 - Last mean reward per episode: 279.88\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 231          |\n",
            "|    ep_rew_mean          | 274          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1335         |\n",
            "|    iterations           | 243          |\n",
            "|    time_elapsed         | 2981         |\n",
            "|    total_timesteps      | 3981312      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0029182825 |\n",
            "|    clip_fraction        | 0.0272       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.538       |\n",
            "|    explained_variance   | 0.981        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.5          |\n",
            "|    n_updates            | 968          |\n",
            "|    policy_gradient_loss | 0.000399     |\n",
            "|    value_loss           | 62.2         |\n",
            "------------------------------------------\n",
            "Num timesteps: 3984000\n",
            "Best mean reward: 286.30 - Last mean reward per episode: 273.15\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 228          |\n",
            "|    ep_rew_mean          | 281          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1336         |\n",
            "|    iterations           | 244          |\n",
            "|    time_elapsed         | 2991         |\n",
            "|    total_timesteps      | 3997696      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0046269363 |\n",
            "|    clip_fraction        | 0.0434       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.579       |\n",
            "|    explained_variance   | 0.962        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 85.7         |\n",
            "|    n_updates            | 972          |\n",
            "|    policy_gradient_loss | 0.00044      |\n",
            "|    value_loss           | 125          |\n",
            "------------------------------------------\n",
            "Num timesteps: 4000000\n",
            "Best mean reward: 286.30 - Last mean reward per episode: 281.67\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 220          |\n",
            "|    ep_rew_mean          | 281          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1337         |\n",
            "|    iterations           | 245          |\n",
            "|    time_elapsed         | 3002         |\n",
            "|    total_timesteps      | 4014080      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0045292415 |\n",
            "|    clip_fraction        | 0.0548       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.541       |\n",
            "|    explained_variance   | 0.997        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.01         |\n",
            "|    n_updates            | 976          |\n",
            "|    policy_gradient_loss | 0.00048      |\n",
            "|    value_loss           | 4.43         |\n",
            "------------------------------------------\n",
            "Num timesteps: 4016000\n",
            "Best mean reward: 286.30 - Last mean reward per episode: 280.97\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 221          |\n",
            "|    ep_rew_mean          | 280          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1338         |\n",
            "|    iterations           | 246          |\n",
            "|    time_elapsed         | 3011         |\n",
            "|    total_timesteps      | 4030464      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040483098 |\n",
            "|    clip_fraction        | 0.0546       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.55        |\n",
            "|    explained_variance   | 0.974        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 15           |\n",
            "|    n_updates            | 980          |\n",
            "|    policy_gradient_loss | -0.0016      |\n",
            "|    value_loss           | 76.6         |\n",
            "------------------------------------------\n",
            "Num timesteps: 4032000\n",
            "Best mean reward: 286.30 - Last mean reward per episode: 279.50\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 220         |\n",
            "|    ep_rew_mean          | 282         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1339        |\n",
            "|    iterations           | 247         |\n",
            "|    time_elapsed         | 3021        |\n",
            "|    total_timesteps      | 4046848     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004486272 |\n",
            "|    clip_fraction        | 0.0519      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.546      |\n",
            "|    explained_variance   | 0.996       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.4         |\n",
            "|    n_updates            | 984         |\n",
            "|    policy_gradient_loss | -7.49e-05   |\n",
            "|    value_loss           | 6.73        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 4048000\n",
            "Best mean reward: 286.30 - Last mean reward per episode: 281.52\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 221         |\n",
            "|    ep_rew_mean          | 283         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1340        |\n",
            "|    iterations           | 248         |\n",
            "|    time_elapsed         | 3031        |\n",
            "|    total_timesteps      | 4063232     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004853585 |\n",
            "|    clip_fraction        | 0.0552      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.55       |\n",
            "|    explained_variance   | 0.997       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.14        |\n",
            "|    n_updates            | 988         |\n",
            "|    policy_gradient_loss | 0.000452    |\n",
            "|    value_loss           | 2.39        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 4064000\n",
            "Best mean reward: 286.30 - Last mean reward per episode: 283.11\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 221          |\n",
            "|    ep_rew_mean          | 282          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1341         |\n",
            "|    iterations           | 249          |\n",
            "|    time_elapsed         | 3040         |\n",
            "|    total_timesteps      | 4079616      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0039514974 |\n",
            "|    clip_fraction        | 0.0396       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.563       |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.29         |\n",
            "|    n_updates            | 992          |\n",
            "|    policy_gradient_loss | 0.00117      |\n",
            "|    value_loss           | 2.8          |\n",
            "------------------------------------------\n",
            "Num timesteps: 4080000\n",
            "Best mean reward: 286.30 - Last mean reward per episode: 282.38\n",
            "Num timesteps: 4096000\n",
            "Best mean reward: 286.30 - Last mean reward per episode: 281.77\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 219          |\n",
            "|    ep_rew_mean          | 282          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1342         |\n",
            "|    iterations           | 250          |\n",
            "|    time_elapsed         | 3051         |\n",
            "|    total_timesteps      | 4096000      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0037523564 |\n",
            "|    clip_fraction        | 0.0495       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.545       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.906        |\n",
            "|    n_updates            | 996          |\n",
            "|    policy_gradient_loss | 0.000692     |\n",
            "|    value_loss           | 1.63         |\n",
            "------------------------------------------\n",
            "Num timesteps: 4112000\n",
            "Best mean reward: 286.30 - Last mean reward per episode: 281.66\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 220          |\n",
            "|    ep_rew_mean          | 282          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1343         |\n",
            "|    iterations           | 251          |\n",
            "|    time_elapsed         | 3060         |\n",
            "|    total_timesteps      | 4112384      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0039269724 |\n",
            "|    clip_fraction        | 0.0385       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.539       |\n",
            "|    explained_variance   | 0.975        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 11.8         |\n",
            "|    n_updates            | 1000         |\n",
            "|    policy_gradient_loss | 0.000309     |\n",
            "|    value_loss           | 79.7         |\n",
            "------------------------------------------\n",
            "Num timesteps: 4128000\n",
            "Best mean reward: 286.30 - Last mean reward per episode: 284.58\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 220         |\n",
            "|    ep_rew_mean          | 285         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1344        |\n",
            "|    iterations           | 252         |\n",
            "|    time_elapsed         | 3069        |\n",
            "|    total_timesteps      | 4128768     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003964645 |\n",
            "|    clip_fraction        | 0.0518      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.564      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.776       |\n",
            "|    n_updates            | 1004        |\n",
            "|    policy_gradient_loss | 0.00128     |\n",
            "|    value_loss           | 1.59        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 4144000\n",
            "Best mean reward: 286.30 - Last mean reward per episode: 281.30\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 219          |\n",
            "|    ep_rew_mean          | 282          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1345         |\n",
            "|    iterations           | 253          |\n",
            "|    time_elapsed         | 3079         |\n",
            "|    total_timesteps      | 4145152      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040779635 |\n",
            "|    clip_fraction        | 0.0515       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.557       |\n",
            "|    explained_variance   | 1            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.861        |\n",
            "|    n_updates            | 1008         |\n",
            "|    policy_gradient_loss | -9.68e-05    |\n",
            "|    value_loss           | 1.48         |\n",
            "------------------------------------------\n",
            "Num timesteps: 4160000\n",
            "Best mean reward: 286.30 - Last mean reward per episode: 278.31\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 217          |\n",
            "|    ep_rew_mean          | 280          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1347         |\n",
            "|    iterations           | 254          |\n",
            "|    time_elapsed         | 3089         |\n",
            "|    total_timesteps      | 4161536      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0037115456 |\n",
            "|    clip_fraction        | 0.0439       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.551       |\n",
            "|    explained_variance   | 1            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.94         |\n",
            "|    n_updates            | 1012         |\n",
            "|    policy_gradient_loss | -0.000331    |\n",
            "|    value_loss           | 1.3          |\n",
            "------------------------------------------\n",
            "Num timesteps: 4176000\n",
            "Best mean reward: 286.30 - Last mean reward per episode: 281.51\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 216          |\n",
            "|    ep_rew_mean          | 281          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1348         |\n",
            "|    iterations           | 255          |\n",
            "|    time_elapsed         | 3098         |\n",
            "|    total_timesteps      | 4177920      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0026444045 |\n",
            "|    clip_fraction        | 0.0209       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.556       |\n",
            "|    explained_variance   | 0.959        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.31         |\n",
            "|    n_updates            | 1016         |\n",
            "|    policy_gradient_loss | 0.000532     |\n",
            "|    value_loss           | 100          |\n",
            "------------------------------------------\n",
            "Num timesteps: 4192000\n",
            "Best mean reward: 286.30 - Last mean reward per episode: 282.44\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 218          |\n",
            "|    ep_rew_mean          | 283          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1349         |\n",
            "|    iterations           | 256          |\n",
            "|    time_elapsed         | 3108         |\n",
            "|    total_timesteps      | 4194304      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041976646 |\n",
            "|    clip_fraction        | 0.0475       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.531       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.694        |\n",
            "|    n_updates            | 1020         |\n",
            "|    policy_gradient_loss | 1.24e-06     |\n",
            "|    value_loss           | 2.45         |\n",
            "------------------------------------------\n",
            "Num timesteps: 4208000\n",
            "Best mean reward: 286.30 - Last mean reward per episode: 280.72\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 216         |\n",
            "|    ep_rew_mean          | 281         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1350        |\n",
            "|    iterations           | 257         |\n",
            "|    time_elapsed         | 3117        |\n",
            "|    total_timesteps      | 4210688     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004447087 |\n",
            "|    clip_fraction        | 0.055       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.55       |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.67        |\n",
            "|    n_updates            | 1024        |\n",
            "|    policy_gradient_loss | 1.83e-06    |\n",
            "|    value_loss           | 1.47        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 4224000\n",
            "Best mean reward: 286.30 - Last mean reward per episode: 282.42\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 216          |\n",
            "|    ep_rew_mean          | 283          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1351         |\n",
            "|    iterations           | 258          |\n",
            "|    time_elapsed         | 3127         |\n",
            "|    total_timesteps      | 4227072      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038415745 |\n",
            "|    clip_fraction        | 0.0403       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.549       |\n",
            "|    explained_variance   | 0.98         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.18         |\n",
            "|    n_updates            | 1028         |\n",
            "|    policy_gradient_loss | -0.000138    |\n",
            "|    value_loss           | 41.9         |\n",
            "------------------------------------------\n",
            "Num timesteps: 4240000\n",
            "Best mean reward: 286.30 - Last mean reward per episode: 280.59\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 214          |\n",
            "|    ep_rew_mean          | 281          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1352         |\n",
            "|    iterations           | 259          |\n",
            "|    time_elapsed         | 3136         |\n",
            "|    total_timesteps      | 4243456      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0047411104 |\n",
            "|    clip_fraction        | 0.0452       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.537       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.791        |\n",
            "|    n_updates            | 1032         |\n",
            "|    policy_gradient_loss | 0.00161      |\n",
            "|    value_loss           | 1.64         |\n",
            "------------------------------------------\n",
            "Num timesteps: 4256000\n",
            "Best mean reward: 286.30 - Last mean reward per episode: 278.22\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 219          |\n",
            "|    ep_rew_mean          | 281          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1353         |\n",
            "|    iterations           | 260          |\n",
            "|    time_elapsed         | 3146         |\n",
            "|    total_timesteps      | 4259840      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0043587023 |\n",
            "|    clip_fraction        | 0.0422       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.555       |\n",
            "|    explained_variance   | 0.979        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.15         |\n",
            "|    n_updates            | 1036         |\n",
            "|    policy_gradient_loss | 0.000727     |\n",
            "|    value_loss           | 26.5         |\n",
            "------------------------------------------\n",
            "Num timesteps: 4272000\n",
            "Best mean reward: 286.30 - Last mean reward per episode: 279.30\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 224          |\n",
            "|    ep_rew_mean          | 279          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1354         |\n",
            "|    iterations           | 261          |\n",
            "|    time_elapsed         | 3157         |\n",
            "|    total_timesteps      | 4276224      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0031514782 |\n",
            "|    clip_fraction        | 0.0249       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.543       |\n",
            "|    explained_variance   | 0.975        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.964        |\n",
            "|    n_updates            | 1040         |\n",
            "|    policy_gradient_loss | 0.000227     |\n",
            "|    value_loss           | 58.2         |\n",
            "------------------------------------------\n",
            "Num timesteps: 4288000\n",
            "Best mean reward: 286.30 - Last mean reward per episode: 278.20\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 226          |\n",
            "|    ep_rew_mean          | 279          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1355         |\n",
            "|    iterations           | 262          |\n",
            "|    time_elapsed         | 3167         |\n",
            "|    total_timesteps      | 4292608      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0036789845 |\n",
            "|    clip_fraction        | 0.0509       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.548       |\n",
            "|    explained_variance   | 0.962        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 16.6         |\n",
            "|    n_updates            | 1044         |\n",
            "|    policy_gradient_loss | -0.000282    |\n",
            "|    value_loss           | 115          |\n",
            "------------------------------------------\n",
            "Num timesteps: 4304000\n",
            "Best mean reward: 286.30 - Last mean reward per episode: 277.71\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 227         |\n",
            "|    ep_rew_mean          | 278         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1356        |\n",
            "|    iterations           | 263         |\n",
            "|    time_elapsed         | 3176        |\n",
            "|    total_timesteps      | 4308992     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005129887 |\n",
            "|    clip_fraction        | 0.0619      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.527      |\n",
            "|    explained_variance   | 0.978       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.37        |\n",
            "|    n_updates            | 1048        |\n",
            "|    policy_gradient_loss | -0.000182   |\n",
            "|    value_loss           | 34.2        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 4320000\n",
            "Best mean reward: 286.30 - Last mean reward per episode: 282.06\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 223         |\n",
            "|    ep_rew_mean          | 283         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1357        |\n",
            "|    iterations           | 264         |\n",
            "|    time_elapsed         | 3187        |\n",
            "|    total_timesteps      | 4325376     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004936824 |\n",
            "|    clip_fraction        | 0.0633      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.547      |\n",
            "|    explained_variance   | 0.993       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.703       |\n",
            "|    n_updates            | 1052        |\n",
            "|    policy_gradient_loss | 0.00114     |\n",
            "|    value_loss           | 12.1        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 4336000\n",
            "Best mean reward: 286.30 - Last mean reward per episode: 283.43\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 232          |\n",
            "|    ep_rew_mean          | 285          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1357         |\n",
            "|    iterations           | 265          |\n",
            "|    time_elapsed         | 3197         |\n",
            "|    total_timesteps      | 4341760      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0047416473 |\n",
            "|    clip_fraction        | 0.0523       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.563       |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.9          |\n",
            "|    n_updates            | 1056         |\n",
            "|    policy_gradient_loss | -0.00027     |\n",
            "|    value_loss           | 3.22         |\n",
            "------------------------------------------\n",
            "Num timesteps: 4352000\n",
            "Best mean reward: 286.30 - Last mean reward per episode: 281.31\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 222          |\n",
            "|    ep_rew_mean          | 279          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1358         |\n",
            "|    iterations           | 266          |\n",
            "|    time_elapsed         | 3206         |\n",
            "|    total_timesteps      | 4358144      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044005737 |\n",
            "|    clip_fraction        | 0.0545       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.562       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.675        |\n",
            "|    n_updates            | 1060         |\n",
            "|    policy_gradient_loss | -0.000524    |\n",
            "|    value_loss           | 2.13         |\n",
            "------------------------------------------\n",
            "Num timesteps: 4368000\n",
            "Best mean reward: 286.30 - Last mean reward per episode: 280.13\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 217          |\n",
            "|    ep_rew_mean          | 282          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1360         |\n",
            "|    iterations           | 267          |\n",
            "|    time_elapsed         | 3216         |\n",
            "|    total_timesteps      | 4374528      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034049659 |\n",
            "|    clip_fraction        | 0.0412       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.547       |\n",
            "|    explained_variance   | 0.976        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.49         |\n",
            "|    n_updates            | 1064         |\n",
            "|    policy_gradient_loss | 0.000113     |\n",
            "|    value_loss           | 75.1         |\n",
            "------------------------------------------\n",
            "Num timesteps: 4384000\n",
            "Best mean reward: 286.30 - Last mean reward per episode: 286.40\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 219          |\n",
            "|    ep_rew_mean          | 287          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1361         |\n",
            "|    iterations           | 268          |\n",
            "|    time_elapsed         | 3225         |\n",
            "|    total_timesteps      | 4390912      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034058634 |\n",
            "|    clip_fraction        | 0.0459       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.508       |\n",
            "|    explained_variance   | 0.997        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.28         |\n",
            "|    n_updates            | 1068         |\n",
            "|    policy_gradient_loss | 0.000482     |\n",
            "|    value_loss           | 3.53         |\n",
            "------------------------------------------\n",
            "Num timesteps: 4400000\n",
            "Best mean reward: 286.40 - Last mean reward per episode: 286.42\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 217          |\n",
            "|    ep_rew_mean          | 285          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1362         |\n",
            "|    iterations           | 269          |\n",
            "|    time_elapsed         | 3234         |\n",
            "|    total_timesteps      | 4407296      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0036800816 |\n",
            "|    clip_fraction        | 0.0482       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.52        |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.727        |\n",
            "|    n_updates            | 1072         |\n",
            "|    policy_gradient_loss | -0.000137    |\n",
            "|    value_loss           | 2.27         |\n",
            "------------------------------------------\n",
            "Num timesteps: 4416000\n",
            "Best mean reward: 286.42 - Last mean reward per episode: 285.60\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 214         |\n",
            "|    ep_rew_mean          | 281         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1363        |\n",
            "|    iterations           | 270         |\n",
            "|    time_elapsed         | 3244        |\n",
            "|    total_timesteps      | 4423680     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003518734 |\n",
            "|    clip_fraction        | 0.0476      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.497      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.801       |\n",
            "|    n_updates            | 1076        |\n",
            "|    policy_gradient_loss | 4.82e-05    |\n",
            "|    value_loss           | 1.69        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 4432000\n",
            "Best mean reward: 286.42 - Last mean reward per episode: 280.94\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 214          |\n",
            "|    ep_rew_mean          | 284          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1364         |\n",
            "|    iterations           | 271          |\n",
            "|    time_elapsed         | 3254         |\n",
            "|    total_timesteps      | 4440064      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0048595117 |\n",
            "|    clip_fraction        | 0.0539       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.505       |\n",
            "|    explained_variance   | 0.977        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.4          |\n",
            "|    n_updates            | 1080         |\n",
            "|    policy_gradient_loss | 0.000148     |\n",
            "|    value_loss           | 60.7         |\n",
            "------------------------------------------\n",
            "Num timesteps: 4448000\n",
            "Best mean reward: 286.42 - Last mean reward per episode: 284.32\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 216          |\n",
            "|    ep_rew_mean          | 286          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1365         |\n",
            "|    iterations           | 272          |\n",
            "|    time_elapsed         | 3263         |\n",
            "|    total_timesteps      | 4456448      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041191243 |\n",
            "|    clip_fraction        | 0.0511       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.503       |\n",
            "|    explained_variance   | 1            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.355        |\n",
            "|    n_updates            | 1084         |\n",
            "|    policy_gradient_loss | -0.000244    |\n",
            "|    value_loss           | 1.27         |\n",
            "------------------------------------------\n",
            "Num timesteps: 4464000\n",
            "Best mean reward: 286.42 - Last mean reward per episode: 283.71\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 223         |\n",
            "|    ep_rew_mean          | 281         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1365        |\n",
            "|    iterations           | 273         |\n",
            "|    time_elapsed         | 3274        |\n",
            "|    total_timesteps      | 4472832     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005033819 |\n",
            "|    clip_fraction        | 0.0647      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.512      |\n",
            "|    explained_variance   | 0.994       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.79        |\n",
            "|    n_updates            | 1088        |\n",
            "|    policy_gradient_loss | -0.000178   |\n",
            "|    value_loss           | 5.99        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 4480000\n",
            "Best mean reward: 286.42 - Last mean reward per episode: 280.56\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 212          |\n",
            "|    ep_rew_mean          | 283          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1366         |\n",
            "|    iterations           | 274          |\n",
            "|    time_elapsed         | 3284         |\n",
            "|    total_timesteps      | 4489216      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0036406952 |\n",
            "|    clip_fraction        | 0.0338       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.507       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.981        |\n",
            "|    n_updates            | 1092         |\n",
            "|    policy_gradient_loss | 0.000831     |\n",
            "|    value_loss           | 2.69         |\n",
            "------------------------------------------\n",
            "Num timesteps: 4496000\n",
            "Best mean reward: 286.42 - Last mean reward per episode: 282.06\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 219          |\n",
            "|    ep_rew_mean          | 281          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1367         |\n",
            "|    iterations           | 275          |\n",
            "|    time_elapsed         | 3293         |\n",
            "|    total_timesteps      | 4505600      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0036782138 |\n",
            "|    clip_fraction        | 0.0377       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.504       |\n",
            "|    explained_variance   | 0.996        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.93         |\n",
            "|    n_updates            | 1096         |\n",
            "|    policy_gradient_loss | -0.000201    |\n",
            "|    value_loss           | 6.25         |\n",
            "------------------------------------------\n",
            "Num timesteps: 4512000\n",
            "Best mean reward: 286.42 - Last mean reward per episode: 280.29\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 212         |\n",
            "|    ep_rew_mean          | 284         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1368        |\n",
            "|    iterations           | 276         |\n",
            "|    time_elapsed         | 3303        |\n",
            "|    total_timesteps      | 4521984     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004025693 |\n",
            "|    clip_fraction        | 0.0436      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.502      |\n",
            "|    explained_variance   | 0.979       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.47        |\n",
            "|    n_updates            | 1100        |\n",
            "|    policy_gradient_loss | 0.000722    |\n",
            "|    value_loss           | 72.8        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 4528000\n",
            "Best mean reward: 286.42 - Last mean reward per episode: 283.64\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 213          |\n",
            "|    ep_rew_mean          | 283          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1369         |\n",
            "|    iterations           | 277          |\n",
            "|    time_elapsed         | 3313         |\n",
            "|    total_timesteps      | 4538368      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0039553195 |\n",
            "|    clip_fraction        | 0.0507       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.512       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.438        |\n",
            "|    n_updates            | 1104         |\n",
            "|    policy_gradient_loss | 0.000857     |\n",
            "|    value_loss           | 2.27         |\n",
            "------------------------------------------\n",
            "Num timesteps: 4544000\n",
            "Best mean reward: 286.42 - Last mean reward per episode: 281.37\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 214         |\n",
            "|    ep_rew_mean          | 279         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1370        |\n",
            "|    iterations           | 278         |\n",
            "|    time_elapsed         | 3323        |\n",
            "|    total_timesteps      | 4554752     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004263947 |\n",
            "|    clip_fraction        | 0.0547      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.497      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.638       |\n",
            "|    n_updates            | 1108        |\n",
            "|    policy_gradient_loss | 0.000412    |\n",
            "|    value_loss           | 1.41        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 4560000\n",
            "Best mean reward: 286.42 - Last mean reward per episode: 281.66\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 220          |\n",
            "|    ep_rew_mean          | 283          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1371         |\n",
            "|    iterations           | 279          |\n",
            "|    time_elapsed         | 3333         |\n",
            "|    total_timesteps      | 4571136      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0027524251 |\n",
            "|    clip_fraction        | 0.0222       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.491       |\n",
            "|    explained_variance   | 0.961        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.25         |\n",
            "|    n_updates            | 1112         |\n",
            "|    policy_gradient_loss | -0.000526    |\n",
            "|    value_loss           | 80.5         |\n",
            "------------------------------------------\n",
            "Num timesteps: 4576000\n",
            "Best mean reward: 286.42 - Last mean reward per episode: 285.98\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 213          |\n",
            "|    ep_rew_mean          | 283          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1372         |\n",
            "|    iterations           | 280          |\n",
            "|    time_elapsed         | 3342         |\n",
            "|    total_timesteps      | 4587520      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040124888 |\n",
            "|    clip_fraction        | 0.0451       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.484       |\n",
            "|    explained_variance   | 0.992        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.61         |\n",
            "|    n_updates            | 1116         |\n",
            "|    policy_gradient_loss | 0.000194     |\n",
            "|    value_loss           | 16.2         |\n",
            "------------------------------------------\n",
            "Num timesteps: 4592000\n",
            "Best mean reward: 286.42 - Last mean reward per episode: 283.31\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 214          |\n",
            "|    ep_rew_mean          | 281          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1373         |\n",
            "|    iterations           | 281          |\n",
            "|    time_elapsed         | 3352         |\n",
            "|    total_timesteps      | 4603904      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038772633 |\n",
            "|    clip_fraction        | 0.0522       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.491       |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.34         |\n",
            "|    n_updates            | 1120         |\n",
            "|    policy_gradient_loss | 0.000216     |\n",
            "|    value_loss           | 5.11         |\n",
            "------------------------------------------\n",
            "Num timesteps: 4608000\n",
            "Best mean reward: 286.42 - Last mean reward per episode: 279.45\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 225          |\n",
            "|    ep_rew_mean          | 280          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1374         |\n",
            "|    iterations           | 282          |\n",
            "|    time_elapsed         | 3362         |\n",
            "|    total_timesteps      | 4620288      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040718922 |\n",
            "|    clip_fraction        | 0.0423       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.504       |\n",
            "|    explained_variance   | 0.98         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.91         |\n",
            "|    n_updates            | 1124         |\n",
            "|    policy_gradient_loss | 0.00102      |\n",
            "|    value_loss           | 41.7         |\n",
            "------------------------------------------\n",
            "Num timesteps: 4624000\n",
            "Best mean reward: 286.42 - Last mean reward per episode: 280.02\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 225         |\n",
            "|    ep_rew_mean          | 281         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1375        |\n",
            "|    iterations           | 283         |\n",
            "|    time_elapsed         | 3371        |\n",
            "|    total_timesteps      | 4636672     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004257102 |\n",
            "|    clip_fraction        | 0.0538      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.512      |\n",
            "|    explained_variance   | 0.981       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.51        |\n",
            "|    n_updates            | 1128        |\n",
            "|    policy_gradient_loss | -0.000703   |\n",
            "|    value_loss           | 28          |\n",
            "-----------------------------------------\n",
            "Num timesteps: 4640000\n",
            "Best mean reward: 286.42 - Last mean reward per episode: 279.50\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 214          |\n",
            "|    ep_rew_mean          | 282          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1376         |\n",
            "|    iterations           | 284          |\n",
            "|    time_elapsed         | 3380         |\n",
            "|    total_timesteps      | 4653056      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0039968155 |\n",
            "|    clip_fraction        | 0.0539       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.496       |\n",
            "|    explained_variance   | 0.973        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.25         |\n",
            "|    n_updates            | 1132         |\n",
            "|    policy_gradient_loss | 0.00064      |\n",
            "|    value_loss           | 85.1         |\n",
            "------------------------------------------\n",
            "Num timesteps: 4656000\n",
            "Best mean reward: 286.42 - Last mean reward per episode: 281.70\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 212          |\n",
            "|    ep_rew_mean          | 283          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1377         |\n",
            "|    iterations           | 285          |\n",
            "|    time_elapsed         | 3390         |\n",
            "|    total_timesteps      | 4669440      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0046548033 |\n",
            "|    clip_fraction        | 0.0617       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.503       |\n",
            "|    explained_variance   | 0.977        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.01         |\n",
            "|    n_updates            | 1136         |\n",
            "|    policy_gradient_loss | 0.000844     |\n",
            "|    value_loss           | 82.2         |\n",
            "------------------------------------------\n",
            "Num timesteps: 4672000\n",
            "Best mean reward: 286.42 - Last mean reward per episode: 283.50\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 222          |\n",
            "|    ep_rew_mean          | 287          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1378         |\n",
            "|    iterations           | 286          |\n",
            "|    time_elapsed         | 3400         |\n",
            "|    total_timesteps      | 4685824      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0052690003 |\n",
            "|    clip_fraction        | 0.0646       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.499       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.61         |\n",
            "|    n_updates            | 1140         |\n",
            "|    policy_gradient_loss | 0.00135      |\n",
            "|    value_loss           | 2.16         |\n",
            "------------------------------------------\n",
            "Num timesteps: 4688000\n",
            "Best mean reward: 286.42 - Last mean reward per episode: 286.69\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 224          |\n",
            "|    ep_rew_mean          | 284          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1379         |\n",
            "|    iterations           | 287          |\n",
            "|    time_elapsed         | 3409         |\n",
            "|    total_timesteps      | 4702208      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0037927616 |\n",
            "|    clip_fraction        | 0.0385       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.499       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.28         |\n",
            "|    n_updates            | 1144         |\n",
            "|    policy_gradient_loss | 0.000552     |\n",
            "|    value_loss           | 2.28         |\n",
            "------------------------------------------\n",
            "Num timesteps: 4704000\n",
            "Best mean reward: 286.69 - Last mean reward per episode: 282.84\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 216         |\n",
            "|    ep_rew_mean          | 286         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1380        |\n",
            "|    iterations           | 288         |\n",
            "|    time_elapsed         | 3419        |\n",
            "|    total_timesteps      | 4718592     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003441046 |\n",
            "|    clip_fraction        | 0.0388      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.499      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.658       |\n",
            "|    n_updates            | 1148        |\n",
            "|    policy_gradient_loss | 0.000176    |\n",
            "|    value_loss           | 1.68        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 4720000\n",
            "Best mean reward: 286.69 - Last mean reward per episode: 285.11\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 218         |\n",
            "|    ep_rew_mean          | 285         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1380        |\n",
            "|    iterations           | 289         |\n",
            "|    time_elapsed         | 3428        |\n",
            "|    total_timesteps      | 4734976     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003274826 |\n",
            "|    clip_fraction        | 0.0415      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.491      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.14        |\n",
            "|    n_updates            | 1152        |\n",
            "|    policy_gradient_loss | 0.000425    |\n",
            "|    value_loss           | 1.52        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 4736000\n",
            "Best mean reward: 286.69 - Last mean reward per episode: 284.68\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 215          |\n",
            "|    ep_rew_mean          | 282          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1382         |\n",
            "|    iterations           | 290          |\n",
            "|    time_elapsed         | 3437         |\n",
            "|    total_timesteps      | 4751360      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0042402446 |\n",
            "|    clip_fraction        | 0.0459       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.494       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.46         |\n",
            "|    n_updates            | 1156         |\n",
            "|    policy_gradient_loss | 5.51e-05     |\n",
            "|    value_loss           | 2.78         |\n",
            "------------------------------------------\n",
            "Num timesteps: 4752000\n",
            "Best mean reward: 286.69 - Last mean reward per episode: 282.32\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 213          |\n",
            "|    ep_rew_mean          | 288          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1383         |\n",
            "|    iterations           | 291          |\n",
            "|    time_elapsed         | 3447         |\n",
            "|    total_timesteps      | 4767744      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0030603653 |\n",
            "|    clip_fraction        | 0.0463       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.484       |\n",
            "|    explained_variance   | 0.976        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.43         |\n",
            "|    n_updates            | 1160         |\n",
            "|    policy_gradient_loss | 0.000217     |\n",
            "|    value_loss           | 54.2         |\n",
            "------------------------------------------\n",
            "Num timesteps: 4768000\n",
            "Best mean reward: 286.69 - Last mean reward per episode: 288.05\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "Num timesteps: 4784000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 286.46\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 216         |\n",
            "|    ep_rew_mean          | 286         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1383        |\n",
            "|    iterations           | 292         |\n",
            "|    time_elapsed         | 3457        |\n",
            "|    total_timesteps      | 4784128     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003815362 |\n",
            "|    clip_fraction        | 0.0538      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.481      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.9         |\n",
            "|    n_updates            | 1164        |\n",
            "|    policy_gradient_loss | -0.000621   |\n",
            "|    value_loss           | 3.34        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 4800000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 282.60\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 222         |\n",
            "|    ep_rew_mean          | 282         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1384        |\n",
            "|    iterations           | 293         |\n",
            "|    time_elapsed         | 3466        |\n",
            "|    total_timesteps      | 4800512     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003303961 |\n",
            "|    clip_fraction        | 0.037       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.469      |\n",
            "|    explained_variance   | 0.997       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.06        |\n",
            "|    n_updates            | 1168        |\n",
            "|    policy_gradient_loss | 0.00057     |\n",
            "|    value_loss           | 5.12        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 4816000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 282.35\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 211          |\n",
            "|    ep_rew_mean          | 283          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1385         |\n",
            "|    iterations           | 294          |\n",
            "|    time_elapsed         | 3475         |\n",
            "|    total_timesteps      | 4816896      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040838523 |\n",
            "|    clip_fraction        | 0.0448       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.473       |\n",
            "|    explained_variance   | 0.982        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.1          |\n",
            "|    n_updates            | 1172         |\n",
            "|    policy_gradient_loss | 0.000498     |\n",
            "|    value_loss           | 50           |\n",
            "------------------------------------------\n",
            "Num timesteps: 4832000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 283.34\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 213         |\n",
            "|    ep_rew_mean          | 283         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1386        |\n",
            "|    iterations           | 295         |\n",
            "|    time_elapsed         | 3485        |\n",
            "|    total_timesteps      | 4833280     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004794485 |\n",
            "|    clip_fraction        | 0.0654      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.486      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.532       |\n",
            "|    n_updates            | 1176        |\n",
            "|    policy_gradient_loss | 0.000336    |\n",
            "|    value_loss           | 1.33        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 4848000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 276.23\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 215          |\n",
            "|    ep_rew_mean          | 277          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1387         |\n",
            "|    iterations           | 296          |\n",
            "|    time_elapsed         | 3495         |\n",
            "|    total_timesteps      | 4849664      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034741345 |\n",
            "|    clip_fraction        | 0.0454       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.467       |\n",
            "|    explained_variance   | 0.98         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 11.4         |\n",
            "|    n_updates            | 1180         |\n",
            "|    policy_gradient_loss | -0.000847    |\n",
            "|    value_loss           | 62.6         |\n",
            "------------------------------------------\n",
            "Num timesteps: 4864000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 282.71\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 209          |\n",
            "|    ep_rew_mean          | 284          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1388         |\n",
            "|    iterations           | 297          |\n",
            "|    time_elapsed         | 3504         |\n",
            "|    total_timesteps      | 4866048      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0029584777 |\n",
            "|    clip_fraction        | 0.0356       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.485       |\n",
            "|    explained_variance   | 0.983        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.704        |\n",
            "|    n_updates            | 1184         |\n",
            "|    policy_gradient_loss | 0.000678     |\n",
            "|    value_loss           | 61.2         |\n",
            "------------------------------------------\n",
            "Num timesteps: 4880000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 285.82\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 219         |\n",
            "|    ep_rew_mean          | 287         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1389        |\n",
            "|    iterations           | 298         |\n",
            "|    time_elapsed         | 3514        |\n",
            "|    total_timesteps      | 4882432     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004257446 |\n",
            "|    clip_fraction        | 0.0457      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.457      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.26        |\n",
            "|    n_updates            | 1188        |\n",
            "|    policy_gradient_loss | 0.00121     |\n",
            "|    value_loss           | 3.5         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 4896000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 282.97\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 212         |\n",
            "|    ep_rew_mean          | 284         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1390        |\n",
            "|    iterations           | 299         |\n",
            "|    time_elapsed         | 3523        |\n",
            "|    total_timesteps      | 4898816     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003110522 |\n",
            "|    clip_fraction        | 0.0338      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.467      |\n",
            "|    explained_variance   | 0.997       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.29        |\n",
            "|    n_updates            | 1192        |\n",
            "|    policy_gradient_loss | 0.000549    |\n",
            "|    value_loss           | 4.57        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 4912000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 281.02\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 212         |\n",
            "|    ep_rew_mean          | 284         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1391        |\n",
            "|    iterations           | 300         |\n",
            "|    time_elapsed         | 3533        |\n",
            "|    total_timesteps      | 4915200     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003795479 |\n",
            "|    clip_fraction        | 0.0477      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.467      |\n",
            "|    explained_variance   | 0.983       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.87        |\n",
            "|    n_updates            | 1196        |\n",
            "|    policy_gradient_loss | 0.000301    |\n",
            "|    value_loss           | 53.8        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 4928000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 284.49\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 211          |\n",
            "|    ep_rew_mean          | 284          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1392         |\n",
            "|    iterations           | 301          |\n",
            "|    time_elapsed         | 3542         |\n",
            "|    total_timesteps      | 4931584      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044523054 |\n",
            "|    clip_fraction        | 0.0565       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.472       |\n",
            "|    explained_variance   | 0.984        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 46.4         |\n",
            "|    n_updates            | 1200         |\n",
            "|    policy_gradient_loss | 0.000261     |\n",
            "|    value_loss           | 41.5         |\n",
            "------------------------------------------\n",
            "Num timesteps: 4944000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 279.45\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 210          |\n",
            "|    ep_rew_mean          | 278          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1393         |\n",
            "|    iterations           | 302          |\n",
            "|    time_elapsed         | 3551         |\n",
            "|    total_timesteps      | 4947968      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044247704 |\n",
            "|    clip_fraction        | 0.0457       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.453       |\n",
            "|    explained_variance   | 0.988        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.913        |\n",
            "|    n_updates            | 1204         |\n",
            "|    policy_gradient_loss | 0.00065      |\n",
            "|    value_loss           | 43.9         |\n",
            "------------------------------------------\n",
            "Num timesteps: 4960000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 279.64\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 221          |\n",
            "|    ep_rew_mean          | 280          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1394         |\n",
            "|    iterations           | 303          |\n",
            "|    time_elapsed         | 3561         |\n",
            "|    total_timesteps      | 4964352      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040041367 |\n",
            "|    clip_fraction        | 0.0421       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.463       |\n",
            "|    explained_variance   | 0.986        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.9          |\n",
            "|    n_updates            | 1208         |\n",
            "|    policy_gradient_loss | 0.00113      |\n",
            "|    value_loss           | 41.1         |\n",
            "------------------------------------------\n",
            "Num timesteps: 4976000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 280.70\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 219          |\n",
            "|    ep_rew_mean          | 280          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1394         |\n",
            "|    iterations           | 304          |\n",
            "|    time_elapsed         | 3570         |\n",
            "|    total_timesteps      | 4980736      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0037210793 |\n",
            "|    clip_fraction        | 0.0471       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.459       |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.31         |\n",
            "|    n_updates            | 1212         |\n",
            "|    policy_gradient_loss | -0.000189    |\n",
            "|    value_loss           | 3.36         |\n",
            "------------------------------------------\n",
            "Num timesteps: 4992000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 285.60\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 212          |\n",
            "|    ep_rew_mean          | 280          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1395         |\n",
            "|    iterations           | 305          |\n",
            "|    time_elapsed         | 3580         |\n",
            "|    total_timesteps      | 4997120      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0028155386 |\n",
            "|    clip_fraction        | 0.0253       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.469       |\n",
            "|    explained_variance   | 0.958        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 101          |\n",
            "|    n_updates            | 1216         |\n",
            "|    policy_gradient_loss | 0.000809     |\n",
            "|    value_loss           | 132          |\n",
            "------------------------------------------\n",
            "Num timesteps: 5008000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 282.47\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 211         |\n",
            "|    ep_rew_mean          | 280         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1396        |\n",
            "|    iterations           | 306         |\n",
            "|    time_elapsed         | 3589        |\n",
            "|    total_timesteps      | 5013504     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005489601 |\n",
            "|    clip_fraction        | 0.0624      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.459      |\n",
            "|    explained_variance   | 0.97        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.78        |\n",
            "|    n_updates            | 1220        |\n",
            "|    policy_gradient_loss | 0.000277    |\n",
            "|    value_loss           | 95.6        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 5024000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 285.20\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 213         |\n",
            "|    ep_rew_mean          | 283         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1397        |\n",
            "|    iterations           | 307         |\n",
            "|    time_elapsed         | 3598        |\n",
            "|    total_timesteps      | 5029888     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004315345 |\n",
            "|    clip_fraction        | 0.0616      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.443      |\n",
            "|    explained_variance   | 0.998       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.12        |\n",
            "|    n_updates            | 1224        |\n",
            "|    policy_gradient_loss | 0.00139     |\n",
            "|    value_loss           | 3.39        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 5040000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 277.89\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 217          |\n",
            "|    ep_rew_mean          | 278          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1398         |\n",
            "|    iterations           | 308          |\n",
            "|    time_elapsed         | 3608         |\n",
            "|    total_timesteps      | 5046272      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0028048297 |\n",
            "|    clip_fraction        | 0.0362       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.464       |\n",
            "|    explained_variance   | 0.98         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.977        |\n",
            "|    n_updates            | 1228         |\n",
            "|    policy_gradient_loss | 0.00102      |\n",
            "|    value_loss           | 74.8         |\n",
            "------------------------------------------\n",
            "Num timesteps: 5056000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 280.42\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 218          |\n",
            "|    ep_rew_mean          | 284          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1399         |\n",
            "|    iterations           | 309          |\n",
            "|    time_elapsed         | 3617         |\n",
            "|    total_timesteps      | 5062656      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0037814572 |\n",
            "|    clip_fraction        | 0.0402       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.454       |\n",
            "|    explained_variance   | 0.978        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.44         |\n",
            "|    n_updates            | 1232         |\n",
            "|    policy_gradient_loss | 0.00101      |\n",
            "|    value_loss           | 78.9         |\n",
            "------------------------------------------\n",
            "Num timesteps: 5072000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 283.93\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 235         |\n",
            "|    ep_rew_mean          | 280         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1400        |\n",
            "|    iterations           | 310         |\n",
            "|    time_elapsed         | 3626        |\n",
            "|    total_timesteps      | 5079040     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003918932 |\n",
            "|    clip_fraction        | 0.0511      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.463      |\n",
            "|    explained_variance   | 0.998       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.41        |\n",
            "|    n_updates            | 1236        |\n",
            "|    policy_gradient_loss | -0.000451   |\n",
            "|    value_loss           | 6.98        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 5088000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 279.40\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 225          |\n",
            "|    ep_rew_mean          | 273          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1401         |\n",
            "|    iterations           | 311          |\n",
            "|    time_elapsed         | 3636         |\n",
            "|    total_timesteps      | 5095424      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0028270339 |\n",
            "|    clip_fraction        | 0.0202       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.444       |\n",
            "|    explained_variance   | 0.979        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.88         |\n",
            "|    n_updates            | 1240         |\n",
            "|    policy_gradient_loss | -0.000296    |\n",
            "|    value_loss           | 88.7         |\n",
            "------------------------------------------\n",
            "Num timesteps: 5104000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 274.02\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 226          |\n",
            "|    ep_rew_mean          | 275          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1401         |\n",
            "|    iterations           | 312          |\n",
            "|    time_elapsed         | 3646         |\n",
            "|    total_timesteps      | 5111808      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034577842 |\n",
            "|    clip_fraction        | 0.0333       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.474       |\n",
            "|    explained_variance   | 0.932        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 17.2         |\n",
            "|    n_updates            | 1244         |\n",
            "|    policy_gradient_loss | -0.00054     |\n",
            "|    value_loss           | 230          |\n",
            "------------------------------------------\n",
            "Num timesteps: 5120000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 275.47\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 218         |\n",
            "|    ep_rew_mean          | 281         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1402        |\n",
            "|    iterations           | 313         |\n",
            "|    time_elapsed         | 3656        |\n",
            "|    total_timesteps      | 5128192     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005594721 |\n",
            "|    clip_fraction        | 0.0672      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.462      |\n",
            "|    explained_variance   | 0.977       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 24.2        |\n",
            "|    n_updates            | 1248        |\n",
            "|    policy_gradient_loss | 0.000303    |\n",
            "|    value_loss           | 66.4        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 5136000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 283.50\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 209          |\n",
            "|    ep_rew_mean          | 286          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1403         |\n",
            "|    iterations           | 314          |\n",
            "|    time_elapsed         | 3665         |\n",
            "|    total_timesteps      | 5144576      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044620424 |\n",
            "|    clip_fraction        | 0.0528       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.461       |\n",
            "|    explained_variance   | 0.98         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.76         |\n",
            "|    n_updates            | 1252         |\n",
            "|    policy_gradient_loss | 2.54e-05     |\n",
            "|    value_loss           | 67.6         |\n",
            "------------------------------------------\n",
            "Num timesteps: 5152000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 286.99\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 234          |\n",
            "|    ep_rew_mean          | 282          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1404         |\n",
            "|    iterations           | 315          |\n",
            "|    time_elapsed         | 3675         |\n",
            "|    total_timesteps      | 5160960      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038744286 |\n",
            "|    clip_fraction        | 0.0499       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.462       |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.16         |\n",
            "|    n_updates            | 1256         |\n",
            "|    policy_gradient_loss | 0.00174      |\n",
            "|    value_loss           | 4.22         |\n",
            "------------------------------------------\n",
            "Num timesteps: 5168000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 282.01\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 226          |\n",
            "|    ep_rew_mean          | 282          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1404         |\n",
            "|    iterations           | 316          |\n",
            "|    time_elapsed         | 3685         |\n",
            "|    total_timesteps      | 5177344      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0028429157 |\n",
            "|    clip_fraction        | 0.0389       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.468       |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.909        |\n",
            "|    n_updates            | 1260         |\n",
            "|    policy_gradient_loss | 0.000293     |\n",
            "|    value_loss           | 3.93         |\n",
            "------------------------------------------\n",
            "Num timesteps: 5184000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 283.92\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 208         |\n",
            "|    ep_rew_mean          | 274         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1405        |\n",
            "|    iterations           | 317         |\n",
            "|    time_elapsed         | 3694        |\n",
            "|    total_timesteps      | 5193728     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005025589 |\n",
            "|    clip_fraction        | 0.0482      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.451      |\n",
            "|    explained_variance   | 0.982       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.23        |\n",
            "|    n_updates            | 1264        |\n",
            "|    policy_gradient_loss | 0.000256    |\n",
            "|    value_loss           | 67.4        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 5200000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 277.00\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 208         |\n",
            "|    ep_rew_mean          | 279         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1406        |\n",
            "|    iterations           | 318         |\n",
            "|    time_elapsed         | 3703        |\n",
            "|    total_timesteps      | 5210112     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002448169 |\n",
            "|    clip_fraction        | 0.0317      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.45       |\n",
            "|    explained_variance   | 0.953       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 11.5        |\n",
            "|    n_updates            | 1268        |\n",
            "|    policy_gradient_loss | 0.000582    |\n",
            "|    value_loss           | 159         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 5216000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 276.40\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 211          |\n",
            "|    ep_rew_mean          | 279          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1407         |\n",
            "|    iterations           | 319          |\n",
            "|    time_elapsed         | 3712         |\n",
            "|    total_timesteps      | 5226496      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0042053545 |\n",
            "|    clip_fraction        | 0.046        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.448       |\n",
            "|    explained_variance   | 0.947        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 286          |\n",
            "|    n_updates            | 1272         |\n",
            "|    policy_gradient_loss | -0.00104     |\n",
            "|    value_loss           | 209          |\n",
            "------------------------------------------\n",
            "Num timesteps: 5232000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 284.57\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 223         |\n",
            "|    ep_rew_mean          | 287         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1408        |\n",
            "|    iterations           | 320         |\n",
            "|    time_elapsed         | 3721        |\n",
            "|    total_timesteps      | 5242880     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005140208 |\n",
            "|    clip_fraction        | 0.0578      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.442      |\n",
            "|    explained_variance   | 0.987       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.96        |\n",
            "|    n_updates            | 1276        |\n",
            "|    policy_gradient_loss | 0.000565    |\n",
            "|    value_loss           | 24.8        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 5248000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 286.89\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 219          |\n",
            "|    ep_rew_mean          | 286          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1409         |\n",
            "|    iterations           | 321          |\n",
            "|    time_elapsed         | 3730         |\n",
            "|    total_timesteps      | 5259264      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041092765 |\n",
            "|    clip_fraction        | 0.0525       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.466       |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.06         |\n",
            "|    n_updates            | 1280         |\n",
            "|    policy_gradient_loss | 0.000803     |\n",
            "|    value_loss           | 2.86         |\n",
            "------------------------------------------\n",
            "Num timesteps: 5264000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 286.73\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 216          |\n",
            "|    ep_rew_mean          | 285          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1410         |\n",
            "|    iterations           | 322          |\n",
            "|    time_elapsed         | 3740         |\n",
            "|    total_timesteps      | 5275648      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044913488 |\n",
            "|    clip_fraction        | 0.0447       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.447       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.534        |\n",
            "|    n_updates            | 1284         |\n",
            "|    policy_gradient_loss | -6.53e-05    |\n",
            "|    value_loss           | 2.9          |\n",
            "------------------------------------------\n",
            "Num timesteps: 5280000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 285.22\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 237          |\n",
            "|    ep_rew_mean          | 279          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1411         |\n",
            "|    iterations           | 323          |\n",
            "|    time_elapsed         | 3750         |\n",
            "|    total_timesteps      | 5292032      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0031548678 |\n",
            "|    clip_fraction        | 0.0367       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.461       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.34         |\n",
            "|    n_updates            | 1288         |\n",
            "|    policy_gradient_loss | 0.000466     |\n",
            "|    value_loss           | 2.98         |\n",
            "------------------------------------------\n",
            "Num timesteps: 5296000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 278.24\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 231         |\n",
            "|    ep_rew_mean          | 281         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1411        |\n",
            "|    iterations           | 324         |\n",
            "|    time_elapsed         | 3759        |\n",
            "|    total_timesteps      | 5308416     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003029228 |\n",
            "|    clip_fraction        | 0.0257      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.472      |\n",
            "|    explained_variance   | 0.965       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 15.5        |\n",
            "|    n_updates            | 1292        |\n",
            "|    policy_gradient_loss | 0.000279    |\n",
            "|    value_loss           | 130         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 5312000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 281.67\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 212          |\n",
            "|    ep_rew_mean          | 284          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1412         |\n",
            "|    iterations           | 325          |\n",
            "|    time_elapsed         | 3768         |\n",
            "|    total_timesteps      | 5324800      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0039980654 |\n",
            "|    clip_fraction        | 0.051        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.444       |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.543        |\n",
            "|    n_updates            | 1296         |\n",
            "|    policy_gradient_loss | 0.00124      |\n",
            "|    value_loss           | 3.75         |\n",
            "------------------------------------------\n",
            "Num timesteps: 5328000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 284.40\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 218         |\n",
            "|    ep_rew_mean          | 280         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1413        |\n",
            "|    iterations           | 326         |\n",
            "|    time_elapsed         | 3778        |\n",
            "|    total_timesteps      | 5341184     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004173417 |\n",
            "|    clip_fraction        | 0.0433      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.433      |\n",
            "|    explained_variance   | 0.981       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 128         |\n",
            "|    n_updates            | 1300        |\n",
            "|    policy_gradient_loss | 0.000563    |\n",
            "|    value_loss           | 74.7        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 5344000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 280.24\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 222         |\n",
            "|    ep_rew_mean          | 281         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1414        |\n",
            "|    iterations           | 327         |\n",
            "|    time_elapsed         | 3787        |\n",
            "|    total_timesteps      | 5357568     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003803315 |\n",
            "|    clip_fraction        | 0.0482      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.434      |\n",
            "|    explained_variance   | 0.98        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.23        |\n",
            "|    n_updates            | 1304        |\n",
            "|    policy_gradient_loss | -3.47e-05   |\n",
            "|    value_loss           | 77.3        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 5360000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 281.38\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 209          |\n",
            "|    ep_rew_mean          | 281          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1415         |\n",
            "|    iterations           | 328          |\n",
            "|    time_elapsed         | 3796         |\n",
            "|    total_timesteps      | 5373952      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035275777 |\n",
            "|    clip_fraction        | 0.048        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.442       |\n",
            "|    explained_variance   | 0.979        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.62         |\n",
            "|    n_updates            | 1308         |\n",
            "|    policy_gradient_loss | 0.00122      |\n",
            "|    value_loss           | 76.2         |\n",
            "------------------------------------------\n",
            "Num timesteps: 5376000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 281.89\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 209          |\n",
            "|    ep_rew_mean          | 285          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1416         |\n",
            "|    iterations           | 329          |\n",
            "|    time_elapsed         | 3805         |\n",
            "|    total_timesteps      | 5390336      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0026909702 |\n",
            "|    clip_fraction        | 0.0377       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.428       |\n",
            "|    explained_variance   | 0.961        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 35           |\n",
            "|    n_updates            | 1312         |\n",
            "|    policy_gradient_loss | 0.000987     |\n",
            "|    value_loss           | 151          |\n",
            "------------------------------------------\n",
            "Num timesteps: 5392000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 284.47\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 210         |\n",
            "|    ep_rew_mean          | 285         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1417        |\n",
            "|    iterations           | 330         |\n",
            "|    time_elapsed         | 3815        |\n",
            "|    total_timesteps      | 5406720     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004932526 |\n",
            "|    clip_fraction        | 0.0551      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.434      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.871       |\n",
            "|    n_updates            | 1316        |\n",
            "|    policy_gradient_loss | 0.000186    |\n",
            "|    value_loss           | 3.29        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 5408000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 286.15\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 208         |\n",
            "|    ep_rew_mean          | 283         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1418        |\n",
            "|    iterations           | 331         |\n",
            "|    time_elapsed         | 3823        |\n",
            "|    total_timesteps      | 5423104     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004965846 |\n",
            "|    clip_fraction        | 0.0486      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.457      |\n",
            "|    explained_variance   | 0.982       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.82        |\n",
            "|    n_updates            | 1320        |\n",
            "|    policy_gradient_loss | 0.000392    |\n",
            "|    value_loss           | 60.1        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 5424000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 284.39\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 207          |\n",
            "|    ep_rew_mean          | 280          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1418         |\n",
            "|    iterations           | 332          |\n",
            "|    time_elapsed         | 3833         |\n",
            "|    total_timesteps      | 5439488      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0030701128 |\n",
            "|    clip_fraction        | 0.0342       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.453       |\n",
            "|    explained_variance   | 0.981        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.37         |\n",
            "|    n_updates            | 1324         |\n",
            "|    policy_gradient_loss | 0.000385     |\n",
            "|    value_loss           | 61.6         |\n",
            "------------------------------------------\n",
            "Num timesteps: 5440000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 279.44\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 208          |\n",
            "|    ep_rew_mean          | 284          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1419         |\n",
            "|    iterations           | 333          |\n",
            "|    time_elapsed         | 3842         |\n",
            "|    total_timesteps      | 5455872      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0039605256 |\n",
            "|    clip_fraction        | 0.0441       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.449       |\n",
            "|    explained_variance   | 0.963        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.03         |\n",
            "|    n_updates            | 1328         |\n",
            "|    policy_gradient_loss | 0.000154     |\n",
            "|    value_loss           | 137          |\n",
            "------------------------------------------\n",
            "Num timesteps: 5456000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 284.11\n",
            "Num timesteps: 5472000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 284.68\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 211          |\n",
            "|    ep_rew_mean          | 285          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1420         |\n",
            "|    iterations           | 334          |\n",
            "|    time_elapsed         | 3851         |\n",
            "|    total_timesteps      | 5472256      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0042111194 |\n",
            "|    clip_fraction        | 0.0522       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.446       |\n",
            "|    explained_variance   | 0.981        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.61         |\n",
            "|    n_updates            | 1332         |\n",
            "|    policy_gradient_loss | 0.000568     |\n",
            "|    value_loss           | 55.4         |\n",
            "------------------------------------------\n",
            "Num timesteps: 5488000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 283.19\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 214         |\n",
            "|    ep_rew_mean          | 284         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1421        |\n",
            "|    iterations           | 335         |\n",
            "|    time_elapsed         | 3861        |\n",
            "|    total_timesteps      | 5488640     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004208713 |\n",
            "|    clip_fraction        | 0.0428      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.443      |\n",
            "|    explained_variance   | 0.973       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.11        |\n",
            "|    n_updates            | 1336        |\n",
            "|    policy_gradient_loss | 0.000907    |\n",
            "|    value_loss           | 84.7        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 5504000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 284.28\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 224        |\n",
            "|    ep_rew_mean          | 284        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 1422       |\n",
            "|    iterations           | 336        |\n",
            "|    time_elapsed         | 3870       |\n",
            "|    total_timesteps      | 5505024    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00560061 |\n",
            "|    clip_fraction        | 0.0626     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.448     |\n",
            "|    explained_variance   | 0.981      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 73.3       |\n",
            "|    n_updates            | 1340       |\n",
            "|    policy_gradient_loss | 0.00107    |\n",
            "|    value_loss           | 68.7       |\n",
            "----------------------------------------\n",
            "Num timesteps: 5520000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 285.23\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 211          |\n",
            "|    ep_rew_mean          | 285          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1422         |\n",
            "|    iterations           | 337          |\n",
            "|    time_elapsed         | 3880         |\n",
            "|    total_timesteps      | 5521408      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040258183 |\n",
            "|    clip_fraction        | 0.0513       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.442       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.665        |\n",
            "|    n_updates            | 1344         |\n",
            "|    policy_gradient_loss | 0.00165      |\n",
            "|    value_loss           | 2.35         |\n",
            "------------------------------------------\n",
            "Num timesteps: 5536000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 282.55\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 204          |\n",
            "|    ep_rew_mean          | 282          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1423         |\n",
            "|    iterations           | 338          |\n",
            "|    time_elapsed         | 3889         |\n",
            "|    total_timesteps      | 5537792      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038886575 |\n",
            "|    clip_fraction        | 0.0428       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.441       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.397        |\n",
            "|    n_updates            | 1348         |\n",
            "|    policy_gradient_loss | 0.000135     |\n",
            "|    value_loss           | 1.46         |\n",
            "------------------------------------------\n",
            "Num timesteps: 5552000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 280.05\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 205         |\n",
            "|    ep_rew_mean          | 279         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1424        |\n",
            "|    iterations           | 339         |\n",
            "|    time_elapsed         | 3898        |\n",
            "|    total_timesteps      | 5554176     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003956582 |\n",
            "|    clip_fraction        | 0.0524      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.44       |\n",
            "|    explained_variance   | 0.981       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 155         |\n",
            "|    n_updates            | 1352        |\n",
            "|    policy_gradient_loss | -0.00071    |\n",
            "|    value_loss           | 66.6        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 5568000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 282.32\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 214          |\n",
            "|    ep_rew_mean          | 284          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1425         |\n",
            "|    iterations           | 340          |\n",
            "|    time_elapsed         | 3908         |\n",
            "|    total_timesteps      | 5570560      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034704867 |\n",
            "|    clip_fraction        | 0.043        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.433       |\n",
            "|    explained_variance   | 0.984        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 17.3         |\n",
            "|    n_updates            | 1356         |\n",
            "|    policy_gradient_loss | 0.000555     |\n",
            "|    value_loss           | 35.4         |\n",
            "------------------------------------------\n",
            "Num timesteps: 5584000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 288.00\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 213          |\n",
            "|    ep_rew_mean          | 290          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1425         |\n",
            "|    iterations           | 341          |\n",
            "|    time_elapsed         | 3918         |\n",
            "|    total_timesteps      | 5586944      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035712055 |\n",
            "|    clip_fraction        | 0.0524       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.427       |\n",
            "|    explained_variance   | 0.995        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.871        |\n",
            "|    n_updates            | 1360         |\n",
            "|    policy_gradient_loss | 0.000564     |\n",
            "|    value_loss           | 16.8         |\n",
            "------------------------------------------\n",
            "Num timesteps: 5600000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 285.31\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 205          |\n",
            "|    ep_rew_mean          | 285          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1426         |\n",
            "|    iterations           | 342          |\n",
            "|    time_elapsed         | 3927         |\n",
            "|    total_timesteps      | 5603328      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0027867053 |\n",
            "|    clip_fraction        | 0.03         |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.409       |\n",
            "|    explained_variance   | 0.992        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 11.2         |\n",
            "|    n_updates            | 1364         |\n",
            "|    policy_gradient_loss | 0.000741     |\n",
            "|    value_loss           | 17.4         |\n",
            "------------------------------------------\n",
            "Num timesteps: 5616000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 283.30\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 205         |\n",
            "|    ep_rew_mean          | 282         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1427        |\n",
            "|    iterations           | 343         |\n",
            "|    time_elapsed         | 3936        |\n",
            "|    total_timesteps      | 5619712     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003695875 |\n",
            "|    clip_fraction        | 0.0507      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.423      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.404       |\n",
            "|    n_updates            | 1368        |\n",
            "|    policy_gradient_loss | 0.00187     |\n",
            "|    value_loss           | 1.35        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 5632000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 284.15\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 206          |\n",
            "|    ep_rew_mean          | 286          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1428         |\n",
            "|    iterations           | 344          |\n",
            "|    time_elapsed         | 3946         |\n",
            "|    total_timesteps      | 5636096      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0039037939 |\n",
            "|    clip_fraction        | 0.0447       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.418       |\n",
            "|    explained_variance   | 1            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.49         |\n",
            "|    n_updates            | 1372         |\n",
            "|    policy_gradient_loss | 0.000315     |\n",
            "|    value_loss           | 1.09         |\n",
            "------------------------------------------\n",
            "Num timesteps: 5648000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 285.98\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 212          |\n",
            "|    ep_rew_mean          | 285          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1429         |\n",
            "|    iterations           | 345          |\n",
            "|    time_elapsed         | 3955         |\n",
            "|    total_timesteps      | 5652480      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0031812228 |\n",
            "|    clip_fraction        | 0.0388       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.411       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.575        |\n",
            "|    n_updates            | 1376         |\n",
            "|    policy_gradient_loss | 0.000107     |\n",
            "|    value_loss           | 1.97         |\n",
            "------------------------------------------\n",
            "Num timesteps: 5664000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 287.31\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 211        |\n",
            "|    ep_rew_mean          | 286        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 1429       |\n",
            "|    iterations           | 346        |\n",
            "|    time_elapsed         | 3964       |\n",
            "|    total_timesteps      | 5668864    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00291022 |\n",
            "|    clip_fraction        | 0.0319     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.434     |\n",
            "|    explained_variance   | 0.965      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 2.38       |\n",
            "|    n_updates            | 1380       |\n",
            "|    policy_gradient_loss | -0.000105  |\n",
            "|    value_loss           | 149        |\n",
            "----------------------------------------\n",
            "Num timesteps: 5680000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 285.02\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 213          |\n",
            "|    ep_rew_mean          | 284          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1430         |\n",
            "|    iterations           | 347          |\n",
            "|    time_elapsed         | 3974         |\n",
            "|    total_timesteps      | 5685248      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0048240717 |\n",
            "|    clip_fraction        | 0.0658       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.433       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.554        |\n",
            "|    n_updates            | 1384         |\n",
            "|    policy_gradient_loss | -0.000728    |\n",
            "|    value_loss           | 1.66         |\n",
            "------------------------------------------\n",
            "Num timesteps: 5696000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 277.93\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 201          |\n",
            "|    ep_rew_mean          | 277          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1430         |\n",
            "|    iterations           | 348          |\n",
            "|    time_elapsed         | 3984         |\n",
            "|    total_timesteps      | 5701632      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0037751675 |\n",
            "|    clip_fraction        | 0.0416       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.438       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.412        |\n",
            "|    n_updates            | 1388         |\n",
            "|    policy_gradient_loss | -0.000451    |\n",
            "|    value_loss           | 1.72         |\n",
            "------------------------------------------\n",
            "Num timesteps: 5712000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 272.75\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 210          |\n",
            "|    ep_rew_mean          | 277          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1431         |\n",
            "|    iterations           | 349          |\n",
            "|    time_elapsed         | 3993         |\n",
            "|    total_timesteps      | 5718016      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0023244466 |\n",
            "|    clip_fraction        | 0.0137       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.441       |\n",
            "|    explained_variance   | 0.939        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 166          |\n",
            "|    n_updates            | 1392         |\n",
            "|    policy_gradient_loss | -0.000163    |\n",
            "|    value_loss           | 255          |\n",
            "------------------------------------------\n",
            "Num timesteps: 5728000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 285.68\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 203         |\n",
            "|    ep_rew_mean          | 287         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1432        |\n",
            "|    iterations           | 350         |\n",
            "|    time_elapsed         | 4002        |\n",
            "|    total_timesteps      | 5734400     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004241801 |\n",
            "|    clip_fraction        | 0.0408      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.428      |\n",
            "|    explained_variance   | 0.981       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.832       |\n",
            "|    n_updates            | 1396        |\n",
            "|    policy_gradient_loss | 0.000317    |\n",
            "|    value_loss           | 73.8        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 5744000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 284.53\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 213          |\n",
            "|    ep_rew_mean          | 283          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1433         |\n",
            "|    iterations           | 351          |\n",
            "|    time_elapsed         | 4012         |\n",
            "|    total_timesteps      | 5750784      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040017534 |\n",
            "|    clip_fraction        | 0.0455       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.429       |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.26         |\n",
            "|    n_updates            | 1400         |\n",
            "|    policy_gradient_loss | 0.00042      |\n",
            "|    value_loss           | 4.47         |\n",
            "------------------------------------------\n",
            "Num timesteps: 5760000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 277.90\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 214          |\n",
            "|    ep_rew_mean          | 282          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1434         |\n",
            "|    iterations           | 352          |\n",
            "|    time_elapsed         | 4021         |\n",
            "|    total_timesteps      | 5767168      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0028565903 |\n",
            "|    clip_fraction        | 0.026        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.459       |\n",
            "|    explained_variance   | 0.972        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.86         |\n",
            "|    n_updates            | 1404         |\n",
            "|    policy_gradient_loss | 0.000436     |\n",
            "|    value_loss           | 112          |\n",
            "------------------------------------------\n",
            "Num timesteps: 5776000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 284.49\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 210          |\n",
            "|    ep_rew_mean          | 286          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1434         |\n",
            "|    iterations           | 353          |\n",
            "|    time_elapsed         | 4030         |\n",
            "|    total_timesteps      | 5783552      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041043316 |\n",
            "|    clip_fraction        | 0.0461       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.441       |\n",
            "|    explained_variance   | 0.983        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.76         |\n",
            "|    n_updates            | 1408         |\n",
            "|    policy_gradient_loss | 0.000299     |\n",
            "|    value_loss           | 73.3         |\n",
            "------------------------------------------\n",
            "Num timesteps: 5792000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 284.92\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 204          |\n",
            "|    ep_rew_mean          | 286          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1435         |\n",
            "|    iterations           | 354          |\n",
            "|    time_elapsed         | 4039         |\n",
            "|    total_timesteps      | 5799936      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0036545477 |\n",
            "|    clip_fraction        | 0.0443       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.437       |\n",
            "|    explained_variance   | 0.983        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 183          |\n",
            "|    n_updates            | 1412         |\n",
            "|    policy_gradient_loss | 0.00115      |\n",
            "|    value_loss           | 68.3         |\n",
            "------------------------------------------\n",
            "Num timesteps: 5808000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 282.31\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 219          |\n",
            "|    ep_rew_mean          | 283          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1436         |\n",
            "|    iterations           | 355          |\n",
            "|    time_elapsed         | 4049         |\n",
            "|    total_timesteps      | 5816320      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0036093502 |\n",
            "|    clip_fraction        | 0.0385       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.439       |\n",
            "|    explained_variance   | 0.996        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.61         |\n",
            "|    n_updates            | 1416         |\n",
            "|    policy_gradient_loss | 0.000995     |\n",
            "|    value_loss           | 9.39         |\n",
            "------------------------------------------\n",
            "Num timesteps: 5824000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 280.75\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 199         |\n",
            "|    ep_rew_mean          | 277         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1437        |\n",
            "|    iterations           | 356         |\n",
            "|    time_elapsed         | 4057        |\n",
            "|    total_timesteps      | 5832704     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003526255 |\n",
            "|    clip_fraction        | 0.0466      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.439      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.28        |\n",
            "|    n_updates            | 1420        |\n",
            "|    policy_gradient_loss | 0.00044     |\n",
            "|    value_loss           | 2.39        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 5840000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 278.30\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 199          |\n",
            "|    ep_rew_mean          | 280          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1438         |\n",
            "|    iterations           | 357          |\n",
            "|    time_elapsed         | 4067         |\n",
            "|    total_timesteps      | 5849088      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0033879653 |\n",
            "|    clip_fraction        | 0.0313       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.439       |\n",
            "|    explained_variance   | 0.968        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.16         |\n",
            "|    n_updates            | 1424         |\n",
            "|    policy_gradient_loss | 0.000502     |\n",
            "|    value_loss           | 126          |\n",
            "------------------------------------------\n",
            "Num timesteps: 5856000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 281.63\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 202         |\n",
            "|    ep_rew_mean          | 281         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1438        |\n",
            "|    iterations           | 358         |\n",
            "|    time_elapsed         | 4077        |\n",
            "|    total_timesteps      | 5865472     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003581524 |\n",
            "|    clip_fraction        | 0.0338      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.431      |\n",
            "|    explained_variance   | 0.965       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.61        |\n",
            "|    n_updates            | 1428        |\n",
            "|    policy_gradient_loss | 0.000595    |\n",
            "|    value_loss           | 122         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 5872000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 278.83\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 209         |\n",
            "|    ep_rew_mean          | 279         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1439        |\n",
            "|    iterations           | 359         |\n",
            "|    time_elapsed         | 4086        |\n",
            "|    total_timesteps      | 5881856     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003238691 |\n",
            "|    clip_fraction        | 0.0424      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.43       |\n",
            "|    explained_variance   | 0.974       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 15.4        |\n",
            "|    n_updates            | 1432        |\n",
            "|    policy_gradient_loss | 0.000182    |\n",
            "|    value_loss           | 97          |\n",
            "-----------------------------------------\n",
            "Num timesteps: 5888000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 282.42\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | 283          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1440         |\n",
            "|    iterations           | 360          |\n",
            "|    time_elapsed         | 4095         |\n",
            "|    total_timesteps      | 5898240      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038783508 |\n",
            "|    clip_fraction        | 0.0384       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.422       |\n",
            "|    explained_variance   | 0.957        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 49.7         |\n",
            "|    n_updates            | 1436         |\n",
            "|    policy_gradient_loss | 0.000353     |\n",
            "|    value_loss           | 152          |\n",
            "------------------------------------------\n",
            "Num timesteps: 5904000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 278.08\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 210          |\n",
            "|    ep_rew_mean          | 278          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1441         |\n",
            "|    iterations           | 361          |\n",
            "|    time_elapsed         | 4104         |\n",
            "|    total_timesteps      | 5914624      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0053766607 |\n",
            "|    clip_fraction        | 0.0585       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.436       |\n",
            "|    explained_variance   | 0.969        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.71         |\n",
            "|    n_updates            | 1440         |\n",
            "|    policy_gradient_loss | 0.00145      |\n",
            "|    value_loss           | 105          |\n",
            "------------------------------------------\n",
            "Num timesteps: 5920000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 274.74\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 211          |\n",
            "|    ep_rew_mean          | 280          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1441         |\n",
            "|    iterations           | 362          |\n",
            "|    time_elapsed         | 4113         |\n",
            "|    total_timesteps      | 5931008      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040701716 |\n",
            "|    clip_fraction        | 0.0518       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.445       |\n",
            "|    explained_variance   | 0.953        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.95         |\n",
            "|    n_updates            | 1444         |\n",
            "|    policy_gradient_loss | 0.000244     |\n",
            "|    value_loss           | 197          |\n",
            "------------------------------------------\n",
            "Num timesteps: 5936000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 282.06\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 211         |\n",
            "|    ep_rew_mean          | 288         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1442        |\n",
            "|    iterations           | 363         |\n",
            "|    time_elapsed         | 4122        |\n",
            "|    total_timesteps      | 5947392     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004010846 |\n",
            "|    clip_fraction        | 0.0512      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.435      |\n",
            "|    explained_variance   | 0.976       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 138         |\n",
            "|    n_updates            | 1448        |\n",
            "|    policy_gradient_loss | 0.000194    |\n",
            "|    value_loss           | 77.4        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 5952000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 281.28\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 212          |\n",
            "|    ep_rew_mean          | 279          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1443         |\n",
            "|    iterations           | 364          |\n",
            "|    time_elapsed         | 4131         |\n",
            "|    total_timesteps      | 5963776      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0033250705 |\n",
            "|    clip_fraction        | 0.0437       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.426       |\n",
            "|    explained_variance   | 0.978        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.64         |\n",
            "|    n_updates            | 1452         |\n",
            "|    policy_gradient_loss | 0.000569     |\n",
            "|    value_loss           | 87.1         |\n",
            "------------------------------------------\n",
            "Num timesteps: 5968000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 277.28\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 215         |\n",
            "|    ep_rew_mean          | 270         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1444        |\n",
            "|    iterations           | 365         |\n",
            "|    time_elapsed         | 4141        |\n",
            "|    total_timesteps      | 5980160     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003174448 |\n",
            "|    clip_fraction        | 0.0413      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.428      |\n",
            "|    explained_variance   | 0.969       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.86        |\n",
            "|    n_updates            | 1456        |\n",
            "|    policy_gradient_loss | -0.000291   |\n",
            "|    value_loss           | 114         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 5984000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 268.57\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 201          |\n",
            "|    ep_rew_mean          | 272          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1444         |\n",
            "|    iterations           | 366          |\n",
            "|    time_elapsed         | 4150         |\n",
            "|    total_timesteps      | 5996544      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0031915288 |\n",
            "|    clip_fraction        | 0.0333       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.441       |\n",
            "|    explained_variance   | 0.924        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 91.1         |\n",
            "|    n_updates            | 1460         |\n",
            "|    policy_gradient_loss | 8.82e-05     |\n",
            "|    value_loss           | 323          |\n",
            "------------------------------------------\n",
            "Num timesteps: 6000000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 278.09\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 217          |\n",
            "|    ep_rew_mean          | 279          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1445         |\n",
            "|    iterations           | 367          |\n",
            "|    time_elapsed         | 4159         |\n",
            "|    total_timesteps      | 6012928      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035014246 |\n",
            "|    clip_fraction        | 0.0352       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.425       |\n",
            "|    explained_variance   | 0.927        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 25.3         |\n",
            "|    n_updates            | 1464         |\n",
            "|    policy_gradient_loss | 0.000189     |\n",
            "|    value_loss           | 285          |\n",
            "------------------------------------------\n",
            "Num timesteps: 6016000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 279.50\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 207         |\n",
            "|    ep_rew_mean          | 279         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1446        |\n",
            "|    iterations           | 368         |\n",
            "|    time_elapsed         | 4168        |\n",
            "|    total_timesteps      | 6029312     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003978048 |\n",
            "|    clip_fraction        | 0.0508      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.437      |\n",
            "|    explained_variance   | 0.982       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.56        |\n",
            "|    n_updates            | 1468        |\n",
            "|    policy_gradient_loss | 0.00185     |\n",
            "|    value_loss           | 66          |\n",
            "-----------------------------------------\n",
            "Num timesteps: 6032000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 281.27\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 213          |\n",
            "|    ep_rew_mean          | 284          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1447         |\n",
            "|    iterations           | 369          |\n",
            "|    time_elapsed         | 4177         |\n",
            "|    total_timesteps      | 6045696      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044801524 |\n",
            "|    clip_fraction        | 0.0417       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.434       |\n",
            "|    explained_variance   | 0.967        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 58.8         |\n",
            "|    n_updates            | 1472         |\n",
            "|    policy_gradient_loss | 0.000433     |\n",
            "|    value_loss           | 134          |\n",
            "------------------------------------------\n",
            "Num timesteps: 6048000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 284.47\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 220          |\n",
            "|    ep_rew_mean          | 284          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1447         |\n",
            "|    iterations           | 370          |\n",
            "|    time_elapsed         | 4188         |\n",
            "|    total_timesteps      | 6062080      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0032977164 |\n",
            "|    clip_fraction        | 0.051        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.445       |\n",
            "|    explained_variance   | 0.997        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.958        |\n",
            "|    n_updates            | 1476         |\n",
            "|    policy_gradient_loss | 0.000864     |\n",
            "|    value_loss           | 6.22         |\n",
            "------------------------------------------\n",
            "Num timesteps: 6064000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 282.58\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 229         |\n",
            "|    ep_rew_mean          | 280         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1447        |\n",
            "|    iterations           | 371         |\n",
            "|    time_elapsed         | 4198        |\n",
            "|    total_timesteps      | 6078464     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004325114 |\n",
            "|    clip_fraction        | 0.0453      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.441      |\n",
            "|    explained_variance   | 0.982       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.85        |\n",
            "|    n_updates            | 1480        |\n",
            "|    policy_gradient_loss | 0.000321    |\n",
            "|    value_loss           | 73          |\n",
            "-----------------------------------------\n",
            "Num timesteps: 6080000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 279.35\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 234          |\n",
            "|    ep_rew_mean          | 282          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1448         |\n",
            "|    iterations           | 372          |\n",
            "|    time_elapsed         | 4207         |\n",
            "|    total_timesteps      | 6094848      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0036904614 |\n",
            "|    clip_fraction        | 0.0367       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.482       |\n",
            "|    explained_variance   | 0.968        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.85         |\n",
            "|    n_updates            | 1484         |\n",
            "|    policy_gradient_loss | 5.56e-05     |\n",
            "|    value_loss           | 153          |\n",
            "------------------------------------------\n",
            "Num timesteps: 6096000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 282.29\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | 282          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1449         |\n",
            "|    iterations           | 373          |\n",
            "|    time_elapsed         | 4216         |\n",
            "|    total_timesteps      | 6111232      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0049319677 |\n",
            "|    clip_fraction        | 0.058        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.451       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.6          |\n",
            "|    n_updates            | 1488         |\n",
            "|    policy_gradient_loss | 0.00119      |\n",
            "|    value_loss           | 2.09         |\n",
            "------------------------------------------\n",
            "Num timesteps: 6112000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 281.59\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 199         |\n",
            "|    ep_rew_mean          | 282         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1450        |\n",
            "|    iterations           | 374         |\n",
            "|    time_elapsed         | 4225        |\n",
            "|    total_timesteps      | 6127616     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003269726 |\n",
            "|    clip_fraction        | 0.0301      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.449      |\n",
            "|    explained_variance   | 0.949       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 399         |\n",
            "|    n_updates            | 1492        |\n",
            "|    policy_gradient_loss | -0.000226   |\n",
            "|    value_loss           | 210         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 6128000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 281.61\n",
            "Num timesteps: 6144000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 284.78\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 203          |\n",
            "|    ep_rew_mean          | 285          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1451         |\n",
            "|    iterations           | 375          |\n",
            "|    time_elapsed         | 4233         |\n",
            "|    total_timesteps      | 6144000      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040233163 |\n",
            "|    clip_fraction        | 0.043        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.443       |\n",
            "|    explained_variance   | 0.966        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 34.5         |\n",
            "|    n_updates            | 1496         |\n",
            "|    policy_gradient_loss | -2.36e-05    |\n",
            "|    value_loss           | 120          |\n",
            "------------------------------------------\n",
            "Num timesteps: 6160000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 275.86\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 205          |\n",
            "|    ep_rew_mean          | 276          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1451         |\n",
            "|    iterations           | 376          |\n",
            "|    time_elapsed         | 4244         |\n",
            "|    total_timesteps      | 6160384      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0037253243 |\n",
            "|    clip_fraction        | 0.0551       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.424       |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.13         |\n",
            "|    n_updates            | 1500         |\n",
            "|    policy_gradient_loss | 0.00109      |\n",
            "|    value_loss           | 3.95         |\n",
            "------------------------------------------\n",
            "Num timesteps: 6176000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 276.19\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 199          |\n",
            "|    ep_rew_mean          | 277          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1452         |\n",
            "|    iterations           | 377          |\n",
            "|    time_elapsed         | 4253         |\n",
            "|    total_timesteps      | 6176768      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019179609 |\n",
            "|    clip_fraction        | 0.0126       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.457       |\n",
            "|    explained_variance   | 0.922        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 185          |\n",
            "|    n_updates            | 1504         |\n",
            "|    policy_gradient_loss | -0.000132    |\n",
            "|    value_loss           | 336          |\n",
            "------------------------------------------\n",
            "Num timesteps: 6192000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 275.53\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 198          |\n",
            "|    ep_rew_mean          | 275          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1453         |\n",
            "|    iterations           | 378          |\n",
            "|    time_elapsed         | 4262         |\n",
            "|    total_timesteps      | 6193152      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0028693613 |\n",
            "|    clip_fraction        | 0.0283       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.441       |\n",
            "|    explained_variance   | 0.935        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.57         |\n",
            "|    n_updates            | 1508         |\n",
            "|    policy_gradient_loss | 0.000679     |\n",
            "|    value_loss           | 159          |\n",
            "------------------------------------------\n",
            "Num timesteps: 6208000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 277.73\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 212          |\n",
            "|    ep_rew_mean          | 279          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1453         |\n",
            "|    iterations           | 379          |\n",
            "|    time_elapsed         | 4271         |\n",
            "|    total_timesteps      | 6209536      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0029909378 |\n",
            "|    clip_fraction        | 0.0312       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.453       |\n",
            "|    explained_variance   | 0.926        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 34.9         |\n",
            "|    n_updates            | 1512         |\n",
            "|    policy_gradient_loss | 0.000403     |\n",
            "|    value_loss           | 256          |\n",
            "------------------------------------------\n",
            "Num timesteps: 6224000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 276.99\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 195          |\n",
            "|    ep_rew_mean          | 274          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1454         |\n",
            "|    iterations           | 380          |\n",
            "|    time_elapsed         | 4280         |\n",
            "|    total_timesteps      | 6225920      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0032221486 |\n",
            "|    clip_fraction        | 0.0465       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.451       |\n",
            "|    explained_variance   | 0.967        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.29         |\n",
            "|    n_updates            | 1516         |\n",
            "|    policy_gradient_loss | 0.000683     |\n",
            "|    value_loss           | 124          |\n",
            "------------------------------------------\n",
            "Num timesteps: 6240000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 268.87\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 195          |\n",
            "|    ep_rew_mean          | 274          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1455         |\n",
            "|    iterations           | 381          |\n",
            "|    time_elapsed         | 4289         |\n",
            "|    total_timesteps      | 6242304      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016763479 |\n",
            "|    clip_fraction        | 0.0112       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.435       |\n",
            "|    explained_variance   | 0.887        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 157          |\n",
            "|    n_updates            | 1520         |\n",
            "|    policy_gradient_loss | 0.00022      |\n",
            "|    value_loss           | 451          |\n",
            "------------------------------------------\n",
            "Num timesteps: 6256000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 281.50\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 218          |\n",
            "|    ep_rew_mean          | 278          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1455         |\n",
            "|    iterations           | 382          |\n",
            "|    time_elapsed         | 4298         |\n",
            "|    total_timesteps      | 6258688      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0027837781 |\n",
            "|    clip_fraction        | 0.032        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.435       |\n",
            "|    explained_variance   | 0.933        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 33.1         |\n",
            "|    n_updates            | 1524         |\n",
            "|    policy_gradient_loss | 9.26e-05     |\n",
            "|    value_loss           | 254          |\n",
            "------------------------------------------\n",
            "Num timesteps: 6272000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 279.49\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 217         |\n",
            "|    ep_rew_mean          | 279         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1456        |\n",
            "|    iterations           | 383         |\n",
            "|    time_elapsed         | 4307        |\n",
            "|    total_timesteps      | 6275072     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004495822 |\n",
            "|    clip_fraction        | 0.0675      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.458      |\n",
            "|    explained_variance   | 0.984       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 11.3        |\n",
            "|    n_updates            | 1528        |\n",
            "|    policy_gradient_loss | 0.00155     |\n",
            "|    value_loss           | 60.9        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 6288000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 279.81\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 222         |\n",
            "|    ep_rew_mean          | 277         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1457        |\n",
            "|    iterations           | 384         |\n",
            "|    time_elapsed         | 4316        |\n",
            "|    total_timesteps      | 6291456     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004137134 |\n",
            "|    clip_fraction        | 0.0371      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.459      |\n",
            "|    explained_variance   | 0.968       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 90.8        |\n",
            "|    n_updates            | 1532        |\n",
            "|    policy_gradient_loss | 0.00027     |\n",
            "|    value_loss           | 124         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 6304000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 281.13\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 209         |\n",
            "|    ep_rew_mean          | 286         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1457        |\n",
            "|    iterations           | 385         |\n",
            "|    time_elapsed         | 4326        |\n",
            "|    total_timesteps      | 6307840     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004031161 |\n",
            "|    clip_fraction        | 0.0515      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.449      |\n",
            "|    explained_variance   | 0.963       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.45        |\n",
            "|    n_updates            | 1536        |\n",
            "|    policy_gradient_loss | 7.48e-05    |\n",
            "|    value_loss           | 150         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 6320000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 285.33\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 225          |\n",
            "|    ep_rew_mean          | 286          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1458         |\n",
            "|    iterations           | 386          |\n",
            "|    time_elapsed         | 4336         |\n",
            "|    total_timesteps      | 6324224      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0045752637 |\n",
            "|    clip_fraction        | 0.0587       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.449       |\n",
            "|    explained_variance   | 0.99         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.22         |\n",
            "|    n_updates            | 1540         |\n",
            "|    policy_gradient_loss | 0.000886     |\n",
            "|    value_loss           | 15.2         |\n",
            "------------------------------------------\n",
            "Num timesteps: 6336000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 278.41\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 217         |\n",
            "|    ep_rew_mean          | 278         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1459        |\n",
            "|    iterations           | 387         |\n",
            "|    time_elapsed         | 4345        |\n",
            "|    total_timesteps      | 6340608     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003722622 |\n",
            "|    clip_fraction        | 0.052       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.451      |\n",
            "|    explained_variance   | 0.982       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 18.7        |\n",
            "|    n_updates            | 1544        |\n",
            "|    policy_gradient_loss | 0.000772    |\n",
            "|    value_loss           | 76.5        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 6352000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 278.91\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 227          |\n",
            "|    ep_rew_mean          | 279          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1459         |\n",
            "|    iterations           | 388          |\n",
            "|    time_elapsed         | 4356         |\n",
            "|    total_timesteps      | 6356992      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0033731908 |\n",
            "|    clip_fraction        | 0.0307       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.467       |\n",
            "|    explained_variance   | 0.977        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 59.2         |\n",
            "|    n_updates            | 1548         |\n",
            "|    policy_gradient_loss | 0.000345     |\n",
            "|    value_loss           | 95.2         |\n",
            "------------------------------------------\n",
            "Num timesteps: 6368000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 279.30\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 222          |\n",
            "|    ep_rew_mean          | 277          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1460         |\n",
            "|    iterations           | 389          |\n",
            "|    time_elapsed         | 4365         |\n",
            "|    total_timesteps      | 6373376      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0056125745 |\n",
            "|    clip_fraction        | 0.0586       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.468       |\n",
            "|    explained_variance   | 0.987        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.9          |\n",
            "|    n_updates            | 1552         |\n",
            "|    policy_gradient_loss | 0.00127      |\n",
            "|    value_loss           | 46.8         |\n",
            "------------------------------------------\n",
            "Num timesteps: 6384000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 277.21\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 205       |\n",
            "|    ep_rew_mean          | 279       |\n",
            "| time/                   |           |\n",
            "|    fps                  | 1460      |\n",
            "|    iterations           | 390       |\n",
            "|    time_elapsed         | 4374      |\n",
            "|    total_timesteps      | 6389760   |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0041888 |\n",
            "|    clip_fraction        | 0.0378    |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -0.45     |\n",
            "|    explained_variance   | 0.954     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 133       |\n",
            "|    n_updates            | 1556      |\n",
            "|    policy_gradient_loss | 0.000221  |\n",
            "|    value_loss           | 210       |\n",
            "---------------------------------------\n",
            "Num timesteps: 6400000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 286.44\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 207        |\n",
            "|    ep_rew_mean          | 287        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 1461       |\n",
            "|    iterations           | 391        |\n",
            "|    time_elapsed         | 4383       |\n",
            "|    total_timesteps      | 6406144    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00426561 |\n",
            "|    clip_fraction        | 0.0428     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.442     |\n",
            "|    explained_variance   | 0.97       |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 3.07       |\n",
            "|    n_updates            | 1560       |\n",
            "|    policy_gradient_loss | 0.00043    |\n",
            "|    value_loss           | 146        |\n",
            "----------------------------------------\n",
            "Num timesteps: 6416000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 282.83\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 216         |\n",
            "|    ep_rew_mean          | 279         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1461        |\n",
            "|    iterations           | 392         |\n",
            "|    time_elapsed         | 4393        |\n",
            "|    total_timesteps      | 6422528     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003827022 |\n",
            "|    clip_fraction        | 0.0472      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.436      |\n",
            "|    explained_variance   | 0.997       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.86        |\n",
            "|    n_updates            | 1564        |\n",
            "|    policy_gradient_loss | 0.00196     |\n",
            "|    value_loss           | 5.48        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 6432000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 279.02\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 209          |\n",
            "|    ep_rew_mean          | 281          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1462         |\n",
            "|    iterations           | 393          |\n",
            "|    time_elapsed         | 4401         |\n",
            "|    total_timesteps      | 6438912      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0030846018 |\n",
            "|    clip_fraction        | 0.0258       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.449       |\n",
            "|    explained_variance   | 0.945        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 100          |\n",
            "|    n_updates            | 1568         |\n",
            "|    policy_gradient_loss | -0.000179    |\n",
            "|    value_loss           | 274          |\n",
            "------------------------------------------\n",
            "Num timesteps: 6448000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 282.22\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 201         |\n",
            "|    ep_rew_mean          | 280         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1463        |\n",
            "|    iterations           | 394         |\n",
            "|    time_elapsed         | 4410        |\n",
            "|    total_timesteps      | 6455296     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004119664 |\n",
            "|    clip_fraction        | 0.0499      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.434      |\n",
            "|    explained_variance   | 0.982       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 17.6        |\n",
            "|    n_updates            | 1572        |\n",
            "|    policy_gradient_loss | -0.000158   |\n",
            "|    value_loss           | 66.7        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 6464000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 283.13\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 196         |\n",
            "|    ep_rew_mean          | 283         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1464        |\n",
            "|    iterations           | 395         |\n",
            "|    time_elapsed         | 4420        |\n",
            "|    total_timesteps      | 6471680     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003481118 |\n",
            "|    clip_fraction        | 0.0363      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.455      |\n",
            "|    explained_variance   | 0.954       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 97          |\n",
            "|    n_updates            | 1576        |\n",
            "|    policy_gradient_loss | 0.000448    |\n",
            "|    value_loss           | 195         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 6480000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 283.75\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 212          |\n",
            "|    ep_rew_mean          | 281          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1464         |\n",
            "|    iterations           | 396          |\n",
            "|    time_elapsed         | 4429         |\n",
            "|    total_timesteps      | 6488064      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038760144 |\n",
            "|    clip_fraction        | 0.0542       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.45        |\n",
            "|    explained_variance   | 0.996        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.7          |\n",
            "|    n_updates            | 1580         |\n",
            "|    policy_gradient_loss | 0.00136      |\n",
            "|    value_loss           | 4.82         |\n",
            "------------------------------------------\n",
            "Num timesteps: 6496000\n",
            "Best mean reward: 288.05 - Last mean reward per episode: 288.59\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 201          |\n",
            "|    ep_rew_mean          | 289          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1465         |\n",
            "|    iterations           | 397          |\n",
            "|    time_elapsed         | 4438         |\n",
            "|    total_timesteps      | 6504448      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034160214 |\n",
            "|    clip_fraction        | 0.0401       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.46        |\n",
            "|    explained_variance   | 0.972        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 76.3         |\n",
            "|    n_updates            | 1584         |\n",
            "|    policy_gradient_loss | 0.000738     |\n",
            "|    value_loss           | 125          |\n",
            "------------------------------------------\n",
            "Num timesteps: 6512000\n",
            "Best mean reward: 288.59 - Last mean reward per episode: 279.40\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 195          |\n",
            "|    ep_rew_mean          | 271          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1466         |\n",
            "|    iterations           | 398          |\n",
            "|    time_elapsed         | 4446         |\n",
            "|    total_timesteps      | 6520832      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0031759024 |\n",
            "|    clip_fraction        | 0.0459       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.442       |\n",
            "|    explained_variance   | 0.983        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.42         |\n",
            "|    n_updates            | 1588         |\n",
            "|    policy_gradient_loss | 0.000801     |\n",
            "|    value_loss           | 75.8         |\n",
            "------------------------------------------\n",
            "Num timesteps: 6528000\n",
            "Best mean reward: 288.59 - Last mean reward per episode: 272.35\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 194          |\n",
            "|    ep_rew_mean          | 278          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1467         |\n",
            "|    iterations           | 399          |\n",
            "|    time_elapsed         | 4455         |\n",
            "|    total_timesteps      | 6537216      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014590673 |\n",
            "|    clip_fraction        | 0.012        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.448       |\n",
            "|    explained_variance   | 0.904        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 98.3         |\n",
            "|    n_updates            | 1592         |\n",
            "|    policy_gradient_loss | -0.000488    |\n",
            "|    value_loss           | 366          |\n",
            "------------------------------------------\n",
            "Num timesteps: 6544000\n",
            "Best mean reward: 288.59 - Last mean reward per episode: 282.65\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | 288         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1467        |\n",
            "|    iterations           | 400         |\n",
            "|    time_elapsed         | 4464        |\n",
            "|    total_timesteps      | 6553600     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004542291 |\n",
            "|    clip_fraction        | 0.0388      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.444      |\n",
            "|    explained_variance   | 0.967       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 36.2        |\n",
            "|    n_updates            | 1596        |\n",
            "|    policy_gradient_loss | 0.000392    |\n",
            "|    value_loss           | 118         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 6560000\n",
            "Best mean reward: 288.59 - Last mean reward per episode: 289.80\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 199          |\n",
            "|    ep_rew_mean          | 290          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1468         |\n",
            "|    iterations           | 401          |\n",
            "|    time_elapsed         | 4473         |\n",
            "|    total_timesteps      | 6569984      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035613468 |\n",
            "|    clip_fraction        | 0.0491       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.451       |\n",
            "|    explained_variance   | 0.994        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 12.3         |\n",
            "|    n_updates            | 1600         |\n",
            "|    policy_gradient_loss | 0.000805     |\n",
            "|    value_loss           | 10.2         |\n",
            "------------------------------------------\n",
            "Num timesteps: 6576000\n",
            "Best mean reward: 289.80 - Last mean reward per episode: 291.48\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 197         |\n",
            "|    ep_rew_mean          | 286         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1469        |\n",
            "|    iterations           | 402         |\n",
            "|    time_elapsed         | 4481        |\n",
            "|    total_timesteps      | 6586368     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004227177 |\n",
            "|    clip_fraction        | 0.052       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.455      |\n",
            "|    explained_variance   | 0.998       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.66        |\n",
            "|    n_updates            | 1604        |\n",
            "|    policy_gradient_loss | 0.000785    |\n",
            "|    value_loss           | 4.89        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 6592000\n",
            "Best mean reward: 291.48 - Last mean reward per episode: 283.83\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 204          |\n",
            "|    ep_rew_mean          | 280          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1470         |\n",
            "|    iterations           | 403          |\n",
            "|    time_elapsed         | 4490         |\n",
            "|    total_timesteps      | 6602752      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0029910123 |\n",
            "|    clip_fraction        | 0.0399       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.451       |\n",
            "|    explained_variance   | 0.983        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.64         |\n",
            "|    n_updates            | 1608         |\n",
            "|    policy_gradient_loss | 0.000323     |\n",
            "|    value_loss           | 75.1         |\n",
            "------------------------------------------\n",
            "Num timesteps: 6608000\n",
            "Best mean reward: 291.48 - Last mean reward per episode: 283.32\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 202          |\n",
            "|    ep_rew_mean          | 283          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1471         |\n",
            "|    iterations           | 404          |\n",
            "|    time_elapsed         | 4499         |\n",
            "|    total_timesteps      | 6619136      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0033088066 |\n",
            "|    clip_fraction        | 0.0454       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.467       |\n",
            "|    explained_variance   | 0.983        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 348          |\n",
            "|    n_updates            | 1612         |\n",
            "|    policy_gradient_loss | 0.000184     |\n",
            "|    value_loss           | 79.3         |\n",
            "------------------------------------------\n",
            "Num timesteps: 6624000\n",
            "Best mean reward: 291.48 - Last mean reward per episode: 284.54\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 193       |\n",
            "|    ep_rew_mean          | 285       |\n",
            "| time/                   |           |\n",
            "|    fps                  | 1471      |\n",
            "|    iterations           | 405       |\n",
            "|    time_elapsed         | 4508      |\n",
            "|    total_timesteps      | 6635520   |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0040694 |\n",
            "|    clip_fraction        | 0.0424    |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -0.444    |\n",
            "|    explained_variance   | 0.969     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 3.87      |\n",
            "|    n_updates            | 1616      |\n",
            "|    policy_gradient_loss | -0.000649 |\n",
            "|    value_loss           | 123       |\n",
            "---------------------------------------\n",
            "Num timesteps: 6640000\n",
            "Best mean reward: 291.48 - Last mean reward per episode: 284.62\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 194          |\n",
            "|    ep_rew_mean          | 282          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1472         |\n",
            "|    iterations           | 406          |\n",
            "|    time_elapsed         | 4517         |\n",
            "|    total_timesteps      | 6651904      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0043102633 |\n",
            "|    clip_fraction        | 0.0569       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.469       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.447        |\n",
            "|    n_updates            | 1620         |\n",
            "|    policy_gradient_loss | 0.000259     |\n",
            "|    value_loss           | 1.74         |\n",
            "------------------------------------------\n",
            "Num timesteps: 6656000\n",
            "Best mean reward: 291.48 - Last mean reward per episode: 283.03\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 210         |\n",
            "|    ep_rew_mean          | 285         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1473        |\n",
            "|    iterations           | 407         |\n",
            "|    time_elapsed         | 4526        |\n",
            "|    total_timesteps      | 6668288     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004179777 |\n",
            "|    clip_fraction        | 0.0362      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.465      |\n",
            "|    explained_variance   | 0.985       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 16.5        |\n",
            "|    n_updates            | 1624        |\n",
            "|    policy_gradient_loss | -0.000264   |\n",
            "|    value_loss           | 61.3        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 6672000\n",
            "Best mean reward: 291.48 - Last mean reward per episode: 286.61\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 203          |\n",
            "|    ep_rew_mean          | 290          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1474         |\n",
            "|    iterations           | 408          |\n",
            "|    time_elapsed         | 4534         |\n",
            "|    total_timesteps      | 6684672      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0042821476 |\n",
            "|    clip_fraction        | 0.0459       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.472       |\n",
            "|    explained_variance   | 0.986        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.07         |\n",
            "|    n_updates            | 1628         |\n",
            "|    policy_gradient_loss | -0.000184    |\n",
            "|    value_loss           | 67.9         |\n",
            "------------------------------------------\n",
            "Num timesteps: 6688000\n",
            "Best mean reward: 291.48 - Last mean reward per episode: 289.00\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 197         |\n",
            "|    ep_rew_mean          | 285         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1474        |\n",
            "|    iterations           | 409         |\n",
            "|    time_elapsed         | 4544        |\n",
            "|    total_timesteps      | 6701056     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004144831 |\n",
            "|    clip_fraction        | 0.0538      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.456      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.749       |\n",
            "|    n_updates            | 1632        |\n",
            "|    policy_gradient_loss | -0.000976   |\n",
            "|    value_loss           | 2.19        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 6704000\n",
            "Best mean reward: 291.48 - Last mean reward per episode: 283.32\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 202          |\n",
            "|    ep_rew_mean          | 286          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1475         |\n",
            "|    iterations           | 410          |\n",
            "|    time_elapsed         | 4553         |\n",
            "|    total_timesteps      | 6717440      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0036547564 |\n",
            "|    clip_fraction        | 0.0395       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.464       |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.37         |\n",
            "|    n_updates            | 1636         |\n",
            "|    policy_gradient_loss | 2.36e-05     |\n",
            "|    value_loss           | 5.32         |\n",
            "------------------------------------------\n",
            "Num timesteps: 6720000\n",
            "Best mean reward: 291.48 - Last mean reward per episode: 285.18\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 193          |\n",
            "|    ep_rew_mean          | 282          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1475         |\n",
            "|    iterations           | 411          |\n",
            "|    time_elapsed         | 4562         |\n",
            "|    total_timesteps      | 6733824      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034408567 |\n",
            "|    clip_fraction        | 0.0459       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.47        |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.709        |\n",
            "|    n_updates            | 1640         |\n",
            "|    policy_gradient_loss | 0.000501     |\n",
            "|    value_loss           | 1.65         |\n",
            "------------------------------------------\n",
            "Num timesteps: 6736000\n",
            "Best mean reward: 291.48 - Last mean reward per episode: 282.16\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 198          |\n",
            "|    ep_rew_mean          | 285          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1476         |\n",
            "|    iterations           | 412          |\n",
            "|    time_elapsed         | 4570         |\n",
            "|    total_timesteps      | 6750208      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0031589991 |\n",
            "|    clip_fraction        | 0.0375       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.46        |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.8          |\n",
            "|    n_updates            | 1644         |\n",
            "|    policy_gradient_loss | 0.00077      |\n",
            "|    value_loss           | 1.96         |\n",
            "------------------------------------------\n",
            "Num timesteps: 6752000\n",
            "Best mean reward: 291.48 - Last mean reward per episode: 285.18\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 199          |\n",
            "|    ep_rew_mean          | 285          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1477         |\n",
            "|    iterations           | 413          |\n",
            "|    time_elapsed         | 4579         |\n",
            "|    total_timesteps      | 6766592      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041306103 |\n",
            "|    clip_fraction        | 0.0555       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.479       |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.844        |\n",
            "|    n_updates            | 1648         |\n",
            "|    policy_gradient_loss | -0.000866    |\n",
            "|    value_loss           | 5.72         |\n",
            "------------------------------------------\n",
            "Num timesteps: 6768000\n",
            "Best mean reward: 291.48 - Last mean reward per episode: 284.62\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 192          |\n",
            "|    ep_rew_mean          | 289          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1478         |\n",
            "|    iterations           | 414          |\n",
            "|    time_elapsed         | 4588         |\n",
            "|    total_timesteps      | 6782976      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034090267 |\n",
            "|    clip_fraction        | 0.0299       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.451       |\n",
            "|    explained_variance   | 0.982        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.07         |\n",
            "|    n_updates            | 1652         |\n",
            "|    policy_gradient_loss | 0.000277     |\n",
            "|    value_loss           | 40.9         |\n",
            "------------------------------------------\n",
            "Num timesteps: 6784000\n",
            "Best mean reward: 291.48 - Last mean reward per episode: 288.31\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 198         |\n",
            "|    ep_rew_mean          | 293         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1479        |\n",
            "|    iterations           | 415         |\n",
            "|    time_elapsed         | 4597        |\n",
            "|    total_timesteps      | 6799360     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004355301 |\n",
            "|    clip_fraction        | 0.0573      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.461      |\n",
            "|    explained_variance   | 0.994       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.4         |\n",
            "|    n_updates            | 1656        |\n",
            "|    policy_gradient_loss | 0.000108    |\n",
            "|    value_loss           | 6.89        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 6800000\n",
            "Best mean reward: 291.48 - Last mean reward per episode: 292.65\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 192          |\n",
            "|    ep_rew_mean          | 289          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1479         |\n",
            "|    iterations           | 416          |\n",
            "|    time_elapsed         | 4606         |\n",
            "|    total_timesteps      | 6815744      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0048969183 |\n",
            "|    clip_fraction        | 0.0544       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.461       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.01         |\n",
            "|    n_updates            | 1660         |\n",
            "|    policy_gradient_loss | 0.000599     |\n",
            "|    value_loss           | 2.58         |\n",
            "------------------------------------------\n",
            "Num timesteps: 6816000\n",
            "Best mean reward: 292.65 - Last mean reward per episode: 288.34\n",
            "Num timesteps: 6832000\n",
            "Best mean reward: 292.65 - Last mean reward per episode: 287.43\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 193          |\n",
            "|    ep_rew_mean          | 288          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1480         |\n",
            "|    iterations           | 417          |\n",
            "|    time_elapsed         | 4615         |\n",
            "|    total_timesteps      | 6832128      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0039422857 |\n",
            "|    clip_fraction        | 0.0453       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.45        |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.611        |\n",
            "|    n_updates            | 1664         |\n",
            "|    policy_gradient_loss | 0.000127     |\n",
            "|    value_loss           | 2.37         |\n",
            "------------------------------------------\n",
            "Num timesteps: 6848000\n",
            "Best mean reward: 292.65 - Last mean reward per episode: 288.46\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 196          |\n",
            "|    ep_rew_mean          | 289          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1481         |\n",
            "|    iterations           | 418          |\n",
            "|    time_elapsed         | 4623         |\n",
            "|    total_timesteps      | 6848512      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0039464748 |\n",
            "|    clip_fraction        | 0.0431       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.452       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.92         |\n",
            "|    n_updates            | 1668         |\n",
            "|    policy_gradient_loss | -1.42e-05    |\n",
            "|    value_loss           | 3.36         |\n",
            "------------------------------------------\n",
            "Num timesteps: 6864000\n",
            "Best mean reward: 292.65 - Last mean reward per episode: 286.99\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 199          |\n",
            "|    ep_rew_mean          | 287          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1481         |\n",
            "|    iterations           | 419          |\n",
            "|    time_elapsed         | 4633         |\n",
            "|    total_timesteps      | 6864896      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0039141662 |\n",
            "|    clip_fraction        | 0.0468       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.444       |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.8          |\n",
            "|    n_updates            | 1672         |\n",
            "|    policy_gradient_loss | 0.000459     |\n",
            "|    value_loss           | 5.52         |\n",
            "------------------------------------------\n",
            "Num timesteps: 6880000\n",
            "Best mean reward: 292.65 - Last mean reward per episode: 279.03\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 191       |\n",
            "|    ep_rew_mean          | 279       |\n",
            "| time/                   |           |\n",
            "|    fps                  | 1482      |\n",
            "|    iterations           | 420       |\n",
            "|    time_elapsed         | 4641      |\n",
            "|    total_timesteps      | 6881280   |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0027627 |\n",
            "|    clip_fraction        | 0.0341    |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -0.44     |\n",
            "|    explained_variance   | 0.98      |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 14.2      |\n",
            "|    n_updates            | 1676      |\n",
            "|    policy_gradient_loss | -0.000241 |\n",
            "|    value_loss           | 43.8      |\n",
            "---------------------------------------\n",
            "Num timesteps: 6896000\n",
            "Best mean reward: 292.65 - Last mean reward per episode: 284.55\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 197          |\n",
            "|    ep_rew_mean          | 284          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1483         |\n",
            "|    iterations           | 421          |\n",
            "|    time_elapsed         | 4650         |\n",
            "|    total_timesteps      | 6897664      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0031697066 |\n",
            "|    clip_fraction        | 0.0285       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.444       |\n",
            "|    explained_variance   | 0.958        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 35.6         |\n",
            "|    n_updates            | 1680         |\n",
            "|    policy_gradient_loss | 0.000287     |\n",
            "|    value_loss           | 170          |\n",
            "------------------------------------------\n",
            "Num timesteps: 6912000\n",
            "Best mean reward: 292.65 - Last mean reward per episode: 285.40\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 199          |\n",
            "|    ep_rew_mean          | 284          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1483         |\n",
            "|    iterations           | 422          |\n",
            "|    time_elapsed         | 4659         |\n",
            "|    total_timesteps      | 6914048      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0062762327 |\n",
            "|    clip_fraction        | 0.0575       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.448       |\n",
            "|    explained_variance   | 0.985        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.934        |\n",
            "|    n_updates            | 1684         |\n",
            "|    policy_gradient_loss | 0.000605     |\n",
            "|    value_loss           | 66.1         |\n",
            "------------------------------------------\n",
            "Num timesteps: 6928000\n",
            "Best mean reward: 292.65 - Last mean reward per episode: 284.82\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 191          |\n",
            "|    ep_rew_mean          | 284          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1484         |\n",
            "|    iterations           | 423          |\n",
            "|    time_elapsed         | 4668         |\n",
            "|    total_timesteps      | 6930432      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0031257786 |\n",
            "|    clip_fraction        | 0.047        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.452       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.906        |\n",
            "|    n_updates            | 1688         |\n",
            "|    policy_gradient_loss | 0.000855     |\n",
            "|    value_loss           | 1.67         |\n",
            "------------------------------------------\n",
            "Num timesteps: 6944000\n",
            "Best mean reward: 292.65 - Last mean reward per episode: 282.75\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 192          |\n",
            "|    ep_rew_mean          | 283          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1485         |\n",
            "|    iterations           | 424          |\n",
            "|    time_elapsed         | 4677         |\n",
            "|    total_timesteps      | 6946816      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0043184147 |\n",
            "|    clip_fraction        | 0.05         |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.462       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.734        |\n",
            "|    n_updates            | 1692         |\n",
            "|    policy_gradient_loss | 0.000291     |\n",
            "|    value_loss           | 2.36         |\n",
            "------------------------------------------\n",
            "Num timesteps: 6960000\n",
            "Best mean reward: 292.65 - Last mean reward per episode: 285.64\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 196         |\n",
            "|    ep_rew_mean          | 286         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1485        |\n",
            "|    iterations           | 425         |\n",
            "|    time_elapsed         | 4687        |\n",
            "|    total_timesteps      | 6963200     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004048973 |\n",
            "|    clip_fraction        | 0.0533      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.468      |\n",
            "|    explained_variance   | 0.991       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.57        |\n",
            "|    n_updates            | 1696        |\n",
            "|    policy_gradient_loss | -0.000525   |\n",
            "|    value_loss           | 18          |\n",
            "-----------------------------------------\n",
            "Num timesteps: 6976000\n",
            "Best mean reward: 292.65 - Last mean reward per episode: 286.93\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 194          |\n",
            "|    ep_rew_mean          | 289          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1486         |\n",
            "|    iterations           | 426          |\n",
            "|    time_elapsed         | 4696         |\n",
            "|    total_timesteps      | 6979584      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0033297376 |\n",
            "|    clip_fraction        | 0.0363       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.461       |\n",
            "|    explained_variance   | 0.996        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.76         |\n",
            "|    n_updates            | 1700         |\n",
            "|    policy_gradient_loss | -0.000254    |\n",
            "|    value_loss           | 6.71         |\n",
            "------------------------------------------\n",
            "Num timesteps: 6992000\n",
            "Best mean reward: 292.65 - Last mean reward per episode: 289.51\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 204          |\n",
            "|    ep_rew_mean          | 289          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1486         |\n",
            "|    iterations           | 427          |\n",
            "|    time_elapsed         | 4706         |\n",
            "|    total_timesteps      | 6995968      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038556228 |\n",
            "|    clip_fraction        | 0.0462       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.454       |\n",
            "|    explained_variance   | 0.997        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.14         |\n",
            "|    n_updates            | 1704         |\n",
            "|    policy_gradient_loss | -6.77e-05    |\n",
            "|    value_loss           | 10.8         |\n",
            "------------------------------------------\n",
            "Num timesteps: 7008000\n",
            "Best mean reward: 292.65 - Last mean reward per episode: 285.81\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 189          |\n",
            "|    ep_rew_mean          | 288          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1487         |\n",
            "|    iterations           | 428          |\n",
            "|    time_elapsed         | 4715         |\n",
            "|    total_timesteps      | 7012352      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034386942 |\n",
            "|    clip_fraction        | 0.0459       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.463       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.893        |\n",
            "|    n_updates            | 1708         |\n",
            "|    policy_gradient_loss | 0.000403     |\n",
            "|    value_loss           | 3.23         |\n",
            "------------------------------------------\n",
            "Num timesteps: 7024000\n",
            "Best mean reward: 292.65 - Last mean reward per episode: 290.41\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 196          |\n",
            "|    ep_rew_mean          | 288          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1487         |\n",
            "|    iterations           | 429          |\n",
            "|    time_elapsed         | 4725         |\n",
            "|    total_timesteps      | 7028736      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0043324837 |\n",
            "|    clip_fraction        | 0.0511       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.445       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.27         |\n",
            "|    n_updates            | 1712         |\n",
            "|    policy_gradient_loss | -7.8e-05     |\n",
            "|    value_loss           | 4.02         |\n",
            "------------------------------------------\n",
            "Num timesteps: 7040000\n",
            "Best mean reward: 292.65 - Last mean reward per episode: 288.55\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 191          |\n",
            "|    ep_rew_mean          | 291          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1487         |\n",
            "|    iterations           | 430          |\n",
            "|    time_elapsed         | 4734         |\n",
            "|    total_timesteps      | 7045120      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041933386 |\n",
            "|    clip_fraction        | 0.0475       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.461       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.652        |\n",
            "|    n_updates            | 1716         |\n",
            "|    policy_gradient_loss | 5.15e-05     |\n",
            "|    value_loss           | 2.45         |\n",
            "------------------------------------------\n",
            "Num timesteps: 7056000\n",
            "Best mean reward: 292.65 - Last mean reward per episode: 290.17\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | 288         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1488        |\n",
            "|    iterations           | 431         |\n",
            "|    time_elapsed         | 4744        |\n",
            "|    total_timesteps      | 7061504     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003586174 |\n",
            "|    clip_fraction        | 0.0449      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.455      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.54        |\n",
            "|    n_updates            | 1720        |\n",
            "|    policy_gradient_loss | -0.00105    |\n",
            "|    value_loss           | 5.27        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 7072000\n",
            "Best mean reward: 292.65 - Last mean reward per episode: 290.92\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 188         |\n",
            "|    ep_rew_mean          | 288         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1489        |\n",
            "|    iterations           | 432         |\n",
            "|    time_elapsed         | 4752        |\n",
            "|    total_timesteps      | 7077888     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004948071 |\n",
            "|    clip_fraction        | 0.0614      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.438      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.439       |\n",
            "|    n_updates            | 1724        |\n",
            "|    policy_gradient_loss | -0.000703   |\n",
            "|    value_loss           | 2.93        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 7088000\n",
            "Best mean reward: 292.65 - Last mean reward per episode: 291.14\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 191         |\n",
            "|    ep_rew_mean          | 290         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1489        |\n",
            "|    iterations           | 433         |\n",
            "|    time_elapsed         | 4761        |\n",
            "|    total_timesteps      | 7094272     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003933766 |\n",
            "|    clip_fraction        | 0.049       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.442      |\n",
            "|    explained_variance   | 0.985       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.21        |\n",
            "|    n_updates            | 1728        |\n",
            "|    policy_gradient_loss | -8.72e-05   |\n",
            "|    value_loss           | 62.4        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 7104000\n",
            "Best mean reward: 292.65 - Last mean reward per episode: 289.92\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 184          |\n",
            "|    ep_rew_mean          | 285          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1490         |\n",
            "|    iterations           | 434          |\n",
            "|    time_elapsed         | 4771         |\n",
            "|    total_timesteps      | 7110656      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0051991013 |\n",
            "|    clip_fraction        | 0.0536       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.428       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.13         |\n",
            "|    n_updates            | 1732         |\n",
            "|    policy_gradient_loss | -0.00104     |\n",
            "|    value_loss           | 2.64         |\n",
            "------------------------------------------\n",
            "Num timesteps: 7120000\n",
            "Best mean reward: 292.65 - Last mean reward per episode: 279.87\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 192          |\n",
            "|    ep_rew_mean          | 283          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1490         |\n",
            "|    iterations           | 435          |\n",
            "|    time_elapsed         | 4780         |\n",
            "|    total_timesteps      | 7127040      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035170803 |\n",
            "|    clip_fraction        | 0.0364       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.421       |\n",
            "|    explained_variance   | 0.988        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.22         |\n",
            "|    n_updates            | 1736         |\n",
            "|    policy_gradient_loss | 0.000623     |\n",
            "|    value_loss           | 28.8         |\n",
            "------------------------------------------\n",
            "Num timesteps: 7136000\n",
            "Best mean reward: 292.65 - Last mean reward per episode: 291.69\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 196          |\n",
            "|    ep_rew_mean          | 286          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1491         |\n",
            "|    iterations           | 436          |\n",
            "|    time_elapsed         | 4789         |\n",
            "|    total_timesteps      | 7143424      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044770883 |\n",
            "|    clip_fraction        | 0.0427       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.424       |\n",
            "|    explained_variance   | 0.979        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.95         |\n",
            "|    n_updates            | 1740         |\n",
            "|    policy_gradient_loss | -0.000274    |\n",
            "|    value_loss           | 101          |\n",
            "------------------------------------------\n",
            "Num timesteps: 7152000\n",
            "Best mean reward: 292.65 - Last mean reward per episode: 283.48\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 204          |\n",
            "|    ep_rew_mean          | 277          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1492         |\n",
            "|    iterations           | 437          |\n",
            "|    time_elapsed         | 4798         |\n",
            "|    total_timesteps      | 7159808      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034821485 |\n",
            "|    clip_fraction        | 0.0322       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.424       |\n",
            "|    explained_variance   | 0.97         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 20.2         |\n",
            "|    n_updates            | 1744         |\n",
            "|    policy_gradient_loss | 0.000507     |\n",
            "|    value_loss           | 77.1         |\n",
            "------------------------------------------\n",
            "Num timesteps: 7168000\n",
            "Best mean reward: 292.65 - Last mean reward per episode: 276.12\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 201          |\n",
            "|    ep_rew_mean          | 275          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1492         |\n",
            "|    iterations           | 438          |\n",
            "|    time_elapsed         | 4808         |\n",
            "|    total_timesteps      | 7176192      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0031096889 |\n",
            "|    clip_fraction        | 0.0336       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.418       |\n",
            "|    explained_variance   | 0.955        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.04         |\n",
            "|    n_updates            | 1748         |\n",
            "|    policy_gradient_loss | -0.000224    |\n",
            "|    value_loss           | 203          |\n",
            "------------------------------------------\n",
            "Num timesteps: 7184000\n",
            "Best mean reward: 292.65 - Last mean reward per episode: 276.77\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 245          |\n",
            "|    ep_rew_mean          | 274          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1492         |\n",
            "|    iterations           | 439          |\n",
            "|    time_elapsed         | 4819         |\n",
            "|    total_timesteps      | 7192576      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0025061402 |\n",
            "|    clip_fraction        | 0.0395       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.45        |\n",
            "|    explained_variance   | 0.974        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.94         |\n",
            "|    n_updates            | 1752         |\n",
            "|    policy_gradient_loss | 0.000784     |\n",
            "|    value_loss           | 127          |\n",
            "------------------------------------------\n",
            "Num timesteps: 7200000\n",
            "Best mean reward: 292.65 - Last mean reward per episode: 280.22\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 190          |\n",
            "|    ep_rew_mean          | 291          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1493         |\n",
            "|    iterations           | 440          |\n",
            "|    time_elapsed         | 4827         |\n",
            "|    total_timesteps      | 7208960      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041988236 |\n",
            "|    clip_fraction        | 0.0628       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.439       |\n",
            "|    explained_variance   | 0.977        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.85         |\n",
            "|    n_updates            | 1756         |\n",
            "|    policy_gradient_loss | 2.36e-06     |\n",
            "|    value_loss           | 85.6         |\n",
            "------------------------------------------\n",
            "Num timesteps: 7216000\n",
            "Best mean reward: 292.65 - Last mean reward per episode: 292.20\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 187          |\n",
            "|    ep_rew_mean          | 285          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1493         |\n",
            "|    iterations           | 441          |\n",
            "|    time_elapsed         | 4837         |\n",
            "|    total_timesteps      | 7225344      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0042492636 |\n",
            "|    clip_fraction        | 0.0419       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.412       |\n",
            "|    explained_variance   | 0.978        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 74.5         |\n",
            "|    n_updates            | 1760         |\n",
            "|    policy_gradient_loss | 0.000317     |\n",
            "|    value_loss           | 78.8         |\n",
            "------------------------------------------\n",
            "Num timesteps: 7232000\n",
            "Best mean reward: 292.65 - Last mean reward per episode: 280.50\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 209          |\n",
            "|    ep_rew_mean          | 276          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1493         |\n",
            "|    iterations           | 442          |\n",
            "|    time_elapsed         | 4848         |\n",
            "|    total_timesteps      | 7241728      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0029245461 |\n",
            "|    clip_fraction        | 0.0364       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.425       |\n",
            "|    explained_variance   | 0.96         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.04         |\n",
            "|    n_updates            | 1764         |\n",
            "|    policy_gradient_loss | -0.000411    |\n",
            "|    value_loss           | 193          |\n",
            "------------------------------------------\n",
            "Num timesteps: 7248000\n",
            "Best mean reward: 292.65 - Last mean reward per episode: 281.62\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 195          |\n",
            "|    ep_rew_mean          | 283          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1494         |\n",
            "|    iterations           | 443          |\n",
            "|    time_elapsed         | 4857         |\n",
            "|    total_timesteps      | 7258112      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0045788367 |\n",
            "|    clip_fraction        | 0.0529       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.444       |\n",
            "|    explained_variance   | 0.986        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.55         |\n",
            "|    n_updates            | 1768         |\n",
            "|    policy_gradient_loss | 0.00054      |\n",
            "|    value_loss           | 79.1         |\n",
            "------------------------------------------\n",
            "Num timesteps: 7264000\n",
            "Best mean reward: 292.65 - Last mean reward per episode: 282.38\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 191          |\n",
            "|    ep_rew_mean          | 288          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1494         |\n",
            "|    iterations           | 444          |\n",
            "|    time_elapsed         | 4866         |\n",
            "|    total_timesteps      | 7274496      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040501603 |\n",
            "|    clip_fraction        | 0.0467       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.431       |\n",
            "|    explained_variance   | 0.967        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 18.7         |\n",
            "|    n_updates            | 1772         |\n",
            "|    policy_gradient_loss | 7.75e-05     |\n",
            "|    value_loss           | 110          |\n",
            "------------------------------------------\n",
            "Num timesteps: 7280000\n",
            "Best mean reward: 292.65 - Last mean reward per episode: 286.86\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 185          |\n",
            "|    ep_rew_mean          | 284          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1495         |\n",
            "|    iterations           | 445          |\n",
            "|    time_elapsed         | 4875         |\n",
            "|    total_timesteps      | 7290880      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0051676324 |\n",
            "|    clip_fraction        | 0.0556       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.418       |\n",
            "|    explained_variance   | 0.99         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.31         |\n",
            "|    n_updates            | 1776         |\n",
            "|    policy_gradient_loss | 0.00101      |\n",
            "|    value_loss           | 15.9         |\n",
            "------------------------------------------\n",
            "Num timesteps: 7296000\n",
            "Best mean reward: 292.65 - Last mean reward per episode: 281.49\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 205          |\n",
            "|    ep_rew_mean          | 288          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1495         |\n",
            "|    iterations           | 446          |\n",
            "|    time_elapsed         | 4884         |\n",
            "|    total_timesteps      | 7307264      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0028654197 |\n",
            "|    clip_fraction        | 0.0254       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.432       |\n",
            "|    explained_variance   | 0.969        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 227          |\n",
            "|    n_updates            | 1780         |\n",
            "|    policy_gradient_loss | 0.000203     |\n",
            "|    value_loss           | 142          |\n",
            "------------------------------------------\n",
            "Num timesteps: 7312000\n",
            "Best mean reward: 292.65 - Last mean reward per episode: 285.97\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 184          |\n",
            "|    ep_rew_mean          | 289          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1496         |\n",
            "|    iterations           | 447          |\n",
            "|    time_elapsed         | 4893         |\n",
            "|    total_timesteps      | 7323648      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0056020757 |\n",
            "|    clip_fraction        | 0.0562       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.412       |\n",
            "|    explained_variance   | 0.986        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.18         |\n",
            "|    n_updates            | 1784         |\n",
            "|    policy_gradient_loss | -0.000323    |\n",
            "|    value_loss           | 69.2         |\n",
            "------------------------------------------\n",
            "Num timesteps: 7328000\n",
            "Best mean reward: 292.65 - Last mean reward per episode: 289.98\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 181          |\n",
            "|    ep_rew_mean          | 277          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1497         |\n",
            "|    iterations           | 448          |\n",
            "|    time_elapsed         | 4902         |\n",
            "|    total_timesteps      | 7340032      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0032452615 |\n",
            "|    clip_fraction        | 0.044        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.393       |\n",
            "|    explained_variance   | 0.986        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.74         |\n",
            "|    n_updates            | 1788         |\n",
            "|    policy_gradient_loss | 0.000318     |\n",
            "|    value_loss           | 68.8         |\n",
            "------------------------------------------\n",
            "Num timesteps: 7344000\n",
            "Best mean reward: 292.65 - Last mean reward per episode: 273.35\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 183          |\n",
            "|    ep_rew_mean          | 279          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1497         |\n",
            "|    iterations           | 449          |\n",
            "|    time_elapsed         | 4911         |\n",
            "|    total_timesteps      | 7356416      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0023935577 |\n",
            "|    clip_fraction        | 0.0206       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.391       |\n",
            "|    explained_variance   | 0.93         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 252          |\n",
            "|    n_updates            | 1792         |\n",
            "|    policy_gradient_loss | 4.81e-06     |\n",
            "|    value_loss           | 321          |\n",
            "------------------------------------------\n",
            "Num timesteps: 7360000\n",
            "Best mean reward: 292.65 - Last mean reward per episode: 283.09\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 185          |\n",
            "|    ep_rew_mean          | 287          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1498         |\n",
            "|    iterations           | 450          |\n",
            "|    time_elapsed         | 4920         |\n",
            "|    total_timesteps      | 7372800      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034575483 |\n",
            "|    clip_fraction        | 0.0476       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.402       |\n",
            "|    explained_variance   | 0.964        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 43.5         |\n",
            "|    n_updates            | 1796         |\n",
            "|    policy_gradient_loss | 0.000739     |\n",
            "|    value_loss           | 175          |\n",
            "------------------------------------------\n",
            "Num timesteps: 7376000\n",
            "Best mean reward: 292.65 - Last mean reward per episode: 287.45\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 191         |\n",
            "|    ep_rew_mean          | 289         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1499        |\n",
            "|    iterations           | 451         |\n",
            "|    time_elapsed         | 4929        |\n",
            "|    total_timesteps      | 7389184     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004386369 |\n",
            "|    clip_fraction        | 0.0546      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.381      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.81        |\n",
            "|    n_updates            | 1800        |\n",
            "|    policy_gradient_loss | 0.000491    |\n",
            "|    value_loss           | 2.72        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 7392000\n",
            "Best mean reward: 292.65 - Last mean reward per episode: 289.43\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 188          |\n",
            "|    ep_rew_mean          | 290          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1499         |\n",
            "|    iterations           | 452          |\n",
            "|    time_elapsed         | 4938         |\n",
            "|    total_timesteps      | 7405568      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035847966 |\n",
            "|    clip_fraction        | 0.0462       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.409       |\n",
            "|    explained_variance   | 0.987        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 35.9         |\n",
            "|    n_updates            | 1804         |\n",
            "|    policy_gradient_loss | 0.000303     |\n",
            "|    value_loss           | 65.6         |\n",
            "------------------------------------------\n",
            "Num timesteps: 7408000\n",
            "Best mean reward: 292.65 - Last mean reward per episode: 289.15\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 197         |\n",
            "|    ep_rew_mean          | 292         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1499        |\n",
            "|    iterations           | 453         |\n",
            "|    time_elapsed         | 4947        |\n",
            "|    total_timesteps      | 7421952     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003583594 |\n",
            "|    clip_fraction        | 0.0503      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.403      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.994       |\n",
            "|    n_updates            | 1808        |\n",
            "|    policy_gradient_loss | 0.000957    |\n",
            "|    value_loss           | 3.74        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 7424000\n",
            "Best mean reward: 292.65 - Last mean reward per episode: 291.07\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 192          |\n",
            "|    ep_rew_mean          | 293          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1500         |\n",
            "|    iterations           | 454          |\n",
            "|    time_elapsed         | 4957         |\n",
            "|    total_timesteps      | 7438336      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0042818654 |\n",
            "|    clip_fraction        | 0.0525       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.408       |\n",
            "|    explained_variance   | 0.986        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.805        |\n",
            "|    n_updates            | 1812         |\n",
            "|    policy_gradient_loss | -9.69e-05    |\n",
            "|    value_loss           | 71.4         |\n",
            "------------------------------------------\n",
            "Num timesteps: 7440000\n",
            "Best mean reward: 292.65 - Last mean reward per episode: 293.66\n",
            "Saving new best model to log_dir_PPO/best_model.zip\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 199         |\n",
            "|    ep_rew_mean          | 288         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1501        |\n",
            "|    iterations           | 455         |\n",
            "|    time_elapsed         | 4966        |\n",
            "|    total_timesteps      | 7454720     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003502529 |\n",
            "|    clip_fraction        | 0.0442      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.421      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.553       |\n",
            "|    n_updates            | 1816        |\n",
            "|    policy_gradient_loss | -0.000865   |\n",
            "|    value_loss           | 1.82        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 7456000\n",
            "Best mean reward: 293.66 - Last mean reward per episode: 286.28\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 193        |\n",
            "|    ep_rew_mean          | 281        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 1501       |\n",
            "|    iterations           | 456        |\n",
            "|    time_elapsed         | 4975       |\n",
            "|    total_timesteps      | 7471104    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00478977 |\n",
            "|    clip_fraction        | 0.057      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.424     |\n",
            "|    explained_variance   | 1          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1          |\n",
            "|    n_updates            | 1820       |\n",
            "|    policy_gradient_loss | -0.000364  |\n",
            "|    value_loss           | 1.47       |\n",
            "----------------------------------------\n",
            "Num timesteps: 7472000\n",
            "Best mean reward: 293.66 - Last mean reward per episode: 278.73\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 194          |\n",
            "|    ep_rew_mean          | 286          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1502         |\n",
            "|    iterations           | 457          |\n",
            "|    time_elapsed         | 4984         |\n",
            "|    total_timesteps      | 7487488      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0025903627 |\n",
            "|    clip_fraction        | 0.0248       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.423       |\n",
            "|    explained_variance   | 0.965        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.5          |\n",
            "|    n_updates            | 1824         |\n",
            "|    policy_gradient_loss | -0.000402    |\n",
            "|    value_loss           | 65.7         |\n",
            "------------------------------------------\n",
            "Num timesteps: 7488000\n",
            "Best mean reward: 293.66 - Last mean reward per episode: 285.99\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 187          |\n",
            "|    ep_rew_mean          | 289          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1502         |\n",
            "|    iterations           | 458          |\n",
            "|    time_elapsed         | 4994         |\n",
            "|    total_timesteps      | 7503872      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0028063413 |\n",
            "|    clip_fraction        | 0.0341       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.403       |\n",
            "|    explained_variance   | 0.995        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 30.3         |\n",
            "|    n_updates            | 1828         |\n",
            "|    policy_gradient_loss | 0.000884     |\n",
            "|    value_loss           | 23.7         |\n",
            "------------------------------------------\n",
            "Num timesteps: 7504000\n",
            "Best mean reward: 293.66 - Last mean reward per episode: 289.30\n",
            "Num timesteps: 7520000\n",
            "Best mean reward: 293.66 - Last mean reward per episode: 280.74\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 180         |\n",
            "|    ep_rew_mean          | 279         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1502        |\n",
            "|    iterations           | 459         |\n",
            "|    time_elapsed         | 5003        |\n",
            "|    total_timesteps      | 7520256     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002800755 |\n",
            "|    clip_fraction        | 0.038       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.392      |\n",
            "|    explained_variance   | 0.989       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.49        |\n",
            "|    n_updates            | 1832        |\n",
            "|    policy_gradient_loss | -7.23e-05   |\n",
            "|    value_loss           | 41.5        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 7536000\n",
            "Best mean reward: 293.66 - Last mean reward per episode: 286.02\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 192          |\n",
            "|    ep_rew_mean          | 287          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1503         |\n",
            "|    iterations           | 460          |\n",
            "|    time_elapsed         | 5012         |\n",
            "|    total_timesteps      | 7536640      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020764233 |\n",
            "|    clip_fraction        | 0.0171       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.415       |\n",
            "|    explained_variance   | 0.943        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 38.7         |\n",
            "|    n_updates            | 1836         |\n",
            "|    policy_gradient_loss | -0.000208    |\n",
            "|    value_loss           | 240          |\n",
            "------------------------------------------\n",
            "Num timesteps: 7552000\n",
            "Best mean reward: 293.66 - Last mean reward per episode: 286.99\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 184          |\n",
            "|    ep_rew_mean          | 286          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1504         |\n",
            "|    iterations           | 461          |\n",
            "|    time_elapsed         | 5021         |\n",
            "|    total_timesteps      | 7553024      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0033042177 |\n",
            "|    clip_fraction        | 0.0444       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.393       |\n",
            "|    explained_variance   | 0.981        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.12         |\n",
            "|    n_updates            | 1840         |\n",
            "|    policy_gradient_loss | 0.000379     |\n",
            "|    value_loss           | 76           |\n",
            "------------------------------------------\n",
            "Num timesteps: 7568000\n",
            "Best mean reward: 293.66 - Last mean reward per episode: 281.72\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 184          |\n",
            "|    ep_rew_mean          | 281          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1504         |\n",
            "|    iterations           | 462          |\n",
            "|    time_elapsed         | 5030         |\n",
            "|    total_timesteps      | 7569408      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0036630384 |\n",
            "|    clip_fraction        | 0.0551       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.404       |\n",
            "|    explained_variance   | 0.991        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.95         |\n",
            "|    n_updates            | 1844         |\n",
            "|    policy_gradient_loss | 0.000952     |\n",
            "|    value_loss           | 25           |\n",
            "------------------------------------------\n",
            "Num timesteps: 7584000\n",
            "Best mean reward: 293.66 - Last mean reward per episode: 283.62\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 190         |\n",
            "|    ep_rew_mean          | 283         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1505        |\n",
            "|    iterations           | 463         |\n",
            "|    time_elapsed         | 5040        |\n",
            "|    total_timesteps      | 7585792     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004647797 |\n",
            "|    clip_fraction        | 0.0535      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.406      |\n",
            "|    explained_variance   | 0.981       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 243         |\n",
            "|    n_updates            | 1848        |\n",
            "|    policy_gradient_loss | 0.000739    |\n",
            "|    value_loss           | 92.7        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 7600000\n",
            "Best mean reward: 293.66 - Last mean reward per episode: 285.80\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 192         |\n",
            "|    ep_rew_mean          | 289         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1505        |\n",
            "|    iterations           | 464         |\n",
            "|    time_elapsed         | 5049        |\n",
            "|    total_timesteps      | 7602176     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003934593 |\n",
            "|    clip_fraction        | 0.042       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.41       |\n",
            "|    explained_variance   | 0.962       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 132         |\n",
            "|    n_updates            | 1852        |\n",
            "|    policy_gradient_loss | -0.0004     |\n",
            "|    value_loss           | 136         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 7616000\n",
            "Best mean reward: 293.66 - Last mean reward per episode: 289.86\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 185          |\n",
            "|    ep_rew_mean          | 289          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1506         |\n",
            "|    iterations           | 465          |\n",
            "|    time_elapsed         | 5058         |\n",
            "|    total_timesteps      | 7618560      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0045947474 |\n",
            "|    clip_fraction        | 0.0535       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.401       |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.45         |\n",
            "|    n_updates            | 1856         |\n",
            "|    policy_gradient_loss | 0.000454     |\n",
            "|    value_loss           | 3.59         |\n",
            "------------------------------------------\n",
            "Num timesteps: 7632000\n",
            "Best mean reward: 293.66 - Last mean reward per episode: 285.66\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 201          |\n",
            "|    ep_rew_mean          | 287          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1506         |\n",
            "|    iterations           | 466          |\n",
            "|    time_elapsed         | 5067         |\n",
            "|    total_timesteps      | 7634944      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0031868787 |\n",
            "|    clip_fraction        | 0.0342       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.439       |\n",
            "|    explained_variance   | 0.984        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.01         |\n",
            "|    n_updates            | 1860         |\n",
            "|    policy_gradient_loss | 0.000619     |\n",
            "|    value_loss           | 79.1         |\n",
            "------------------------------------------\n",
            "Num timesteps: 7648000\n",
            "Best mean reward: 293.66 - Last mean reward per episode: 285.10\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 188         |\n",
            "|    ep_rew_mean          | 285         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1507        |\n",
            "|    iterations           | 467         |\n",
            "|    time_elapsed         | 5076        |\n",
            "|    total_timesteps      | 7651328     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003688064 |\n",
            "|    clip_fraction        | 0.0427      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.39       |\n",
            "|    explained_variance   | 0.993       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.492       |\n",
            "|    n_updates            | 1864        |\n",
            "|    policy_gradient_loss | 0.00103     |\n",
            "|    value_loss           | 17.1        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 7664000\n",
            "Best mean reward: 293.66 - Last mean reward per episode: 282.36\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 181         |\n",
            "|    ep_rew_mean          | 282         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1507        |\n",
            "|    iterations           | 468         |\n",
            "|    time_elapsed         | 5084        |\n",
            "|    total_timesteps      | 7667712     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003863043 |\n",
            "|    clip_fraction        | 0.0398      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.388      |\n",
            "|    explained_variance   | 0.987       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.47        |\n",
            "|    n_updates            | 1868        |\n",
            "|    policy_gradient_loss | 0.0011      |\n",
            "|    value_loss           | 58.6        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 7680000\n",
            "Best mean reward: 293.66 - Last mean reward per episode: 277.43\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 180         |\n",
            "|    ep_rew_mean          | 277         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1508        |\n",
            "|    iterations           | 469         |\n",
            "|    time_elapsed         | 5093        |\n",
            "|    total_timesteps      | 7684096     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004208424 |\n",
            "|    clip_fraction        | 0.0389      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.396      |\n",
            "|    explained_variance   | 0.958       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 243         |\n",
            "|    n_updates            | 1872        |\n",
            "|    policy_gradient_loss | -0.00116    |\n",
            "|    value_loss           | 209         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 7696000\n",
            "Best mean reward: 293.66 - Last mean reward per episode: 272.70\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 179          |\n",
            "|    ep_rew_mean          | 268          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1509         |\n",
            "|    iterations           | 470          |\n",
            "|    time_elapsed         | 5102         |\n",
            "|    total_timesteps      | 7700480      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0026544924 |\n",
            "|    clip_fraction        | 0.0225       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.395       |\n",
            "|    explained_variance   | 0.937        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 84.9         |\n",
            "|    n_updates            | 1876         |\n",
            "|    policy_gradient_loss | 0.000294     |\n",
            "|    value_loss           | 282          |\n",
            "------------------------------------------\n",
            "Num timesteps: 7712000\n",
            "Best mean reward: 293.66 - Last mean reward per episode: 270.80\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 185          |\n",
            "|    ep_rew_mean          | 273          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1509         |\n",
            "|    iterations           | 471          |\n",
            "|    time_elapsed         | 5110         |\n",
            "|    total_timesteps      | 7716864      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0028536692 |\n",
            "|    clip_fraction        | 0.0276       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.402       |\n",
            "|    explained_variance   | 0.92         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 212          |\n",
            "|    n_updates            | 1880         |\n",
            "|    policy_gradient_loss | -0.000962    |\n",
            "|    value_loss           | 379          |\n",
            "------------------------------------------\n",
            "Num timesteps: 7728000\n",
            "Best mean reward: 293.66 - Last mean reward per episode: 283.98\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 183         |\n",
            "|    ep_rew_mean          | 290         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1510        |\n",
            "|    iterations           | 472         |\n",
            "|    time_elapsed         | 5120        |\n",
            "|    total_timesteps      | 7733248     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005133033 |\n",
            "|    clip_fraction        | 0.0535      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.402      |\n",
            "|    explained_variance   | 0.929       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 31.8        |\n",
            "|    n_updates            | 1884        |\n",
            "|    policy_gradient_loss | -0.00154    |\n",
            "|    value_loss           | 267         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 7744000\n",
            "Best mean reward: 293.66 - Last mean reward per episode: 291.92\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 184         |\n",
            "|    ep_rew_mean          | 290         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1510        |\n",
            "|    iterations           | 473         |\n",
            "|    time_elapsed         | 5128        |\n",
            "|    total_timesteps      | 7749632     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005358273 |\n",
            "|    clip_fraction        | 0.074       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.378      |\n",
            "|    explained_variance   | 0.99        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.18        |\n",
            "|    n_updates            | 1888        |\n",
            "|    policy_gradient_loss | -0.000628   |\n",
            "|    value_loss           | 9.35        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 7760000\n",
            "Best mean reward: 293.66 - Last mean reward per episode: 283.06\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 180          |\n",
            "|    ep_rew_mean          | 284          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1511         |\n",
            "|    iterations           | 474          |\n",
            "|    time_elapsed         | 5137         |\n",
            "|    total_timesteps      | 7766016      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0058147237 |\n",
            "|    clip_fraction        | 0.0582       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.434       |\n",
            "|    explained_variance   | 0.997        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.506        |\n",
            "|    n_updates            | 1892         |\n",
            "|    policy_gradient_loss | 0.0021       |\n",
            "|    value_loss           | 2.54         |\n",
            "------------------------------------------\n",
            "Num timesteps: 7776000\n",
            "Best mean reward: 293.66 - Last mean reward per episode: 284.31\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 185          |\n",
            "|    ep_rew_mean          | 284          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1512         |\n",
            "|    iterations           | 475          |\n",
            "|    time_elapsed         | 5146         |\n",
            "|    total_timesteps      | 7782400      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0032617163 |\n",
            "|    clip_fraction        | 0.0327       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.429       |\n",
            "|    explained_variance   | 0.973        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 47.3         |\n",
            "|    n_updates            | 1896         |\n",
            "|    policy_gradient_loss | 0.000712     |\n",
            "|    value_loss           | 87.7         |\n",
            "------------------------------------------\n",
            "Num timesteps: 7792000\n",
            "Best mean reward: 293.66 - Last mean reward per episode: 283.02\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 183          |\n",
            "|    ep_rew_mean          | 280          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1512         |\n",
            "|    iterations           | 476          |\n",
            "|    time_elapsed         | 5155         |\n",
            "|    total_timesteps      | 7798784      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041372515 |\n",
            "|    clip_fraction        | 0.0452       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.425       |\n",
            "|    explained_variance   | 0.963        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.61         |\n",
            "|    n_updates            | 1900         |\n",
            "|    policy_gradient_loss | 0.000737     |\n",
            "|    value_loss           | 102          |\n",
            "------------------------------------------\n",
            "Num timesteps: 7808000\n",
            "Best mean reward: 293.66 - Last mean reward per episode: 286.14\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 187         |\n",
            "|    ep_rew_mean          | 290         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1513        |\n",
            "|    iterations           | 477         |\n",
            "|    time_elapsed         | 5164        |\n",
            "|    total_timesteps      | 7815168     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003758595 |\n",
            "|    clip_fraction        | 0.0326      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.415      |\n",
            "|    explained_variance   | 0.96        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 21          |\n",
            "|    n_updates            | 1904        |\n",
            "|    policy_gradient_loss | -7.04e-05   |\n",
            "|    value_loss           | 188         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 7824000\n",
            "Best mean reward: 293.66 - Last mean reward per episode: 292.68\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 190          |\n",
            "|    ep_rew_mean          | 293          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1513         |\n",
            "|    iterations           | 478          |\n",
            "|    time_elapsed         | 5173         |\n",
            "|    total_timesteps      | 7831552      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0049436027 |\n",
            "|    clip_fraction        | 0.0605       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.413       |\n",
            "|    explained_variance   | 0.997        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.37         |\n",
            "|    n_updates            | 1908         |\n",
            "|    policy_gradient_loss | 0.000592     |\n",
            "|    value_loss           | 4.53         |\n",
            "------------------------------------------\n",
            "Num timesteps: 7840000\n",
            "Best mean reward: 293.66 - Last mean reward per episode: 286.76\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 188         |\n",
            "|    ep_rew_mean          | 284         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1514        |\n",
            "|    iterations           | 479         |\n",
            "|    time_elapsed         | 5182        |\n",
            "|    total_timesteps      | 7847936     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004732148 |\n",
            "|    clip_fraction        | 0.0453      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.428      |\n",
            "|    explained_variance   | 0.997       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.74        |\n",
            "|    n_updates            | 1912        |\n",
            "|    policy_gradient_loss | 0.000671    |\n",
            "|    value_loss           | 4.92        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 7856000\n",
            "Best mean reward: 293.66 - Last mean reward per episode: 287.83\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 195          |\n",
            "|    ep_rew_mean          | 285          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1514         |\n",
            "|    iterations           | 480          |\n",
            "|    time_elapsed         | 5191         |\n",
            "|    total_timesteps      | 7864320      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0026881308 |\n",
            "|    clip_fraction        | 0.0337       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.429       |\n",
            "|    explained_variance   | 0.971        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 13           |\n",
            "|    n_updates            | 1916         |\n",
            "|    policy_gradient_loss | 0.000193     |\n",
            "|    value_loss           | 112          |\n",
            "------------------------------------------\n",
            "Num timesteps: 7872000\n",
            "Best mean reward: 293.66 - Last mean reward per episode: 284.29\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 187        |\n",
            "|    ep_rew_mean          | 289        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 1515       |\n",
            "|    iterations           | 481        |\n",
            "|    time_elapsed         | 5200       |\n",
            "|    total_timesteps      | 7880704    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00449434 |\n",
            "|    clip_fraction        | 0.0449     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.403     |\n",
            "|    explained_variance   | 0.968      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 181        |\n",
            "|    n_updates            | 1920       |\n",
            "|    policy_gradient_loss | 0.00034    |\n",
            "|    value_loss           | 140        |\n",
            "----------------------------------------\n",
            "Num timesteps: 7888000\n",
            "Best mean reward: 293.66 - Last mean reward per episode: 288.00\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 190          |\n",
            "|    ep_rew_mean          | 287          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1515         |\n",
            "|    iterations           | 482          |\n",
            "|    time_elapsed         | 5209         |\n",
            "|    total_timesteps      | 7897088      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0047683204 |\n",
            "|    clip_fraction        | 0.06         |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.409       |\n",
            "|    explained_variance   | 0.98         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.89         |\n",
            "|    n_updates            | 1924         |\n",
            "|    policy_gradient_loss | -0.000413    |\n",
            "|    value_loss           | 61.3         |\n",
            "------------------------------------------\n",
            "Num timesteps: 7904000\n",
            "Best mean reward: 293.66 - Last mean reward per episode: 286.99\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 184          |\n",
            "|    ep_rew_mean          | 283          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1516         |\n",
            "|    iterations           | 483          |\n",
            "|    time_elapsed         | 5218         |\n",
            "|    total_timesteps      | 7913472      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038311407 |\n",
            "|    clip_fraction        | 0.0483       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.407       |\n",
            "|    explained_variance   | 0.98         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.47         |\n",
            "|    n_updates            | 1928         |\n",
            "|    policy_gradient_loss | 0.000805     |\n",
            "|    value_loss           | 86.5         |\n",
            "------------------------------------------\n",
            "Num timesteps: 7920000\n",
            "Best mean reward: 293.66 - Last mean reward per episode: 283.13\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 184          |\n",
            "|    ep_rew_mean          | 283          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1516         |\n",
            "|    iterations           | 484          |\n",
            "|    time_elapsed         | 5227         |\n",
            "|    total_timesteps      | 7929856      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0039566243 |\n",
            "|    clip_fraction        | 0.0601       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.434       |\n",
            "|    explained_variance   | 0.982        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.79         |\n",
            "|    n_updates            | 1932         |\n",
            "|    policy_gradient_loss | 0.000905     |\n",
            "|    value_loss           | 64.6         |\n",
            "------------------------------------------\n",
            "Num timesteps: 7936000\n",
            "Best mean reward: 293.66 - Last mean reward per episode: 288.78\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 185          |\n",
            "|    ep_rew_mean          | 289          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1517         |\n",
            "|    iterations           | 485          |\n",
            "|    time_elapsed         | 5236         |\n",
            "|    total_timesteps      | 7946240      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0032283044 |\n",
            "|    clip_fraction        | 0.046        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.406       |\n",
            "|    explained_variance   | 0.984        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.8          |\n",
            "|    n_updates            | 1936         |\n",
            "|    policy_gradient_loss | 0.000594     |\n",
            "|    value_loss           | 58.9         |\n",
            "------------------------------------------\n",
            "Num timesteps: 7952000\n",
            "Best mean reward: 293.66 - Last mean reward per episode: 283.51\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 183         |\n",
            "|    ep_rew_mean          | 282         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1517        |\n",
            "|    iterations           | 486         |\n",
            "|    time_elapsed         | 5246        |\n",
            "|    total_timesteps      | 7962624     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004338309 |\n",
            "|    clip_fraction        | 0.0544      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.411      |\n",
            "|    explained_variance   | 0.996       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.855       |\n",
            "|    n_updates            | 1940        |\n",
            "|    policy_gradient_loss | 0.0014      |\n",
            "|    value_loss           | 5.96        |\n",
            "-----------------------------------------\n",
            "Num timesteps: 7968000\n",
            "Best mean reward: 293.66 - Last mean reward per episode: 286.16\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 188         |\n",
            "|    ep_rew_mean          | 288         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1518        |\n",
            "|    iterations           | 487         |\n",
            "|    time_elapsed         | 5254        |\n",
            "|    total_timesteps      | 7979008     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004679527 |\n",
            "|    clip_fraction        | 0.0445      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.41       |\n",
            "|    explained_variance   | 0.969       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 307         |\n",
            "|    n_updates            | 1944        |\n",
            "|    policy_gradient_loss | -0.000546   |\n",
            "|    value_loss           | 112         |\n",
            "-----------------------------------------\n",
            "Num timesteps: 7984000\n",
            "Best mean reward: 293.66 - Last mean reward per episode: 289.68\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 190          |\n",
            "|    ep_rew_mean          | 284          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1519         |\n",
            "|    iterations           | 488          |\n",
            "|    time_elapsed         | 5263         |\n",
            "|    total_timesteps      | 7995392      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0053099794 |\n",
            "|    clip_fraction        | 0.0566       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.406       |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.15         |\n",
            "|    n_updates            | 1948         |\n",
            "|    policy_gradient_loss | -8.74e-05    |\n",
            "|    value_loss           | 4.12         |\n",
            "------------------------------------------\n",
            "Num timesteps: 8000000\n",
            "Best mean reward: 293.66 - Last mean reward per episode: 283.32\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 185          |\n",
            "|    ep_rew_mean          | 284          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1519         |\n",
            "|    iterations           | 489          |\n",
            "|    time_elapsed         | 5272         |\n",
            "|    total_timesteps      | 8011776      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019658052 |\n",
            "|    clip_fraction        | 0.0275       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.397       |\n",
            "|    explained_variance   | 0.956        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.24         |\n",
            "|    n_updates            | 1952         |\n",
            "|    policy_gradient_loss | 0.000217     |\n",
            "|    value_loss           | 183          |\n",
            "------------------------------------------\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<stable_baselines3.ppo.ppo.PPO at 0x7f05d7b4deb0>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_from_file = False\n",
        "# Hyperparameters are from RL_Zoo\n",
        "# https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/ppo.yml\n",
        "\n",
        "policy = \"MlpPolicy\"\n",
        "n_steps = 1024\n",
        "batch_size = 64\n",
        "n_epochs = 4\n",
        "n_envs = 16\n",
        "n_timesteps = 8e6\n",
        "gamma = 0.999\n",
        "gae_lambda = 0.98\n",
        "ent_coef = 0.01\n",
        "\n",
        "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=\"log_dir_PPO/\")\n",
        "\n",
        "# env\n",
        "env = make_vec_env(\"LunarLander-v2\", n_envs=n_envs, monitor_dir=\"log_dir_PPO/\")\n",
        "\n",
        "# instantiate the agent\n",
        "if train_from_file:\n",
        "  model = PPO.load(path=\"log_dir_PPO/best_model.zip\", env=env)\n",
        "else:\n",
        "  model = PPO(\n",
        "      policy,\n",
        "      env,\n",
        "      n_steps = n_steps,\n",
        "      ent_coef= ent_coef,\n",
        "      batch_size=batch_size,\n",
        "      n_epochs=n_epochs,\n",
        "      gamma=gamma,\n",
        "      gae_lambda=gae_lambda,\n",
        "      tensorboard_log=\"./TensorBoardLog/\", verbose=1)\n",
        "\n",
        "# train the agent\n",
        "model.learn(total_timesteps=n_timesteps, callback=callback)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b1dc8fa",
      "metadata": {
        "id": "5b1dc8fa"
      },
      "source": [
        "# Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "366b80a8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "366b80a8",
        "outputId": "7de4adc0-0ca5-4e7a-f70f-c18a4c1c2fd7"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def moving_average(values, window):\n",
        "    \"\"\"\n",
        "    Smooth values by doing a moving average\n",
        "    :param values: (numpy array)\n",
        "    :param window: (int)\n",
        "    :return: (numpy array)\n",
        "    \"\"\"\n",
        "    weights = np.repeat(1.0, window) / window\n",
        "    return np.convolve(values, weights, 'valid')\n",
        "\n",
        "def plot_results(log_folder, title='Learning Curve'):\n",
        "    \"\"\"\n",
        "    plot the results\n",
        "\n",
        "    :param log_folder: (str) the save location of the results to plot\n",
        "    :param title: (str) the title of the task to plot\n",
        "    \"\"\"\n",
        "\n",
        "    x, y = ts2xy(load_results(log_folder), 'timesteps')\n",
        "    y = moving_average(y, window=100)\n",
        "    # Truncate x\n",
        "    x = x[len(x) - len(y):]\n",
        "    fig = plt.figure(title, figsize=(12,5))\n",
        "    plt.plot(x, y)\n",
        "    plt.xlabel('Number of Timesteps')\n",
        "    plt.ylabel('Rewards')\n",
        "    plt.title(title + \" Smoothed PPO\")\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "plot_results(\"log_dir_PPO\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b00f2a81",
      "metadata": {
        "id": "b00f2a81"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "815393a0",
      "metadata": {
        "id": "815393a0"
      },
      "outputs": [],
      "source": [
        "env = make_vec_env(\"LunarLander-v2\", n_envs=1,monitor_dir=\"log_dir_PPO\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "63611e6e",
      "metadata": {
        "id": "63611e6e"
      },
      "outputs": [],
      "source": [
        "model = PPO.load(path=\"log_dir_PPO/best_model.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b06e1a3",
      "metadata": {
        "id": "3b06e1a3"
      },
      "source": [
        "#### Stable Baseline 3 Evaluation Function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "9d4fd326",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d4fd326",
        "outputId": "24d33fe8-2815-4805-9e01-899576a77037"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean & Std Reward after 10 max run is 287.43837139999994 & 15.012367302568675\n"
          ]
        }
      ],
      "source": [
        "mean_reward, std_reward = evaluate_policy(model, env,n_eval_episodes=10, render=True, deterministic=True)\n",
        "print(\"Mean & Std Reward after {} max run is {} & {}\".format(10,mean_reward, std_reward))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e49c5168",
      "metadata": {
        "id": "e49c5168"
      },
      "source": [
        "# GIF of a Train Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "60cc63dc",
      "metadata": {
        "id": "60cc63dc"
      },
      "outputs": [],
      "source": [
        "env = make_vec_env(\"LunarLander-v2\", n_envs=1)\n",
        "model = PPO.load(path=\"log_dir_PPO/best_model.zip\")\n",
        "\n",
        "images = []\n",
        "obs = env.reset()\n",
        "img = env.render(mode=\"rgb_array\")\n",
        "for i in range(1000):\n",
        "    images.append(img)\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, _, _ ,_ = env.step(action)\n",
        "    img = env.render(mode=\"rgb_array\")\n",
        "\n",
        "imageio.mimsave(\"lunar lander_PPO.gif\", [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3afc060b",
      "metadata": {
        "id": "3afc060b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "857970f990130bbcaee778cf1846f7875676d945310dca1379fe4b5ef3d258a5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
