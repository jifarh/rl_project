{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "57601915",
      "metadata": {
        "id": "57601915",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3d12e96-6003-4cd0-a0e5-44c1a9e71147"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting shap\n",
            "  Downloading shap-0.45.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (538 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m538.2/538.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (1.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.66.2)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (24.0)\n",
            "Collecting slicer==0.0.7 (from shap)\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.58.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.41.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2023.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (3.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->shap) (1.16.0)\n",
            "Installing collected packages: slicer, shap\n",
            "Successfully installed shap-0.45.0 slicer-0.0.7\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.25.2)\n",
            "Collecting swig\n",
            "  Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.2.1\n",
            "Collecting Box2D\n",
            "  Downloading Box2D-2.3.2.tar.gz (427 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.9/427.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: Box2D\n",
            "  Building wheel for Box2D (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Box2D: filename=Box2D-2.3.2-cp310-cp310-linux_x86_64.whl size=2394339 sha256=dd4b99d788eb523262ad9530b504c2d82aa9890c4e087ed6688ae622c68de112\n",
            "  Stored in directory: /root/.cache/pip/wheels/eb/cb/be/e663f3ce9aba6580611c0febaf7cd3cf7603f87047de2a52f9\n",
            "Successfully built Box2D\n",
            "Installing collected packages: Box2D\n",
            "Successfully installed Box2D-2.3.2\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
            "Collecting pyglet==1.5.27\n",
            "  Downloading pyglet-1.5.27-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyglet\n",
            "Successfully installed pyglet-1.5.27\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement stable-baseline3 (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for stable-baseline3\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting gymnasium[all]\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (4.10.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[all])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting shimmy[atari]<1.0,>=0.1.0 (from gymnasium[all])\n",
            "  Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[all])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (2.5.2)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (4.2.1)\n",
            "Collecting mujoco-py<2.2,>=2.1 (from gymnasium[all])\n",
            "  Downloading mujoco_py-2.1.2.14-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cython<3 (from gymnasium[all])\n",
            "  Downloading Cython-0.29.37-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mujoco>=2.3.3 (from gymnasium[all])\n",
            "  Downloading mujoco-3.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (2.31.6)\n",
            "Requirement already satisfied: jax>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (0.4.23)\n",
            "Requirement already satisfied: jaxlib>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (0.4.23+cuda12.cudnn89)\n",
            "Collecting lz4>=3.1.0 (from gymnasium[all])\n",
            "  Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python>=3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (4.8.0.76)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (3.7.1)\n",
            "Requirement already satisfied: moviepy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (1.0.3)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (2.2.1+cu121)\n",
            "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio>=2.14.1->gymnasium[all]) (9.4.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.0->gymnasium[all]) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.0->gymnasium[all]) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.0->gymnasium[all]) (1.11.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gymnasium[all]) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gymnasium[all]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gymnasium[all]) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gymnasium[all]) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gymnasium[all]) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gymnasium[all]) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gymnasium[all]) (2.8.2)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy>=1.0.0->gymnasium[all]) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy>=1.0.0->gymnasium[all]) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy>=1.0.0->gymnasium[all]) (2.31.0)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy>=1.0.0->gymnasium[all]) (0.1.10)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy>=1.0.0->gymnasium[all]) (0.4.9)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.3.3->gymnasium[all]) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.3.3->gymnasium[all]) (1.7.0)\n",
            "Collecting glfw (from mujoco>=2.3.3->gymnasium[all])\n",
            "  Downloading glfw-2.7.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (211 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.8/211.8 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyopengl in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.3.3->gymnasium[all]) (3.1.7)\n",
            "Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.10/dist-packages (from mujoco-py<2.2,>=2.1->gymnasium[all]) (1.16.0)\n",
            "Collecting fasteners~=0.15 (from mujoco-py<2.2,>=2.1->gymnasium[all])\n",
            "  Downloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0->gymnasium[all])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->gymnasium[all]) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->gymnasium[all]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->gymnasium[all]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->gymnasium[all]) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->gymnasium[all]) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.0.0->gymnasium[all])\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.0.0->gymnasium[all])\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.0.0->gymnasium[all])\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.0.0->gymnasium[all])\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.0.0->gymnasium[all])\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.0.0->gymnasium[all])\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.0.0->gymnasium[all])\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.0.0->gymnasium[all])\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.0.0->gymnasium[all])\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.0.0->gymnasium[all])\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.0.0->gymnasium[all])\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->gymnasium[all]) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->gymnasium[all])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0->gymnasium[all]) (6.3.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.10->mujoco-py<2.2,>=2.1->gymnasium[all]) (2.21)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy>=1.0.0->gymnasium[all]) (67.7.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->gymnasium[all]) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[all]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[all]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[all]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[all]) (2024.2.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[all]) (3.18.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->gymnasium[all]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->gymnasium[all]) (1.3.0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2376102 sha256=5b610430214a9553f257ab7b72a65cfedff761b6a7e59483d617f2c12feff764\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: glfw, farama-notifications, box2d-py, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lz4, gymnasium, fasteners, cython, ale-py, shimmy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, mujoco-py, nvidia-cusolver-cu12, mujoco\n",
            "  Attempting uninstall: cython\n",
            "    Found existing installation: Cython 3.0.9\n",
            "    Uninstalling Cython-3.0.9:\n",
            "      Successfully uninstalled Cython-3.0.9\n",
            "Successfully installed ale-py-0.8.1 box2d-py-2.3.5 cython-0.29.37 farama-notifications-0.0.4 fasteners-0.19 glfw-2.7.0 gymnasium-0.29.1 lz4-4.3.3 mujoco-3.1.3 mujoco-py-2.1.2.14 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 shimmy-0.2.1\n",
            "Collecting stable_baselines3\n",
            "  Downloading stable_baselines3-2.2.1-py3-none-any.whl (181 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.7/181.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.25.2)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.2.1+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (4.10.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->stable_baselines3) (12.4.99)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable_baselines3) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable_baselines3) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable_baselines3) (1.3.0)\n",
            "Installing collected packages: stable_baselines3\n",
            "Successfully installed stable_baselines3-2.2.1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install shap\n",
        "!pip install opencv-python\n",
        "!pip install swig\n",
        "!pip install Box2D\n",
        "\n",
        "\n",
        "# !pip install box2d pygame\n",
        "\n",
        "\n",
        "!pip install gym\n",
        "!pip install pyglet==1.5.27\n",
        "!pip install stable-baseline3\n",
        "!pip install \"gymnasium[all]\"\n",
        "\n",
        "!pip install stable_baselines3\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "PtTyQewr3gxt"
      },
      "id": "PtTyQewr3gxt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b00a128f",
      "metadata": {
        "id": "b00a128f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import imageio\n",
        "import os\n",
        "from stable_baselines3 import DQN, A2C\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack, SubprocVecEnv, VecNormalize\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common import results_plotter\n",
        "import gymnasium  as gym\n",
        "import matplotlib.pyplot as plt\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "import scipy.stats as stats\n",
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "import tensorflow as tf\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# seeds\n",
        "# Set seed for numpy\n",
        "np.random.seed(100)\n",
        "\n",
        "# Set seed for Python random module\n",
        "import random\n",
        "random.seed(100)\n",
        "\n",
        "# Set seed for TensorFlow\n",
        "tf.random.set_seed(100)\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "if tf.test.gpu_device_name():\n",
        "    print('Default GPU Device:', tf.test.gpu_device_name())\n",
        "else:\n",
        "    print(\"GPU not found. Please ensure that GPU is enabled in Colab.\")"
      ],
      "metadata": {
        "id": "YtZN-eC7NwuS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c9cdd26-c266-4012-8463-48462279e0b1"
      },
      "id": "YtZN-eC7NwuS",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Default GPU Device: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "780afb92",
      "metadata": {
        "id": "780afb92"
      },
      "source": [
        "<h1> Important Libraries To Install </h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2826cd85",
      "metadata": {
        "id": "2826cd85"
      },
      "source": [
        "<h1> Parameter & Environment Information </h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87ef75ca",
      "metadata": {
        "id": "87ef75ca"
      },
      "source": [
        "<p>\n",
        "    This environment is part of the Box2D environments.\n",
        "</p>\n",
        "\n",
        "<ul>\n",
        "    <li> Action Space Discrete(4) </li>\n",
        "    <li> Observation Shape (8,) </li>\n",
        "    <li> Observation High [1.5 1.5 5. 5. 3.14 5. 1. 1. ] </li>\n",
        "    <li> Observation Low [-1.5 -1.5 -5. -5. -3.14 -5. -0. -0. ] </li>\n",
        "    <li> Import gymnasium.make(\"LunarLander-v2\") </li>\n",
        "</ul>\n",
        "\n",
        "<h3> Description </h3>\n",
        "<p>This environment is a classic rocket trajectory optimization problem. According to Pontryagin’s maximum principle, it is optimal to fire the engine at full throttle or turn it off. This is the reason why this environment has discrete actions: engine on or off.\n",
        "\n",
        "There are two environment versions: discrete or continuous. The landing pad is always at coordinates (0,0). The coordinates are the first two numbers in the state vector. Landing outside of the landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt.</p>\n",
        "\n",
        "<h3> Action Space </h3>\n",
        "<p>\n",
        "There are four discrete actions available:\n",
        "\n",
        "* 0: do nothing\n",
        "* 1: fire left orientation engine\n",
        "* 2: fire main engine\n",
        "* 3: fire right orientation engine\n",
        "\n",
        "</p>\n",
        "\n",
        "<h3> Observation Space </h3>\n",
        "<p>\n",
        "The state is an 8-dimensional vector: the coordinates of the lander in x & y, its linear velocities in x & y, its angle, its angular velocity, and two booleans that represent whether each leg is in contact with the ground or not.\n",
        "</p>\n",
        "\n",
        "<h3> Reward </h3>\n",
        "<p>\n",
        "After every step a reward is granted. The total reward of an episode is the sum of the rewards for all the steps within that episode.\n",
        "\n",
        "For each step, the reward:\n",
        "\n",
        "* is increased/decreased the closer/further the lander is to the landing pad.\n",
        "* is increased/decreased the slower/faster the lander is moving.\n",
        "* is decreased the more the lander is tilted (angle not horizontal).\n",
        "* is increased by 10 points for each leg that is in contact with the ground.\n",
        "* is decreased by 0.03 points each frame a side engine is firing.\n",
        "* is decreased by 0.3 points each frame the main engine is firing.\n",
        "\n",
        "The episode receive an additional reward of -100 or +100 points for crashing or landing safely respectively.\n",
        "\n",
        "An episode is considered a solution if it scores at least 200 points.\n",
        "</p>\n",
        "\n",
        "<h3> Starting State </h3>\n",
        "\n",
        "<p>The lander starts at the top center of the viewport with a random initial force applied to its center of mass.</p>\n",
        "\n",
        "<h3> Episode Termination </h3>\n",
        "<p> The episode finishes if:<br>\n",
        "    \n",
        "1. the lander crashes (the lander body gets in contact with the moon);<br>\n",
        "2. the lander gets outside of the viewport (x coordinate is greater than 1);<br>\n",
        "3. the lander is not awake. From the Box2D docs, a body which is not awake is a body which doesn’t move and doesn’t collide with any other body:<br>\n",
        "\n",
        "When Box2D determines that a body (or group of bodies) has come to rest, the body enters a sleep state which has very little CPU overhead. If a body is awake and collides with a sleeping body, then the sleeping body wakes up. Bodies will also wake up if a joint or contact attached to them is destroyed.\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3d0543a5",
      "metadata": {
        "scrolled": true,
        "id": "3d0543a5"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b3fb45b2",
      "metadata": {
        "id": "b3fb45b2",
        "outputId": "7a40d9ca-d5c2-47ca-f823-c7c18048e0aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Action inter is descrete 4\n",
            "Shape of Observation is (8,)\n"
          ]
        }
      ],
      "source": [
        "print(\"The Action inter is descrete {}\".format(env.action_space.n))\n",
        "print(\"Shape of Observation is {}\".format(env.observation_space.sample().shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b9ff9c9",
      "metadata": {
        "id": "8b9ff9c9"
      },
      "source": [
        "<h1> Baseline Model. </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d35101af",
      "metadata": {
        "scrolled": true,
        "id": "d35101af",
        "outputId": "da78a772-4679-4e5a-c9cb-f8c17406c4f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Reward after 10 max run is -1.3681605472415346\n"
          ]
        }
      ],
      "source": [
        "rewards = []\n",
        "obs = env.reset()\n",
        "done = False\n",
        "MAX_RUN = 10\n",
        "\n",
        "for i in range(MAX_RUN):\n",
        "    while not done:\n",
        "        env.render()\n",
        "        action_sample = env.action_space.sample()\n",
        "        # let's take a step in the environment\n",
        "        obs, rwd, done, info ,_  = env.step(action_sample)\n",
        "        rewards.append(rwd)\n",
        "env.close()\n",
        "print(\"Mean Reward after {} max run is {}\".format(MAX_RUN, np.mean(np.array(rewards))))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac737551",
      "metadata": {
        "id": "ac737551"
      },
      "source": [
        "<h1> Reinforcement Learning For Training The Model </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "bdaa0e55",
      "metadata": {
        "id": "bdaa0e55"
      },
      "outputs": [],
      "source": [
        "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
        "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
        "\n",
        "    :param check_freq: (int)\n",
        "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
        "      It must contains the file created by the ``Monitor`` wrapper.\n",
        "    :param verbose: (int)\n",
        "    \"\"\"\n",
        "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
        "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
        "        self.check_freq = check_freq\n",
        "        self.log_dir = log_dir\n",
        "        self.save_path = os.path.join(log_dir, 'best_model')\n",
        "        self.best_mean_reward = -np.inf\n",
        "\n",
        "    def _init_callback(self) -> None:\n",
        "        # Create folder if needed\n",
        "        if self.save_path is not None:\n",
        "            os.makedirs(self.save_path, exist_ok=True)\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.n_calls % self.check_freq == 0:\n",
        "\n",
        "          # Retrieve training reward\n",
        "          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
        "          if len(x) > 0:\n",
        "              # Mean training reward over the last 100 episodes\n",
        "              mean_reward = np.mean(y[-100:])\n",
        "              if self.verbose > 0:\n",
        "                print(f\"Num timesteps: {self.num_timesteps}\")\n",
        "                print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n",
        "\n",
        "              # New best model, you could save the agent here\n",
        "              if mean_reward > self.best_mean_reward:\n",
        "                  self.best_mean_reward = mean_reward\n",
        "                  # Example for saving best model\n",
        "                  if self.verbose > 0:\n",
        "                    print(f\"Saving new best model to {self.save_path}.zip\")\n",
        "                  self.model.save(self.save_path)\n",
        "\n",
        "        return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "2d311d6a",
      "metadata": {
        "scrolled": true,
        "id": "2d311d6a",
        "outputId": "19628099-224c-4f44-a524-92d9412a1a50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "|    fps              | 401      |\n",
            "|    time_elapsed     | 6372     |\n",
            "|    total_timesteps  | 2561581  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0976   |\n",
            "|    n_updates        | 627895   |\n",
            "----------------------------------\n",
            "Num timesteps: 2562000\n",
            "Best mean reward: 241.42 - Last mean reward per episode: 238.02\n",
            "Num timesteps: 2563000\n",
            "Best mean reward: 241.42 - Last mean reward per episode: 240.53\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 470      |\n",
            "|    ep_rew_mean      | 240      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4456     |\n",
            "|    fps              | 402      |\n",
            "|    time_elapsed     | 6375     |\n",
            "|    total_timesteps  | 2563377  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0414   |\n",
            "|    n_updates        | 628344   |\n",
            "----------------------------------\n",
            "Num timesteps: 2564000\n",
            "Best mean reward: 241.42 - Last mean reward per episode: 240.84\n",
            "Num timesteps: 2565000\n",
            "Best mean reward: 241.42 - Last mean reward per episode: 240.66\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 467      |\n",
            "|    ep_rew_mean      | 240      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4460     |\n",
            "|    fps              | 402      |\n",
            "|    time_elapsed     | 6379     |\n",
            "|    total_timesteps  | 2565248  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.125    |\n",
            "|    n_updates        | 628811   |\n",
            "----------------------------------\n",
            "Num timesteps: 2566000\n",
            "Best mean reward: 241.42 - Last mean reward per episode: 240.38\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 465      |\n",
            "|    ep_rew_mean      | 241      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4464     |\n",
            "|    fps              | 402      |\n",
            "|    time_elapsed     | 6382     |\n",
            "|    total_timesteps  | 2566851  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.354    |\n",
            "|    n_updates        | 629212   |\n",
            "----------------------------------\n",
            "Num timesteps: 2567000\n",
            "Best mean reward: 241.42 - Last mean reward per episode: 240.58\n",
            "Num timesteps: 2568000\n",
            "Best mean reward: 241.42 - Last mean reward per episode: 239.87\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 466      |\n",
            "|    ep_rew_mean      | 240      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4468     |\n",
            "|    fps              | 402      |\n",
            "|    time_elapsed     | 6385     |\n",
            "|    total_timesteps  | 2568569  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.106    |\n",
            "|    n_updates        | 629642   |\n",
            "----------------------------------\n",
            "Num timesteps: 2569000\n",
            "Best mean reward: 241.42 - Last mean reward per episode: 240.10\n",
            "Num timesteps: 2570000\n",
            "Best mean reward: 241.42 - Last mean reward per episode: 239.71\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 471      |\n",
            "|    ep_rew_mean      | 240      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4472     |\n",
            "|    fps              | 402      |\n",
            "|    time_elapsed     | 6389     |\n",
            "|    total_timesteps  | 2570470  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.106    |\n",
            "|    n_updates        | 630117   |\n",
            "----------------------------------\n",
            "Num timesteps: 2571000\n",
            "Best mean reward: 241.42 - Last mean reward per episode: 240.18\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 468      |\n",
            "|    ep_rew_mean      | 240      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4476     |\n",
            "|    fps              | 402      |\n",
            "|    time_elapsed     | 6391     |\n",
            "|    total_timesteps  | 2571817  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.772    |\n",
            "|    n_updates        | 630454   |\n",
            "----------------------------------\n",
            "Num timesteps: 2572000\n",
            "Best mean reward: 241.42 - Last mean reward per episode: 240.12\n",
            "Num timesteps: 2573000\n",
            "Best mean reward: 241.42 - Last mean reward per episode: 240.25\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 467      |\n",
            "|    ep_rew_mean      | 240      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4480     |\n",
            "|    fps              | 402      |\n",
            "|    time_elapsed     | 6395     |\n",
            "|    total_timesteps  | 2573514  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.136    |\n",
            "|    n_updates        | 630878   |\n",
            "----------------------------------\n",
            "Num timesteps: 2574000\n",
            "Best mean reward: 241.42 - Last mean reward per episode: 239.95\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 461      |\n",
            "|    ep_rew_mean      | 240      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4484     |\n",
            "|    fps              | 402      |\n",
            "|    time_elapsed     | 6396     |\n",
            "|    total_timesteps  | 2574585  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.532    |\n",
            "|    n_updates        | 631146   |\n",
            "----------------------------------\n",
            "Num timesteps: 2575000\n",
            "Best mean reward: 241.42 - Last mean reward per episode: 240.43\n",
            "Num timesteps: 2576000\n",
            "Best mean reward: 241.42 - Last mean reward per episode: 239.99\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 461      |\n",
            "|    ep_rew_mean      | 240      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4488     |\n",
            "|    fps              | 402      |\n",
            "|    time_elapsed     | 6399     |\n",
            "|    total_timesteps  | 2576282  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.104    |\n",
            "|    n_updates        | 631570   |\n",
            "----------------------------------\n",
            "Num timesteps: 2577000\n",
            "Best mean reward: 241.42 - Last mean reward per episode: 240.50\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 456      |\n",
            "|    ep_rew_mean      | 238      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4492     |\n",
            "|    fps              | 402      |\n",
            "|    time_elapsed     | 6401     |\n",
            "|    total_timesteps  | 2577339  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.03     |\n",
            "|    n_updates        | 631834   |\n",
            "----------------------------------\n",
            "Num timesteps: 2578000\n",
            "Best mean reward: 241.42 - Last mean reward per episode: 237.78\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 451      |\n",
            "|    ep_rew_mean      | 238      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4496     |\n",
            "|    fps              | 402      |\n",
            "|    time_elapsed     | 6403     |\n",
            "|    total_timesteps  | 2578594  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.244    |\n",
            "|    n_updates        | 632148   |\n",
            "----------------------------------\n",
            "Num timesteps: 2579000\n",
            "Best mean reward: 241.42 - Last mean reward per episode: 237.92\n",
            "Num timesteps: 2580000\n",
            "Best mean reward: 241.42 - Last mean reward per episode: 238.00\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 448      |\n",
            "|    ep_rew_mean      | 238      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4500     |\n",
            "|    fps              | 402      |\n",
            "|    time_elapsed     | 6406     |\n",
            "|    total_timesteps  | 2580083  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.135    |\n",
            "|    n_updates        | 632520   |\n",
            "----------------------------------\n",
            "Num timesteps: 2581000\n",
            "Best mean reward: 241.42 - Last mean reward per episode: 238.68\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 445      |\n",
            "|    ep_rew_mean      | 239      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4504     |\n",
            "|    fps              | 402      |\n",
            "|    time_elapsed     | 6408     |\n",
            "|    total_timesteps  | 2581482  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.138    |\n",
            "|    n_updates        | 632870   |\n",
            "----------------------------------\n",
            "Num timesteps: 2582000\n",
            "Best mean reward: 241.42 - Last mean reward per episode: 239.47\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 443      |\n",
            "|    ep_rew_mean      | 240      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4508     |\n",
            "|    fps              | 402      |\n",
            "|    time_elapsed     | 6411     |\n",
            "|    total_timesteps  | 2582963  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.15     |\n",
            "|    n_updates        | 633240   |\n",
            "----------------------------------\n",
            "Num timesteps: 2583000\n",
            "Best mean reward: 241.42 - Last mean reward per episode: 239.50\n",
            "Num timesteps: 2584000\n",
            "Best mean reward: 241.42 - Last mean reward per episode: 239.62\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 437      |\n",
            "|    ep_rew_mean      | 240      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4512     |\n",
            "|    fps              | 402      |\n",
            "|    time_elapsed     | 6413     |\n",
            "|    total_timesteps  | 2584347  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.354    |\n",
            "|    n_updates        | 633586   |\n",
            "----------------------------------\n",
            "Num timesteps: 2585000\n",
            "Best mean reward: 241.42 - Last mean reward per episode: 241.08\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 423      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4516     |\n",
            "|    fps              | 402      |\n",
            "|    time_elapsed     | 6416     |\n",
            "|    total_timesteps  | 2585672  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.713    |\n",
            "|    n_updates        | 633917   |\n",
            "----------------------------------\n",
            "Num timesteps: 2586000\n",
            "Best mean reward: 241.42 - Last mean reward per episode: 242.65\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 2587000\n",
            "Best mean reward: 242.65 - Last mean reward per episode: 242.49\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 414      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4520     |\n",
            "|    fps              | 403      |\n",
            "|    time_elapsed     | 6420     |\n",
            "|    total_timesteps  | 2587580  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.112    |\n",
            "|    n_updates        | 634394   |\n",
            "----------------------------------\n",
            "Num timesteps: 2588000\n",
            "Best mean reward: 242.65 - Last mean reward per episode: 243.38\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 2589000\n",
            "Best mean reward: 243.38 - Last mean reward per episode: 244.82\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 403      |\n",
            "|    ep_rew_mean      | 245      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4524     |\n",
            "|    fps              | 403      |\n",
            "|    time_elapsed     | 6423     |\n",
            "|    total_timesteps  | 2589160  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.106    |\n",
            "|    n_updates        | 634789   |\n",
            "----------------------------------\n",
            "Num timesteps: 2590000\n",
            "Best mean reward: 244.82 - Last mean reward per episode: 245.97\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 401      |\n",
            "|    ep_rew_mean      | 246      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4528     |\n",
            "|    fps              | 403      |\n",
            "|    time_elapsed     | 6425     |\n",
            "|    total_timesteps  | 2590726  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.123    |\n",
            "|    n_updates        | 635181   |\n",
            "----------------------------------\n",
            "Num timesteps: 2591000\n",
            "Best mean reward: 245.97 - Last mean reward per episode: 245.83\n",
            "Num timesteps: 2592000\n",
            "Best mean reward: 245.97 - Last mean reward per episode: 246.20\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 400      |\n",
            "|    ep_rew_mean      | 247      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4532     |\n",
            "|    fps              | 403      |\n",
            "|    time_elapsed     | 6429     |\n",
            "|    total_timesteps  | 2592566  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0828   |\n",
            "|    n_updates        | 635641   |\n",
            "----------------------------------\n",
            "Num timesteps: 2593000\n",
            "Best mean reward: 246.20 - Last mean reward per episode: 246.56\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 2594000\n",
            "Best mean reward: 246.56 - Last mean reward per episode: 246.08\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 396      |\n",
            "|    ep_rew_mean      | 246      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4536     |\n",
            "|    fps              | 403      |\n",
            "|    time_elapsed     | 6433     |\n",
            "|    total_timesteps  | 2594398  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.129    |\n",
            "|    n_updates        | 636099   |\n",
            "----------------------------------\n",
            "Num timesteps: 2595000\n",
            "Best mean reward: 246.56 - Last mean reward per episode: 246.10\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 393      |\n",
            "|    ep_rew_mean      | 247      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4540     |\n",
            "|    fps              | 403      |\n",
            "|    time_elapsed     | 6435     |\n",
            "|    total_timesteps  | 2595755  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.13     |\n",
            "|    n_updates        | 636438   |\n",
            "----------------------------------\n",
            "Num timesteps: 2596000\n",
            "Best mean reward: 246.56 - Last mean reward per episode: 246.75\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 2597000\n",
            "Best mean reward: 246.75 - Last mean reward per episode: 246.31\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 389      |\n",
            "|    ep_rew_mean      | 247      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4544     |\n",
            "|    fps              | 403      |\n",
            "|    time_elapsed     | 6437     |\n",
            "|    total_timesteps  | 2597225  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0696   |\n",
            "|    n_updates        | 636806   |\n",
            "----------------------------------\n",
            "Num timesteps: 2598000\n",
            "Best mean reward: 246.75 - Last mean reward per episode: 246.32\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 384      |\n",
            "|    ep_rew_mean      | 247      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4548     |\n",
            "|    fps              | 403      |\n",
            "|    time_elapsed     | 6440     |\n",
            "|    total_timesteps  | 2598358  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0897   |\n",
            "|    n_updates        | 637089   |\n",
            "----------------------------------\n",
            "Num timesteps: 2599000\n",
            "Best mean reward: 246.75 - Last mean reward per episode: 246.37\n",
            "Num timesteps: 2600000\n",
            "Best mean reward: 246.75 - Last mean reward per episode: 246.50\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 385      |\n",
            "|    ep_rew_mean      | 247      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4552     |\n",
            "|    fps              | 403      |\n",
            "|    time_elapsed     | 6443     |\n",
            "|    total_timesteps  | 2600117  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.92     |\n",
            "|    n_updates        | 637529   |\n",
            "----------------------------------\n",
            "Num timesteps: 2601000\n",
            "Best mean reward: 246.75 - Last mean reward per episode: 247.23\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 386      |\n",
            "|    ep_rew_mean      | 247      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4556     |\n",
            "|    fps              | 403      |\n",
            "|    time_elapsed     | 6447     |\n",
            "|    total_timesteps  | 2601972  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.327    |\n",
            "|    n_updates        | 637992   |\n",
            "----------------------------------\n",
            "Num timesteps: 2602000\n",
            "Best mean reward: 247.23 - Last mean reward per episode: 247.33\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 2603000\n",
            "Best mean reward: 247.33 - Last mean reward per episode: 247.19\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 382      |\n",
            "|    ep_rew_mean      | 248      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4560     |\n",
            "|    fps              | 403      |\n",
            "|    time_elapsed     | 6449     |\n",
            "|    total_timesteps  | 2603436  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.124    |\n",
            "|    n_updates        | 638358   |\n",
            "----------------------------------\n",
            "Num timesteps: 2604000\n",
            "Best mean reward: 247.33 - Last mean reward per episode: 248.10\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 381      |\n",
            "|    ep_rew_mean      | 248      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4564     |\n",
            "|    fps              | 403      |\n",
            "|    time_elapsed     | 6452     |\n",
            "|    total_timesteps  | 2604952  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0987   |\n",
            "|    n_updates        | 638737   |\n",
            "----------------------------------\n",
            "Num timesteps: 2605000\n",
            "Best mean reward: 248.10 - Last mean reward per episode: 247.72\n",
            "Num timesteps: 2606000\n",
            "Best mean reward: 248.10 - Last mean reward per episode: 248.88\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 381      |\n",
            "|    ep_rew_mean      | 249      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4568     |\n",
            "|    fps              | 403      |\n",
            "|    time_elapsed     | 6456     |\n",
            "|    total_timesteps  | 2606670  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0781   |\n",
            "|    n_updates        | 639167   |\n",
            "----------------------------------\n",
            "Num timesteps: 2607000\n",
            "Best mean reward: 248.88 - Last mean reward per episode: 248.86\n",
            "Num timesteps: 2608000\n",
            "Best mean reward: 248.88 - Last mean reward per episode: 249.52\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 377      |\n",
            "|    ep_rew_mean      | 250      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4572     |\n",
            "|    fps              | 403      |\n",
            "|    time_elapsed     | 6459     |\n",
            "|    total_timesteps  | 2608185  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0783   |\n",
            "|    n_updates        | 639546   |\n",
            "----------------------------------\n",
            "Num timesteps: 2609000\n",
            "Best mean reward: 249.52 - Last mean reward per episode: 249.41\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 381      |\n",
            "|    ep_rew_mean      | 250      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4576     |\n",
            "|    fps              | 403      |\n",
            "|    time_elapsed     | 6462     |\n",
            "|    total_timesteps  | 2609887  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0632   |\n",
            "|    n_updates        | 639971   |\n",
            "----------------------------------\n",
            "Num timesteps: 2610000\n",
            "Best mean reward: 249.52 - Last mean reward per episode: 249.63\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 2611000\n",
            "Best mean reward: 249.63 - Last mean reward per episode: 248.99\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 384      |\n",
            "|    ep_rew_mean      | 250      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4580     |\n",
            "|    fps              | 403      |\n",
            "|    time_elapsed     | 6466     |\n",
            "|    total_timesteps  | 2611864  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0818   |\n",
            "|    n_updates        | 640465   |\n",
            "----------------------------------\n",
            "Num timesteps: 2612000\n",
            "Best mean reward: 249.63 - Last mean reward per episode: 249.67\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 2613000\n",
            "Best mean reward: 249.67 - Last mean reward per episode: 248.67\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 390      |\n",
            "|    ep_rew_mean      | 249      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4584     |\n",
            "|    fps              | 403      |\n",
            "|    time_elapsed     | 6469     |\n",
            "|    total_timesteps  | 2613601  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0659   |\n",
            "|    n_updates        | 640900   |\n",
            "----------------------------------\n",
            "Num timesteps: 2614000\n",
            "Best mean reward: 249.67 - Last mean reward per episode: 248.54\n",
            "Num timesteps: 2615000\n",
            "Best mean reward: 249.67 - Last mean reward per episode: 248.27\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 389      |\n",
            "|    ep_rew_mean      | 249      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4588     |\n",
            "|    fps              | 404      |\n",
            "|    time_elapsed     | 6472     |\n",
            "|    total_timesteps  | 2615175  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0755   |\n",
            "|    n_updates        | 641293   |\n",
            "----------------------------------\n",
            "Num timesteps: 2616000\n",
            "Best mean reward: 249.67 - Last mean reward per episode: 248.72\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 395      |\n",
            "|    ep_rew_mean      | 251      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4592     |\n",
            "|    fps              | 404      |\n",
            "|    time_elapsed     | 6475     |\n",
            "|    total_timesteps  | 2616795  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.26     |\n",
            "|    n_updates        | 641698   |\n",
            "----------------------------------\n",
            "Num timesteps: 2617000\n",
            "Best mean reward: 249.67 - Last mean reward per episode: 251.09\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 2618000\n",
            "Best mean reward: 251.09 - Last mean reward per episode: 251.25\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 395      |\n",
            "|    ep_rew_mean      | 251      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4596     |\n",
            "|    fps              | 404      |\n",
            "|    time_elapsed     | 6478     |\n",
            "|    total_timesteps  | 2618090  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0687   |\n",
            "|    n_updates        | 642022   |\n",
            "----------------------------------\n",
            "Num timesteps: 2619000\n",
            "Best mean reward: 251.25 - Last mean reward per episode: 251.25\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 396      |\n",
            "|    ep_rew_mean      | 251      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4600     |\n",
            "|    fps              | 404      |\n",
            "|    time_elapsed     | 6481     |\n",
            "|    total_timesteps  | 2619679  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.131    |\n",
            "|    n_updates        | 642419   |\n",
            "----------------------------------\n",
            "Num timesteps: 2620000\n",
            "Best mean reward: 251.25 - Last mean reward per episode: 251.33\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 2621000\n",
            "Best mean reward: 251.33 - Last mean reward per episode: 250.03\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 398      |\n",
            "|    ep_rew_mean      | 250      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4604     |\n",
            "|    fps              | 404      |\n",
            "|    time_elapsed     | 6483     |\n",
            "|    total_timesteps  | 2621293  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.267    |\n",
            "|    n_updates        | 642823   |\n",
            "----------------------------------\n",
            "Num timesteps: 2622000\n",
            "Best mean reward: 251.33 - Last mean reward per episode: 249.38\n",
            "Num timesteps: 2623000\n",
            "Best mean reward: 251.33 - Last mean reward per episode: 248.77\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 405      |\n",
            "|    ep_rew_mean      | 249      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4608     |\n",
            "|    fps              | 404      |\n",
            "|    time_elapsed     | 6488     |\n",
            "|    total_timesteps  | 2623422  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.11     |\n",
            "|    n_updates        | 643355   |\n",
            "----------------------------------\n",
            "Num timesteps: 2624000\n",
            "Best mean reward: 251.33 - Last mean reward per episode: 248.72\n",
            "Num timesteps: 2625000\n",
            "Best mean reward: 251.33 - Last mean reward per episode: 248.73\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 408      |\n",
            "|    ep_rew_mean      | 249      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4612     |\n",
            "|    fps              | 404      |\n",
            "|    time_elapsed     | 6491     |\n",
            "|    total_timesteps  | 2625148  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.33     |\n",
            "|    n_updates        | 643786   |\n",
            "----------------------------------\n",
            "Num timesteps: 2626000\n",
            "Best mean reward: 251.33 - Last mean reward per episode: 249.19\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 413      |\n",
            "|    ep_rew_mean      | 248      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4616     |\n",
            "|    fps              | 404      |\n",
            "|    time_elapsed     | 6495     |\n",
            "|    total_timesteps  | 2626949  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.53     |\n",
            "|    n_updates        | 644237   |\n",
            "----------------------------------\n",
            "Num timesteps: 2627000\n",
            "Best mean reward: 251.33 - Last mean reward per episode: 248.22\n",
            "Num timesteps: 2628000\n",
            "Best mean reward: 251.33 - Last mean reward per episode: 248.55\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 407      |\n",
            "|    ep_rew_mean      | 249      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4620     |\n",
            "|    fps              | 404      |\n",
            "|    time_elapsed     | 6497     |\n",
            "|    total_timesteps  | 2628310  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0959   |\n",
            "|    n_updates        | 644577   |\n",
            "----------------------------------\n",
            "Num timesteps: 2629000\n",
            "Best mean reward: 251.33 - Last mean reward per episode: 248.67\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 406      |\n",
            "|    ep_rew_mean      | 248      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4624     |\n",
            "|    fps              | 404      |\n",
            "|    time_elapsed     | 6500     |\n",
            "|    total_timesteps  | 2629775  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.082    |\n",
            "|    n_updates        | 644943   |\n",
            "----------------------------------\n",
            "Num timesteps: 2630000\n",
            "Best mean reward: 251.33 - Last mean reward per episode: 248.25\n",
            "Num timesteps: 2631000\n",
            "Best mean reward: 251.33 - Last mean reward per episode: 247.66\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 403      |\n",
            "|    ep_rew_mean      | 248      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4628     |\n",
            "|    fps              | 404      |\n",
            "|    time_elapsed     | 6502     |\n",
            "|    total_timesteps  | 2631054  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.106    |\n",
            "|    n_updates        | 645263   |\n",
            "----------------------------------\n",
            "Num timesteps: 2632000\n",
            "Best mean reward: 251.33 - Last mean reward per episode: 247.86\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 400      |\n",
            "|    ep_rew_mean      | 248      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4632     |\n",
            "|    fps              | 404      |\n",
            "|    time_elapsed     | 6505     |\n",
            "|    total_timesteps  | 2632531  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.251    |\n",
            "|    n_updates        | 645632   |\n",
            "----------------------------------\n",
            "Num timesteps: 2633000\n",
            "Best mean reward: 251.33 - Last mean reward per episode: 248.98\n",
            "Num timesteps: 2634000\n",
            "Best mean reward: 251.33 - Last mean reward per episode: 249.69\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 396      |\n",
            "|    ep_rew_mean      | 250      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4636     |\n",
            "|    fps              | 404      |\n",
            "|    time_elapsed     | 6508     |\n",
            "|    total_timesteps  | 2634046  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.08     |\n",
            "|    n_updates        | 646011   |\n",
            "----------------------------------\n",
            "Num timesteps: 2635000\n",
            "Best mean reward: 251.33 - Last mean reward per episode: 250.06\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 396      |\n",
            "|    ep_rew_mean      | 250      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4640     |\n",
            "|    fps              | 404      |\n",
            "|    time_elapsed     | 6510     |\n",
            "|    total_timesteps  | 2635383  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.486    |\n",
            "|    n_updates        | 646345   |\n",
            "----------------------------------\n",
            "Num timesteps: 2636000\n",
            "Best mean reward: 251.33 - Last mean reward per episode: 250.35\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 395      |\n",
            "|    ep_rew_mean      | 250      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4644     |\n",
            "|    fps              | 404      |\n",
            "|    time_elapsed     | 6512     |\n",
            "|    total_timesteps  | 2636686  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0706   |\n",
            "|    n_updates        | 646671   |\n",
            "----------------------------------\n",
            "Num timesteps: 2637000\n",
            "Best mean reward: 251.33 - Last mean reward per episode: 250.30\n",
            "Num timesteps: 2638000\n",
            "Best mean reward: 251.33 - Last mean reward per episode: 250.51\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 397      |\n",
            "|    ep_rew_mean      | 250      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4648     |\n",
            "|    fps              | 404      |\n",
            "|    time_elapsed     | 6515     |\n",
            "|    total_timesteps  | 2638062  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.123    |\n",
            "|    n_updates        | 647015   |\n",
            "----------------------------------\n",
            "Num timesteps: 2639000\n",
            "Best mean reward: 251.33 - Last mean reward per episode: 250.94\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 392      |\n",
            "|    ep_rew_mean      | 251      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4652     |\n",
            "|    fps              | 404      |\n",
            "|    time_elapsed     | 6517     |\n",
            "|    total_timesteps  | 2639273  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.706    |\n",
            "|    n_updates        | 647318   |\n",
            "----------------------------------\n",
            "Num timesteps: 2640000\n",
            "Best mean reward: 251.33 - Last mean reward per episode: 250.56\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 388      |\n",
            "|    ep_rew_mean      | 251      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4656     |\n",
            "|    fps              | 405      |\n",
            "|    time_elapsed     | 6519     |\n",
            "|    total_timesteps  | 2640757  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.132    |\n",
            "|    n_updates        | 647689   |\n",
            "----------------------------------\n",
            "Num timesteps: 2641000\n",
            "Best mean reward: 251.33 - Last mean reward per episode: 250.97\n",
            "Num timesteps: 2642000\n",
            "Best mean reward: 251.33 - Last mean reward per episode: 251.50\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 391      |\n",
            "|    ep_rew_mean      | 251      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4660     |\n",
            "|    fps              | 405      |\n",
            "|    time_elapsed     | 6523     |\n",
            "|    total_timesteps  | 2642505  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.835    |\n",
            "|    n_updates        | 648126   |\n",
            "----------------------------------\n",
            "Num timesteps: 2643000\n",
            "Best mean reward: 251.50 - Last mean reward per episode: 251.21\n",
            "Num timesteps: 2644000\n",
            "Best mean reward: 251.50 - Last mean reward per episode: 251.84\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 394      |\n",
            "|    ep_rew_mean      | 251      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4664     |\n",
            "|    fps              | 405      |\n",
            "|    time_elapsed     | 6527     |\n",
            "|    total_timesteps  | 2644325  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.04     |\n",
            "|    n_updates        | 648581   |\n",
            "----------------------------------\n",
            "Num timesteps: 2645000\n",
            "Best mean reward: 251.84 - Last mean reward per episode: 250.89\n",
            "Num timesteps: 2646000\n",
            "Best mean reward: 251.84 - Last mean reward per episode: 250.54\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 394      |\n",
            "|    ep_rew_mean      | 250      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4668     |\n",
            "|    fps              | 405      |\n",
            "|    time_elapsed     | 6530     |\n",
            "|    total_timesteps  | 2646086  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.652    |\n",
            "|    n_updates        | 649021   |\n",
            "----------------------------------\n",
            "Num timesteps: 2647000\n",
            "Best mean reward: 251.84 - Last mean reward per episode: 250.28\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 394      |\n",
            "|    ep_rew_mean      | 250      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4672     |\n",
            "|    fps              | 405      |\n",
            "|    time_elapsed     | 6533     |\n",
            "|    total_timesteps  | 2647579  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.187    |\n",
            "|    n_updates        | 649394   |\n",
            "----------------------------------\n",
            "Num timesteps: 2648000\n",
            "Best mean reward: 251.84 - Last mean reward per episode: 249.83\n",
            "Num timesteps: 2649000\n",
            "Best mean reward: 251.84 - Last mean reward per episode: 249.54\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 394      |\n",
            "|    ep_rew_mean      | 249      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4676     |\n",
            "|    fps              | 405      |\n",
            "|    time_elapsed     | 6537     |\n",
            "|    total_timesteps  | 2649252  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.123    |\n",
            "|    n_updates        | 649812   |\n",
            "----------------------------------\n",
            "Num timesteps: 2650000\n",
            "Best mean reward: 251.84 - Last mean reward per episode: 249.13\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 390      |\n",
            "|    ep_rew_mean      | 249      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4680     |\n",
            "|    fps              | 405      |\n",
            "|    time_elapsed     | 6540     |\n",
            "|    total_timesteps  | 2650864  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.142    |\n",
            "|    n_updates        | 650215   |\n",
            "----------------------------------\n",
            "Num timesteps: 2651000\n",
            "Best mean reward: 251.84 - Last mean reward per episode: 249.26\n",
            "Num timesteps: 2652000\n",
            "Best mean reward: 251.84 - Last mean reward per episode: 250.59\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 387      |\n",
            "|    ep_rew_mean      | 251      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4684     |\n",
            "|    fps              | 405      |\n",
            "|    time_elapsed     | 6542     |\n",
            "|    total_timesteps  | 2652292  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.189    |\n",
            "|    n_updates        | 650572   |\n",
            "----------------------------------\n",
            "Num timesteps: 2653000\n",
            "Best mean reward: 251.84 - Last mean reward per episode: 250.84\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 386      |\n",
            "|    ep_rew_mean      | 251      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4688     |\n",
            "|    fps              | 405      |\n",
            "|    time_elapsed     | 6545     |\n",
            "|    total_timesteps  | 2653777  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.146    |\n",
            "|    n_updates        | 650944   |\n",
            "----------------------------------\n",
            "Num timesteps: 2654000\n",
            "Best mean reward: 251.84 - Last mean reward per episode: 251.12\n",
            "Num timesteps: 2655000\n",
            "Best mean reward: 251.84 - Last mean reward per episode: 251.57\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 387      |\n",
            "|    ep_rew_mean      | 252      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4692     |\n",
            "|    fps              | 405      |\n",
            "|    time_elapsed     | 6548     |\n",
            "|    total_timesteps  | 2655492  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.37     |\n",
            "|    n_updates        | 651372   |\n",
            "----------------------------------\n",
            "Num timesteps: 2656000\n",
            "Best mean reward: 251.84 - Last mean reward per episode: 252.02\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 389      |\n",
            "|    ep_rew_mean      | 252      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4696     |\n",
            "|    fps              | 405      |\n",
            "|    time_elapsed     | 6551     |\n",
            "|    total_timesteps  | 2656973  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.811    |\n",
            "|    n_updates        | 651743   |\n",
            "----------------------------------\n",
            "Num timesteps: 2657000\n",
            "Best mean reward: 252.02 - Last mean reward per episode: 251.51\n",
            "Num timesteps: 2658000\n",
            "Best mean reward: 252.02 - Last mean reward per episode: 251.53\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 390      |\n",
            "|    ep_rew_mean      | 252      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4700     |\n",
            "|    fps              | 405      |\n",
            "|    time_elapsed     | 6554     |\n",
            "|    total_timesteps  | 2658655  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.188    |\n",
            "|    n_updates        | 652163   |\n",
            "----------------------------------\n",
            "Num timesteps: 2659000\n",
            "Best mean reward: 252.02 - Last mean reward per episode: 252.20\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 2660000\n",
            "Best mean reward: 252.20 - Last mean reward per episode: 251.76\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 392      |\n",
            "|    ep_rew_mean      | 253      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4704     |\n",
            "|    fps              | 405      |\n",
            "|    time_elapsed     | 6558     |\n",
            "|    total_timesteps  | 2660466  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.155    |\n",
            "|    n_updates        | 652616   |\n",
            "----------------------------------\n",
            "Num timesteps: 2661000\n",
            "Best mean reward: 252.20 - Last mean reward per episode: 253.39\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 385      |\n",
            "|    ep_rew_mean      | 254      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4708     |\n",
            "|    fps              | 405      |\n",
            "|    time_elapsed     | 6561     |\n",
            "|    total_timesteps  | 2661936  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0815   |\n",
            "|    n_updates        | 652983   |\n",
            "----------------------------------\n",
            "Num timesteps: 2662000\n",
            "Best mean reward: 253.39 - Last mean reward per episode: 254.47\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 2663000\n",
            "Best mean reward: 254.47 - Last mean reward per episode: 254.78\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 385      |\n",
            "|    ep_rew_mean      | 254      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4712     |\n",
            "|    fps              | 405      |\n",
            "|    time_elapsed     | 6564     |\n",
            "|    total_timesteps  | 2663620  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.123    |\n",
            "|    n_updates        | 653404   |\n",
            "----------------------------------\n",
            "Num timesteps: 2664000\n",
            "Best mean reward: 254.78 - Last mean reward per episode: 254.39\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 379      |\n",
            "|    ep_rew_mean      | 255      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4716     |\n",
            "|    fps              | 405      |\n",
            "|    time_elapsed     | 6566     |\n",
            "|    total_timesteps  | 2664875  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.208    |\n",
            "|    n_updates        | 653718   |\n",
            "----------------------------------\n",
            "Num timesteps: 2665000\n",
            "Best mean reward: 254.78 - Last mean reward per episode: 254.90\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 2666000\n",
            "Best mean reward: 254.90 - Last mean reward per episode: 255.26\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 380      |\n",
            "|    ep_rew_mean      | 255      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4720     |\n",
            "|    fps              | 405      |\n",
            "|    time_elapsed     | 6568     |\n",
            "|    total_timesteps  | 2666304  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.113    |\n",
            "|    n_updates        | 654075   |\n",
            "----------------------------------\n",
            "Num timesteps: 2667000\n",
            "Best mean reward: 255.26 - Last mean reward per episode: 255.85\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 380      |\n",
            "|    ep_rew_mean      | 256      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4724     |\n",
            "|    fps              | 405      |\n",
            "|    time_elapsed     | 6571     |\n",
            "|    total_timesteps  | 2667761  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.179    |\n",
            "|    n_updates        | 654440   |\n",
            "----------------------------------\n",
            "Num timesteps: 2668000\n",
            "Best mean reward: 255.85 - Last mean reward per episode: 255.99\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 2669000\n",
            "Best mean reward: 255.99 - Last mean reward per episode: 255.90\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 383      |\n",
            "|    ep_rew_mean      | 256      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4728     |\n",
            "|    fps              | 406      |\n",
            "|    time_elapsed     | 6574     |\n",
            "|    total_timesteps  | 2669375  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0953   |\n",
            "|    n_updates        | 654843   |\n",
            "----------------------------------\n",
            "Num timesteps: 2670000\n",
            "Best mean reward: 255.99 - Last mean reward per episode: 256.97\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 381      |\n",
            "|    ep_rew_mean      | 257      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4732     |\n",
            "|    fps              | 406      |\n",
            "|    time_elapsed     | 6576     |\n",
            "|    total_timesteps  | 2670604  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.141    |\n",
            "|    n_updates        | 655150   |\n",
            "----------------------------------\n",
            "Num timesteps: 2671000\n",
            "Best mean reward: 256.97 - Last mean reward per episode: 257.40\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 2672000\n",
            "Best mean reward: 257.40 - Last mean reward per episode: 256.35\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 382      |\n",
            "|    ep_rew_mean      | 257      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4736     |\n",
            "|    fps              | 406      |\n",
            "|    time_elapsed     | 6579     |\n",
            "|    total_timesteps  | 2672246  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.102    |\n",
            "|    n_updates        | 655561   |\n",
            "----------------------------------\n",
            "Num timesteps: 2673000\n",
            "Best mean reward: 257.40 - Last mean reward per episode: 256.51\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 386      |\n",
            "|    ep_rew_mean      | 256      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4740     |\n",
            "|    fps              | 406      |\n",
            "|    time_elapsed     | 6582     |\n",
            "|    total_timesteps  | 2673984  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.16     |\n",
            "|    n_updates        | 655995   |\n",
            "----------------------------------\n",
            "Num timesteps: 2674000\n",
            "Best mean reward: 257.40 - Last mean reward per episode: 256.02\n",
            "Num timesteps: 2675000\n",
            "Best mean reward: 257.40 - Last mean reward per episode: 256.38\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 389      |\n",
            "|    ep_rew_mean      | 256      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4744     |\n",
            "|    fps              | 406      |\n",
            "|    time_elapsed     | 6585     |\n",
            "|    total_timesteps  | 2675573  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.97     |\n",
            "|    n_updates        | 656393   |\n",
            "----------------------------------\n",
            "Num timesteps: 2676000\n",
            "Best mean reward: 257.40 - Last mean reward per episode: 256.29\n",
            "Num timesteps: 2677000\n",
            "Best mean reward: 257.40 - Last mean reward per episode: 256.10\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 392      |\n",
            "|    ep_rew_mean      | 256      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4748     |\n",
            "|    fps              | 406      |\n",
            "|    time_elapsed     | 6588     |\n",
            "|    total_timesteps  | 2677214  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.117    |\n",
            "|    n_updates        | 656803   |\n",
            "----------------------------------\n",
            "Num timesteps: 2678000\n",
            "Best mean reward: 257.40 - Last mean reward per episode: 255.79\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 393      |\n",
            "|    ep_rew_mean      | 256      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4752     |\n",
            "|    fps              | 406      |\n",
            "|    time_elapsed     | 6590     |\n",
            "|    total_timesteps  | 2678573  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.132    |\n",
            "|    n_updates        | 657143   |\n",
            "----------------------------------\n",
            "Num timesteps: 2679000\n",
            "Best mean reward: 257.40 - Last mean reward per episode: 256.17\n",
            "Num timesteps: 2680000\n",
            "Best mean reward: 257.40 - Last mean reward per episode: 256.14\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 396      |\n",
            "|    ep_rew_mean      | 256      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4756     |\n",
            "|    fps              | 406      |\n",
            "|    time_elapsed     | 6594     |\n",
            "|    total_timesteps  | 2680351  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.119    |\n",
            "|    n_updates        | 657587   |\n",
            "----------------------------------\n",
            "Num timesteps: 2681000\n",
            "Best mean reward: 257.40 - Last mean reward per episode: 256.04\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 390      |\n",
            "|    ep_rew_mean      | 256      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4760     |\n",
            "|    fps              | 406      |\n",
            "|    time_elapsed     | 6596     |\n",
            "|    total_timesteps  | 2681524  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0964   |\n",
            "|    n_updates        | 657880   |\n",
            "----------------------------------\n",
            "Num timesteps: 2682000\n",
            "Best mean reward: 257.40 - Last mean reward per episode: 255.97\n",
            "Num timesteps: 2683000\n",
            "Best mean reward: 257.40 - Last mean reward per episode: 255.36\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 389      |\n",
            "|    ep_rew_mean      | 256      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4764     |\n",
            "|    fps              | 406      |\n",
            "|    time_elapsed     | 6599     |\n",
            "|    total_timesteps  | 2683264  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.121    |\n",
            "|    n_updates        | 658315   |\n",
            "----------------------------------\n",
            "Num timesteps: 2684000\n",
            "Best mean reward: 257.40 - Last mean reward per episode: 256.03\n",
            "Num timesteps: 2685000\n",
            "Best mean reward: 257.40 - Last mean reward per episode: 256.85\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 393      |\n",
            "|    ep_rew_mean      | 256      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4768     |\n",
            "|    fps              | 406      |\n",
            "|    time_elapsed     | 6603     |\n",
            "|    total_timesteps  | 2685349  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.417    |\n",
            "|    n_updates        | 658837   |\n",
            "----------------------------------\n",
            "Num timesteps: 2686000\n",
            "Best mean reward: 257.40 - Last mean reward per episode: 256.14\n",
            "Num timesteps: 2687000\n",
            "Best mean reward: 257.40 - Last mean reward per episode: 256.04\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 395      |\n",
            "|    ep_rew_mean      | 256      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4772     |\n",
            "|    fps              | 406      |\n",
            "|    time_elapsed     | 6606     |\n",
            "|    total_timesteps  | 2687093  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.289    |\n",
            "|    n_updates        | 659273   |\n",
            "----------------------------------\n",
            "Num timesteps: 2688000\n",
            "Best mean reward: 257.40 - Last mean reward per episode: 256.85\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 393      |\n",
            "|    ep_rew_mean      | 257      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4776     |\n",
            "|    fps              | 406      |\n",
            "|    time_elapsed     | 6609     |\n",
            "|    total_timesteps  | 2688565  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.175    |\n",
            "|    n_updates        | 659641   |\n",
            "----------------------------------\n",
            "Num timesteps: 2689000\n",
            "Best mean reward: 257.40 - Last mean reward per episode: 257.14\n",
            "Num timesteps: 2690000\n",
            "Best mean reward: 257.40 - Last mean reward per episode: 257.72\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 392      |\n",
            "|    ep_rew_mean      | 258      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4780     |\n",
            "|    fps              | 406      |\n",
            "|    time_elapsed     | 6612     |\n",
            "|    total_timesteps  | 2690065  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.937    |\n",
            "|    n_updates        | 660016   |\n",
            "----------------------------------\n",
            "Num timesteps: 2691000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 256.87\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 395      |\n",
            "|    ep_rew_mean      | 257      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4784     |\n",
            "|    fps              | 406      |\n",
            "|    time_elapsed     | 6615     |\n",
            "|    total_timesteps  | 2691799  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.127    |\n",
            "|    n_updates        | 660449   |\n",
            "----------------------------------\n",
            "Num timesteps: 2692000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 256.74\n",
            "Num timesteps: 2693000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 256.96\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 400      |\n",
            "|    ep_rew_mean      | 257      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4788     |\n",
            "|    fps              | 406      |\n",
            "|    time_elapsed     | 6619     |\n",
            "|    total_timesteps  | 2693779  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0695   |\n",
            "|    n_updates        | 660944   |\n",
            "----------------------------------\n",
            "Num timesteps: 2694000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 256.77\n",
            "Num timesteps: 2695000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 255.97\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 403      |\n",
            "|    ep_rew_mean      | 255      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4792     |\n",
            "|    fps              | 406      |\n",
            "|    time_elapsed     | 6623     |\n",
            "|    total_timesteps  | 2695810  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.65     |\n",
            "|    n_updates        | 661452   |\n",
            "----------------------------------\n",
            "Num timesteps: 2696000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 255.21\n",
            "Num timesteps: 2697000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 255.16\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 409      |\n",
            "|    ep_rew_mean      | 254      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4796     |\n",
            "|    fps              | 407      |\n",
            "|    time_elapsed     | 6627     |\n",
            "|    total_timesteps  | 2697919  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.06     |\n",
            "|    n_updates        | 661979   |\n",
            "----------------------------------\n",
            "Num timesteps: 2698000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 254.47\n",
            "Num timesteps: 2699000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 253.90\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 413      |\n",
            "|    ep_rew_mean      | 253      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4800     |\n",
            "|    fps              | 407      |\n",
            "|    time_elapsed     | 6632     |\n",
            "|    total_timesteps  | 2699963  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.78     |\n",
            "|    n_updates        | 662490   |\n",
            "----------------------------------\n",
            "Num timesteps: 2700000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 253.27\n",
            "Num timesteps: 2701000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 253.94\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 406      |\n",
            "|    ep_rew_mean      | 253      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4804     |\n",
            "|    fps              | 407      |\n",
            "|    time_elapsed     | 6634     |\n",
            "|    total_timesteps  | 2701088  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.174    |\n",
            "|    n_updates        | 662771   |\n",
            "----------------------------------\n",
            "Num timesteps: 2702000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 253.41\n",
            "Num timesteps: 2703000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 253.18\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 412      |\n",
            "|    ep_rew_mean      | 254      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4808     |\n",
            "|    fps              | 407      |\n",
            "|    time_elapsed     | 6638     |\n",
            "|    total_timesteps  | 2703183  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.83     |\n",
            "|    n_updates        | 663295   |\n",
            "----------------------------------\n",
            "Num timesteps: 2704000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 253.67\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 408      |\n",
            "|    ep_rew_mean      | 254      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4812     |\n",
            "|    fps              | 407      |\n",
            "|    time_elapsed     | 6640     |\n",
            "|    total_timesteps  | 2704377  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.431    |\n",
            "|    n_updates        | 663594   |\n",
            "----------------------------------\n",
            "Num timesteps: 2705000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 253.48\n",
            "Num timesteps: 2706000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 253.59\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 413      |\n",
            "|    ep_rew_mean      | 254      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4816     |\n",
            "|    fps              | 407      |\n",
            "|    time_elapsed     | 6643     |\n",
            "|    total_timesteps  | 2706158  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0994   |\n",
            "|    n_updates        | 664039   |\n",
            "----------------------------------\n",
            "Num timesteps: 2707000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 253.62\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 411      |\n",
            "|    ep_rew_mean      | 253      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4820     |\n",
            "|    fps              | 407      |\n",
            "|    time_elapsed     | 6645     |\n",
            "|    total_timesteps  | 2707391  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.146    |\n",
            "|    n_updates        | 664347   |\n",
            "----------------------------------\n",
            "Num timesteps: 2708000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 253.46\n",
            "Num timesteps: 2709000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 253.24\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 412      |\n",
            "|    ep_rew_mean      | 253      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4824     |\n",
            "|    fps              | 407      |\n",
            "|    time_elapsed     | 6648     |\n",
            "|    total_timesteps  | 2709008  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.111    |\n",
            "|    n_updates        | 664751   |\n",
            "----------------------------------\n",
            "Num timesteps: 2710000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 253.62\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 416      |\n",
            "|    ep_rew_mean      | 253      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4828     |\n",
            "|    fps              | 407      |\n",
            "|    time_elapsed     | 6652     |\n",
            "|    total_timesteps  | 2710930  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.066    |\n",
            "|    n_updates        | 665232   |\n",
            "----------------------------------\n",
            "Num timesteps: 2711000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 253.12\n",
            "Num timesteps: 2712000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 252.03\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 415      |\n",
            "|    ep_rew_mean      | 252      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4832     |\n",
            "|    fps              | 407      |\n",
            "|    time_elapsed     | 6654     |\n",
            "|    total_timesteps  | 2712095  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.123    |\n",
            "|    n_updates        | 665523   |\n",
            "----------------------------------\n",
            "Num timesteps: 2713000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 252.96\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 412      |\n",
            "|    ep_rew_mean      | 253      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4836     |\n",
            "|    fps              | 407      |\n",
            "|    time_elapsed     | 6657     |\n",
            "|    total_timesteps  | 2713460  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0797   |\n",
            "|    n_updates        | 665864   |\n",
            "----------------------------------\n",
            "Num timesteps: 2714000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 253.24\n",
            "Num timesteps: 2715000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 253.90\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 414      |\n",
            "|    ep_rew_mean      | 253      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4840     |\n",
            "|    fps              | 407      |\n",
            "|    time_elapsed     | 6660     |\n",
            "|    total_timesteps  | 2715434  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.103    |\n",
            "|    n_updates        | 666358   |\n",
            "----------------------------------\n",
            "Num timesteps: 2716000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 252.27\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 410      |\n",
            "|    ep_rew_mean      | 252      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4844     |\n",
            "|    fps              | 407      |\n",
            "|    time_elapsed     | 6663     |\n",
            "|    total_timesteps  | 2716620  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0861   |\n",
            "|    n_updates        | 666654   |\n",
            "----------------------------------\n",
            "Num timesteps: 2717000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 251.92\n",
            "Num timesteps: 2718000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 251.43\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 414      |\n",
            "|    ep_rew_mean      | 252      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4848     |\n",
            "|    fps              | 407      |\n",
            "|    time_elapsed     | 6667     |\n",
            "|    total_timesteps  | 2718603  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.49     |\n",
            "|    n_updates        | 667150   |\n",
            "----------------------------------\n",
            "Num timesteps: 2719000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 252.46\n",
            "Num timesteps: 2720000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 251.71\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 416      |\n",
            "|    ep_rew_mean      | 251      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4852     |\n",
            "|    fps              | 407      |\n",
            "|    time_elapsed     | 6670     |\n",
            "|    total_timesteps  | 2720164  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.151    |\n",
            "|    n_updates        | 667540   |\n",
            "----------------------------------\n",
            "Num timesteps: 2721000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 251.17\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 410      |\n",
            "|    ep_rew_mean      | 251      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4856     |\n",
            "|    fps              | 407      |\n",
            "|    time_elapsed     | 6672     |\n",
            "|    total_timesteps  | 2721361  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.93     |\n",
            "|    n_updates        | 667840   |\n",
            "----------------------------------\n",
            "Num timesteps: 2722000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 251.31\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 408      |\n",
            "|    ep_rew_mean      | 252      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4860     |\n",
            "|    fps              | 407      |\n",
            "|    time_elapsed     | 6674     |\n",
            "|    total_timesteps  | 2722371  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0761   |\n",
            "|    n_updates        | 668092   |\n",
            "----------------------------------\n",
            "Num timesteps: 2723000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 252.07\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 407      |\n",
            "|    ep_rew_mean      | 253      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4864     |\n",
            "|    fps              | 407      |\n",
            "|    time_elapsed     | 6677     |\n",
            "|    total_timesteps  | 2723936  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.34     |\n",
            "|    n_updates        | 668483   |\n",
            "----------------------------------\n",
            "Num timesteps: 2724000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 252.64\n",
            "Num timesteps: 2725000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 252.67\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 405      |\n",
            "|    ep_rew_mean      | 253      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4868     |\n",
            "|    fps              | 408      |\n",
            "|    time_elapsed     | 6680     |\n",
            "|    total_timesteps  | 2725843  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.06     |\n",
            "|    n_updates        | 668960   |\n",
            "----------------------------------\n",
            "Num timesteps: 2726000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 253.12\n",
            "Num timesteps: 2727000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 251.78\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 403      |\n",
            "|    ep_rew_mean      | 251      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4872     |\n",
            "|    fps              | 408      |\n",
            "|    time_elapsed     | 6683     |\n",
            "|    total_timesteps  | 2727437  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.117    |\n",
            "|    n_updates        | 669359   |\n",
            "----------------------------------\n",
            "Num timesteps: 2728000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 251.39\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 401      |\n",
            "|    ep_rew_mean      | 252      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4876     |\n",
            "|    fps              | 408      |\n",
            "|    time_elapsed     | 6686     |\n",
            "|    total_timesteps  | 2728693  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0951   |\n",
            "|    n_updates        | 669673   |\n",
            "----------------------------------\n",
            "Num timesteps: 2729000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 252.10\n",
            "Num timesteps: 2730000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 251.82\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 401      |\n",
            "|    ep_rew_mean      | 252      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4880     |\n",
            "|    fps              | 408      |\n",
            "|    time_elapsed     | 6688     |\n",
            "|    total_timesteps  | 2730145  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.132    |\n",
            "|    n_updates        | 670036   |\n",
            "----------------------------------\n",
            "Num timesteps: 2731000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 252.33\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 398      |\n",
            "|    ep_rew_mean      | 253      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4884     |\n",
            "|    fps              | 408      |\n",
            "|    time_elapsed     | 6691     |\n",
            "|    total_timesteps  | 2731627  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.108    |\n",
            "|    n_updates        | 670406   |\n",
            "----------------------------------\n",
            "Num timesteps: 2732000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 252.73\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 391      |\n",
            "|    ep_rew_mean      | 254      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4888     |\n",
            "|    fps              | 408      |\n",
            "|    time_elapsed     | 6693     |\n",
            "|    total_timesteps  | 2732920  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.243    |\n",
            "|    n_updates        | 670729   |\n",
            "----------------------------------\n",
            "Num timesteps: 2733000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 253.68\n",
            "Num timesteps: 2734000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 254.37\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 385      |\n",
            "|    ep_rew_mean      | 255      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4892     |\n",
            "|    fps              | 408      |\n",
            "|    time_elapsed     | 6695     |\n",
            "|    total_timesteps  | 2734315  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.36     |\n",
            "|    n_updates        | 671078   |\n",
            "----------------------------------\n",
            "Num timesteps: 2735000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 255.05\n",
            "Num timesteps: 2736000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 254.80\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 382      |\n",
            "|    ep_rew_mean      | 256      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4896     |\n",
            "|    fps              | 408      |\n",
            "|    time_elapsed     | 6699     |\n",
            "|    total_timesteps  | 2736139  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.424    |\n",
            "|    n_updates        | 671534   |\n",
            "----------------------------------\n",
            "Num timesteps: 2737000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 256.05\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 378      |\n",
            "|    ep_rew_mean      | 257      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4900     |\n",
            "|    fps              | 408      |\n",
            "|    time_elapsed     | 6702     |\n",
            "|    total_timesteps  | 2737742  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.146    |\n",
            "|    n_updates        | 671935   |\n",
            "----------------------------------\n",
            "Num timesteps: 2738000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 257.01\n",
            "Num timesteps: 2739000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 257.03\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 381      |\n",
            "|    ep_rew_mean      | 257      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4904     |\n",
            "|    fps              | 408      |\n",
            "|    time_elapsed     | 6704     |\n",
            "|    total_timesteps  | 2739204  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.117    |\n",
            "|    n_updates        | 672300   |\n",
            "----------------------------------\n",
            "Num timesteps: 2740000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 257.08\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 374      |\n",
            "|    ep_rew_mean      | 257      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4908     |\n",
            "|    fps              | 408      |\n",
            "|    time_elapsed     | 6706     |\n",
            "|    total_timesteps  | 2740571  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.63     |\n",
            "|    n_updates        | 672642   |\n",
            "----------------------------------\n",
            "Num timesteps: 2741000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 256.47\n",
            "Num timesteps: 2742000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 257.08\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 377      |\n",
            "|    ep_rew_mean      | 257      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4912     |\n",
            "|    fps              | 408      |\n",
            "|    time_elapsed     | 6709     |\n",
            "|    total_timesteps  | 2742041  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.255    |\n",
            "|    n_updates        | 673010   |\n",
            "----------------------------------\n",
            "Num timesteps: 2743000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 257.46\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 371      |\n",
            "|    ep_rew_mean      | 257      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4916     |\n",
            "|    fps              | 408      |\n",
            "|    time_elapsed     | 6711     |\n",
            "|    total_timesteps  | 2743222  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.591    |\n",
            "|    n_updates        | 673305   |\n",
            "----------------------------------\n",
            "Num timesteps: 2744000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 255.50\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 369      |\n",
            "|    ep_rew_mean      | 255      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4920     |\n",
            "|    fps              | 408      |\n",
            "|    time_elapsed     | 6713     |\n",
            "|    total_timesteps  | 2744311  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.96     |\n",
            "|    n_updates        | 673577   |\n",
            "----------------------------------\n",
            "Num timesteps: 2745000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 255.61\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 365      |\n",
            "|    ep_rew_mean      | 256      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4924     |\n",
            "|    fps              | 408      |\n",
            "|    time_elapsed     | 6715     |\n",
            "|    total_timesteps  | 2745477  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.525    |\n",
            "|    n_updates        | 673869   |\n",
            "----------------------------------\n",
            "Num timesteps: 2746000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 256.08\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 359      |\n",
            "|    ep_rew_mean      | 257      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4928     |\n",
            "|    fps              | 408      |\n",
            "|    time_elapsed     | 6717     |\n",
            "|    total_timesteps  | 2746795  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.141    |\n",
            "|    n_updates        | 674198   |\n",
            "----------------------------------\n",
            "Num timesteps: 2747000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 256.92\n",
            "Num timesteps: 2748000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 257.10\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 360      |\n",
            "|    ep_rew_mean      | 257      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4932     |\n",
            "|    fps              | 408      |\n",
            "|    time_elapsed     | 6719     |\n",
            "|    total_timesteps  | 2748050  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0827   |\n",
            "|    n_updates        | 674512   |\n",
            "----------------------------------\n",
            "Num timesteps: 2749000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 257.03\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 362      |\n",
            "|    ep_rew_mean      | 257      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4936     |\n",
            "|    fps              | 408      |\n",
            "|    time_elapsed     | 6723     |\n",
            "|    total_timesteps  | 2749690  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.107    |\n",
            "|    n_updates        | 674922   |\n",
            "----------------------------------\n",
            "Num timesteps: 2750000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 256.83\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 354      |\n",
            "|    ep_rew_mean      | 257      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4940     |\n",
            "|    fps              | 409      |\n",
            "|    time_elapsed     | 6724     |\n",
            "|    total_timesteps  | 2750803  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0963   |\n",
            "|    n_updates        | 675200   |\n",
            "----------------------------------\n",
            "Num timesteps: 2751000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 257.26\n",
            "Num timesteps: 2752000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 257.47\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 356      |\n",
            "|    ep_rew_mean      | 258      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4944     |\n",
            "|    fps              | 409      |\n",
            "|    time_elapsed     | 6727     |\n",
            "|    total_timesteps  | 2752179  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.87     |\n",
            "|    n_updates        | 675544   |\n",
            "----------------------------------\n",
            "Num timesteps: 2753000\n",
            "Best mean reward: 257.72 - Last mean reward per episode: 258.44\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 350      |\n",
            "|    ep_rew_mean      | 258      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4948     |\n",
            "|    fps              | 409      |\n",
            "|    time_elapsed     | 6729     |\n",
            "|    total_timesteps  | 2753621  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.121    |\n",
            "|    n_updates        | 675905   |\n",
            "----------------------------------\n",
            "Num timesteps: 2754000\n",
            "Best mean reward: 258.44 - Last mean reward per episode: 257.71\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 345      |\n",
            "|    ep_rew_mean      | 259      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4952     |\n",
            "|    fps              | 409      |\n",
            "|    time_elapsed     | 6731     |\n",
            "|    total_timesteps  | 2754651  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0471   |\n",
            "|    n_updates        | 676162   |\n",
            "----------------------------------\n",
            "Num timesteps: 2755000\n",
            "Best mean reward: 258.44 - Last mean reward per episode: 259.15\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 2756000\n",
            "Best mean reward: 259.15 - Last mean reward per episode: 258.45\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 352      |\n",
            "|    ep_rew_mean      | 258      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4956     |\n",
            "|    fps              | 409      |\n",
            "|    time_elapsed     | 6735     |\n",
            "|    total_timesteps  | 2756587  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0595   |\n",
            "|    n_updates        | 676646   |\n",
            "----------------------------------\n",
            "Num timesteps: 2757000\n",
            "Best mean reward: 259.15 - Last mean reward per episode: 258.39\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 355      |\n",
            "|    ep_rew_mean      | 258      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4960     |\n",
            "|    fps              | 409      |\n",
            "|    time_elapsed     | 6737     |\n",
            "|    total_timesteps  | 2757894  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.129    |\n",
            "|    n_updates        | 676973   |\n",
            "----------------------------------\n",
            "Num timesteps: 2758000\n",
            "Best mean reward: 259.15 - Last mean reward per episode: 257.59\n",
            "Num timesteps: 2759000\n",
            "Best mean reward: 259.15 - Last mean reward per episode: 256.89\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 352      |\n",
            "|    ep_rew_mean      | 257      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4964     |\n",
            "|    fps              | 409      |\n",
            "|    time_elapsed     | 6739     |\n",
            "|    total_timesteps  | 2759118  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.707    |\n",
            "|    n_updates        | 677279   |\n",
            "----------------------------------\n",
            "Num timesteps: 2760000\n",
            "Best mean reward: 259.15 - Last mean reward per episode: 256.70\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 349      |\n",
            "|    ep_rew_mean      | 257      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4968     |\n",
            "|    fps              | 409      |\n",
            "|    time_elapsed     | 6742     |\n",
            "|    total_timesteps  | 2760775  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.316    |\n",
            "|    n_updates        | 677693   |\n",
            "----------------------------------\n",
            "Num timesteps: 2761000\n",
            "Best mean reward: 259.15 - Last mean reward per episode: 257.42\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 345      |\n",
            "|    ep_rew_mean      | 260      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4972     |\n",
            "|    fps              | 409      |\n",
            "|    time_elapsed     | 6744     |\n",
            "|    total_timesteps  | 2761966  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.21     |\n",
            "|    n_updates        | 677991   |\n",
            "----------------------------------\n",
            "Num timesteps: 2762000\n",
            "Best mean reward: 259.15 - Last mean reward per episode: 259.91\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 2763000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 259.23\n",
            "Num timesteps: 2764000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 258.74\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 357      |\n",
            "|    ep_rew_mean      | 258      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4976     |\n",
            "|    fps              | 409      |\n",
            "|    time_elapsed     | 6750     |\n",
            "|    total_timesteps  | 2764403  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.47     |\n",
            "|    n_updates        | 678600   |\n",
            "----------------------------------\n",
            "Num timesteps: 2765000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 258.10\n",
            "Num timesteps: 2766000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 258.30\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 361      |\n",
            "|    ep_rew_mean      | 258      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4980     |\n",
            "|    fps              | 409      |\n",
            "|    time_elapsed     | 6753     |\n",
            "|    total_timesteps  | 2766239  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.04     |\n",
            "|    n_updates        | 679059   |\n",
            "----------------------------------\n",
            "Num timesteps: 2767000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 257.69\n",
            "Num timesteps: 2768000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 257.57\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 365      |\n",
            "|    ep_rew_mean      | 257      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4984     |\n",
            "|    fps              | 409      |\n",
            "|    time_elapsed     | 6756     |\n",
            "|    total_timesteps  | 2768127  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.108    |\n",
            "|    n_updates        | 679531   |\n",
            "----------------------------------\n",
            "Num timesteps: 2769000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 256.74\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 364      |\n",
            "|    ep_rew_mean      | 256      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4988     |\n",
            "|    fps              | 409      |\n",
            "|    time_elapsed     | 6758     |\n",
            "|    total_timesteps  | 2769356  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0472   |\n",
            "|    n_updates        | 679838   |\n",
            "----------------------------------\n",
            "Num timesteps: 2770000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 256.16\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 361      |\n",
            "|    ep_rew_mean      | 256      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4992     |\n",
            "|    fps              | 409      |\n",
            "|    time_elapsed     | 6760     |\n",
            "|    total_timesteps  | 2770456  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.324    |\n",
            "|    n_updates        | 680113   |\n",
            "----------------------------------\n",
            "Num timesteps: 2771000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 256.05\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 355      |\n",
            "|    ep_rew_mean      | 257      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4996     |\n",
            "|    fps              | 409      |\n",
            "|    time_elapsed     | 6762     |\n",
            "|    total_timesteps  | 2771612  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.281    |\n",
            "|    n_updates        | 680402   |\n",
            "----------------------------------\n",
            "Num timesteps: 2772000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 256.68\n",
            "Num timesteps: 2773000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 255.50\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 358      |\n",
            "|    ep_rew_mean      | 255      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5000     |\n",
            "|    fps              | 409      |\n",
            "|    time_elapsed     | 6765     |\n",
            "|    total_timesteps  | 2773536  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0677   |\n",
            "|    n_updates        | 680883   |\n",
            "----------------------------------\n",
            "Num timesteps: 2774000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 255.27\n",
            "Num timesteps: 2775000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 255.21\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 362      |\n",
            "|    ep_rew_mean      | 255      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5004     |\n",
            "|    fps              | 409      |\n",
            "|    time_elapsed     | 6769     |\n",
            "|    total_timesteps  | 2775402  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0807   |\n",
            "|    n_updates        | 681350   |\n",
            "----------------------------------\n",
            "Num timesteps: 2776000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 255.22\n",
            "Num timesteps: 2777000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 255.64\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 368      |\n",
            "|    ep_rew_mean      | 254      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5008     |\n",
            "|    fps              | 410      |\n",
            "|    time_elapsed     | 6773     |\n",
            "|    total_timesteps  | 2777340  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.193    |\n",
            "|    n_updates        | 681834   |\n",
            "----------------------------------\n",
            "Num timesteps: 2778000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 254.47\n",
            "Num timesteps: 2779000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 253.91\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 373      |\n",
            "|    ep_rew_mean      | 253      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5012     |\n",
            "|    fps              | 410      |\n",
            "|    time_elapsed     | 6777     |\n",
            "|    total_timesteps  | 2779292  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.125    |\n",
            "|    n_updates        | 682322   |\n",
            "----------------------------------\n",
            "Num timesteps: 2780000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 253.17\n",
            "Num timesteps: 2781000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 252.59\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 380      |\n",
            "|    ep_rew_mean      | 253      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5016     |\n",
            "|    fps              | 410      |\n",
            "|    time_elapsed     | 6781     |\n",
            "|    total_timesteps  | 2781235  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.641    |\n",
            "|    n_updates        | 682808   |\n",
            "----------------------------------\n",
            "Num timesteps: 2782000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 254.57\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 385      |\n",
            "|    ep_rew_mean      | 254      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5020     |\n",
            "|    fps              | 410      |\n",
            "|    time_elapsed     | 6784     |\n",
            "|    total_timesteps  | 2782806  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.108    |\n",
            "|    n_updates        | 683201   |\n",
            "----------------------------------\n",
            "Num timesteps: 2783000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 254.21\n",
            "Num timesteps: 2784000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 254.17\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 386      |\n",
            "|    ep_rew_mean      | 254      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5024     |\n",
            "|    fps              | 410      |\n",
            "|    time_elapsed     | 6786     |\n",
            "|    total_timesteps  | 2784056  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0947   |\n",
            "|    n_updates        | 683513   |\n",
            "----------------------------------\n",
            "Num timesteps: 2785000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 254.02\n",
            "Num timesteps: 2786000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 254.06\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 394      |\n",
            "|    ep_rew_mean      | 253      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5028     |\n",
            "|    fps              | 410      |\n",
            "|    time_elapsed     | 6790     |\n",
            "|    total_timesteps  | 2786173  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.212    |\n",
            "|    n_updates        | 684043   |\n",
            "----------------------------------\n",
            "Num timesteps: 2787000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 251.46\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 393      |\n",
            "|    ep_rew_mean      | 251      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5032     |\n",
            "|    fps              | 410      |\n",
            "|    time_elapsed     | 6793     |\n",
            "|    total_timesteps  | 2787394  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.958    |\n",
            "|    n_updates        | 684348   |\n",
            "----------------------------------\n",
            "Num timesteps: 2788000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 250.92\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 391      |\n",
            "|    ep_rew_mean      | 251      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5036     |\n",
            "|    fps              | 410      |\n",
            "|    time_elapsed     | 6795     |\n",
            "|    total_timesteps  | 2788782  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.196    |\n",
            "|    n_updates        | 684695   |\n",
            "----------------------------------\n",
            "Num timesteps: 2789000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 250.68\n",
            "Num timesteps: 2790000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 250.71\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 392      |\n",
            "|    ep_rew_mean      | 251      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5040     |\n",
            "|    fps              | 410      |\n",
            "|    time_elapsed     | 6797     |\n",
            "|    total_timesteps  | 2790034  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.104    |\n",
            "|    n_updates        | 685008   |\n",
            "----------------------------------\n",
            "Num timesteps: 2791000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 251.11\n",
            "Num timesteps: 2792000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 250.86\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 399      |\n",
            "|    ep_rew_mean      | 251      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5044     |\n",
            "|    fps              | 410      |\n",
            "|    time_elapsed     | 6801     |\n",
            "|    total_timesteps  | 2792109  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.72     |\n",
            "|    n_updates        | 685527   |\n",
            "----------------------------------\n",
            "Num timesteps: 2793000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 250.88\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 399      |\n",
            "|    ep_rew_mean      | 250      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5048     |\n",
            "|    fps              | 410      |\n",
            "|    time_elapsed     | 6804     |\n",
            "|    total_timesteps  | 2793524  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.16     |\n",
            "|    n_updates        | 685880   |\n",
            "----------------------------------\n",
            "Num timesteps: 2794000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 250.05\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 403      |\n",
            "|    ep_rew_mean      | 249      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5052     |\n",
            "|    fps              | 410      |\n",
            "|    time_elapsed     | 6807     |\n",
            "|    total_timesteps  | 2794929  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.117    |\n",
            "|    n_updates        | 686232   |\n",
            "----------------------------------\n",
            "Num timesteps: 2795000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 249.44\n",
            "Num timesteps: 2796000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 248.96\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 402      |\n",
            "|    ep_rew_mean      | 249      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5056     |\n",
            "|    fps              | 410      |\n",
            "|    time_elapsed     | 6810     |\n",
            "|    total_timesteps  | 2796751  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.107    |\n",
            "|    n_updates        | 686687   |\n",
            "----------------------------------\n",
            "Num timesteps: 2797000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 249.16\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 399      |\n",
            "|    ep_rew_mean      | 250      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5060     |\n",
            "|    fps              | 410      |\n",
            "|    time_elapsed     | 6812     |\n",
            "|    total_timesteps  | 2797808  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.71     |\n",
            "|    n_updates        | 686951   |\n",
            "----------------------------------\n",
            "Num timesteps: 2798000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 249.83\n",
            "Num timesteps: 2799000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 250.63\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 399      |\n",
            "|    ep_rew_mean      | 251      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5064     |\n",
            "|    fps              | 410      |\n",
            "|    time_elapsed     | 6814     |\n",
            "|    total_timesteps  | 2799064  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0633   |\n",
            "|    n_updates        | 687265   |\n",
            "----------------------------------\n",
            "Num timesteps: 2800000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 250.92\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 396      |\n",
            "|    ep_rew_mean      | 251      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5068     |\n",
            "|    fps              | 410      |\n",
            "|    time_elapsed     | 6817     |\n",
            "|    total_timesteps  | 2800419  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.257    |\n",
            "|    n_updates        | 687604   |\n",
            "----------------------------------\n",
            "Num timesteps: 2801000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 250.53\n",
            "Num timesteps: 2802000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 248.76\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 410      |\n",
            "|    ep_rew_mean      | 249      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5072     |\n",
            "|    fps              | 410      |\n",
            "|    time_elapsed     | 6822     |\n",
            "|    total_timesteps  | 2802930  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.148    |\n",
            "|    n_updates        | 688232   |\n",
            "----------------------------------\n",
            "Num timesteps: 2803000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 248.62\n",
            "Num timesteps: 2804000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 250.43\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 399      |\n",
            "|    ep_rew_mean      | 250      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5076     |\n",
            "|    fps              | 410      |\n",
            "|    time_elapsed     | 6824     |\n",
            "|    total_timesteps  | 2804281  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.34     |\n",
            "|    n_updates        | 688570   |\n",
            "----------------------------------\n",
            "Num timesteps: 2805000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 250.42\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 396      |\n",
            "|    ep_rew_mean      | 250      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5080     |\n",
            "|    fps              | 410      |\n",
            "|    time_elapsed     | 6827     |\n",
            "|    total_timesteps  | 2805801  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.456    |\n",
            "|    n_updates        | 688950   |\n",
            "----------------------------------\n",
            "Num timesteps: 2806000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 249.52\n",
            "Num timesteps: 2807000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 249.61\n",
            "Num timesteps: 2808000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 249.50\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 403      |\n",
            "|    ep_rew_mean      | 248      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5084     |\n",
            "|    fps              | 411      |\n",
            "|    time_elapsed     | 6833     |\n",
            "|    total_timesteps  | 2808457  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.91     |\n",
            "|    n_updates        | 689614   |\n",
            "----------------------------------\n",
            "Num timesteps: 2809000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 248.28\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 404      |\n",
            "|    ep_rew_mean      | 248      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5088     |\n",
            "|    fps              | 411      |\n",
            "|    time_elapsed     | 6835     |\n",
            "|    total_timesteps  | 2809786  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.658    |\n",
            "|    n_updates        | 689946   |\n",
            "----------------------------------\n",
            "Num timesteps: 2810000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 248.15\n",
            "Num timesteps: 2811000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 248.12\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 407      |\n",
            "|    ep_rew_mean      | 248      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5092     |\n",
            "|    fps              | 411      |\n",
            "|    time_elapsed     | 6837     |\n",
            "|    total_timesteps  | 2811181  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.605    |\n",
            "|    n_updates        | 690295   |\n",
            "----------------------------------\n",
            "Num timesteps: 2812000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 247.98\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 413      |\n",
            "|    ep_rew_mean      | 247      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5096     |\n",
            "|    fps              | 411      |\n",
            "|    time_elapsed     | 6841     |\n",
            "|    total_timesteps  | 2812960  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.131    |\n",
            "|    n_updates        | 690739   |\n",
            "----------------------------------\n",
            "Num timesteps: 2813000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 247.20\n",
            "Num timesteps: 2814000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 247.84\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 411      |\n",
            "|    ep_rew_mean      | 248      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5100     |\n",
            "|    fps              | 411      |\n",
            "|    time_elapsed     | 6844     |\n",
            "|    total_timesteps  | 2814599  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.181    |\n",
            "|    n_updates        | 691149   |\n",
            "----------------------------------\n",
            "Num timesteps: 2815000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 248.03\n",
            "Num timesteps: 2816000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 247.85\n",
            "Num timesteps: 2817000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 245.66\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 422      |\n",
            "|    ep_rew_mean      | 245      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5104     |\n",
            "|    fps              | 411      |\n",
            "|    time_elapsed     | 6851     |\n",
            "|    total_timesteps  | 2817576  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.136    |\n",
            "|    n_updates        | 691893   |\n",
            "----------------------------------\n",
            "Num timesteps: 2818000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 245.20\n",
            "Num timesteps: 2819000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 245.04\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 419      |\n",
            "|    ep_rew_mean      | 246      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5108     |\n",
            "|    fps              | 411      |\n",
            "|    time_elapsed     | 6854     |\n",
            "|    total_timesteps  | 2819202  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.92     |\n",
            "|    n_updates        | 692300   |\n",
            "----------------------------------\n",
            "Num timesteps: 2820000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 245.95\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 416      |\n",
            "|    ep_rew_mean      | 247      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5112     |\n",
            "|    fps              | 411      |\n",
            "|    time_elapsed     | 6857     |\n",
            "|    total_timesteps  | 2820875  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.101    |\n",
            "|    n_updates        | 692718   |\n",
            "----------------------------------\n",
            "Num timesteps: 2821000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 247.06\n",
            "Num timesteps: 2822000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 246.49\n",
            "Num timesteps: 2823000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 246.88\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 419      |\n",
            "|    ep_rew_mean      | 246      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5116     |\n",
            "|    fps              | 411      |\n",
            "|    time_elapsed     | 6863     |\n",
            "|    total_timesteps  | 2823172  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.215    |\n",
            "|    n_updates        | 693292   |\n",
            "----------------------------------\n",
            "Num timesteps: 2824000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 246.97\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 419      |\n",
            "|    ep_rew_mean      | 247      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5120     |\n",
            "|    fps              | 411      |\n",
            "|    time_elapsed     | 6865     |\n",
            "|    total_timesteps  | 2824708  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.804    |\n",
            "|    n_updates        | 693676   |\n",
            "----------------------------------\n",
            "Num timesteps: 2825000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 246.78\n",
            "Num timesteps: 2826000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 246.88\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 420      |\n",
            "|    ep_rew_mean      | 247      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5124     |\n",
            "|    fps              | 411      |\n",
            "|    time_elapsed     | 6868     |\n",
            "|    total_timesteps  | 2826106  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.42     |\n",
            "|    n_updates        | 694026   |\n",
            "----------------------------------\n",
            "Num timesteps: 2827000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 246.51\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 410      |\n",
            "|    ep_rew_mean      | 247      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5128     |\n",
            "|    fps              | 411      |\n",
            "|    time_elapsed     | 6870     |\n",
            "|    total_timesteps  | 2827222  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0937   |\n",
            "|    n_updates        | 694305   |\n",
            "----------------------------------\n",
            "Num timesteps: 2828000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 249.09\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 412      |\n",
            "|    ep_rew_mean      | 247      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5132     |\n",
            "|    fps              | 411      |\n",
            "|    time_elapsed     | 6872     |\n",
            "|    total_timesteps  | 2828559  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.035    |\n",
            "|    n_updates        | 694639   |\n",
            "----------------------------------\n",
            "Num timesteps: 2829000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 247.20\n",
            "Num timesteps: 2830000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 245.16\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 416      |\n",
            "|    ep_rew_mean      | 245      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5136     |\n",
            "|    fps              | 411      |\n",
            "|    time_elapsed     | 6876     |\n",
            "|    total_timesteps  | 2830398  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.79     |\n",
            "|    n_updates        | 695099   |\n",
            "----------------------------------\n",
            "Num timesteps: 2831000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 245.36\n",
            "Num timesteps: 2832000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 243.04\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 423      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5140     |\n",
            "|    fps              | 411      |\n",
            "|    time_elapsed     | 6879     |\n",
            "|    total_timesteps  | 2832315  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.103    |\n",
            "|    n_updates        | 695578   |\n",
            "----------------------------------\n",
            "Num timesteps: 2833000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 240.96\n",
            "Num timesteps: 2834000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 240.87\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 419      |\n",
            "|    ep_rew_mean      | 241      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5144     |\n",
            "|    fps              | 411      |\n",
            "|    time_elapsed     | 6883     |\n",
            "|    total_timesteps  | 2834034  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.84     |\n",
            "|    n_updates        | 696008   |\n",
            "----------------------------------\n",
            "Num timesteps: 2835000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 240.55\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 420      |\n",
            "|    ep_rew_mean      | 241      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5148     |\n",
            "|    fps              | 411      |\n",
            "|    time_elapsed     | 6886     |\n",
            "|    total_timesteps  | 2835542  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.71     |\n",
            "|    n_updates        | 696385   |\n",
            "----------------------------------\n",
            "Num timesteps: 2836000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 241.01\n",
            "Num timesteps: 2837000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 241.18\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 423      |\n",
            "|    ep_rew_mean      | 242      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5152     |\n",
            "|    fps              | 411      |\n",
            "|    time_elapsed     | 6889     |\n",
            "|    total_timesteps  | 2837188  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.123    |\n",
            "|    n_updates        | 696796   |\n",
            "----------------------------------\n",
            "Num timesteps: 2838000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 241.84\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 420      |\n",
            "|    ep_rew_mean      | 242      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5156     |\n",
            "|    fps              | 411      |\n",
            "|    time_elapsed     | 6891     |\n",
            "|    total_timesteps  | 2838761  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.112    |\n",
            "|    n_updates        | 697190   |\n",
            "----------------------------------\n",
            "Num timesteps: 2839000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 242.07\n",
            "Num timesteps: 2840000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 241.39\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 423      |\n",
            "|    ep_rew_mean      | 241      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5160     |\n",
            "|    fps              | 411      |\n",
            "|    time_elapsed     | 6894     |\n",
            "|    total_timesteps  | 2840079  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.173    |\n",
            "|    n_updates        | 697519   |\n",
            "----------------------------------\n",
            "Num timesteps: 2841000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 241.27\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 424      |\n",
            "|    ep_rew_mean      | 241      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5164     |\n",
            "|    fps              | 412      |\n",
            "|    time_elapsed     | 6896     |\n",
            "|    total_timesteps  | 2841415  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.52     |\n",
            "|    n_updates        | 697853   |\n",
            "----------------------------------\n",
            "Num timesteps: 2842000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 241.01\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 423      |\n",
            "|    ep_rew_mean      | 242      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5168     |\n",
            "|    fps              | 412      |\n",
            "|    time_elapsed     | 6898     |\n",
            "|    total_timesteps  | 2842703  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0607   |\n",
            "|    n_updates        | 698175   |\n",
            "----------------------------------\n",
            "Num timesteps: 2843000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 241.84\n",
            "Num timesteps: 2844000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 243.43\n",
            "Num timesteps: 2845000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 243.36\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 422      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5172     |\n",
            "|    fps              | 412      |\n",
            "|    time_elapsed     | 6903     |\n",
            "|    total_timesteps  | 2845170  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.198    |\n",
            "|    n_updates        | 698792   |\n",
            "----------------------------------\n",
            "Num timesteps: 2846000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 242.77\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 424      |\n",
            "|    ep_rew_mean      | 242      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5176     |\n",
            "|    fps              | 412      |\n",
            "|    time_elapsed     | 6906     |\n",
            "|    total_timesteps  | 2846707  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.149    |\n",
            "|    n_updates        | 699176   |\n",
            "----------------------------------\n",
            "Num timesteps: 2847000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 242.48\n",
            "Num timesteps: 2848000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 242.24\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 432      |\n",
            "|    ep_rew_mean      | 242      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5180     |\n",
            "|    fps              | 412      |\n",
            "|    time_elapsed     | 6911     |\n",
            "|    total_timesteps  | 2848965  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.713    |\n",
            "|    n_updates        | 699741   |\n",
            "----------------------------------\n",
            "Num timesteps: 2849000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 241.68\n",
            "Num timesteps: 2850000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 241.61\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 419      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5184     |\n",
            "|    fps              | 412      |\n",
            "|    time_elapsed     | 6913     |\n",
            "|    total_timesteps  | 2850400  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0808   |\n",
            "|    n_updates        | 700099   |\n",
            "----------------------------------\n",
            "Num timesteps: 2851000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 242.86\n",
            "Num timesteps: 2852000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 242.28\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 428      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5188     |\n",
            "|    fps              | 412      |\n",
            "|    time_elapsed     | 6917     |\n",
            "|    total_timesteps  | 2852602  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.182    |\n",
            "|    n_updates        | 700650   |\n",
            "----------------------------------\n",
            "Num timesteps: 2853000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 242.64\n",
            "Num timesteps: 2854000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 241.43\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 437      |\n",
            "|    ep_rew_mean      | 241      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5192     |\n",
            "|    fps              | 412      |\n",
            "|    time_elapsed     | 6922     |\n",
            "|    total_timesteps  | 2854841  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.186    |\n",
            "|    n_updates        | 701210   |\n",
            "----------------------------------\n",
            "Num timesteps: 2855000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 241.02\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 430      |\n",
            "|    ep_rew_mean      | 242      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5196     |\n",
            "|    fps              | 412      |\n",
            "|    time_elapsed     | 6924     |\n",
            "|    total_timesteps  | 2855941  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.124    |\n",
            "|    n_updates        | 701485   |\n",
            "----------------------------------\n",
            "Num timesteps: 2856000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 241.91\n",
            "Num timesteps: 2857000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 241.65\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 432      |\n",
            "|    ep_rew_mean      | 241      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5200     |\n",
            "|    fps              | 412      |\n",
            "|    time_elapsed     | 6927     |\n",
            "|    total_timesteps  | 2857777  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0574   |\n",
            "|    n_updates        | 701944   |\n",
            "----------------------------------\n",
            "Num timesteps: 2858000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 241.43\n",
            "Num timesteps: 2859000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 244.27\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 416      |\n",
            "|    ep_rew_mean      | 245      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5204     |\n",
            "|    fps              | 412      |\n",
            "|    time_elapsed     | 6930     |\n",
            "|    total_timesteps  | 2859175  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.196    |\n",
            "|    n_updates        | 702293   |\n",
            "----------------------------------\n",
            "Num timesteps: 2860000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 245.40\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 413      |\n",
            "|    ep_rew_mean      | 245      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5208     |\n",
            "|    fps              | 412      |\n",
            "|    time_elapsed     | 6932     |\n",
            "|    total_timesteps  | 2860519  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.131    |\n",
            "|    n_updates        | 702629   |\n",
            "----------------------------------\n",
            "Num timesteps: 2861000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 245.61\n",
            "Num timesteps: 2862000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 244.22\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 415      |\n",
            "|    ep_rew_mean      | 244      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5212     |\n",
            "|    fps              | 412      |\n",
            "|    time_elapsed     | 6936     |\n",
            "|    total_timesteps  | 2862353  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.34     |\n",
            "|    n_updates        | 703088   |\n",
            "----------------------------------\n",
            "Num timesteps: 2863000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 245.06\n",
            "Num timesteps: 2864000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 244.56\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 409      |\n",
            "|    ep_rew_mean      | 245      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5216     |\n",
            "|    fps              | 412      |\n",
            "|    time_elapsed     | 6939     |\n",
            "|    total_timesteps  | 2864070  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0866   |\n",
            "|    n_updates        | 703517   |\n",
            "----------------------------------\n",
            "Num timesteps: 2865000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 245.02\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 410      |\n",
            "|    ep_rew_mean      | 245      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5220     |\n",
            "|    fps              | 412      |\n",
            "|    time_elapsed     | 6942     |\n",
            "|    total_timesteps  | 2865744  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.162    |\n",
            "|    n_updates        | 703935   |\n",
            "----------------------------------\n",
            "Num timesteps: 2866000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 245.02\n",
            "Num timesteps: 2867000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 244.32\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 413      |\n",
            "|    ep_rew_mean      | 244      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5224     |\n",
            "|    fps              | 412      |\n",
            "|    time_elapsed     | 6946     |\n",
            "|    total_timesteps  | 2867404  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0663   |\n",
            "|    n_updates        | 704350   |\n",
            "----------------------------------\n",
            "Num timesteps: 2868000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 244.05\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 417      |\n",
            "|    ep_rew_mean      | 244      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5228     |\n",
            "|    fps              | 412      |\n",
            "|    time_elapsed     | 6948     |\n",
            "|    total_timesteps  | 2868950  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.21     |\n",
            "|    n_updates        | 704737   |\n",
            "----------------------------------\n",
            "Num timesteps: 2869000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 243.87\n",
            "Num timesteps: 2870000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 243.49\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 423      |\n",
            "|    ep_rew_mean      | 245      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5232     |\n",
            "|    fps              | 412      |\n",
            "|    time_elapsed     | 6952     |\n",
            "|    total_timesteps  | 2870836  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.9      |\n",
            "|    n_updates        | 705208   |\n",
            "----------------------------------\n",
            "Num timesteps: 2871000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 245.48\n",
            "Num timesteps: 2872000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 247.09\n",
            "Num timesteps: 2873000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 245.80\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 427      |\n",
            "|    ep_rew_mean      | 246      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5236     |\n",
            "|    fps              | 412      |\n",
            "|    time_elapsed     | 6957     |\n",
            "|    total_timesteps  | 2873111  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0854   |\n",
            "|    n_updates        | 705777   |\n",
            "----------------------------------\n",
            "Num timesteps: 2874000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 248.31\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 422      |\n",
            "|    ep_rew_mean      | 248      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5240     |\n",
            "|    fps              | 413      |\n",
            "|    time_elapsed     | 6959     |\n",
            "|    total_timesteps  | 2874509  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.28     |\n",
            "|    n_updates        | 706127   |\n",
            "----------------------------------\n",
            "Num timesteps: 2875000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 248.19\n",
            "Num timesteps: 2876000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 250.24\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 420      |\n",
            "|    ep_rew_mean      | 250      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5244     |\n",
            "|    fps              | 413      |\n",
            "|    time_elapsed     | 6962     |\n",
            "|    total_timesteps  | 2876049  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.95     |\n",
            "|    n_updates        | 706512   |\n",
            "----------------------------------\n",
            "Num timesteps: 2877000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 250.29\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 424      |\n",
            "|    ep_rew_mean      | 251      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5248     |\n",
            "|    fps              | 413      |\n",
            "|    time_elapsed     | 6965     |\n",
            "|    total_timesteps  | 2877932  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.254    |\n",
            "|    n_updates        | 706982   |\n",
            "----------------------------------\n",
            "Num timesteps: 2878000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 250.82\n",
            "Num timesteps: 2879000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 249.19\n",
            "Num timesteps: 2880000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 249.43\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 428      |\n",
            "|    ep_rew_mean      | 249      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5252     |\n",
            "|    fps              | 413      |\n",
            "|    time_elapsed     | 6970     |\n",
            "|    total_timesteps  | 2880038  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.18     |\n",
            "|    n_updates        | 707509   |\n",
            "----------------------------------\n",
            "Num timesteps: 2881000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 248.87\n",
            "Num timesteps: 2882000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 247.61\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 438      |\n",
            "|    ep_rew_mean      | 247      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5256     |\n",
            "|    fps              | 413      |\n",
            "|    time_elapsed     | 6976     |\n",
            "|    total_timesteps  | 2882511  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.17     |\n",
            "|    n_updates        | 708127   |\n",
            "----------------------------------\n",
            "Num timesteps: 2883000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 247.10\n",
            "Num timesteps: 2884000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 247.87\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 443      |\n",
            "|    ep_rew_mean      | 247      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5260     |\n",
            "|    fps              | 413      |\n",
            "|    time_elapsed     | 6980     |\n",
            "|    total_timesteps  | 2884401  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.63     |\n",
            "|    n_updates        | 708600   |\n",
            "----------------------------------\n",
            "Num timesteps: 2885000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 247.09\n",
            "Num timesteps: 2886000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 245.55\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 454      |\n",
            "|    ep_rew_mean      | 245      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5264     |\n",
            "|    fps              | 413      |\n",
            "|    time_elapsed     | 6984     |\n",
            "|    total_timesteps  | 2886814  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.34     |\n",
            "|    n_updates        | 709203   |\n",
            "----------------------------------\n",
            "Num timesteps: 2887000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 245.44\n",
            "Num timesteps: 2888000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 244.92\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 458      |\n",
            "|    ep_rew_mean      | 245      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5268     |\n",
            "|    fps              | 413      |\n",
            "|    time_elapsed     | 6987     |\n",
            "|    total_timesteps  | 2888479  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.134    |\n",
            "|    n_updates        | 709619   |\n",
            "----------------------------------\n",
            "Num timesteps: 2889000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 244.69\n",
            "Num timesteps: 2890000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 244.92\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 452      |\n",
            "|    ep_rew_mean      | 244      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5272     |\n",
            "|    fps              | 413      |\n",
            "|    time_elapsed     | 6991     |\n",
            "|    total_timesteps  | 2890325  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.28     |\n",
            "|    n_updates        | 710081   |\n",
            "----------------------------------\n",
            "Num timesteps: 2891000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 243.72\n",
            "Num timesteps: 2892000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 242.58\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 460      |\n",
            "|    ep_rew_mean      | 242      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5276     |\n",
            "|    fps              | 413      |\n",
            "|    time_elapsed     | 6996     |\n",
            "|    total_timesteps  | 2892681  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.17     |\n",
            "|    n_updates        | 710670   |\n",
            "----------------------------------\n",
            "Num timesteps: 2893000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 242.14\n",
            "Num timesteps: 2894000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 241.47\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 458      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5280     |\n",
            "|    fps              | 413      |\n",
            "|    time_elapsed     | 6999     |\n",
            "|    total_timesteps  | 2894754  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.87     |\n",
            "|    n_updates        | 711188   |\n",
            "----------------------------------\n",
            "Num timesteps: 2895000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 242.76\n",
            "Num timesteps: 2896000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 242.83\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 458      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5284     |\n",
            "|    fps              | 413      |\n",
            "|    time_elapsed     | 7002     |\n",
            "|    total_timesteps  | 2896207  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.194    |\n",
            "|    n_updates        | 711551   |\n",
            "----------------------------------\n",
            "Num timesteps: 2897000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 243.13\n",
            "Num timesteps: 2898000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 242.73\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 455      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5288     |\n",
            "|    fps              | 413      |\n",
            "|    time_elapsed     | 7006     |\n",
            "|    total_timesteps  | 2898089  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.28     |\n",
            "|    n_updates        | 712022   |\n",
            "----------------------------------\n",
            "Num timesteps: 2899000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 244.77\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 448      |\n",
            "|    ep_rew_mean      | 244      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5292     |\n",
            "|    fps              | 413      |\n",
            "|    time_elapsed     | 7009     |\n",
            "|    total_timesteps  | 2899651  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.104    |\n",
            "|    n_updates        | 712412   |\n",
            "----------------------------------\n",
            "Num timesteps: 2900000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 244.29\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 450      |\n",
            "|    ep_rew_mean      | 244      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5296     |\n",
            "|    fps              | 413      |\n",
            "|    time_elapsed     | 7011     |\n",
            "|    total_timesteps  | 2900936  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.187    |\n",
            "|    n_updates        | 712733   |\n",
            "----------------------------------\n",
            "Num timesteps: 2901000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 243.63\n",
            "Num timesteps: 2902000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 243.86\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 449      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5300     |\n",
            "|    fps              | 413      |\n",
            "|    time_elapsed     | 7015     |\n",
            "|    total_timesteps  | 2902637  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.118    |\n",
            "|    n_updates        | 713159   |\n",
            "----------------------------------\n",
            "Num timesteps: 2903000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 243.65\n",
            "Num timesteps: 2904000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 242.70\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 451      |\n",
            "|    ep_rew_mean      | 242      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5304     |\n",
            "|    fps              | 413      |\n",
            "|    time_elapsed     | 7018     |\n",
            "|    total_timesteps  | 2904321  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.55     |\n",
            "|    n_updates        | 713580   |\n",
            "----------------------------------\n",
            "Num timesteps: 2905000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 241.91\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 455      |\n",
            "|    ep_rew_mean      | 242      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5308     |\n",
            "|    fps              | 413      |\n",
            "|    time_elapsed     | 7021     |\n",
            "|    total_timesteps  | 2905987  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.12     |\n",
            "|    n_updates        | 713996   |\n",
            "----------------------------------\n",
            "Num timesteps: 2906000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 241.91\n",
            "Num timesteps: 2907000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 242.45\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 449      |\n",
            "|    ep_rew_mean      | 242      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5312     |\n",
            "|    fps              | 413      |\n",
            "|    time_elapsed     | 7023     |\n",
            "|    total_timesteps  | 2907293  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.116    |\n",
            "|    n_updates        | 714323   |\n",
            "----------------------------------\n",
            "Num timesteps: 2908000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 242.85\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 446      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5316     |\n",
            "|    fps              | 413      |\n",
            "|    time_elapsed     | 7026     |\n",
            "|    total_timesteps  | 2908720  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.945    |\n",
            "|    n_updates        | 714679   |\n",
            "----------------------------------\n",
            "Num timesteps: 2909000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 242.89\n",
            "Num timesteps: 2910000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 243.14\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 450      |\n",
            "|    ep_rew_mean      | 241      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5320     |\n",
            "|    fps              | 414      |\n",
            "|    time_elapsed     | 7030     |\n",
            "|    total_timesteps  | 2910735  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.12     |\n",
            "|    n_updates        | 715183   |\n",
            "----------------------------------\n",
            "Num timesteps: 2911000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 241.07\n",
            "Num timesteps: 2912000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 241.57\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 455      |\n",
            "|    ep_rew_mean      | 241      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5324     |\n",
            "|    fps              | 414      |\n",
            "|    time_elapsed     | 7034     |\n",
            "|    total_timesteps  | 2912920  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.63     |\n",
            "|    n_updates        | 715729   |\n",
            "----------------------------------\n",
            "Num timesteps: 2913000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 240.56\n",
            "Num timesteps: 2914000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 241.13\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 451      |\n",
            "|    ep_rew_mean      | 241      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5328     |\n",
            "|    fps              | 414      |\n",
            "|    time_elapsed     | 7036     |\n",
            "|    total_timesteps  | 2914035  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.08     |\n",
            "|    n_updates        | 716008   |\n",
            "----------------------------------\n",
            "Num timesteps: 2915000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 241.22\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 448      |\n",
            "|    ep_rew_mean      | 241      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5332     |\n",
            "|    fps              | 414      |\n",
            "|    time_elapsed     | 7039     |\n",
            "|    total_timesteps  | 2915627  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.212    |\n",
            "|    n_updates        | 716406   |\n",
            "----------------------------------\n",
            "Num timesteps: 2916000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 241.73\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 438      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5336     |\n",
            "|    fps              | 414      |\n",
            "|    time_elapsed     | 7041     |\n",
            "|    total_timesteps  | 2916931  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.41     |\n",
            "|    n_updates        | 716732   |\n",
            "----------------------------------\n",
            "Num timesteps: 2917000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 243.44\n",
            "Num timesteps: 2918000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 242.46\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 439      |\n",
            "|    ep_rew_mean      | 242      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5340     |\n",
            "|    fps              | 414      |\n",
            "|    time_elapsed     | 7044     |\n",
            "|    total_timesteps  | 2918362  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0739   |\n",
            "|    n_updates        | 717090   |\n",
            "----------------------------------\n",
            "Num timesteps: 2919000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 242.64\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 438      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5344     |\n",
            "|    fps              | 414      |\n",
            "|    time_elapsed     | 7046     |\n",
            "|    total_timesteps  | 2919865  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0738   |\n",
            "|    n_updates        | 717466   |\n",
            "----------------------------------\n",
            "Num timesteps: 2920000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 243.00\n",
            "Num timesteps: 2921000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 242.72\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 432      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5348     |\n",
            "|    fps              | 414      |\n",
            "|    time_elapsed     | 7049     |\n",
            "|    total_timesteps  | 2921149  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0427   |\n",
            "|    n_updates        | 717787   |\n",
            "----------------------------------\n",
            "Num timesteps: 2922000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 243.64\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 429      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5352     |\n",
            "|    fps              | 414      |\n",
            "|    time_elapsed     | 7052     |\n",
            "|    total_timesteps  | 2922926  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.09     |\n",
            "|    n_updates        | 718231   |\n",
            "----------------------------------\n",
            "Num timesteps: 2923000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 243.13\n",
            "Num timesteps: 2924000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 244.86\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 425      |\n",
            "|    ep_rew_mean      | 245      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5356     |\n",
            "|    fps              | 414      |\n",
            "|    time_elapsed     | 7056     |\n",
            "|    total_timesteps  | 2924971  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.176    |\n",
            "|    n_updates        | 718742   |\n",
            "----------------------------------\n",
            "Num timesteps: 2925000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 244.61\n",
            "Num timesteps: 2926000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 244.10\n",
            "Num timesteps: 2927000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 243.92\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 426      |\n",
            "|    ep_rew_mean      | 244      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5360     |\n",
            "|    fps              | 414      |\n",
            "|    time_elapsed     | 7061     |\n",
            "|    total_timesteps  | 2927016  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.88     |\n",
            "|    n_updates        | 719253   |\n",
            "----------------------------------\n",
            "Num timesteps: 2928000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 245.59\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 416      |\n",
            "|    ep_rew_mean      | 246      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5364     |\n",
            "|    fps              | 414      |\n",
            "|    time_elapsed     | 7063     |\n",
            "|    total_timesteps  | 2928411  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.24     |\n",
            "|    n_updates        | 719602   |\n",
            "----------------------------------\n",
            "Num timesteps: 2929000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 245.67\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 412      |\n",
            "|    ep_rew_mean      | 246      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5368     |\n",
            "|    fps              | 414      |\n",
            "|    time_elapsed     | 7065     |\n",
            "|    total_timesteps  | 2929651  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.05     |\n",
            "|    n_updates        | 719912   |\n",
            "----------------------------------\n",
            "Num timesteps: 2930000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 246.16\n",
            "Num timesteps: 2931000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 246.01\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 412      |\n",
            "|    ep_rew_mean      | 248      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5372     |\n",
            "|    fps              | 414      |\n",
            "|    time_elapsed     | 7069     |\n",
            "|    total_timesteps  | 2931555  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.109    |\n",
            "|    n_updates        | 720388   |\n",
            "----------------------------------\n",
            "Num timesteps: 2932000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 247.70\n",
            "Num timesteps: 2933000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 249.36\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 406      |\n",
            "|    ep_rew_mean      | 249      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5376     |\n",
            "|    fps              | 414      |\n",
            "|    time_elapsed     | 7072     |\n",
            "|    total_timesteps  | 2933268  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.204    |\n",
            "|    n_updates        | 720816   |\n",
            "----------------------------------\n",
            "Num timesteps: 2934000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 249.70\n",
            "Num timesteps: 2935000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 250.03\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 404      |\n",
            "|    ep_rew_mean      | 249      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5380     |\n",
            "|    fps              | 414      |\n",
            "|    time_elapsed     | 7076     |\n",
            "|    total_timesteps  | 2935176  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.151    |\n",
            "|    n_updates        | 721293   |\n",
            "----------------------------------\n",
            "Num timesteps: 2936000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 249.56\n",
            "Num timesteps: 2937000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 249.09\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 410      |\n",
            "|    ep_rew_mean      | 249      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5384     |\n",
            "|    fps              | 414      |\n",
            "|    time_elapsed     | 7079     |\n",
            "|    total_timesteps  | 2937164  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0806   |\n",
            "|    n_updates        | 721790   |\n",
            "----------------------------------\n",
            "Num timesteps: 2938000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 248.70\n",
            "Num timesteps: 2939000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 246.92\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 417      |\n",
            "|    ep_rew_mean      | 247      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5388     |\n",
            "|    fps              | 414      |\n",
            "|    time_elapsed     | 7086     |\n",
            "|    total_timesteps  | 2939788  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.133    |\n",
            "|    n_updates        | 722446   |\n",
            "----------------------------------\n",
            "Num timesteps: 2940000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 246.92\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 413      |\n",
            "|    ep_rew_mean      | 247      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5392     |\n",
            "|    fps              | 414      |\n",
            "|    time_elapsed     | 7088     |\n",
            "|    total_timesteps  | 2940985  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0862   |\n",
            "|    n_updates        | 722746   |\n",
            "----------------------------------\n",
            "Num timesteps: 2941000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 247.35\n",
            "Num timesteps: 2942000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 246.14\n",
            "Num timesteps: 2943000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 245.77\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 429      |\n",
            "|    ep_rew_mean      | 245      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5396     |\n",
            "|    fps              | 414      |\n",
            "|    time_elapsed     | 7094     |\n",
            "|    total_timesteps  | 2943831  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.11     |\n",
            "|    n_updates        | 723457   |\n",
            "----------------------------------\n",
            "Num timesteps: 2944000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 245.41\n",
            "Num timesteps: 2945000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 245.68\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 427      |\n",
            "|    ep_rew_mean      | 246      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5400     |\n",
            "|    fps              | 415      |\n",
            "|    time_elapsed     | 7097     |\n",
            "|    total_timesteps  | 2945369  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.54     |\n",
            "|    n_updates        | 723842   |\n",
            "----------------------------------\n",
            "Num timesteps: 2946000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 245.76\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 423      |\n",
            "|    ep_rew_mean      | 247      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5404     |\n",
            "|    fps              | 415      |\n",
            "|    time_elapsed     | 7099     |\n",
            "|    total_timesteps  | 2946622  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.14     |\n",
            "|    n_updates        | 724155   |\n",
            "----------------------------------\n",
            "Num timesteps: 2947000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 246.93\n",
            "Num timesteps: 2948000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 245.64\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 425      |\n",
            "|    ep_rew_mean      | 246      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5408     |\n",
            "|    fps              | 415      |\n",
            "|    time_elapsed     | 7102     |\n",
            "|    total_timesteps  | 2948475  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.97     |\n",
            "|    n_updates        | 724618   |\n",
            "----------------------------------\n",
            "Num timesteps: 2949000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 245.27\n",
            "Num timesteps: 2950000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 244.79\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 431      |\n",
            "|    ep_rew_mean      | 245      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5412     |\n",
            "|    fps              | 415      |\n",
            "|    time_elapsed     | 7106     |\n",
            "|    total_timesteps  | 2950346  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.18     |\n",
            "|    n_updates        | 725086   |\n",
            "----------------------------------\n",
            "Num timesteps: 2951000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 244.62\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 429      |\n",
            "|    ep_rew_mean      | 244      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5416     |\n",
            "|    fps              | 415      |\n",
            "|    time_elapsed     | 7109     |\n",
            "|    total_timesteps  | 2951666  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.112    |\n",
            "|    n_updates        | 725416   |\n",
            "----------------------------------\n",
            "Num timesteps: 2952000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 244.36\n",
            "Num timesteps: 2953000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 243.11\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 425      |\n",
            "|    ep_rew_mean      | 245      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5420     |\n",
            "|    fps              | 415      |\n",
            "|    time_elapsed     | 7111     |\n",
            "|    total_timesteps  | 2953219  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0891   |\n",
            "|    n_updates        | 725804   |\n",
            "----------------------------------\n",
            "Num timesteps: 2954000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 244.72\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 416      |\n",
            "|    ep_rew_mean      | 246      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5424     |\n",
            "|    fps              | 415      |\n",
            "|    time_elapsed     | 7113     |\n",
            "|    total_timesteps  | 2954500  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.511    |\n",
            "|    n_updates        | 726124   |\n",
            "----------------------------------\n",
            "Num timesteps: 2955000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 245.89\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 417      |\n",
            "|    ep_rew_mean      | 246      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5428     |\n",
            "|    fps              | 415      |\n",
            "|    time_elapsed     | 7115     |\n",
            "|    total_timesteps  | 2955706  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.19     |\n",
            "|    n_updates        | 726426   |\n",
            "----------------------------------\n",
            "Num timesteps: 2956000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 246.05\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 412      |\n",
            "|    ep_rew_mean      | 246      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5432     |\n",
            "|    fps              | 415      |\n",
            "|    time_elapsed     | 7117     |\n",
            "|    total_timesteps  | 2956869  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.06     |\n",
            "|    n_updates        | 726717   |\n",
            "----------------------------------\n",
            "Num timesteps: 2957000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 245.95\n",
            "Num timesteps: 2958000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 246.10\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 417      |\n",
            "|    ep_rew_mean      | 245      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5436     |\n",
            "|    fps              | 415      |\n",
            "|    time_elapsed     | 7121     |\n",
            "|    total_timesteps  | 2958614  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0897   |\n",
            "|    n_updates        | 727153   |\n",
            "----------------------------------\n",
            "Num timesteps: 2959000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 245.64\n",
            "Num timesteps: 2960000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 246.18\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 421      |\n",
            "|    ep_rew_mean      | 245      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5440     |\n",
            "|    fps              | 415      |\n",
            "|    time_elapsed     | 7125     |\n",
            "|    total_timesteps  | 2960487  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.13     |\n",
            "|    n_updates        | 727621   |\n",
            "----------------------------------\n",
            "Num timesteps: 2961000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 244.64\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 419      |\n",
            "|    ep_rew_mean      | 245      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5444     |\n",
            "|    fps              | 415      |\n",
            "|    time_elapsed     | 7127     |\n",
            "|    total_timesteps  | 2961780  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.233    |\n",
            "|    n_updates        | 727944   |\n",
            "----------------------------------\n",
            "Num timesteps: 2962000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 244.67\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 418      |\n",
            "|    ep_rew_mean      | 241      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5448     |\n",
            "|    fps              | 415      |\n",
            "|    time_elapsed     | 7129     |\n",
            "|    total_timesteps  | 2962935  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.108    |\n",
            "|    n_updates        | 728233   |\n",
            "----------------------------------\n",
            "Num timesteps: 2963000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 241.10\n",
            "Num timesteps: 2964000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 241.37\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 413      |\n",
            "|    ep_rew_mean      | 242      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5452     |\n",
            "|    fps              | 415      |\n",
            "|    time_elapsed     | 7131     |\n",
            "|    total_timesteps  | 2964228  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.921    |\n",
            "|    n_updates        | 728556   |\n",
            "----------------------------------\n",
            "Num timesteps: 2965000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 242.48\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 408      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5456     |\n",
            "|    fps              | 415      |\n",
            "|    time_elapsed     | 7134     |\n",
            "|    total_timesteps  | 2965810  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.289    |\n",
            "|    n_updates        | 728952   |\n",
            "----------------------------------\n",
            "Num timesteps: 2966000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 242.78\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 399      |\n",
            "|    ep_rew_mean      | 244      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5460     |\n",
            "|    fps              | 415      |\n",
            "|    time_elapsed     | 7136     |\n",
            "|    total_timesteps  | 2966897  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.777    |\n",
            "|    n_updates        | 729224   |\n",
            "----------------------------------\n",
            "Num timesteps: 2967000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 243.69\n",
            "Num timesteps: 2968000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 243.79\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 397      |\n",
            "|    ep_rew_mean      | 244      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5464     |\n",
            "|    fps              | 415      |\n",
            "|    time_elapsed     | 7138     |\n",
            "|    total_timesteps  | 2968105  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.08     |\n",
            "|    n_updates        | 729526   |\n",
            "----------------------------------\n",
            "Num timesteps: 2969000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 243.87\n",
            "Num timesteps: 2970000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 243.20\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 405      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5468     |\n",
            "|    fps              | 415      |\n",
            "|    time_elapsed     | 7142     |\n",
            "|    total_timesteps  | 2970118  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.93     |\n",
            "|    n_updates        | 730029   |\n",
            "----------------------------------\n",
            "Num timesteps: 2971000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 242.21\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 404      |\n",
            "|    ep_rew_mean      | 242      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5472     |\n",
            "|    fps              | 415      |\n",
            "|    time_elapsed     | 7146     |\n",
            "|    total_timesteps  | 2971982  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.108    |\n",
            "|    n_updates        | 730495   |\n",
            "----------------------------------\n",
            "Num timesteps: 2972000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 242.42\n",
            "Num timesteps: 2973000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 241.92\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 402      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5476     |\n",
            "|    fps              | 415      |\n",
            "|    time_elapsed     | 7148     |\n",
            "|    total_timesteps  | 2973487  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0897   |\n",
            "|    n_updates        | 730871   |\n",
            "----------------------------------\n",
            "Num timesteps: 2974000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 242.95\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 395      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5480     |\n",
            "|    fps              | 415      |\n",
            "|    time_elapsed     | 7150     |\n",
            "|    total_timesteps  | 2974657  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.168    |\n",
            "|    n_updates        | 731164   |\n",
            "----------------------------------\n",
            "Num timesteps: 2975000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 243.23\n",
            "Num timesteps: 2976000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 243.16\n",
            "Num timesteps: 2977000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 242.46\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 403      |\n",
            "|    ep_rew_mean      | 242      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5484     |\n",
            "|    fps              | 416      |\n",
            "|    time_elapsed     | 7156     |\n",
            "|    total_timesteps  | 2977481  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.995    |\n",
            "|    n_updates        | 731870   |\n",
            "----------------------------------\n",
            "Num timesteps: 2978000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 242.74\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 392      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5488     |\n",
            "|    fps              | 416      |\n",
            "|    time_elapsed     | 7159     |\n",
            "|    total_timesteps  | 2978994  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0527   |\n",
            "|    n_updates        | 732248   |\n",
            "----------------------------------\n",
            "Num timesteps: 2979000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 242.91\n",
            "Num timesteps: 2980000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 243.12\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 398      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5492     |\n",
            "|    fps              | 416      |\n",
            "|    time_elapsed     | 7162     |\n",
            "|    total_timesteps  | 2980736  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.07     |\n",
            "|    n_updates        | 732683   |\n",
            "----------------------------------\n",
            "Num timesteps: 2981000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 243.23\n",
            "Num timesteps: 2982000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 245.25\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 383      |\n",
            "|    ep_rew_mean      | 246      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5496     |\n",
            "|    fps              | 416      |\n",
            "|    time_elapsed     | 7164     |\n",
            "|    total_timesteps  | 2982166  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.22     |\n",
            "|    n_updates        | 733041   |\n",
            "----------------------------------\n",
            "Num timesteps: 2983000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 245.81\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 382      |\n",
            "|    ep_rew_mean      | 246      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5500     |\n",
            "|    fps              | 416      |\n",
            "|    time_elapsed     | 7167     |\n",
            "|    total_timesteps  | 2983540  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0553   |\n",
            "|    n_updates        | 733384   |\n",
            "----------------------------------\n",
            "Num timesteps: 2984000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 245.57\n",
            "Num timesteps: 2985000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 245.31\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 384      |\n",
            "|    ep_rew_mean      | 245      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5504     |\n",
            "|    fps              | 416      |\n",
            "|    time_elapsed     | 7169     |\n",
            "|    total_timesteps  | 2985006  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.206    |\n",
            "|    n_updates        | 733751   |\n",
            "----------------------------------\n",
            "Num timesteps: 2986000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 245.35\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 382      |\n",
            "|    ep_rew_mean      | 247      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5508     |\n",
            "|    fps              | 416      |\n",
            "|    time_elapsed     | 7172     |\n",
            "|    total_timesteps  | 2986655  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.3      |\n",
            "|    n_updates        | 734163   |\n",
            "----------------------------------\n",
            "Num timesteps: 2987000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 247.33\n",
            "Num timesteps: 2988000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 247.90\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 378      |\n",
            "|    ep_rew_mean      | 248      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5512     |\n",
            "|    fps              | 416      |\n",
            "|    time_elapsed     | 7175     |\n",
            "|    total_timesteps  | 2988161  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.03     |\n",
            "|    n_updates        | 734540   |\n",
            "----------------------------------\n",
            "Num timesteps: 2989000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 247.44\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 381      |\n",
            "|    ep_rew_mean      | 247      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5516     |\n",
            "|    fps              | 416      |\n",
            "|    time_elapsed     | 7178     |\n",
            "|    total_timesteps  | 2989735  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0845   |\n",
            "|    n_updates        | 734933   |\n",
            "----------------------------------\n",
            "Num timesteps: 2990000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 247.05\n",
            "Num timesteps: 2991000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 247.64\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 384      |\n",
            "|    ep_rew_mean      | 246      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5520     |\n",
            "|    fps              | 416      |\n",
            "|    time_elapsed     | 7181     |\n",
            "|    total_timesteps  | 2991668  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.18     |\n",
            "|    n_updates        | 735416   |\n",
            "----------------------------------\n",
            "Num timesteps: 2992000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 246.31\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 384      |\n",
            "|    ep_rew_mean      | 247      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5524     |\n",
            "|    fps              | 416      |\n",
            "|    time_elapsed     | 7183     |\n",
            "|    total_timesteps  | 2992879  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.9      |\n",
            "|    n_updates        | 735719   |\n",
            "----------------------------------\n",
            "Num timesteps: 2993000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 246.60\n",
            "Num timesteps: 2994000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 246.04\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 390      |\n",
            "|    ep_rew_mean      | 245      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5528     |\n",
            "|    fps              | 416      |\n",
            "|    time_elapsed     | 7187     |\n",
            "|    total_timesteps  | 2994738  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.178    |\n",
            "|    n_updates        | 736184   |\n",
            "----------------------------------\n",
            "Num timesteps: 2995000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 245.42\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 390      |\n",
            "|    ep_rew_mean      | 246      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5532     |\n",
            "|    fps              | 416      |\n",
            "|    time_elapsed     | 7189     |\n",
            "|    total_timesteps  | 2995875  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.12     |\n",
            "|    n_updates        | 736468   |\n",
            "----------------------------------\n",
            "Num timesteps: 2996000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 245.85\n",
            "Num timesteps: 2997000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 245.67\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 394      |\n",
            "|    ep_rew_mean      | 245      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5536     |\n",
            "|    fps              | 416      |\n",
            "|    time_elapsed     | 7193     |\n",
            "|    total_timesteps  | 2997996  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.998    |\n",
            "|    n_updates        | 736998   |\n",
            "----------------------------------\n",
            "Num timesteps: 2998000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 245.38\n",
            "Num timesteps: 2999000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 245.62\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 388      |\n",
            "|    ep_rew_mean      | 246      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5540     |\n",
            "|    fps              | 416      |\n",
            "|    time_elapsed     | 7196     |\n",
            "|    total_timesteps  | 2999277  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0676   |\n",
            "|    n_updates        | 737319   |\n",
            "----------------------------------\n",
            "Num timesteps: 3000000\n",
            "Best mean reward: 259.91 - Last mean reward per episode: 246.20\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.dqn.dqn.DQN at 0x7ac7a630a020>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "train_from_file = False\n",
        "# Hyperparameters are from RL_Zoo\n",
        "# https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/dqn.yml\n",
        "\n",
        "\n",
        "n_timesteps = 3e6\n",
        "policy =  'MlpPolicy'\n",
        "learning_rate = 6.3e-4\n",
        "batch_size = 128\n",
        "buffer_size = 50000\n",
        "# learning_starts = 0\n",
        "gamma = 0.99\n",
        "target_update_interval = 250\n",
        "train_freq = 4\n",
        "gradient_steps = -1\n",
        "exploration_fraction = 0.12\n",
        "exploration_final_eps = 0.1\n",
        "policy_kwargs = \"dict(net_arch=[256, 256])\"\n",
        "\n",
        "n_envs = 1\n",
        "\n",
        "\n",
        "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=\"log_dir_DQN/\")\n",
        "\n",
        "# env\n",
        "env = make_vec_env(\"LunarLander-v2\", n_envs=n_envs, monitor_dir=\"log_dir_DQN/\")\n",
        "\n",
        "# instantiate the agent\n",
        "if train_from_file:\n",
        "  model = DQN.load(path=\"log_dir_DQN/best_model.zip\", env=env)\n",
        "else:\n",
        "  model = DQN(\n",
        "      policy,\n",
        "      env,\n",
        "      # learning_rate = learning_rate,\n",
        "      # batch_size = batch_size,\n",
        "      # buffer_size = buffer_size,\n",
        "      # learning_starts = learning_starts,\n",
        "      # gamma = gamma,\n",
        "      # target_update_interval = target_update_interval,\n",
        "      # train_freq = train_freq,\n",
        "      # gradient_steps = gradient_steps,\n",
        "      # exploration_fraction = exploration_fraction,\n",
        "      # exploration_final_eps = exploration_final_eps,\n",
        "      # policy_kwargs = dict(net_arch=[256, 256]),\n",
        "      tensorboard_log=\"./TensorBoardLog/\", verbose=1)\n",
        "\n",
        "# train the agent\n",
        "model.learn(total_timesteps=n_timesteps, callback=callback)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b1dc8fa",
      "metadata": {
        "id": "5b1dc8fa"
      },
      "source": [
        "# Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "366b80a8",
      "metadata": {
        "id": "366b80a8",
        "outputId": "e3a334d7-5726-4a4a-82b5-d3d3401ea4e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "def moving_average(values, window):\n",
        "    \"\"\"\n",
        "    Smooth values by doing a moving average\n",
        "    :param values: (numpy array)\n",
        "    :param window: (int)\n",
        "    :return: (numpy array)\n",
        "    \"\"\"\n",
        "    weights = np.repeat(1.0, window) / window\n",
        "    return np.convolve(values, weights, 'valid')\n",
        "\n",
        "def plot_results(log_folder, title='Learning Curve'):\n",
        "    \"\"\"\n",
        "    plot the results\n",
        "\n",
        "    :param log_folder: (str) the save location of the results to plot\n",
        "    :param title: (str) the title of the task to plot\n",
        "    \"\"\"\n",
        "\n",
        "    x, y = ts2xy(load_results(log_folder), 'timesteps')\n",
        "    y = moving_average(y, window=100)\n",
        "    # Truncate x\n",
        "    x = x[len(x) - len(y):]\n",
        "    fig = plt.figure(title, figsize=(12,5))\n",
        "    plt.plot(x, y)\n",
        "    plt.xlabel('Number of Timesteps')\n",
        "    plt.ylabel('Rewards')\n",
        "    plt.title(title + \" Smoothed DQN\")\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "plot_results(\"log_dir_DQN\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b00f2a81",
      "metadata": {
        "id": "b00f2a81"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "815393a0",
      "metadata": {
        "id": "815393a0"
      },
      "outputs": [],
      "source": [
        "env = make_vec_env(\"LunarLander-v2\", n_envs=1,monitor_dir=\"log_dir_DQN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "63611e6e",
      "metadata": {
        "id": "63611e6e"
      },
      "outputs": [],
      "source": [
        "model = DQN.load(path=\"log_dir_DQN/best_model.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b06e1a3",
      "metadata": {
        "id": "3b06e1a3"
      },
      "source": [
        "#### Stable Baseline 3 Evaluation Function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "9d4fd326",
      "metadata": {
        "id": "9d4fd326",
        "outputId": "b2dc4cc4-6e34-4cf1-8b97-127a57015b9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean & Std Reward after 10 max run is 210.6845727 & 75.29049482875803\n"
          ]
        }
      ],
      "source": [
        "mean_reward, std_reward = evaluate_policy(model, env,n_eval_episodes=10, render=True, deterministic=True)\n",
        "print(\"Mean & Std Reward after {} max run is {} & {}\".format(10,mean_reward, std_reward))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e49c5168",
      "metadata": {
        "id": "e49c5168"
      },
      "source": [
        "# GIF of a Train Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "60cc63dc",
      "metadata": {
        "id": "60cc63dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "def8954a-9a49-4664-d769-e14b4c57d174"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/imageio/plugins/pillow.py:390: DeprecationWarning: The keyword `fps` is no longer supported. Use `duration`(in ms) instead, e.g. `fps=50` == `duration=20` (1000 * 1/50).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "env = make_vec_env(\"LunarLander-v2\", n_envs=1)\n",
        "model = DQN.load(path=\"log_dir_DQN/best_model.zip\")\n",
        "\n",
        "images = []\n",
        "obs = env.reset()\n",
        "img = env.render(mode=\"rgb_array\")\n",
        "for i in range(1000):\n",
        "    images.append(img)\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, _, _ ,_ = env.step(action)\n",
        "    img = env.render(mode=\"rgb_array\")\n",
        "\n",
        "imageio.mimsave(\"lunar lander_DQN.gif\", [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3afc060b",
      "metadata": {
        "id": "3afc060b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "857970f990130bbcaee778cf1846f7875676d945310dca1379fe4b5ef3d258a5"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}