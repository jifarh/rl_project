{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "57601915",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57601915",
        "outputId": "e3d12e96-6003-4cd0-a0e5-44c1a9e71147"
      },
      "outputs": [],
      "source": [
        "\n",
        "# !pip install shap\n",
        "# !pip install opencv-python\n",
        "# !pip install swig\n",
        "# !pip install Box2D\n",
        "\n",
        "\n",
        "# # !pip install box2d pygame\n",
        "\n",
        "\n",
        "# !pip install gym\n",
        "# !pip install pyglet==1.5.27\n",
        "# !pip install stable-baseline3\n",
        "# !pip install \"gymnasium[all]\"\n",
        "\n",
        "# !pip install stable_baselines3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "PtTyQewr3gxt",
      "metadata": {
        "id": "PtTyQewr3gxt"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b00a128f",
      "metadata": {
        "id": "b00a128f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-26 19:24:26.654263: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-03-26 19:24:27.295283: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import imageio\n",
        "import os\n",
        "from stable_baselines3 import DQN, A2C\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack, SubprocVecEnv, VecNormalize\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common import results_plotter\n",
        "import gymnasium  as gym\n",
        "import matplotlib.pyplot as plt\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "import scipy.stats as stats\n",
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "import tensorflow as tf\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "YtZN-eC7NwuS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtZN-eC7NwuS",
        "outputId": "8c9cdd26-c266-4012-8463-48462279e0b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n",
            "GPU not found. Please ensure that GPU is enabled in Colab.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-26 19:24:29.329941: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
          ]
        }
      ],
      "source": [
        "# seeds\n",
        "# Set seed for numpy\n",
        "np.random.seed(100)\n",
        "\n",
        "# Set seed for Python random module\n",
        "import random\n",
        "random.seed(100)\n",
        "\n",
        "# Set seed for TensorFlow\n",
        "tf.random.set_seed(100)\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "if tf.test.gpu_device_name():\n",
        "    print('Default GPU Device:', tf.test.gpu_device_name())\n",
        "else:\n",
        "    print(\"GPU not found. Please ensure that GPU is enabled in Colab.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "780afb92",
      "metadata": {
        "id": "780afb92"
      },
      "source": [
        "<h1> Important Libraries To Install </h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2826cd85",
      "metadata": {
        "id": "2826cd85"
      },
      "source": [
        "<h1> Parameter & Environment Information </h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87ef75ca",
      "metadata": {
        "id": "87ef75ca"
      },
      "source": [
        "<p>\n",
        "    This environment is part of the Box2D environments.\n",
        "</p>\n",
        "\n",
        "<ul>\n",
        "    <li> Action Space Discrete(4) </li>\n",
        "    <li> Observation Shape (8,) </li>\n",
        "    <li> Observation High [1.5 1.5 5. 5. 3.14 5. 1. 1. ] </li>\n",
        "    <li> Observation Low [-1.5 -1.5 -5. -5. -3.14 -5. -0. -0. ] </li>\n",
        "    <li> Import gymnasium.make(\"LunarLander-v2\") </li>\n",
        "</ul>\n",
        "\n",
        "<h3> Description </h3>\n",
        "<p>This environment is a classic rocket trajectory optimization problem. According to Pontryagin’s maximum principle, it is optimal to fire the engine at full throttle or turn it off. This is the reason why this environment has discrete actions: engine on or off.\n",
        "\n",
        "There are two environment versions: discrete or continuous. The landing pad is always at coordinates (0,0). The coordinates are the first two numbers in the state vector. Landing outside of the landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt.</p>\n",
        "\n",
        "<h3> Action Space </h3>\n",
        "<p>\n",
        "There are four discrete actions available:\n",
        "\n",
        "* 0: do nothing\n",
        "* 1: fire left orientation engine\n",
        "* 2: fire main engine\n",
        "* 3: fire right orientation engine\n",
        "\n",
        "</p>\n",
        "\n",
        "<h3> Observation Space </h3>\n",
        "<p>\n",
        "The state is an 8-dimensional vector: the coordinates of the lander in x & y, its linear velocities in x & y, its angle, its angular velocity, and two booleans that represent whether each leg is in contact with the ground or not.\n",
        "</p>\n",
        "\n",
        "<h3> Reward </h3>\n",
        "<p>\n",
        "After every step a reward is granted. The total reward of an episode is the sum of the rewards for all the steps within that episode.\n",
        "\n",
        "For each step, the reward:\n",
        "\n",
        "* is increased/decreased the closer/further the lander is to the landing pad.\n",
        "* is increased/decreased the slower/faster the lander is moving.\n",
        "* is decreased the more the lander is tilted (angle not horizontal).\n",
        "* is increased by 10 points for each leg that is in contact with the ground.\n",
        "* is decreased by 0.03 points each frame a side engine is firing.\n",
        "* is decreased by 0.3 points each frame the main engine is firing.\n",
        "\n",
        "The episode receive an additional reward of -100 or +100 points for crashing or landing safely respectively.\n",
        "\n",
        "An episode is considered a solution if it scores at least 200 points.\n",
        "</p>\n",
        "\n",
        "<h3> Starting State </h3>\n",
        "\n",
        "<p>The lander starts at the top center of the viewport with a random initial force applied to its center of mass.</p>\n",
        "\n",
        "<h3> Episode Termination </h3>\n",
        "<p> The episode finishes if:<br>\n",
        "    \n",
        "1. the lander crashes (the lander body gets in contact with the moon);<br>\n",
        "2. the lander gets outside of the viewport (x coordinate is greater than 1);<br>\n",
        "3. the lander is not awake. From the Box2D docs, a body which is not awake is a body which doesn’t move and doesn’t collide with any other body:<br>\n",
        "\n",
        "When Box2D determines that a body (or group of bodies) has come to rest, the body enters a sleep state which has very little CPU overhead. If a body is awake and collides with a sleeping body, then the sleeping body wakes up. Bodies will also wake up if a joint or contact attached to them is destroyed.\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3d0543a5",
      "metadata": {
        "id": "3d0543a5",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b3fb45b2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3fb45b2",
        "outputId": "7a40d9ca-d5c2-47ca-f823-c7c18048e0aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Action inter is descrete 4\n",
            "Shape of Observation is (8,)\n"
          ]
        }
      ],
      "source": [
        "print(\"The Action inter is descrete {}\".format(env.action_space.n))\n",
        "print(\"Shape of Observation is {}\".format(env.observation_space.sample().shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b9ff9c9",
      "metadata": {
        "id": "8b9ff9c9"
      },
      "source": [
        "<h1> Baseline Model. </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d35101af",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d35101af",
        "outputId": "da78a772-4679-4e5a-c9cb-f8c17406c4f9",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Reward after 10 max run is -2.1645506620428785\n"
          ]
        }
      ],
      "source": [
        "rewards = []\n",
        "obs = env.reset()\n",
        "done = False\n",
        "MAX_RUN = 10\n",
        "\n",
        "for i in range(MAX_RUN):\n",
        "    while not done:\n",
        "        env.render()\n",
        "        action_sample = env.action_space.sample()\n",
        "        # let's take a step in the environment\n",
        "        obs, rwd, done, info ,_  = env.step(action_sample)\n",
        "        rewards.append(rwd)\n",
        "env.close()\n",
        "print(\"Mean Reward after {} max run is {}\".format(MAX_RUN, np.mean(np.array(rewards))))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac737551",
      "metadata": {
        "id": "ac737551"
      },
      "source": [
        "<h1> Reinforcement Learning For Training The Model </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "bdaa0e55",
      "metadata": {
        "id": "bdaa0e55"
      },
      "outputs": [],
      "source": [
        "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
        "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
        "\n",
        "    :param check_freq: (int)\n",
        "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
        "      It must contains the file created by the ``Monitor`` wrapper.\n",
        "    :param verbose: (int)\n",
        "    \"\"\"\n",
        "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
        "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
        "        self.check_freq = check_freq\n",
        "        self.log_dir = log_dir\n",
        "        self.save_path = os.path.join(log_dir, 'best_model')\n",
        "        self.best_mean_reward = -np.inf\n",
        "\n",
        "    def _init_callback(self) -> None:\n",
        "        # Create folder if needed\n",
        "        if self.save_path is not None:\n",
        "            os.makedirs(self.save_path, exist_ok=True)\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.n_calls % self.check_freq == 0:\n",
        "\n",
        "          # Retrieve training reward\n",
        "          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
        "          if len(x) > 0:\n",
        "              # Mean training reward over the last 100 episodes\n",
        "              mean_reward = np.mean(y[-100:])\n",
        "              if self.verbose > 0:\n",
        "                print(f\"Num timesteps: {self.num_timesteps}\")\n",
        "                print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n",
        "\n",
        "              # New best model, you could save the agent here\n",
        "              if mean_reward > self.best_mean_reward:\n",
        "                  self.best_mean_reward = mean_reward\n",
        "                  # Example for saving best model\n",
        "                  if self.verbose > 0:\n",
        "                    print(f\"Saving new best model to {self.save_path}.zip\")\n",
        "                  self.model.save(self.save_path)\n",
        "\n",
        "        return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2d311d6a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d311d6a",
        "outputId": "19628099-224c-4f44-a524-92d9412a1a50",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "Logging to ./TensorBoardLog/DQN_1\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 94       |\n",
            "|    ep_rew_mean      | -176     |\n",
            "|    exploration_rate | 0.999    |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 3532     |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 376      |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 96.5     |\n",
            "|    ep_rew_mean      | -162     |\n",
            "|    exploration_rate | 0.998    |\n",
            "| time/               |          |\n",
            "|    episodes         | 8        |\n",
            "|    fps              | 3654     |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 772      |\n",
            "----------------------------------\n",
            "Num timesteps: 1000\n",
            "Best mean reward: -inf - Last mean reward per episode: -179.18\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 88       |\n",
            "|    ep_rew_mean      | -181     |\n",
            "|    exploration_rate | 0.997    |\n",
            "| time/               |          |\n",
            "|    episodes         | 12       |\n",
            "|    fps              | 3650     |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 1056     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 87.1     |\n",
            "|    ep_rew_mean      | -178     |\n",
            "|    exploration_rate | 0.996    |\n",
            "| time/               |          |\n",
            "|    episodes         | 16       |\n",
            "|    fps              | 3728     |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 1393     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 90.8     |\n",
            "|    ep_rew_mean      | -182     |\n",
            "|    exploration_rate | 0.994    |\n",
            "| time/               |          |\n",
            "|    episodes         | 20       |\n",
            "|    fps              | 3754     |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 1817     |\n",
            "----------------------------------\n",
            "Num timesteps: 2000\n",
            "Best mean reward: -179.18 - Last mean reward per episode: -181.61\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 89.9     |\n",
            "|    ep_rew_mean      | -168     |\n",
            "|    exploration_rate | 0.993    |\n",
            "| time/               |          |\n",
            "|    episodes         | 24       |\n",
            "|    fps              | 3742     |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 2157     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 90.9     |\n",
            "|    ep_rew_mean      | -167     |\n",
            "|    exploration_rate | 0.992    |\n",
            "| time/               |          |\n",
            "|    episodes         | 28       |\n",
            "|    fps              | 3768     |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 2545     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.1     |\n",
            "|    ep_rew_mean      | -167     |\n",
            "|    exploration_rate | 0.991    |\n",
            "| time/               |          |\n",
            "|    episodes         | 32       |\n",
            "|    fps              | 3784     |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 2948     |\n",
            "----------------------------------\n",
            "Num timesteps: 3000\n",
            "Best mean reward: -179.18 - Last mean reward per episode: -166.95\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.4     |\n",
            "|    ep_rew_mean      | -164     |\n",
            "|    exploration_rate | 0.989    |\n",
            "| time/               |          |\n",
            "|    episodes         | 36       |\n",
            "|    fps              | 3763     |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 3328     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91.5     |\n",
            "|    ep_rew_mean      | -165     |\n",
            "|    exploration_rate | 0.988    |\n",
            "| time/               |          |\n",
            "|    episodes         | 40       |\n",
            "|    fps              | 3780     |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 3661     |\n",
            "----------------------------------\n",
            "Num timesteps: 4000\n",
            "Best mean reward: -166.95 - Last mean reward per episode: -160.45\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91.2     |\n",
            "|    ep_rew_mean      | -160     |\n",
            "|    exploration_rate | 0.987    |\n",
            "| time/               |          |\n",
            "|    episodes         | 44       |\n",
            "|    fps              | 3767     |\n",
            "|    time_elapsed     | 1        |\n",
            "|    total_timesteps  | 4013     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91.4     |\n",
            "|    ep_rew_mean      | -159     |\n",
            "|    exploration_rate | 0.986    |\n",
            "| time/               |          |\n",
            "|    episodes         | 48       |\n",
            "|    fps              | 3777     |\n",
            "|    time_elapsed     | 1        |\n",
            "|    total_timesteps  | 4386     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92       |\n",
            "|    ep_rew_mean      | -163     |\n",
            "|    exploration_rate | 0.985    |\n",
            "| time/               |          |\n",
            "|    episodes         | 52       |\n",
            "|    fps              | 3787     |\n",
            "|    time_elapsed     | 1        |\n",
            "|    total_timesteps  | 4785     |\n",
            "----------------------------------\n",
            "Num timesteps: 5000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -162.43\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.3     |\n",
            "|    ep_rew_mean      | -167     |\n",
            "|    exploration_rate | 0.984    |\n",
            "| time/               |          |\n",
            "|    episodes         | 56       |\n",
            "|    fps              | 3786     |\n",
            "|    time_elapsed     | 1        |\n",
            "|    total_timesteps  | 5168     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91.2     |\n",
            "|    ep_rew_mean      | -166     |\n",
            "|    exploration_rate | 0.983    |\n",
            "| time/               |          |\n",
            "|    episodes         | 60       |\n",
            "|    fps              | 3799     |\n",
            "|    time_elapsed     | 1        |\n",
            "|    total_timesteps  | 5473     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91       |\n",
            "|    ep_rew_mean      | -174     |\n",
            "|    exploration_rate | 0.982    |\n",
            "| time/               |          |\n",
            "|    episodes         | 64       |\n",
            "|    fps              | 3806     |\n",
            "|    time_elapsed     | 1        |\n",
            "|    total_timesteps  | 5824     |\n",
            "----------------------------------\n",
            "Num timesteps: 6000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -171.82\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 90.7     |\n",
            "|    ep_rew_mean      | -172     |\n",
            "|    exploration_rate | 0.98     |\n",
            "| time/               |          |\n",
            "|    episodes         | 68       |\n",
            "|    fps              | 3804     |\n",
            "|    time_elapsed     | 1        |\n",
            "|    total_timesteps  | 6167     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 89.7     |\n",
            "|    ep_rew_mean      | -169     |\n",
            "|    exploration_rate | 0.98     |\n",
            "| time/               |          |\n",
            "|    episodes         | 72       |\n",
            "|    fps              | 3813     |\n",
            "|    time_elapsed     | 1        |\n",
            "|    total_timesteps  | 6460     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 89.1     |\n",
            "|    ep_rew_mean      | -173     |\n",
            "|    exploration_rate | 0.979    |\n",
            "| time/               |          |\n",
            "|    episodes         | 76       |\n",
            "|    fps              | 3796     |\n",
            "|    time_elapsed     | 1        |\n",
            "|    total_timesteps  | 6775     |\n",
            "----------------------------------\n",
            "Num timesteps: 7000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -172.67\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 89.9     |\n",
            "|    ep_rew_mean      | -178     |\n",
            "|    exploration_rate | 0.977    |\n",
            "| time/               |          |\n",
            "|    episodes         | 80       |\n",
            "|    fps              | 3796     |\n",
            "|    time_elapsed     | 1        |\n",
            "|    total_timesteps  | 7194     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 90.2     |\n",
            "|    ep_rew_mean      | -177     |\n",
            "|    exploration_rate | 0.976    |\n",
            "| time/               |          |\n",
            "|    episodes         | 84       |\n",
            "|    fps              | 3802     |\n",
            "|    time_elapsed     | 1        |\n",
            "|    total_timesteps  | 7573     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 90.6     |\n",
            "|    ep_rew_mean      | -178     |\n",
            "|    exploration_rate | 0.975    |\n",
            "| time/               |          |\n",
            "|    episodes         | 88       |\n",
            "|    fps              | 3809     |\n",
            "|    time_elapsed     | 2        |\n",
            "|    total_timesteps  | 7969     |\n",
            "----------------------------------\n",
            "Num timesteps: 8000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -177.64\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 90.2     |\n",
            "|    ep_rew_mean      | -175     |\n",
            "|    exploration_rate | 0.974    |\n",
            "| time/               |          |\n",
            "|    episodes         | 92       |\n",
            "|    fps              | 3809     |\n",
            "|    time_elapsed     | 2        |\n",
            "|    total_timesteps  | 8295     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 90.3     |\n",
            "|    ep_rew_mean      | -176     |\n",
            "|    exploration_rate | 0.973    |\n",
            "| time/               |          |\n",
            "|    episodes         | 96       |\n",
            "|    fps              | 3805     |\n",
            "|    time_elapsed     | 2        |\n",
            "|    total_timesteps  | 8665     |\n",
            "----------------------------------\n",
            "Num timesteps: 9000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -175.88\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 90.1     |\n",
            "|    ep_rew_mean      | -176     |\n",
            "|    exploration_rate | 0.971    |\n",
            "| time/               |          |\n",
            "|    episodes         | 100      |\n",
            "|    fps              | 3799     |\n",
            "|    time_elapsed     | 2        |\n",
            "|    total_timesteps  | 9007     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 90.3     |\n",
            "|    ep_rew_mean      | -177     |\n",
            "|    exploration_rate | 0.97     |\n",
            "| time/               |          |\n",
            "|    episodes         | 104      |\n",
            "|    fps              | 3794     |\n",
            "|    time_elapsed     | 2        |\n",
            "|    total_timesteps  | 9404     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 90       |\n",
            "|    ep_rew_mean      | -178     |\n",
            "|    exploration_rate | 0.969    |\n",
            "| time/               |          |\n",
            "|    episodes         | 108      |\n",
            "|    fps              | 3792     |\n",
            "|    time_elapsed     | 2        |\n",
            "|    total_timesteps  | 9774     |\n",
            "----------------------------------\n",
            "Num timesteps: 10000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -176.83\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 90.8     |\n",
            "|    ep_rew_mean      | -177     |\n",
            "|    exploration_rate | 0.968    |\n",
            "| time/               |          |\n",
            "|    episodes         | 112      |\n",
            "|    fps              | 3792     |\n",
            "|    time_elapsed     | 2        |\n",
            "|    total_timesteps  | 10132    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91       |\n",
            "|    ep_rew_mean      | -180     |\n",
            "|    exploration_rate | 0.967    |\n",
            "| time/               |          |\n",
            "|    episodes         | 116      |\n",
            "|    fps              | 3803     |\n",
            "|    time_elapsed     | 2        |\n",
            "|    total_timesteps  | 10496    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 90       |\n",
            "|    ep_rew_mean      | -179     |\n",
            "|    exploration_rate | 0.966    |\n",
            "| time/               |          |\n",
            "|    episodes         | 120      |\n",
            "|    fps              | 3798     |\n",
            "|    time_elapsed     | 2        |\n",
            "|    total_timesteps  | 10821    |\n",
            "----------------------------------\n",
            "Num timesteps: 11000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -178.14\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 90.7     |\n",
            "|    ep_rew_mean      | -179     |\n",
            "|    exploration_rate | 0.964    |\n",
            "| time/               |          |\n",
            "|    episodes         | 124      |\n",
            "|    fps              | 3789     |\n",
            "|    time_elapsed     | 2        |\n",
            "|    total_timesteps  | 11228    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 90.1     |\n",
            "|    ep_rew_mean      | -178     |\n",
            "|    exploration_rate | 0.963    |\n",
            "| time/               |          |\n",
            "|    episodes         | 128      |\n",
            "|    fps              | 3793     |\n",
            "|    time_elapsed     | 3        |\n",
            "|    total_timesteps  | 11554    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 89.1     |\n",
            "|    ep_rew_mean      | -179     |\n",
            "|    exploration_rate | 0.962    |\n",
            "| time/               |          |\n",
            "|    episodes         | 132      |\n",
            "|    fps              | 3799     |\n",
            "|    time_elapsed     | 3        |\n",
            "|    total_timesteps  | 11862    |\n",
            "----------------------------------\n",
            "Num timesteps: 12000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -178.16\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 88.5     |\n",
            "|    ep_rew_mean      | -179     |\n",
            "|    exploration_rate | 0.961    |\n",
            "| time/               |          |\n",
            "|    episodes         | 136      |\n",
            "|    fps              | 3800     |\n",
            "|    time_elapsed     | 3        |\n",
            "|    total_timesteps  | 12180    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 88.3     |\n",
            "|    ep_rew_mean      | -178     |\n",
            "|    exploration_rate | 0.96     |\n",
            "| time/               |          |\n",
            "|    episodes         | 140      |\n",
            "|    fps              | 3805     |\n",
            "|    time_elapsed     | 3        |\n",
            "|    total_timesteps  | 12492    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 88       |\n",
            "|    ep_rew_mean      | -179     |\n",
            "|    exploration_rate | 0.959    |\n",
            "| time/               |          |\n",
            "|    episodes         | 144      |\n",
            "|    fps              | 3811     |\n",
            "|    time_elapsed     | 3        |\n",
            "|    total_timesteps  | 12811    |\n",
            "----------------------------------\n",
            "Num timesteps: 13000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -178.65\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 87.5     |\n",
            "|    ep_rew_mean      | -179     |\n",
            "|    exploration_rate | 0.958    |\n",
            "| time/               |          |\n",
            "|    episodes         | 148      |\n",
            "|    fps              | 3811     |\n",
            "|    time_elapsed     | 3        |\n",
            "|    total_timesteps  | 13131    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 87       |\n",
            "|    ep_rew_mean      | -181     |\n",
            "|    exploration_rate | 0.957    |\n",
            "| time/               |          |\n",
            "|    episodes         | 152      |\n",
            "|    fps              | 3816     |\n",
            "|    time_elapsed     | 3        |\n",
            "|    total_timesteps  | 13486    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 86.4     |\n",
            "|    ep_rew_mean      | -178     |\n",
            "|    exploration_rate | 0.956    |\n",
            "| time/               |          |\n",
            "|    episodes         | 156      |\n",
            "|    fps              | 3818     |\n",
            "|    time_elapsed     | 3        |\n",
            "|    total_timesteps  | 13810    |\n",
            "----------------------------------\n",
            "Num timesteps: 14000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -180.66\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 86.8     |\n",
            "|    ep_rew_mean      | -179     |\n",
            "|    exploration_rate | 0.955    |\n",
            "| time/               |          |\n",
            "|    episodes         | 160      |\n",
            "|    fps              | 3813     |\n",
            "|    time_elapsed     | 3        |\n",
            "|    total_timesteps  | 14149    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 86.9     |\n",
            "|    ep_rew_mean      | -175     |\n",
            "|    exploration_rate | 0.954    |\n",
            "| time/               |          |\n",
            "|    episodes         | 164      |\n",
            "|    fps              | 3813     |\n",
            "|    time_elapsed     | 3        |\n",
            "|    total_timesteps  | 14512    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 87.4     |\n",
            "|    ep_rew_mean      | -180     |\n",
            "|    exploration_rate | 0.953    |\n",
            "| time/               |          |\n",
            "|    episodes         | 168      |\n",
            "|    fps              | 3815     |\n",
            "|    time_elapsed     | 3        |\n",
            "|    total_timesteps  | 14903    |\n",
            "----------------------------------\n",
            "Num timesteps: 15000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -180.12\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 88.8     |\n",
            "|    ep_rew_mean      | -183     |\n",
            "|    exploration_rate | 0.951    |\n",
            "| time/               |          |\n",
            "|    episodes         | 172      |\n",
            "|    fps              | 3804     |\n",
            "|    time_elapsed     | 4        |\n",
            "|    total_timesteps  | 15337    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 89.2     |\n",
            "|    ep_rew_mean      | -182     |\n",
            "|    exploration_rate | 0.95     |\n",
            "| time/               |          |\n",
            "|    episodes         | 176      |\n",
            "|    fps              | 3798     |\n",
            "|    time_elapsed     | 4        |\n",
            "|    total_timesteps  | 15694    |\n",
            "----------------------------------\n",
            "Num timesteps: 16000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -176.13\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 88.8     |\n",
            "|    ep_rew_mean      | -178     |\n",
            "|    exploration_rate | 0.949    |\n",
            "| time/               |          |\n",
            "|    episodes         | 180      |\n",
            "|    fps              | 3797     |\n",
            "|    time_elapsed     | 4        |\n",
            "|    total_timesteps  | 16079    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 88.8     |\n",
            "|    ep_rew_mean      | -177     |\n",
            "|    exploration_rate | 0.948    |\n",
            "| time/               |          |\n",
            "|    episodes         | 184      |\n",
            "|    fps              | 3800     |\n",
            "|    time_elapsed     | 4        |\n",
            "|    total_timesteps  | 16448    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 88.5     |\n",
            "|    ep_rew_mean      | -180     |\n",
            "|    exploration_rate | 0.947    |\n",
            "| time/               |          |\n",
            "|    episodes         | 188      |\n",
            "|    fps              | 3804     |\n",
            "|    time_elapsed     | 4        |\n",
            "|    total_timesteps  | 16819    |\n",
            "----------------------------------\n",
            "Num timesteps: 17000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -179.96\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 88.3     |\n",
            "|    ep_rew_mean      | -179     |\n",
            "|    exploration_rate | 0.946    |\n",
            "| time/               |          |\n",
            "|    episodes         | 192      |\n",
            "|    fps              | 3806     |\n",
            "|    time_elapsed     | 4        |\n",
            "|    total_timesteps  | 17123    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 87.8     |\n",
            "|    ep_rew_mean      | -178     |\n",
            "|    exploration_rate | 0.945    |\n",
            "| time/               |          |\n",
            "|    episodes         | 196      |\n",
            "|    fps              | 3809     |\n",
            "|    time_elapsed     | 4        |\n",
            "|    total_timesteps  | 17443    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 88       |\n",
            "|    ep_rew_mean      | -178     |\n",
            "|    exploration_rate | 0.944    |\n",
            "| time/               |          |\n",
            "|    episodes         | 200      |\n",
            "|    fps              | 3810     |\n",
            "|    time_elapsed     | 4        |\n",
            "|    total_timesteps  | 17802    |\n",
            "----------------------------------\n",
            "Num timesteps: 18000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -177.25\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 87.7     |\n",
            "|    ep_rew_mean      | -176     |\n",
            "|    exploration_rate | 0.942    |\n",
            "| time/               |          |\n",
            "|    episodes         | 204      |\n",
            "|    fps              | 3810     |\n",
            "|    time_elapsed     | 4        |\n",
            "|    total_timesteps  | 18171    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 88.2     |\n",
            "|    ep_rew_mean      | -175     |\n",
            "|    exploration_rate | 0.941    |\n",
            "| time/               |          |\n",
            "|    episodes         | 208      |\n",
            "|    fps              | 3811     |\n",
            "|    time_elapsed     | 4        |\n",
            "|    total_timesteps  | 18599    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 87.8     |\n",
            "|    ep_rew_mean      | -176     |\n",
            "|    exploration_rate | 0.94     |\n",
            "| time/               |          |\n",
            "|    episodes         | 212      |\n",
            "|    fps              | 3810     |\n",
            "|    time_elapsed     | 4        |\n",
            "|    total_timesteps  | 18913    |\n",
            "----------------------------------\n",
            "Num timesteps: 19000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -174.32\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 87.7     |\n",
            "|    ep_rew_mean      | -176     |\n",
            "|    exploration_rate | 0.939    |\n",
            "| time/               |          |\n",
            "|    episodes         | 216      |\n",
            "|    fps              | 3808     |\n",
            "|    time_elapsed     | 5        |\n",
            "|    total_timesteps  | 19266    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 87.7     |\n",
            "|    ep_rew_mean      | -179     |\n",
            "|    exploration_rate | 0.938    |\n",
            "| time/               |          |\n",
            "|    episodes         | 220      |\n",
            "|    fps              | 3810     |\n",
            "|    time_elapsed     | 5        |\n",
            "|    total_timesteps  | 19590    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 87       |\n",
            "|    ep_rew_mean      | -181     |\n",
            "|    exploration_rate | 0.937    |\n",
            "| time/               |          |\n",
            "|    episodes         | 224      |\n",
            "|    fps              | 3812     |\n",
            "|    time_elapsed     | 5        |\n",
            "|    total_timesteps  | 19926    |\n",
            "----------------------------------\n",
            "Num timesteps: 20000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -181.20\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 87.8     |\n",
            "|    ep_rew_mean      | -185     |\n",
            "|    exploration_rate | 0.936    |\n",
            "| time/               |          |\n",
            "|    episodes         | 228      |\n",
            "|    fps              | 3812     |\n",
            "|    time_elapsed     | 5        |\n",
            "|    total_timesteps  | 20329    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 89.1     |\n",
            "|    ep_rew_mean      | -187     |\n",
            "|    exploration_rate | 0.934    |\n",
            "| time/               |          |\n",
            "|    episodes         | 232      |\n",
            "|    fps              | 3810     |\n",
            "|    time_elapsed     | 5        |\n",
            "|    total_timesteps  | 20776    |\n",
            "----------------------------------\n",
            "Num timesteps: 21000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -190.34\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 90       |\n",
            "|    ep_rew_mean      | -190     |\n",
            "|    exploration_rate | 0.933    |\n",
            "| time/               |          |\n",
            "|    episodes         | 236      |\n",
            "|    fps              | 3809     |\n",
            "|    time_elapsed     | 5        |\n",
            "|    total_timesteps  | 21181    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91.4     |\n",
            "|    ep_rew_mean      | -193     |\n",
            "|    exploration_rate | 0.931    |\n",
            "| time/               |          |\n",
            "|    episodes         | 240      |\n",
            "|    fps              | 3809     |\n",
            "|    time_elapsed     | 5        |\n",
            "|    total_timesteps  | 21634    |\n",
            "----------------------------------\n",
            "Num timesteps: 22000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -198.74\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.3     |\n",
            "|    ep_rew_mean      | -199     |\n",
            "|    exploration_rate | 0.93     |\n",
            "| time/               |          |\n",
            "|    episodes         | 244      |\n",
            "|    fps              | 3809     |\n",
            "|    time_elapsed     | 5        |\n",
            "|    total_timesteps  | 22042    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.8     |\n",
            "|    ep_rew_mean      | -196     |\n",
            "|    exploration_rate | 0.929    |\n",
            "| time/               |          |\n",
            "|    episodes         | 248      |\n",
            "|    fps              | 3811     |\n",
            "|    time_elapsed     | 5        |\n",
            "|    total_timesteps  | 22407    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.2     |\n",
            "|    ep_rew_mean      | -189     |\n",
            "|    exploration_rate | 0.928    |\n",
            "| time/               |          |\n",
            "|    episodes         | 252      |\n",
            "|    fps              | 3813     |\n",
            "|    time_elapsed     | 5        |\n",
            "|    total_timesteps  | 22710    |\n",
            "----------------------------------\n",
            "Num timesteps: 23000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -190.59\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.2     |\n",
            "|    ep_rew_mean      | -189     |\n",
            "|    exploration_rate | 0.927    |\n",
            "| time/               |          |\n",
            "|    episodes         | 256      |\n",
            "|    fps              | 3810     |\n",
            "|    time_elapsed     | 6        |\n",
            "|    total_timesteps  | 23027    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 93       |\n",
            "|    ep_rew_mean      | -194     |\n",
            "|    exploration_rate | 0.926    |\n",
            "| time/               |          |\n",
            "|    episodes         | 260      |\n",
            "|    fps              | 3806     |\n",
            "|    time_elapsed     | 6        |\n",
            "|    total_timesteps  | 23448    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.3     |\n",
            "|    ep_rew_mean      | -195     |\n",
            "|    exploration_rate | 0.925    |\n",
            "| time/               |          |\n",
            "|    episodes         | 264      |\n",
            "|    fps              | 3806     |\n",
            "|    time_elapsed     | 6        |\n",
            "|    total_timesteps  | 23744    |\n",
            "----------------------------------\n",
            "Num timesteps: 24000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -191.72\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.2     |\n",
            "|    ep_rew_mean      | -192     |\n",
            "|    exploration_rate | 0.924    |\n",
            "| time/               |          |\n",
            "|    episodes         | 268      |\n",
            "|    fps              | 3802     |\n",
            "|    time_elapsed     | 6        |\n",
            "|    total_timesteps  | 24120    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91.5     |\n",
            "|    ep_rew_mean      | -195     |\n",
            "|    exploration_rate | 0.922    |\n",
            "| time/               |          |\n",
            "|    episodes         | 272      |\n",
            "|    fps              | 3804     |\n",
            "|    time_elapsed     | 6        |\n",
            "|    total_timesteps  | 24487    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91.7     |\n",
            "|    ep_rew_mean      | -193     |\n",
            "|    exploration_rate | 0.921    |\n",
            "| time/               |          |\n",
            "|    episodes         | 276      |\n",
            "|    fps              | 3805     |\n",
            "|    time_elapsed     | 6        |\n",
            "|    total_timesteps  | 24863    |\n",
            "----------------------------------\n",
            "Num timesteps: 25000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -192.91\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92       |\n",
            "|    ep_rew_mean      | -189     |\n",
            "|    exploration_rate | 0.92     |\n",
            "| time/               |          |\n",
            "|    episodes         | 280      |\n",
            "|    fps              | 3802     |\n",
            "|    time_elapsed     | 6        |\n",
            "|    total_timesteps  | 25281    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.6     |\n",
            "|    ep_rew_mean      | -197     |\n",
            "|    exploration_rate | 0.919    |\n",
            "| time/               |          |\n",
            "|    episodes         | 284      |\n",
            "|    fps              | 3801     |\n",
            "|    time_elapsed     | 6        |\n",
            "|    total_timesteps  | 25708    |\n",
            "----------------------------------\n",
            "Num timesteps: 26000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -193.56\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.7     |\n",
            "|    ep_rew_mean      | -190     |\n",
            "|    exploration_rate | 0.917    |\n",
            "| time/               |          |\n",
            "|    episodes         | 288      |\n",
            "|    fps              | 3800     |\n",
            "|    time_elapsed     | 6        |\n",
            "|    total_timesteps  | 26093    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 93       |\n",
            "|    ep_rew_mean      | -192     |\n",
            "|    exploration_rate | 0.916    |\n",
            "| time/               |          |\n",
            "|    episodes         | 292      |\n",
            "|    fps              | 3802     |\n",
            "|    time_elapsed     | 6        |\n",
            "|    total_timesteps  | 26424    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.5     |\n",
            "|    ep_rew_mean      | -190     |\n",
            "|    exploration_rate | 0.915    |\n",
            "| time/               |          |\n",
            "|    episodes         | 296      |\n",
            "|    fps              | 3805     |\n",
            "|    time_elapsed     | 7        |\n",
            "|    total_timesteps  | 26691    |\n",
            "----------------------------------\n",
            "Num timesteps: 27000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -188.72\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.6     |\n",
            "|    ep_rew_mean      | -189     |\n",
            "|    exploration_rate | 0.914    |\n",
            "| time/               |          |\n",
            "|    episodes         | 300      |\n",
            "|    fps              | 3804     |\n",
            "|    time_elapsed     | 7        |\n",
            "|    total_timesteps  | 27059    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 93.2     |\n",
            "|    ep_rew_mean      | -196     |\n",
            "|    exploration_rate | 0.913    |\n",
            "| time/               |          |\n",
            "|    episodes         | 304      |\n",
            "|    fps              | 3800     |\n",
            "|    time_elapsed     | 7        |\n",
            "|    total_timesteps  | 27494    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 93.1     |\n",
            "|    ep_rew_mean      | -201     |\n",
            "|    exploration_rate | 0.912    |\n",
            "| time/               |          |\n",
            "|    episodes         | 308      |\n",
            "|    fps              | 3799     |\n",
            "|    time_elapsed     | 7        |\n",
            "|    total_timesteps  | 27909    |\n",
            "----------------------------------\n",
            "Num timesteps: 28000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -202.24\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 93.7     |\n",
            "|    ep_rew_mean      | -202     |\n",
            "|    exploration_rate | 0.91     |\n",
            "| time/               |          |\n",
            "|    episodes         | 312      |\n",
            "|    fps              | 3799     |\n",
            "|    time_elapsed     | 7        |\n",
            "|    total_timesteps  | 28284    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 94       |\n",
            "|    ep_rew_mean      | -196     |\n",
            "|    exploration_rate | 0.909    |\n",
            "| time/               |          |\n",
            "|    episodes         | 316      |\n",
            "|    fps              | 3801     |\n",
            "|    time_elapsed     | 7        |\n",
            "|    total_timesteps  | 28662    |\n",
            "----------------------------------\n",
            "Num timesteps: 29000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -196.11\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 94.6     |\n",
            "|    ep_rew_mean      | -196     |\n",
            "|    exploration_rate | 0.908    |\n",
            "| time/               |          |\n",
            "|    episodes         | 320      |\n",
            "|    fps              | 3799     |\n",
            "|    time_elapsed     | 7        |\n",
            "|    total_timesteps  | 29050    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 94.8     |\n",
            "|    ep_rew_mean      | -199     |\n",
            "|    exploration_rate | 0.907    |\n",
            "| time/               |          |\n",
            "|    episodes         | 324      |\n",
            "|    fps              | 3801     |\n",
            "|    time_elapsed     | 7        |\n",
            "|    total_timesteps  | 29411    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 94.4     |\n",
            "|    ep_rew_mean      | -198     |\n",
            "|    exploration_rate | 0.906    |\n",
            "| time/               |          |\n",
            "|    episodes         | 328      |\n",
            "|    fps              | 3798     |\n",
            "|    time_elapsed     | 7        |\n",
            "|    total_timesteps  | 29767    |\n",
            "----------------------------------\n",
            "Num timesteps: 30000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -198.27\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 93.4     |\n",
            "|    ep_rew_mean      | -199     |\n",
            "|    exploration_rate | 0.905    |\n",
            "| time/               |          |\n",
            "|    episodes         | 332      |\n",
            "|    fps              | 3797     |\n",
            "|    time_elapsed     | 7        |\n",
            "|    total_timesteps  | 30118    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 93       |\n",
            "|    ep_rew_mean      | -200     |\n",
            "|    exploration_rate | 0.903    |\n",
            "| time/               |          |\n",
            "|    episodes         | 336      |\n",
            "|    fps              | 3799     |\n",
            "|    time_elapsed     | 8        |\n",
            "|    total_timesteps  | 30486    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91.9     |\n",
            "|    ep_rew_mean      | -194     |\n",
            "|    exploration_rate | 0.902    |\n",
            "| time/               |          |\n",
            "|    episodes         | 340      |\n",
            "|    fps              | 3801     |\n",
            "|    time_elapsed     | 8        |\n",
            "|    total_timesteps  | 30824    |\n",
            "----------------------------------\n",
            "Num timesteps: 31000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -191.65\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91.5     |\n",
            "|    ep_rew_mean      | -191     |\n",
            "|    exploration_rate | 0.901    |\n",
            "| time/               |          |\n",
            "|    episodes         | 344      |\n",
            "|    fps              | 3801     |\n",
            "|    time_elapsed     | 8        |\n",
            "|    total_timesteps  | 31194    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91.4     |\n",
            "|    ep_rew_mean      | -195     |\n",
            "|    exploration_rate | 0.9      |\n",
            "| time/               |          |\n",
            "|    episodes         | 348      |\n",
            "|    fps              | 3803     |\n",
            "|    time_elapsed     | 8        |\n",
            "|    total_timesteps  | 31544    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.5     |\n",
            "|    ep_rew_mean      | -201     |\n",
            "|    exploration_rate | 0.899    |\n",
            "| time/               |          |\n",
            "|    episodes         | 352      |\n",
            "|    fps              | 3803     |\n",
            "|    time_elapsed     | 8        |\n",
            "|    total_timesteps  | 31956    |\n",
            "----------------------------------\n",
            "Num timesteps: 32000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -201.18\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.9     |\n",
            "|    ep_rew_mean      | -203     |\n",
            "|    exploration_rate | 0.898    |\n",
            "| time/               |          |\n",
            "|    episodes         | 356      |\n",
            "|    fps              | 3802     |\n",
            "|    time_elapsed     | 8        |\n",
            "|    total_timesteps  | 32321    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.1     |\n",
            "|    ep_rew_mean      | -201     |\n",
            "|    exploration_rate | 0.897    |\n",
            "| time/               |          |\n",
            "|    episodes         | 360      |\n",
            "|    fps              | 3801     |\n",
            "|    time_elapsed     | 8        |\n",
            "|    total_timesteps  | 32658    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.5     |\n",
            "|    ep_rew_mean      | -198     |\n",
            "|    exploration_rate | 0.896    |\n",
            "| time/               |          |\n",
            "|    episodes         | 364      |\n",
            "|    fps              | 3800     |\n",
            "|    time_elapsed     | 8        |\n",
            "|    total_timesteps  | 32993    |\n",
            "----------------------------------\n",
            "Num timesteps: 33000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -197.52\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91.8     |\n",
            "|    ep_rew_mean      | -199     |\n",
            "|    exploration_rate | 0.895    |\n",
            "| time/               |          |\n",
            "|    episodes         | 368      |\n",
            "|    fps              | 3799     |\n",
            "|    time_elapsed     | 8        |\n",
            "|    total_timesteps  | 33296    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.3     |\n",
            "|    ep_rew_mean      | -193     |\n",
            "|    exploration_rate | 0.893    |\n",
            "| time/               |          |\n",
            "|    episodes         | 372      |\n",
            "|    fps              | 3799     |\n",
            "|    time_elapsed     | 8        |\n",
            "|    total_timesteps  | 33715    |\n",
            "----------------------------------\n",
            "Num timesteps: 34000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -195.02\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.5     |\n",
            "|    ep_rew_mean      | -199     |\n",
            "|    exploration_rate | 0.892    |\n",
            "| time/               |          |\n",
            "|    episodes         | 376      |\n",
            "|    fps              | 3797     |\n",
            "|    time_elapsed     | 8        |\n",
            "|    total_timesteps  | 34110    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.2     |\n",
            "|    ep_rew_mean      | -202     |\n",
            "|    exploration_rate | 0.891    |\n",
            "| time/               |          |\n",
            "|    episodes         | 380      |\n",
            "|    fps              | 3798     |\n",
            "|    time_elapsed     | 9        |\n",
            "|    total_timesteps  | 34497    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91.8     |\n",
            "|    ep_rew_mean      | -200     |\n",
            "|    exploration_rate | 0.89     |\n",
            "| time/               |          |\n",
            "|    episodes         | 384      |\n",
            "|    fps              | 3799     |\n",
            "|    time_elapsed     | 9        |\n",
            "|    total_timesteps  | 34889    |\n",
            "----------------------------------\n",
            "Num timesteps: 35000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -199.44\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.5     |\n",
            "|    ep_rew_mean      | -200     |\n",
            "|    exploration_rate | 0.888    |\n",
            "| time/               |          |\n",
            "|    episodes         | 388      |\n",
            "|    fps              | 3797     |\n",
            "|    time_elapsed     | 9        |\n",
            "|    total_timesteps  | 35344    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.6     |\n",
            "|    ep_rew_mean      | -206     |\n",
            "|    exploration_rate | 0.887    |\n",
            "| time/               |          |\n",
            "|    episodes         | 392      |\n",
            "|    fps              | 3798     |\n",
            "|    time_elapsed     | 9        |\n",
            "|    total_timesteps  | 35684    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 93       |\n",
            "|    ep_rew_mean      | -204     |\n",
            "|    exploration_rate | 0.886    |\n",
            "| time/               |          |\n",
            "|    episodes         | 396      |\n",
            "|    fps              | 3798     |\n",
            "|    time_elapsed     | 9        |\n",
            "|    total_timesteps  | 35988    |\n",
            "----------------------------------\n",
            "Num timesteps: 36000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -204.50\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 93.1     |\n",
            "|    ep_rew_mean      | -206     |\n",
            "|    exploration_rate | 0.885    |\n",
            "| time/               |          |\n",
            "|    episodes         | 400      |\n",
            "|    fps              | 3795     |\n",
            "|    time_elapsed     | 9        |\n",
            "|    total_timesteps  | 36366    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.2     |\n",
            "|    ep_rew_mean      | -201     |\n",
            "|    exploration_rate | 0.884    |\n",
            "| time/               |          |\n",
            "|    episodes         | 404      |\n",
            "|    fps              | 3795     |\n",
            "|    time_elapsed     | 9        |\n",
            "|    total_timesteps  | 36716    |\n",
            "----------------------------------\n",
            "Num timesteps: 37000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -195.60\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91.5     |\n",
            "|    ep_rew_mean      | -195     |\n",
            "|    exploration_rate | 0.883    |\n",
            "| time/               |          |\n",
            "|    episodes         | 408      |\n",
            "|    fps              | 3795     |\n",
            "|    time_elapsed     | 9        |\n",
            "|    total_timesteps  | 37057    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91.4     |\n",
            "|    ep_rew_mean      | -194     |\n",
            "|    exploration_rate | 0.881    |\n",
            "| time/               |          |\n",
            "|    episodes         | 412      |\n",
            "|    fps              | 3796     |\n",
            "|    time_elapsed     | 9        |\n",
            "|    total_timesteps  | 37426    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 90.7     |\n",
            "|    ep_rew_mean      | -194     |\n",
            "|    exploration_rate | 0.881    |\n",
            "| time/               |          |\n",
            "|    episodes         | 416      |\n",
            "|    fps              | 3798     |\n",
            "|    time_elapsed     | 9        |\n",
            "|    total_timesteps  | 37729    |\n",
            "----------------------------------\n",
            "Num timesteps: 38000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -193.05\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 90.2     |\n",
            "|    ep_rew_mean      | -192     |\n",
            "|    exploration_rate | 0.879    |\n",
            "| time/               |          |\n",
            "|    episodes         | 420      |\n",
            "|    fps              | 3797     |\n",
            "|    time_elapsed     | 10       |\n",
            "|    total_timesteps  | 38071    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 90.7     |\n",
            "|    ep_rew_mean      | -190     |\n",
            "|    exploration_rate | 0.878    |\n",
            "| time/               |          |\n",
            "|    episodes         | 424      |\n",
            "|    fps              | 3798     |\n",
            "|    time_elapsed     | 10       |\n",
            "|    total_timesteps  | 38477    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91.4     |\n",
            "|    ep_rew_mean      | -186     |\n",
            "|    exploration_rate | 0.877    |\n",
            "| time/               |          |\n",
            "|    episodes         | 428      |\n",
            "|    fps              | 3798     |\n",
            "|    time_elapsed     | 10       |\n",
            "|    total_timesteps  | 38911    |\n",
            "----------------------------------\n",
            "Num timesteps: 39000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -185.62\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91.6     |\n",
            "|    ep_rew_mean      | -182     |\n",
            "|    exploration_rate | 0.876    |\n",
            "| time/               |          |\n",
            "|    episodes         | 432      |\n",
            "|    fps              | 3798     |\n",
            "|    time_elapsed     | 10       |\n",
            "|    total_timesteps  | 39281    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91.8     |\n",
            "|    ep_rew_mean      | -181     |\n",
            "|    exploration_rate | 0.874    |\n",
            "| time/               |          |\n",
            "|    episodes         | 436      |\n",
            "|    fps              | 3799     |\n",
            "|    time_elapsed     | 10       |\n",
            "|    total_timesteps  | 39668    |\n",
            "----------------------------------\n",
            "Num timesteps: 40000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -183.67\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92       |\n",
            "|    ep_rew_mean      | -187     |\n",
            "|    exploration_rate | 0.873    |\n",
            "| time/               |          |\n",
            "|    episodes         | 440      |\n",
            "|    fps              | 3795     |\n",
            "|    time_elapsed     | 10       |\n",
            "|    total_timesteps  | 40020    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.5     |\n",
            "|    ep_rew_mean      | -186     |\n",
            "|    exploration_rate | 0.872    |\n",
            "| time/               |          |\n",
            "|    episodes         | 444      |\n",
            "|    fps              | 3796     |\n",
            "|    time_elapsed     | 10       |\n",
            "|    total_timesteps  | 40440    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.8     |\n",
            "|    ep_rew_mean      | -188     |\n",
            "|    exploration_rate | 0.871    |\n",
            "| time/               |          |\n",
            "|    episodes         | 448      |\n",
            "|    fps              | 3795     |\n",
            "|    time_elapsed     | 10       |\n",
            "|    total_timesteps  | 40823    |\n",
            "----------------------------------\n",
            "Num timesteps: 41000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -187.77\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.5     |\n",
            "|    ep_rew_mean      | -185     |\n",
            "|    exploration_rate | 0.869    |\n",
            "| time/               |          |\n",
            "|    episodes         | 452      |\n",
            "|    fps              | 3794     |\n",
            "|    time_elapsed     | 10       |\n",
            "|    total_timesteps  | 41211    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 93       |\n",
            "|    ep_rew_mean      | -182     |\n",
            "|    exploration_rate | 0.868    |\n",
            "| time/               |          |\n",
            "|    episodes         | 456      |\n",
            "|    fps              | 3795     |\n",
            "|    time_elapsed     | 10       |\n",
            "|    total_timesteps  | 41624    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.9     |\n",
            "|    ep_rew_mean      | -179     |\n",
            "|    exploration_rate | 0.867    |\n",
            "| time/               |          |\n",
            "|    episodes         | 460      |\n",
            "|    fps              | 3797     |\n",
            "|    time_elapsed     | 11       |\n",
            "|    total_timesteps  | 41944    |\n",
            "----------------------------------\n",
            "Num timesteps: 42000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -179.18\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 93.3     |\n",
            "|    ep_rew_mean      | -181     |\n",
            "|    exploration_rate | 0.866    |\n",
            "| time/               |          |\n",
            "|    episodes         | 464      |\n",
            "|    fps              | 3797     |\n",
            "|    time_elapsed     | 11       |\n",
            "|    total_timesteps  | 42324    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 94.4     |\n",
            "|    ep_rew_mean      | -184     |\n",
            "|    exploration_rate | 0.865    |\n",
            "| time/               |          |\n",
            "|    episodes         | 468      |\n",
            "|    fps              | 3797     |\n",
            "|    time_elapsed     | 11       |\n",
            "|    total_timesteps  | 42740    |\n",
            "----------------------------------\n",
            "Num timesteps: 43000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -187.00\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 94.2     |\n",
            "|    ep_rew_mean      | -188     |\n",
            "|    exploration_rate | 0.863    |\n",
            "| time/               |          |\n",
            "|    episodes         | 472      |\n",
            "|    fps              | 3794     |\n",
            "|    time_elapsed     | 11       |\n",
            "|    total_timesteps  | 43137    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 94.2     |\n",
            "|    ep_rew_mean      | -186     |\n",
            "|    exploration_rate | 0.862    |\n",
            "| time/               |          |\n",
            "|    episodes         | 476      |\n",
            "|    fps              | 3795     |\n",
            "|    time_elapsed     | 11       |\n",
            "|    total_timesteps  | 43530    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 93.8     |\n",
            "|    ep_rew_mean      | -188     |\n",
            "|    exploration_rate | 0.861    |\n",
            "| time/               |          |\n",
            "|    episodes         | 480      |\n",
            "|    fps              | 3795     |\n",
            "|    time_elapsed     | 11       |\n",
            "|    total_timesteps  | 43873    |\n",
            "----------------------------------\n",
            "Num timesteps: 44000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -183.67\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.8     |\n",
            "|    ep_rew_mean      | -183     |\n",
            "|    exploration_rate | 0.86     |\n",
            "| time/               |          |\n",
            "|    episodes         | 484      |\n",
            "|    fps              | 3795     |\n",
            "|    time_elapsed     | 11       |\n",
            "|    total_timesteps  | 44173    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91.6     |\n",
            "|    ep_rew_mean      | -183     |\n",
            "|    exploration_rate | 0.859    |\n",
            "| time/               |          |\n",
            "|    episodes         | 488      |\n",
            "|    fps              | 3797     |\n",
            "|    time_elapsed     | 11       |\n",
            "|    total_timesteps  | 44507    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91.6     |\n",
            "|    ep_rew_mean      | -177     |\n",
            "|    exploration_rate | 0.858    |\n",
            "| time/               |          |\n",
            "|    episodes         | 492      |\n",
            "|    fps              | 3793     |\n",
            "|    time_elapsed     | 11       |\n",
            "|    total_timesteps  | 44843    |\n",
            "----------------------------------\n",
            "Num timesteps: 45000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -175.69\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92       |\n",
            "|    ep_rew_mean      | -176     |\n",
            "|    exploration_rate | 0.857    |\n",
            "| time/               |          |\n",
            "|    episodes         | 496      |\n",
            "|    fps              | 3793     |\n",
            "|    time_elapsed     | 11       |\n",
            "|    total_timesteps  | 45191    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91.5     |\n",
            "|    ep_rew_mean      | -173     |\n",
            "|    exploration_rate | 0.856    |\n",
            "| time/               |          |\n",
            "|    episodes         | 500      |\n",
            "|    fps              | 3794     |\n",
            "|    time_elapsed     | 11       |\n",
            "|    total_timesteps  | 45512    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91.4     |\n",
            "|    ep_rew_mean      | -171     |\n",
            "|    exploration_rate | 0.855    |\n",
            "| time/               |          |\n",
            "|    episodes         | 504      |\n",
            "|    fps              | 3795     |\n",
            "|    time_elapsed     | 12       |\n",
            "|    total_timesteps  | 45858    |\n",
            "----------------------------------\n",
            "Num timesteps: 46000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -171.12\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91.5     |\n",
            "|    ep_rew_mean      | -174     |\n",
            "|    exploration_rate | 0.854    |\n",
            "| time/               |          |\n",
            "|    episodes         | 508      |\n",
            "|    fps              | 3794     |\n",
            "|    time_elapsed     | 12       |\n",
            "|    total_timesteps  | 46205    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91.5     |\n",
            "|    ep_rew_mean      | -174     |\n",
            "|    exploration_rate | 0.852    |\n",
            "| time/               |          |\n",
            "|    episodes         | 512      |\n",
            "|    fps              | 3794     |\n",
            "|    time_elapsed     | 12       |\n",
            "|    total_timesteps  | 46580    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91.4     |\n",
            "|    ep_rew_mean      | -177     |\n",
            "|    exploration_rate | 0.852    |\n",
            "| time/               |          |\n",
            "|    episodes         | 516      |\n",
            "|    fps              | 3794     |\n",
            "|    time_elapsed     | 12       |\n",
            "|    total_timesteps  | 46872    |\n",
            "----------------------------------\n",
            "Num timesteps: 47000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -177.50\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91.4     |\n",
            "|    ep_rew_mean      | -176     |\n",
            "|    exploration_rate | 0.851    |\n",
            "| time/               |          |\n",
            "|    episodes         | 520      |\n",
            "|    fps              | 3794     |\n",
            "|    time_elapsed     | 12       |\n",
            "|    total_timesteps  | 47210    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91.8     |\n",
            "|    ep_rew_mean      | -176     |\n",
            "|    exploration_rate | 0.849    |\n",
            "| time/               |          |\n",
            "|    episodes         | 524      |\n",
            "|    fps              | 3794     |\n",
            "|    time_elapsed     | 12       |\n",
            "|    total_timesteps  | 47659    |\n",
            "----------------------------------\n",
            "Num timesteps: 48000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -178.59\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91.1     |\n",
            "|    ep_rew_mean      | -179     |\n",
            "|    exploration_rate | 0.848    |\n",
            "| time/               |          |\n",
            "|    episodes         | 528      |\n",
            "|    fps              | 3793     |\n",
            "|    time_elapsed     | 12       |\n",
            "|    total_timesteps  | 48020    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91.3     |\n",
            "|    ep_rew_mean      | -179     |\n",
            "|    exploration_rate | 0.847    |\n",
            "| time/               |          |\n",
            "|    episodes         | 532      |\n",
            "|    fps              | 3794     |\n",
            "|    time_elapsed     | 12       |\n",
            "|    total_timesteps  | 48415    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91.6     |\n",
            "|    ep_rew_mean      | -178     |\n",
            "|    exploration_rate | 0.845    |\n",
            "| time/               |          |\n",
            "|    episodes         | 536      |\n",
            "|    fps              | 3794     |\n",
            "|    time_elapsed     | 12       |\n",
            "|    total_timesteps  | 48825    |\n",
            "----------------------------------\n",
            "Num timesteps: 49000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -178.26\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91.5     |\n",
            "|    ep_rew_mean      | -176     |\n",
            "|    exploration_rate | 0.844    |\n",
            "| time/               |          |\n",
            "|    episodes         | 540      |\n",
            "|    fps              | 3792     |\n",
            "|    time_elapsed     | 12       |\n",
            "|    total_timesteps  | 49171    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91.3     |\n",
            "|    ep_rew_mean      | -176     |\n",
            "|    exploration_rate | 0.843    |\n",
            "| time/               |          |\n",
            "|    episodes         | 544      |\n",
            "|    fps              | 3792     |\n",
            "|    time_elapsed     | 13       |\n",
            "|    total_timesteps  | 49570    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91       |\n",
            "|    ep_rew_mean      | -174     |\n",
            "|    exploration_rate | 0.842    |\n",
            "| time/               |          |\n",
            "|    episodes         | 548      |\n",
            "|    fps              | 3792     |\n",
            "|    time_elapsed     | 13       |\n",
            "|    total_timesteps  | 49927    |\n",
            "----------------------------------\n",
            "Num timesteps: 50000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -173.92\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 90.6     |\n",
            "|    ep_rew_mean      | -178     |\n",
            "|    exploration_rate | 0.841    |\n",
            "| time/               |          |\n",
            "|    episodes         | 552      |\n",
            "|    fps              | 3748     |\n",
            "|    time_elapsed     | 13       |\n",
            "|    total_timesteps  | 50269    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.24     |\n",
            "|    n_updates        | 67       |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 90.3     |\n",
            "|    ep_rew_mean      | -179     |\n",
            "|    exploration_rate | 0.84     |\n",
            "| time/               |          |\n",
            "|    episodes         | 556      |\n",
            "|    fps              | 3694     |\n",
            "|    time_elapsed     | 13       |\n",
            "|    total_timesteps  | 50654    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.82     |\n",
            "|    n_updates        | 163      |\n",
            "----------------------------------\n",
            "Num timesteps: 51000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -177.00\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 90.7     |\n",
            "|    ep_rew_mean      | -179     |\n",
            "|    exploration_rate | 0.838    |\n",
            "| time/               |          |\n",
            "|    episodes         | 560      |\n",
            "|    fps              | 3644     |\n",
            "|    time_elapsed     | 13       |\n",
            "|    total_timesteps  | 51012    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.55     |\n",
            "|    n_updates        | 252      |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91       |\n",
            "|    ep_rew_mean      | -179     |\n",
            "|    exploration_rate | 0.837    |\n",
            "| time/               |          |\n",
            "|    episodes         | 564      |\n",
            "|    fps              | 3589     |\n",
            "|    time_elapsed     | 14       |\n",
            "|    total_timesteps  | 51421    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.27     |\n",
            "|    n_updates        | 355      |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91.1     |\n",
            "|    ep_rew_mean      | -175     |\n",
            "|    exploration_rate | 0.836    |\n",
            "| time/               |          |\n",
            "|    episodes         | 568      |\n",
            "|    fps              | 3534     |\n",
            "|    time_elapsed     | 14       |\n",
            "|    total_timesteps  | 51847    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.93     |\n",
            "|    n_updates        | 461      |\n",
            "----------------------------------\n",
            "Num timesteps: 52000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -175.45\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.4     |\n",
            "|    ep_rew_mean      | -172     |\n",
            "|    exploration_rate | 0.834    |\n",
            "| time/               |          |\n",
            "|    episodes         | 572      |\n",
            "|    fps              | 3467     |\n",
            "|    time_elapsed     | 15       |\n",
            "|    total_timesteps  | 52377    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.13     |\n",
            "|    n_updates        | 594      |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.7     |\n",
            "|    ep_rew_mean      | -174     |\n",
            "|    exploration_rate | 0.833    |\n",
            "| time/               |          |\n",
            "|    episodes         | 576      |\n",
            "|    fps              | 3420     |\n",
            "|    time_elapsed     | 15       |\n",
            "|    total_timesteps  | 52801    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.36     |\n",
            "|    n_updates        | 700      |\n",
            "----------------------------------\n",
            "Num timesteps: 53000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -175.78\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 93.6     |\n",
            "|    ep_rew_mean      | -179     |\n",
            "|    exploration_rate | 0.831    |\n",
            "| time/               |          |\n",
            "|    episodes         | 580      |\n",
            "|    fps              | 3376     |\n",
            "|    time_elapsed     | 15       |\n",
            "|    total_timesteps  | 53234    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 6.97     |\n",
            "|    n_updates        | 808      |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 94.4     |\n",
            "|    ep_rew_mean      | -186     |\n",
            "|    exploration_rate | 0.83     |\n",
            "| time/               |          |\n",
            "|    episodes         | 584      |\n",
            "|    fps              | 3339     |\n",
            "|    time_elapsed     | 16       |\n",
            "|    total_timesteps  | 53616    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.612    |\n",
            "|    n_updates        | 903      |\n",
            "----------------------------------\n",
            "Num timesteps: 54000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -193.93\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 95       |\n",
            "|    ep_rew_mean      | -196     |\n",
            "|    exploration_rate | 0.829    |\n",
            "| time/               |          |\n",
            "|    episodes         | 588      |\n",
            "|    fps              | 3301     |\n",
            "|    time_elapsed     | 16       |\n",
            "|    total_timesteps  | 54006    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.18     |\n",
            "|    n_updates        | 1001     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 96.7     |\n",
            "|    ep_rew_mean      | -198     |\n",
            "|    exploration_rate | 0.827    |\n",
            "| time/               |          |\n",
            "|    episodes         | 592      |\n",
            "|    fps              | 3248     |\n",
            "|    time_elapsed     | 16       |\n",
            "|    total_timesteps  | 54515    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.81     |\n",
            "|    n_updates        | 1128     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 97.4     |\n",
            "|    ep_rew_mean      | -201     |\n",
            "|    exploration_rate | 0.826    |\n",
            "| time/               |          |\n",
            "|    episodes         | 596      |\n",
            "|    fps              | 3208     |\n",
            "|    time_elapsed     | 17       |\n",
            "|    total_timesteps  | 54932    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.98     |\n",
            "|    n_updates        | 1232     |\n",
            "----------------------------------\n",
            "Num timesteps: 55000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -201.19\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 98.5     |\n",
            "|    ep_rew_mean      | -203     |\n",
            "|    exploration_rate | 0.825    |\n",
            "| time/               |          |\n",
            "|    episodes         | 600      |\n",
            "|    fps              | 3165     |\n",
            "|    time_elapsed     | 17       |\n",
            "|    total_timesteps  | 55367    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.747    |\n",
            "|    n_updates        | 1341     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -206     |\n",
            "|    exploration_rate | 0.823    |\n",
            "| time/               |          |\n",
            "|    episodes         | 604      |\n",
            "|    fps              | 3117     |\n",
            "|    time_elapsed     | 17       |\n",
            "|    total_timesteps  | 55909    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.727    |\n",
            "|    n_updates        | 1477     |\n",
            "----------------------------------\n",
            "Num timesteps: 56000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -207.35\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -207     |\n",
            "|    exploration_rate | 0.822    |\n",
            "| time/               |          |\n",
            "|    episodes         | 608      |\n",
            "|    fps              | 3082     |\n",
            "|    time_elapsed     | 18       |\n",
            "|    total_timesteps  | 56319    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.737    |\n",
            "|    n_updates        | 1579     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 102      |\n",
            "|    ep_rew_mean      | -209     |\n",
            "|    exploration_rate | 0.82     |\n",
            "| time/               |          |\n",
            "|    episodes         | 612      |\n",
            "|    fps              | 3047     |\n",
            "|    time_elapsed     | 18       |\n",
            "|    total_timesteps  | 56757    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.834    |\n",
            "|    n_updates        | 1689     |\n",
            "----------------------------------\n",
            "Num timesteps: 57000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -205.35\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 103      |\n",
            "|    ep_rew_mean      | -205     |\n",
            "|    exploration_rate | 0.819    |\n",
            "| time/               |          |\n",
            "|    episodes         | 616      |\n",
            "|    fps              | 3010     |\n",
            "|    time_elapsed     | 19       |\n",
            "|    total_timesteps  | 57201    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 6.63     |\n",
            "|    n_updates        | 1800     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 104      |\n",
            "|    ep_rew_mean      | -207     |\n",
            "|    exploration_rate | 0.818    |\n",
            "| time/               |          |\n",
            "|    episodes         | 620      |\n",
            "|    fps              | 2979     |\n",
            "|    time_elapsed     | 19       |\n",
            "|    total_timesteps  | 57616    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.17     |\n",
            "|    n_updates        | 1903     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 103      |\n",
            "|    ep_rew_mean      | -205     |\n",
            "|    exploration_rate | 0.816    |\n",
            "| time/               |          |\n",
            "|    episodes         | 624      |\n",
            "|    fps              | 2954     |\n",
            "|    time_elapsed     | 19       |\n",
            "|    total_timesteps  | 57974    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.41     |\n",
            "|    n_updates        | 1993     |\n",
            "----------------------------------\n",
            "Num timesteps: 58000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -205.00\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 104      |\n",
            "|    ep_rew_mean      | -204     |\n",
            "|    exploration_rate | 0.815    |\n",
            "| time/               |          |\n",
            "|    episodes         | 628      |\n",
            "|    fps              | 2925     |\n",
            "|    time_elapsed     | 19       |\n",
            "|    total_timesteps  | 58397    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.437    |\n",
            "|    n_updates        | 2099     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 104      |\n",
            "|    ep_rew_mean      | -202     |\n",
            "|    exploration_rate | 0.814    |\n",
            "| time/               |          |\n",
            "|    episodes         | 632      |\n",
            "|    fps              | 2901     |\n",
            "|    time_elapsed     | 20       |\n",
            "|    total_timesteps  | 58778    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.437    |\n",
            "|    n_updates        | 2194     |\n",
            "----------------------------------\n",
            "Num timesteps: 59000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -198.46\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 103      |\n",
            "|    ep_rew_mean      | -198     |\n",
            "|    exploration_rate | 0.813    |\n",
            "| time/               |          |\n",
            "|    episodes         | 636      |\n",
            "|    fps              | 2878     |\n",
            "|    time_elapsed     | 20       |\n",
            "|    total_timesteps  | 59158    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.49     |\n",
            "|    n_updates        | 2289     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 105      |\n",
            "|    ep_rew_mean      | -196     |\n",
            "|    exploration_rate | 0.811    |\n",
            "| time/               |          |\n",
            "|    episodes         | 640      |\n",
            "|    fps              | 2847     |\n",
            "|    time_elapsed     | 20       |\n",
            "|    total_timesteps  | 59655    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.921    |\n",
            "|    n_updates        | 2413     |\n",
            "----------------------------------\n",
            "Num timesteps: 60000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -193.29\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 105      |\n",
            "|    ep_rew_mean      | -195     |\n",
            "|    exploration_rate | 0.81     |\n",
            "| time/               |          |\n",
            "|    episodes         | 644      |\n",
            "|    fps              | 2816     |\n",
            "|    time_elapsed     | 21       |\n",
            "|    total_timesteps  | 60118    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.04     |\n",
            "|    n_updates        | 2529     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 107      |\n",
            "|    ep_rew_mean      | -193     |\n",
            "|    exploration_rate | 0.808    |\n",
            "| time/               |          |\n",
            "|    episodes         | 648      |\n",
            "|    fps              | 2786     |\n",
            "|    time_elapsed     | 21       |\n",
            "|    total_timesteps  | 60598    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.51     |\n",
            "|    n_updates        | 2649     |\n",
            "----------------------------------\n",
            "Num timesteps: 61000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -189.99\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 107      |\n",
            "|    ep_rew_mean      | -187     |\n",
            "|    exploration_rate | 0.807    |\n",
            "| time/               |          |\n",
            "|    episodes         | 652      |\n",
            "|    fps              | 2763     |\n",
            "|    time_elapsed     | 22       |\n",
            "|    total_timesteps  | 61002    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.49     |\n",
            "|    n_updates        | 2750     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 108      |\n",
            "|    ep_rew_mean      | -186     |\n",
            "|    exploration_rate | 0.805    |\n",
            "| time/               |          |\n",
            "|    episodes         | 656      |\n",
            "|    fps              | 2736     |\n",
            "|    time_elapsed     | 22       |\n",
            "|    total_timesteps  | 61482    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.306    |\n",
            "|    n_updates        | 2870     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 109      |\n",
            "|    ep_rew_mean      | -185     |\n",
            "|    exploration_rate | 0.804    |\n",
            "| time/               |          |\n",
            "|    episodes         | 660      |\n",
            "|    fps              | 2715     |\n",
            "|    time_elapsed     | 22       |\n",
            "|    total_timesteps  | 61881    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.517    |\n",
            "|    n_updates        | 2970     |\n",
            "----------------------------------\n",
            "Num timesteps: 62000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -185.18\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 110      |\n",
            "|    ep_rew_mean      | -182     |\n",
            "|    exploration_rate | 0.802    |\n",
            "| time/               |          |\n",
            "|    episodes         | 664      |\n",
            "|    fps              | 2684     |\n",
            "|    time_elapsed     | 23       |\n",
            "|    total_timesteps  | 62431    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.676    |\n",
            "|    n_updates        | 3107     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 110      |\n",
            "|    ep_rew_mean      | -179     |\n",
            "|    exploration_rate | 0.801    |\n",
            "| time/               |          |\n",
            "|    episodes         | 668      |\n",
            "|    fps              | 2664     |\n",
            "|    time_elapsed     | 23       |\n",
            "|    total_timesteps  | 62856    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.595    |\n",
            "|    n_updates        | 3213     |\n",
            "----------------------------------\n",
            "Num timesteps: 63000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -178.39\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 109      |\n",
            "|    ep_rew_mean      | -180     |\n",
            "|    exploration_rate | 0.8      |\n",
            "| time/               |          |\n",
            "|    episodes         | 672      |\n",
            "|    fps              | 2644     |\n",
            "|    time_elapsed     | 23       |\n",
            "|    total_timesteps  | 63272    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.826    |\n",
            "|    n_updates        | 3317     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 109      |\n",
            "|    ep_rew_mean      | -173     |\n",
            "|    exploration_rate | 0.798    |\n",
            "| time/               |          |\n",
            "|    episodes         | 676      |\n",
            "|    fps              | 2623     |\n",
            "|    time_elapsed     | 24       |\n",
            "|    total_timesteps  | 63702    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3        |\n",
            "|    n_updates        | 3425     |\n",
            "----------------------------------\n",
            "Num timesteps: 64000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -168.87\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 109      |\n",
            "|    ep_rew_mean      | -165     |\n",
            "|    exploration_rate | 0.797    |\n",
            "| time/               |          |\n",
            "|    episodes         | 680      |\n",
            "|    fps              | 2602     |\n",
            "|    time_elapsed     | 24       |\n",
            "|    total_timesteps  | 64098    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 6.64     |\n",
            "|    n_updates        | 3524     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 109      |\n",
            "|    ep_rew_mean      | -158     |\n",
            "|    exploration_rate | 0.796    |\n",
            "| time/               |          |\n",
            "|    episodes         | 684      |\n",
            "|    fps              | 2583     |\n",
            "|    time_elapsed     | 24       |\n",
            "|    total_timesteps  | 64528    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.13     |\n",
            "|    n_updates        | 3631     |\n",
            "----------------------------------\n",
            "Num timesteps: 65000\n",
            "Best mean reward: -160.45 - Last mean reward per episode: -153.99\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 110      |\n",
            "|    ep_rew_mean      | -153     |\n",
            "|    exploration_rate | 0.794    |\n",
            "| time/               |          |\n",
            "|    episodes         | 688      |\n",
            "|    fps              | 2561     |\n",
            "|    time_elapsed     | 25       |\n",
            "|    total_timesteps  | 65005    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.458    |\n",
            "|    n_updates        | 3751     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 110      |\n",
            "|    ep_rew_mean      | -152     |\n",
            "|    exploration_rate | 0.793    |\n",
            "| time/               |          |\n",
            "|    episodes         | 692      |\n",
            "|    fps              | 2540     |\n",
            "|    time_elapsed     | 25       |\n",
            "|    total_timesteps  | 65479    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.872    |\n",
            "|    n_updates        | 3869     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 110      |\n",
            "|    ep_rew_mean      | -153     |\n",
            "|    exploration_rate | 0.791    |\n",
            "| time/               |          |\n",
            "|    episodes         | 696      |\n",
            "|    fps              | 2523     |\n",
            "|    time_elapsed     | 26       |\n",
            "|    total_timesteps  | 65883    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.41     |\n",
            "|    n_updates        | 3970     |\n",
            "----------------------------------\n",
            "Num timesteps: 66000\n",
            "Best mean reward: -153.99 - Last mean reward per episode: -154.69\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 110      |\n",
            "|    ep_rew_mean      | -151     |\n",
            "|    exploration_rate | 0.79     |\n",
            "| time/               |          |\n",
            "|    episodes         | 700      |\n",
            "|    fps              | 2504     |\n",
            "|    time_elapsed     | 26       |\n",
            "|    total_timesteps  | 66347    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.557    |\n",
            "|    n_updates        | 4086     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 109      |\n",
            "|    ep_rew_mean      | -150     |\n",
            "|    exploration_rate | 0.788    |\n",
            "| time/               |          |\n",
            "|    episodes         | 704      |\n",
            "|    fps              | 2482     |\n",
            "|    time_elapsed     | 26       |\n",
            "|    total_timesteps  | 66857    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.48     |\n",
            "|    n_updates        | 4214     |\n",
            "----------------------------------\n",
            "Num timesteps: 67000\n",
            "Best mean reward: -153.99 - Last mean reward per episode: -148.54\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 111      |\n",
            "|    ep_rew_mean      | -146     |\n",
            "|    exploration_rate | 0.787    |\n",
            "| time/               |          |\n",
            "|    episodes         | 708      |\n",
            "|    fps              | 2461     |\n",
            "|    time_elapsed     | 27       |\n",
            "|    total_timesteps  | 67376    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.65     |\n",
            "|    n_updates        | 4343     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 110      |\n",
            "|    ep_rew_mean      | -140     |\n",
            "|    exploration_rate | 0.785    |\n",
            "| time/               |          |\n",
            "|    episodes         | 712      |\n",
            "|    fps              | 2446     |\n",
            "|    time_elapsed     | 27       |\n",
            "|    total_timesteps  | 67779    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.47     |\n",
            "|    n_updates        | 4444     |\n",
            "----------------------------------\n",
            "Num timesteps: 68000\n",
            "Best mean reward: -148.54 - Last mean reward per episode: -141.23\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 111      |\n",
            "|    ep_rew_mean      | -140     |\n",
            "|    exploration_rate | 0.784    |\n",
            "| time/               |          |\n",
            "|    episodes         | 716      |\n",
            "|    fps              | 2425     |\n",
            "|    time_elapsed     | 28       |\n",
            "|    total_timesteps  | 68289    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.14     |\n",
            "|    n_updates        | 4572     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 112      |\n",
            "|    ep_rew_mean      | -138     |\n",
            "|    exploration_rate | 0.782    |\n",
            "| time/               |          |\n",
            "|    episodes         | 720      |\n",
            "|    fps              | 2407     |\n",
            "|    time_elapsed     | 28       |\n",
            "|    total_timesteps  | 68780    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.481    |\n",
            "|    n_updates        | 4694     |\n",
            "----------------------------------\n",
            "Num timesteps: 69000\n",
            "Best mean reward: -141.23 - Last mean reward per episode: -136.26\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 113      |\n",
            "|    ep_rew_mean      | -138     |\n",
            "|    exploration_rate | 0.781    |\n",
            "| time/               |          |\n",
            "|    episodes         | 724      |\n",
            "|    fps              | 2389     |\n",
            "|    time_elapsed     | 29       |\n",
            "|    total_timesteps  | 69300    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.467    |\n",
            "|    n_updates        | 4824     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 114      |\n",
            "|    ep_rew_mean      | -137     |\n",
            "|    exploration_rate | 0.779    |\n",
            "| time/               |          |\n",
            "|    episodes         | 728      |\n",
            "|    fps              | 2373     |\n",
            "|    time_elapsed     | 29       |\n",
            "|    total_timesteps  | 69762    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.497    |\n",
            "|    n_updates        | 4940     |\n",
            "----------------------------------\n",
            "Num timesteps: 70000\n",
            "Best mean reward: -136.26 - Last mean reward per episode: -138.59\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 114      |\n",
            "|    ep_rew_mean      | -139     |\n",
            "|    exploration_rate | 0.778    |\n",
            "| time/               |          |\n",
            "|    episodes         | 732      |\n",
            "|    fps              | 2360     |\n",
            "|    time_elapsed     | 29       |\n",
            "|    total_timesteps  | 70179    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.33     |\n",
            "|    n_updates        | 5044     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 115      |\n",
            "|    ep_rew_mean      | -141     |\n",
            "|    exploration_rate | 0.776    |\n",
            "| time/               |          |\n",
            "|    episodes         | 736      |\n",
            "|    fps              | 2344     |\n",
            "|    time_elapsed     | 30       |\n",
            "|    total_timesteps  | 70655    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.15     |\n",
            "|    n_updates        | 5163     |\n",
            "----------------------------------\n",
            "Num timesteps: 71000\n",
            "Best mean reward: -136.26 - Last mean reward per episode: -140.67\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 114      |\n",
            "|    ep_rew_mean      | -142     |\n",
            "|    exploration_rate | 0.775    |\n",
            "| time/               |          |\n",
            "|    episodes         | 740      |\n",
            "|    fps              | 2331     |\n",
            "|    time_elapsed     | 30       |\n",
            "|    total_timesteps  | 71067    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.76     |\n",
            "|    n_updates        | 5266     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 114      |\n",
            "|    ep_rew_mean      | -141     |\n",
            "|    exploration_rate | 0.773    |\n",
            "| time/               |          |\n",
            "|    episodes         | 744      |\n",
            "|    fps              | 2316     |\n",
            "|    time_elapsed     | 30       |\n",
            "|    total_timesteps  | 71560    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.479    |\n",
            "|    n_updates        | 5389     |\n",
            "----------------------------------\n",
            "Num timesteps: 72000\n",
            "Best mean reward: -136.26 - Last mean reward per episode: -141.62\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 115      |\n",
            "|    ep_rew_mean      | -140     |\n",
            "|    exploration_rate | 0.772    |\n",
            "| time/               |          |\n",
            "|    episodes         | 748      |\n",
            "|    fps              | 2301     |\n",
            "|    time_elapsed     | 31       |\n",
            "|    total_timesteps  | 72052    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.08     |\n",
            "|    n_updates        | 5512     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 114      |\n",
            "|    ep_rew_mean      | -142     |\n",
            "|    exploration_rate | 0.771    |\n",
            "| time/               |          |\n",
            "|    episodes         | 752      |\n",
            "|    fps              | 2290     |\n",
            "|    time_elapsed     | 31       |\n",
            "|    total_timesteps  | 72438    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.03     |\n",
            "|    n_updates        | 5609     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 114      |\n",
            "|    ep_rew_mean      | -140     |\n",
            "|    exploration_rate | 0.769    |\n",
            "| time/               |          |\n",
            "|    episodes         | 756      |\n",
            "|    fps              | 2278     |\n",
            "|    time_elapsed     | 31       |\n",
            "|    total_timesteps  | 72855    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.23     |\n",
            "|    n_updates        | 5713     |\n",
            "----------------------------------\n",
            "Num timesteps: 73000\n",
            "Best mean reward: -136.26 - Last mean reward per episode: -142.52\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 114      |\n",
            "|    ep_rew_mean      | -141     |\n",
            "|    exploration_rate | 0.768    |\n",
            "| time/               |          |\n",
            "|    episodes         | 760      |\n",
            "|    fps              | 2264     |\n",
            "|    time_elapsed     | 32       |\n",
            "|    total_timesteps  | 73248    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.41     |\n",
            "|    n_updates        | 5811     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 112      |\n",
            "|    ep_rew_mean      | -142     |\n",
            "|    exploration_rate | 0.767    |\n",
            "| time/               |          |\n",
            "|    episodes         | 764      |\n",
            "|    fps              | 2255     |\n",
            "|    time_elapsed     | 32       |\n",
            "|    total_timesteps  | 73656    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.758    |\n",
            "|    n_updates        | 5913     |\n",
            "----------------------------------\n",
            "Num timesteps: 74000\n",
            "Best mean reward: -136.26 - Last mean reward per episode: -138.77\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 112      |\n",
            "|    ep_rew_mean      | -136     |\n",
            "|    exploration_rate | 0.766    |\n",
            "| time/               |          |\n",
            "|    episodes         | 768      |\n",
            "|    fps              | 2245     |\n",
            "|    time_elapsed     | 32       |\n",
            "|    total_timesteps  | 74031    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.809    |\n",
            "|    n_updates        | 6007     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 112      |\n",
            "|    ep_rew_mean      | -136     |\n",
            "|    exploration_rate | 0.764    |\n",
            "| time/               |          |\n",
            "|    episodes         | 772      |\n",
            "|    fps              | 2230     |\n",
            "|    time_elapsed     | 33       |\n",
            "|    total_timesteps  | 74507    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.467    |\n",
            "|    n_updates        | 6126     |\n",
            "----------------------------------\n",
            "Num timesteps: 75000\n",
            "Best mean reward: -136.26 - Last mean reward per episode: -134.48\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 113      |\n",
            "|    ep_rew_mean      | -134     |\n",
            "|    exploration_rate | 0.762    |\n",
            "| time/               |          |\n",
            "|    episodes         | 776      |\n",
            "|    fps              | 2217     |\n",
            "|    time_elapsed     | 33       |\n",
            "|    total_timesteps  | 75031    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.841    |\n",
            "|    n_updates        | 6257     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 114      |\n",
            "|    ep_rew_mean      | -133     |\n",
            "|    exploration_rate | 0.761    |\n",
            "| time/               |          |\n",
            "|    episodes         | 780      |\n",
            "|    fps              | 2207     |\n",
            "|    time_elapsed     | 34       |\n",
            "|    total_timesteps  | 75478    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.716    |\n",
            "|    n_updates        | 6369     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 113      |\n",
            "|    ep_rew_mean      | -133     |\n",
            "|    exploration_rate | 0.76     |\n",
            "| time/               |          |\n",
            "|    episodes         | 784      |\n",
            "|    fps              | 2198     |\n",
            "|    time_elapsed     | 34       |\n",
            "|    total_timesteps  | 75854    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.241    |\n",
            "|    n_updates        | 6463     |\n",
            "----------------------------------\n",
            "Num timesteps: 76000\n",
            "Best mean reward: -134.48 - Last mean reward per episode: -133.10\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 113      |\n",
            "|    ep_rew_mean      | -129     |\n",
            "|    exploration_rate | 0.758    |\n",
            "| time/               |          |\n",
            "|    episodes         | 788      |\n",
            "|    fps              | 2187     |\n",
            "|    time_elapsed     | 34       |\n",
            "|    total_timesteps  | 76266    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.48     |\n",
            "|    n_updates        | 6566     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 113      |\n",
            "|    ep_rew_mean      | -126     |\n",
            "|    exploration_rate | 0.757    |\n",
            "| time/               |          |\n",
            "|    episodes         | 792      |\n",
            "|    fps              | 2177     |\n",
            "|    time_elapsed     | 35       |\n",
            "|    total_timesteps  | 76733    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.06     |\n",
            "|    n_updates        | 6683     |\n",
            "----------------------------------\n",
            "Num timesteps: 77000\n",
            "Best mean reward: -133.10 - Last mean reward per episode: -124.52\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 113      |\n",
            "|    ep_rew_mean      | -122     |\n",
            "|    exploration_rate | 0.756    |\n",
            "| time/               |          |\n",
            "|    episodes         | 796      |\n",
            "|    fps              | 2166     |\n",
            "|    time_elapsed     | 35       |\n",
            "|    total_timesteps  | 77140    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.17     |\n",
            "|    n_updates        | 6784     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 112      |\n",
            "|    ep_rew_mean      | -122     |\n",
            "|    exploration_rate | 0.754    |\n",
            "| time/               |          |\n",
            "|    episodes         | 800      |\n",
            "|    fps              | 2157     |\n",
            "|    time_elapsed     | 35       |\n",
            "|    total_timesteps  | 77564    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4        |\n",
            "|    n_updates        | 6890     |\n",
            "----------------------------------\n",
            "Num timesteps: 78000\n",
            "Best mean reward: -124.52 - Last mean reward per episode: -118.52\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 112      |\n",
            "|    ep_rew_mean      | -118     |\n",
            "|    exploration_rate | 0.753    |\n",
            "| time/               |          |\n",
            "|    episodes         | 804      |\n",
            "|    fps              | 2147     |\n",
            "|    time_elapsed     | 36       |\n",
            "|    total_timesteps  | 78091    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.464    |\n",
            "|    n_updates        | 7022     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 112      |\n",
            "|    ep_rew_mean      | -117     |\n",
            "|    exploration_rate | 0.751    |\n",
            "| time/               |          |\n",
            "|    episodes         | 808      |\n",
            "|    fps              | 2137     |\n",
            "|    time_elapsed     | 36       |\n",
            "|    total_timesteps  | 78565    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.85     |\n",
            "|    n_updates        | 7141     |\n",
            "----------------------------------\n",
            "Num timesteps: 79000\n",
            "Best mean reward: -118.52 - Last mean reward per episode: -116.43\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 112      |\n",
            "|    ep_rew_mean      | -117     |\n",
            "|    exploration_rate | 0.75     |\n",
            "| time/               |          |\n",
            "|    episodes         | 812      |\n",
            "|    fps              | 2128     |\n",
            "|    time_elapsed     | 37       |\n",
            "|    total_timesteps  | 79003    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.484    |\n",
            "|    n_updates        | 7250     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 112      |\n",
            "|    ep_rew_mean      | -120     |\n",
            "|    exploration_rate | 0.748    |\n",
            "| time/               |          |\n",
            "|    episodes         | 816      |\n",
            "|    fps              | 2118     |\n",
            "|    time_elapsed     | 37       |\n",
            "|    total_timesteps  | 79482    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.01     |\n",
            "|    n_updates        | 7370     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 111      |\n",
            "|    ep_rew_mean      | -119     |\n",
            "|    exploration_rate | 0.747    |\n",
            "| time/               |          |\n",
            "|    episodes         | 820      |\n",
            "|    fps              | 2110     |\n",
            "|    time_elapsed     | 37       |\n",
            "|    total_timesteps  | 79894    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.882    |\n",
            "|    n_updates        | 7473     |\n",
            "----------------------------------\n",
            "Num timesteps: 80000\n",
            "Best mean reward: -116.43 - Last mean reward per episode: -119.45\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 110      |\n",
            "|    ep_rew_mean      | -119     |\n",
            "|    exploration_rate | 0.746    |\n",
            "| time/               |          |\n",
            "|    episodes         | 824      |\n",
            "|    fps              | 2101     |\n",
            "|    time_elapsed     | 38       |\n",
            "|    total_timesteps  | 80347    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.5      |\n",
            "|    n_updates        | 7586     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 110      |\n",
            "|    ep_rew_mean      | -117     |\n",
            "|    exploration_rate | 0.744    |\n",
            "| time/               |          |\n",
            "|    episodes         | 828      |\n",
            "|    fps              | 2094     |\n",
            "|    time_elapsed     | 38       |\n",
            "|    total_timesteps  | 80765    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.539    |\n",
            "|    n_updates        | 7691     |\n",
            "----------------------------------\n",
            "Num timesteps: 81000\n",
            "Best mean reward: -116.43 - Last mean reward per episode: -115.54\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 110      |\n",
            "|    ep_rew_mean      | -114     |\n",
            "|    exploration_rate | 0.743    |\n",
            "| time/               |          |\n",
            "|    episodes         | 832      |\n",
            "|    fps              | 2085     |\n",
            "|    time_elapsed     | 38       |\n",
            "|    total_timesteps  | 81211    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.459    |\n",
            "|    n_updates        | 7802     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 110      |\n",
            "|    ep_rew_mean      | -113     |\n",
            "|    exploration_rate | 0.741    |\n",
            "| time/               |          |\n",
            "|    episodes         | 836      |\n",
            "|    fps              | 2077     |\n",
            "|    time_elapsed     | 39       |\n",
            "|    total_timesteps  | 81641    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.704    |\n",
            "|    n_updates        | 7910     |\n",
            "----------------------------------\n",
            "Num timesteps: 82000\n",
            "Best mean reward: -115.54 - Last mean reward per episode: -111.66\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 110      |\n",
            "|    ep_rew_mean      | -110     |\n",
            "|    exploration_rate | 0.74     |\n",
            "| time/               |          |\n",
            "|    episodes         | 840      |\n",
            "|    fps              | 2069     |\n",
            "|    time_elapsed     | 39       |\n",
            "|    total_timesteps  | 82063    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.64     |\n",
            "|    n_updates        | 8015     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 110      |\n",
            "|    ep_rew_mean      | -110     |\n",
            "|    exploration_rate | 0.739    |\n",
            "| time/               |          |\n",
            "|    episodes         | 844      |\n",
            "|    fps              | 2061     |\n",
            "|    time_elapsed     | 40       |\n",
            "|    total_timesteps  | 82557    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.82     |\n",
            "|    n_updates        | 8139     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 109      |\n",
            "|    ep_rew_mean      | -108     |\n",
            "|    exploration_rate | 0.737    |\n",
            "| time/               |          |\n",
            "|    episodes         | 848      |\n",
            "|    fps              | 2055     |\n",
            "|    time_elapsed     | 40       |\n",
            "|    total_timesteps  | 82911    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.514    |\n",
            "|    n_updates        | 8227     |\n",
            "----------------------------------\n",
            "Num timesteps: 83000\n",
            "Best mean reward: -111.66 - Last mean reward per episode: -108.09\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 109      |\n",
            "|    ep_rew_mean      | -107     |\n",
            "|    exploration_rate | 0.736    |\n",
            "| time/               |          |\n",
            "|    episodes         | 852      |\n",
            "|    fps              | 2046     |\n",
            "|    time_elapsed     | 40       |\n",
            "|    total_timesteps  | 83347    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.63     |\n",
            "|    n_updates        | 8336     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 110      |\n",
            "|    ep_rew_mean      | -105     |\n",
            "|    exploration_rate | 0.735    |\n",
            "| time/               |          |\n",
            "|    episodes         | 856      |\n",
            "|    fps              | 2037     |\n",
            "|    time_elapsed     | 41       |\n",
            "|    total_timesteps  | 83836    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.77     |\n",
            "|    n_updates        | 8458     |\n",
            "----------------------------------\n",
            "Num timesteps: 84000\n",
            "Best mean reward: -108.09 - Last mean reward per episode: -103.45\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 111      |\n",
            "|    ep_rew_mean      | -104     |\n",
            "|    exploration_rate | 0.733    |\n",
            "| time/               |          |\n",
            "|    episodes         | 860      |\n",
            "|    fps              | 2028     |\n",
            "|    time_elapsed     | 41       |\n",
            "|    total_timesteps  | 84389    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.487    |\n",
            "|    n_updates        | 8597     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 113      |\n",
            "|    ep_rew_mean      | -105     |\n",
            "|    exploration_rate | 0.731    |\n",
            "| time/               |          |\n",
            "|    episodes         | 864      |\n",
            "|    fps              | 2018     |\n",
            "|    time_elapsed     | 42       |\n",
            "|    total_timesteps  | 84926    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.589    |\n",
            "|    n_updates        | 8731     |\n",
            "----------------------------------\n",
            "Num timesteps: 85000\n",
            "Best mean reward: -103.45 - Last mean reward per episode: -105.07\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 115      |\n",
            "|    ep_rew_mean      | -109     |\n",
            "|    exploration_rate | 0.729    |\n",
            "| time/               |          |\n",
            "|    episodes         | 868      |\n",
            "|    fps              | 2009     |\n",
            "|    time_elapsed     | 42       |\n",
            "|    total_timesteps  | 85493    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.45     |\n",
            "|    n_updates        | 8873     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 114      |\n",
            "|    ep_rew_mean      | -108     |\n",
            "|    exploration_rate | 0.728    |\n",
            "| time/               |          |\n",
            "|    episodes         | 872      |\n",
            "|    fps              | 2003     |\n",
            "|    time_elapsed     | 42       |\n",
            "|    total_timesteps  | 85928    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.81     |\n",
            "|    n_updates        | 8981     |\n",
            "----------------------------------\n",
            "Num timesteps: 86000\n",
            "Best mean reward: -103.45 - Last mean reward per episode: -107.61\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 113      |\n",
            "|    ep_rew_mean      | -108     |\n",
            "|    exploration_rate | 0.727    |\n",
            "| time/               |          |\n",
            "|    episodes         | 876      |\n",
            "|    fps              | 1997     |\n",
            "|    time_elapsed     | 43       |\n",
            "|    total_timesteps  | 86361    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.545    |\n",
            "|    n_updates        | 9090     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 113      |\n",
            "|    ep_rew_mean      | -109     |\n",
            "|    exploration_rate | 0.725    |\n",
            "| time/               |          |\n",
            "|    episodes         | 880      |\n",
            "|    fps              | 1992     |\n",
            "|    time_elapsed     | 43       |\n",
            "|    total_timesteps  | 86774    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.8      |\n",
            "|    n_updates        | 9193     |\n",
            "----------------------------------\n",
            "Num timesteps: 87000\n",
            "Best mean reward: -103.45 - Last mean reward per episode: -110.63\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 114      |\n",
            "|    ep_rew_mean      | -110     |\n",
            "|    exploration_rate | 0.724    |\n",
            "| time/               |          |\n",
            "|    episodes         | 884      |\n",
            "|    fps              | 1986     |\n",
            "|    time_elapsed     | 43       |\n",
            "|    total_timesteps  | 87239    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.77     |\n",
            "|    n_updates        | 9309     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 115      |\n",
            "|    ep_rew_mean      | -109     |\n",
            "|    exploration_rate | 0.722    |\n",
            "| time/               |          |\n",
            "|    episodes         | 888      |\n",
            "|    fps              | 1979     |\n",
            "|    time_elapsed     | 44       |\n",
            "|    total_timesteps  | 87755    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.898    |\n",
            "|    n_updates        | 9438     |\n",
            "----------------------------------\n",
            "Num timesteps: 88000\n",
            "Best mean reward: -103.45 - Last mean reward per episode: -108.85\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 115      |\n",
            "|    ep_rew_mean      | -109     |\n",
            "|    exploration_rate | 0.721    |\n",
            "| time/               |          |\n",
            "|    episodes         | 892      |\n",
            "|    fps              | 1973     |\n",
            "|    time_elapsed     | 44       |\n",
            "|    total_timesteps  | 88260    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.947    |\n",
            "|    n_updates        | 9564     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 115      |\n",
            "|    ep_rew_mean      | -110     |\n",
            "|    exploration_rate | 0.719    |\n",
            "| time/               |          |\n",
            "|    episodes         | 896      |\n",
            "|    fps              | 1968     |\n",
            "|    time_elapsed     | 45       |\n",
            "|    total_timesteps  | 88641    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.93     |\n",
            "|    n_updates        | 9660     |\n",
            "----------------------------------\n",
            "Num timesteps: 89000\n",
            "Best mean reward: -103.45 - Last mean reward per episode: -108.58\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 116      |\n",
            "|    ep_rew_mean      | -109     |\n",
            "|    exploration_rate | 0.718    |\n",
            "| time/               |          |\n",
            "|    episodes         | 900      |\n",
            "|    fps              | 1960     |\n",
            "|    time_elapsed     | 45       |\n",
            "|    total_timesteps  | 89129    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.433    |\n",
            "|    n_updates        | 9782     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 116      |\n",
            "|    ep_rew_mean      | -109     |\n",
            "|    exploration_rate | 0.716    |\n",
            "| time/               |          |\n",
            "|    episodes         | 904      |\n",
            "|    fps              | 1952     |\n",
            "|    time_elapsed     | 45       |\n",
            "|    total_timesteps  | 89671    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.49     |\n",
            "|    n_updates        | 9917     |\n",
            "----------------------------------\n",
            "Num timesteps: 90000\n",
            "Best mean reward: -103.45 - Last mean reward per episode: -107.43\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 116      |\n",
            "|    ep_rew_mean      | -107     |\n",
            "|    exploration_rate | 0.714    |\n",
            "| time/               |          |\n",
            "|    episodes         | 908      |\n",
            "|    fps              | 1945     |\n",
            "|    time_elapsed     | 46       |\n",
            "|    total_timesteps  | 90168    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.62     |\n",
            "|    n_updates        | 10041    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 116      |\n",
            "|    ep_rew_mean      | -109     |\n",
            "|    exploration_rate | 0.713    |\n",
            "| time/               |          |\n",
            "|    episodes         | 912      |\n",
            "|    fps              | 1940     |\n",
            "|    time_elapsed     | 46       |\n",
            "|    total_timesteps  | 90566    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.3      |\n",
            "|    n_updates        | 10141    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 115      |\n",
            "|    ep_rew_mean      | -103     |\n",
            "|    exploration_rate | 0.712    |\n",
            "| time/               |          |\n",
            "|    episodes         | 916      |\n",
            "|    fps              | 1934     |\n",
            "|    time_elapsed     | 47       |\n",
            "|    total_timesteps  | 90970    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.532    |\n",
            "|    n_updates        | 10242    |\n",
            "----------------------------------\n",
            "Num timesteps: 91000\n",
            "Best mean reward: -103.45 - Last mean reward per episode: -103.04\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 115      |\n",
            "|    ep_rew_mean      | -100     |\n",
            "|    exploration_rate | 0.711    |\n",
            "| time/               |          |\n",
            "|    episodes         | 920      |\n",
            "|    fps              | 1928     |\n",
            "|    time_elapsed     | 47       |\n",
            "|    total_timesteps  | 91392    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.2      |\n",
            "|    n_updates        | 10347    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 115      |\n",
            "|    ep_rew_mean      | -99.1    |\n",
            "|    exploration_rate | 0.709    |\n",
            "| time/               |          |\n",
            "|    episodes         | 924      |\n",
            "|    fps              | 1921     |\n",
            "|    time_elapsed     | 47       |\n",
            "|    total_timesteps  | 91863    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.777    |\n",
            "|    n_updates        | 10465    |\n",
            "----------------------------------\n",
            "Num timesteps: 92000\n",
            "Best mean reward: -103.04 - Last mean reward per episode: -98.57\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 115      |\n",
            "|    ep_rew_mean      | -100     |\n",
            "|    exploration_rate | 0.708    |\n",
            "| time/               |          |\n",
            "|    episodes         | 928      |\n",
            "|    fps              | 1915     |\n",
            "|    time_elapsed     | 48       |\n",
            "|    total_timesteps  | 92294    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.888    |\n",
            "|    n_updates        | 10573    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 115      |\n",
            "|    ep_rew_mean      | -100     |\n",
            "|    exploration_rate | 0.706    |\n",
            "| time/               |          |\n",
            "|    episodes         | 932      |\n",
            "|    fps              | 1909     |\n",
            "|    time_elapsed     | 48       |\n",
            "|    total_timesteps  | 92749    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.546    |\n",
            "|    n_updates        | 10687    |\n",
            "----------------------------------\n",
            "Num timesteps: 93000\n",
            "Best mean reward: -98.57 - Last mean reward per episode: -101.91\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 116      |\n",
            "|    ep_rew_mean      | -102     |\n",
            "|    exploration_rate | 0.705    |\n",
            "| time/               |          |\n",
            "|    episodes         | 936      |\n",
            "|    fps              | 1904     |\n",
            "|    time_elapsed     | 48       |\n",
            "|    total_timesteps  | 93226    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.707    |\n",
            "|    n_updates        | 10806    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 116      |\n",
            "|    ep_rew_mean      | -104     |\n",
            "|    exploration_rate | 0.703    |\n",
            "| time/               |          |\n",
            "|    episodes         | 940      |\n",
            "|    fps              | 1899     |\n",
            "|    time_elapsed     | 49       |\n",
            "|    total_timesteps  | 93668    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.338    |\n",
            "|    n_updates        | 10916    |\n",
            "----------------------------------\n",
            "Num timesteps: 94000\n",
            "Best mean reward: -98.57 - Last mean reward per episode: -104.82\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 115      |\n",
            "|    ep_rew_mean      | -104     |\n",
            "|    exploration_rate | 0.702    |\n",
            "| time/               |          |\n",
            "|    episodes         | 944      |\n",
            "|    fps              | 1894     |\n",
            "|    time_elapsed     | 49       |\n",
            "|    total_timesteps  | 94044    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.501    |\n",
            "|    n_updates        | 11010    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 116      |\n",
            "|    ep_rew_mean      | -103     |\n",
            "|    exploration_rate | 0.701    |\n",
            "| time/               |          |\n",
            "|    episodes         | 948      |\n",
            "|    fps              | 1889     |\n",
            "|    time_elapsed     | 50       |\n",
            "|    total_timesteps  | 94496    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.417    |\n",
            "|    n_updates        | 11123    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 116      |\n",
            "|    ep_rew_mean      | -102     |\n",
            "|    exploration_rate | 0.699    |\n",
            "| time/               |          |\n",
            "|    episodes         | 952      |\n",
            "|    fps              | 1883     |\n",
            "|    time_elapsed     | 50       |\n",
            "|    total_timesteps  | 94936    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.355    |\n",
            "|    n_updates        | 11233    |\n",
            "----------------------------------\n",
            "Num timesteps: 95000\n",
            "Best mean reward: -98.57 - Last mean reward per episode: -102.29\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 116      |\n",
            "|    ep_rew_mean      | -103     |\n",
            "|    exploration_rate | 0.698    |\n",
            "| time/               |          |\n",
            "|    episodes         | 956      |\n",
            "|    fps              | 1876     |\n",
            "|    time_elapsed     | 50       |\n",
            "|    total_timesteps  | 95475    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.442    |\n",
            "|    n_updates        | 11368    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 116      |\n",
            "|    ep_rew_mean      | -104     |\n",
            "|    exploration_rate | 0.696    |\n",
            "| time/               |          |\n",
            "|    episodes         | 960      |\n",
            "|    fps              | 1871     |\n",
            "|    time_elapsed     | 51       |\n",
            "|    total_timesteps  | 95961    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.503    |\n",
            "|    n_updates        | 11490    |\n",
            "----------------------------------\n",
            "Num timesteps: 96000\n",
            "Best mean reward: -98.57 - Last mean reward per episode: -103.91\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 114      |\n",
            "|    ep_rew_mean      | -101     |\n",
            "|    exploration_rate | 0.695    |\n",
            "| time/               |          |\n",
            "|    episodes         | 964      |\n",
            "|    fps              | 1866     |\n",
            "|    time_elapsed     | 51       |\n",
            "|    total_timesteps  | 96366    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.04     |\n",
            "|    n_updates        | 11591    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 114      |\n",
            "|    ep_rew_mean      | -98      |\n",
            "|    exploration_rate | 0.693    |\n",
            "| time/               |          |\n",
            "|    episodes         | 968      |\n",
            "|    fps              | 1861     |\n",
            "|    time_elapsed     | 52       |\n",
            "|    total_timesteps  | 96858    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.693    |\n",
            "|    n_updates        | 11714    |\n",
            "----------------------------------\n",
            "Num timesteps: 97000\n",
            "Best mean reward: -98.57 - Last mean reward per episode: -98.30\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 114      |\n",
            "|    ep_rew_mean      | -99.6    |\n",
            "|    exploration_rate | 0.692    |\n",
            "| time/               |          |\n",
            "|    episodes         | 972      |\n",
            "|    fps              | 1855     |\n",
            "|    time_elapsed     | 52       |\n",
            "|    total_timesteps  | 97303    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 6.62     |\n",
            "|    n_updates        | 11825    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 114      |\n",
            "|    ep_rew_mean      | -98.9    |\n",
            "|    exploration_rate | 0.69     |\n",
            "| time/               |          |\n",
            "|    episodes         | 976      |\n",
            "|    fps              | 1850     |\n",
            "|    time_elapsed     | 52       |\n",
            "|    total_timesteps  | 97781    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.349    |\n",
            "|    n_updates        | 11945    |\n",
            "----------------------------------\n",
            "Num timesteps: 98000\n",
            "Best mean reward: -98.30 - Last mean reward per episode: -98.77\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 114      |\n",
            "|    ep_rew_mean      | -98.4    |\n",
            "|    exploration_rate | 0.689    |\n",
            "| time/               |          |\n",
            "|    episodes         | 980      |\n",
            "|    fps              | 1845     |\n",
            "|    time_elapsed     | 53       |\n",
            "|    total_timesteps  | 98222    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.855    |\n",
            "|    n_updates        | 12055    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 115      |\n",
            "|    ep_rew_mean      | -97.2    |\n",
            "|    exploration_rate | 0.687    |\n",
            "| time/               |          |\n",
            "|    episodes         | 984      |\n",
            "|    fps              | 1839     |\n",
            "|    time_elapsed     | 53       |\n",
            "|    total_timesteps  | 98713    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.392    |\n",
            "|    n_updates        | 12178    |\n",
            "----------------------------------\n",
            "Num timesteps: 99000\n",
            "Best mean reward: -98.30 - Last mean reward per episode: -96.69\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 113      |\n",
            "|    ep_rew_mean      | -97.3    |\n",
            "|    exploration_rate | 0.686    |\n",
            "| time/               |          |\n",
            "|    episodes         | 988      |\n",
            "|    fps              | 1835     |\n",
            "|    time_elapsed     | 53       |\n",
            "|    total_timesteps  | 99088    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.09     |\n",
            "|    n_updates        | 12271    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 114      |\n",
            "|    ep_rew_mean      | -97.5    |\n",
            "|    exploration_rate | 0.684    |\n",
            "| time/               |          |\n",
            "|    episodes         | 992      |\n",
            "|    fps              | 1829     |\n",
            "|    time_elapsed     | 54       |\n",
            "|    total_timesteps  | 99646    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.297    |\n",
            "|    n_updates        | 12411    |\n",
            "----------------------------------\n",
            "Num timesteps: 100000\n",
            "Best mean reward: -96.69 - Last mean reward per episode: -97.16\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 115      |\n",
            "|    ep_rew_mean      | -99.1    |\n",
            "|    exploration_rate | 0.683    |\n",
            "| time/               |          |\n",
            "|    episodes         | 996      |\n",
            "|    fps              | 1823     |\n",
            "|    time_elapsed     | 54       |\n",
            "|    total_timesteps  | 100176   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 6.73     |\n",
            "|    n_updates        | 12543    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 115      |\n",
            "|    ep_rew_mean      | -99      |\n",
            "|    exploration_rate | 0.681    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1000     |\n",
            "|    fps              | 1819     |\n",
            "|    time_elapsed     | 55       |\n",
            "|    total_timesteps  | 100586   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.591    |\n",
            "|    n_updates        | 12646    |\n",
            "----------------------------------\n",
            "Num timesteps: 101000\n",
            "Best mean reward: -96.69 - Last mean reward per episode: -100.28\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 114      |\n",
            "|    ep_rew_mean      | -99.7    |\n",
            "|    exploration_rate | 0.68     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1004     |\n",
            "|    fps              | 1814     |\n",
            "|    time_elapsed     | 55       |\n",
            "|    total_timesteps  | 101079   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.916    |\n",
            "|    n_updates        | 12769    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 114      |\n",
            "|    ep_rew_mean      | -102     |\n",
            "|    exploration_rate | 0.678    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1008     |\n",
            "|    fps              | 1808     |\n",
            "|    time_elapsed     | 56       |\n",
            "|    total_timesteps  | 101611   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.53     |\n",
            "|    n_updates        | 12902    |\n",
            "----------------------------------\n",
            "Num timesteps: 102000\n",
            "Best mean reward: -96.69 - Last mean reward per episode: -101.89\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 115      |\n",
            "|    ep_rew_mean      | -101     |\n",
            "|    exploration_rate | 0.677    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1012     |\n",
            "|    fps              | 1803     |\n",
            "|    time_elapsed     | 56       |\n",
            "|    total_timesteps  | 102100   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.55     |\n",
            "|    n_updates        | 13024    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 119      |\n",
            "|    ep_rew_mean      | -102     |\n",
            "|    exploration_rate | 0.674    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1016     |\n",
            "|    fps              | 1795     |\n",
            "|    time_elapsed     | 57       |\n",
            "|    total_timesteps  | 102832   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.483    |\n",
            "|    n_updates        | 13207    |\n",
            "----------------------------------\n",
            "Num timesteps: 103000\n",
            "Best mean reward: -96.69 - Last mean reward per episode: -103.16\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 119      |\n",
            "|    ep_rew_mean      | -103     |\n",
            "|    exploration_rate | 0.673    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1020     |\n",
            "|    fps              | 1790     |\n",
            "|    time_elapsed     | 57       |\n",
            "|    total_timesteps  | 103330   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.354    |\n",
            "|    n_updates        | 13332    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 120      |\n",
            "|    ep_rew_mean      | -101     |\n",
            "|    exploration_rate | 0.671    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1024     |\n",
            "|    fps              | 1785     |\n",
            "|    time_elapsed     | 58       |\n",
            "|    total_timesteps  | 103889   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 7.13     |\n",
            "|    n_updates        | 13472    |\n",
            "----------------------------------\n",
            "Num timesteps: 104000\n",
            "Best mean reward: -96.69 - Last mean reward per episode: -101.07\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 121      |\n",
            "|    ep_rew_mean      | -100     |\n",
            "|    exploration_rate | 0.669    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1028     |\n",
            "|    fps              | 1780     |\n",
            "|    time_elapsed     | 58       |\n",
            "|    total_timesteps  | 104408   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.222    |\n",
            "|    n_updates        | 13601    |\n",
            "----------------------------------\n",
            "Num timesteps: 105000\n",
            "Best mean reward: -96.69 - Last mean reward per episode: -100.50\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 123      |\n",
            "|    ep_rew_mean      | -100     |\n",
            "|    exploration_rate | 0.667    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1032     |\n",
            "|    fps              | 1774     |\n",
            "|    time_elapsed     | 59       |\n",
            "|    total_timesteps  | 105020   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.459    |\n",
            "|    n_updates        | 13754    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 122      |\n",
            "|    ep_rew_mean      | -98      |\n",
            "|    exploration_rate | 0.666    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1036     |\n",
            "|    fps              | 1770     |\n",
            "|    time_elapsed     | 59       |\n",
            "|    total_timesteps  | 105472   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.64     |\n",
            "|    n_updates        | 13867    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 123      |\n",
            "|    ep_rew_mean      | -95      |\n",
            "|    exploration_rate | 0.664    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1040     |\n",
            "|    fps              | 1766     |\n",
            "|    time_elapsed     | 60       |\n",
            "|    total_timesteps  | 105979   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.37     |\n",
            "|    n_updates        | 13994    |\n",
            "----------------------------------\n",
            "Num timesteps: 106000\n",
            "Best mean reward: -96.69 - Last mean reward per episode: -95.03\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 125      |\n",
            "|    ep_rew_mean      | -93.8    |\n",
            "|    exploration_rate | 0.663    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1044     |\n",
            "|    fps              | 1760     |\n",
            "|    time_elapsed     | 60       |\n",
            "|    total_timesteps  | 106546   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.424    |\n",
            "|    n_updates        | 14136    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 125      |\n",
            "|    ep_rew_mean      | -95.3    |\n",
            "|    exploration_rate | 0.661    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1048     |\n",
            "|    fps              | 1757     |\n",
            "|    time_elapsed     | 60       |\n",
            "|    total_timesteps  | 106983   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.384    |\n",
            "|    n_updates        | 14245    |\n",
            "----------------------------------\n",
            "Num timesteps: 107000\n",
            "Best mean reward: -95.03 - Last mean reward per episode: -95.28\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 126      |\n",
            "|    ep_rew_mean      | -94.5    |\n",
            "|    exploration_rate | 0.66     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1052     |\n",
            "|    fps              | 1752     |\n",
            "|    time_elapsed     | 61       |\n",
            "|    total_timesteps  | 107499   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.568    |\n",
            "|    n_updates        | 14374    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 125      |\n",
            "|    ep_rew_mean      | -93.2    |\n",
            "|    exploration_rate | 0.658    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1056     |\n",
            "|    fps              | 1748     |\n",
            "|    time_elapsed     | 61       |\n",
            "|    total_timesteps  | 107958   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.708    |\n",
            "|    n_updates        | 14489    |\n",
            "----------------------------------\n",
            "Num timesteps: 108000\n",
            "Best mean reward: -95.03 - Last mean reward per episode: -93.15\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 126      |\n",
            "|    ep_rew_mean      | -90.9    |\n",
            "|    exploration_rate | 0.656    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1060     |\n",
            "|    fps              | 1742     |\n",
            "|    time_elapsed     | 62       |\n",
            "|    total_timesteps  | 108531   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1        |\n",
            "|    n_updates        | 14632    |\n",
            "----------------------------------\n",
            "Num timesteps: 109000\n",
            "Best mean reward: -93.15 - Last mean reward per episode: -90.63\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 127      |\n",
            "|    ep_rew_mean      | -90.5    |\n",
            "|    exploration_rate | 0.655    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1064     |\n",
            "|    fps              | 1737     |\n",
            "|    time_elapsed     | 62       |\n",
            "|    total_timesteps  | 109091   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.654    |\n",
            "|    n_updates        | 14772    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 127      |\n",
            "|    ep_rew_mean      | -93      |\n",
            "|    exploration_rate | 0.653    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1068     |\n",
            "|    fps              | 1733     |\n",
            "|    time_elapsed     | 63       |\n",
            "|    total_timesteps  | 109597   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.522    |\n",
            "|    n_updates        | 14899    |\n",
            "----------------------------------\n",
            "Num timesteps: 110000\n",
            "Best mean reward: -90.63 - Last mean reward per episode: -92.68\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 129      |\n",
            "|    ep_rew_mean      | -90.1    |\n",
            "|    exploration_rate | 0.651    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1072     |\n",
            "|    fps              | 1728     |\n",
            "|    time_elapsed     | 63       |\n",
            "|    total_timesteps  | 110212   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.463    |\n",
            "|    n_updates        | 15052    |\n",
            "----------------------------------\n",
            "Num timesteps: 111000\n",
            "Best mean reward: -90.63 - Last mean reward per episode: -91.94\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 138      |\n",
            "|    ep_rew_mean      | -89.5    |\n",
            "|    exploration_rate | 0.647    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1076     |\n",
            "|    fps              | 1700     |\n",
            "|    time_elapsed     | 65       |\n",
            "|    total_timesteps  | 111558   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.643    |\n",
            "|    n_updates        | 15389    |\n",
            "----------------------------------\n",
            "Num timesteps: 112000\n",
            "Best mean reward: -90.63 - Last mean reward per episode: -88.96\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 139      |\n",
            "|    ep_rew_mean      | -88.5    |\n",
            "|    exploration_rate | 0.645    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1080     |\n",
            "|    fps              | 1696     |\n",
            "|    time_elapsed     | 66       |\n",
            "|    total_timesteps  | 112080   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.361    |\n",
            "|    n_updates        | 15519    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 137      |\n",
            "|    ep_rew_mean      | -85.9    |\n",
            "|    exploration_rate | 0.644    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1084     |\n",
            "|    fps              | 1693     |\n",
            "|    time_elapsed     | 66       |\n",
            "|    total_timesteps  | 112431   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.497    |\n",
            "|    n_updates        | 15607    |\n",
            "----------------------------------\n",
            "Num timesteps: 113000\n",
            "Best mean reward: -88.96 - Last mean reward per episode: -85.94\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 139      |\n",
            "|    ep_rew_mean      | -85.5    |\n",
            "|    exploration_rate | 0.642    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1088     |\n",
            "|    fps              | 1689     |\n",
            "|    time_elapsed     | 66       |\n",
            "|    total_timesteps  | 113020   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.329    |\n",
            "|    n_updates        | 15754    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 140      |\n",
            "|    ep_rew_mean      | -86.2    |\n",
            "|    exploration_rate | 0.64     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1092     |\n",
            "|    fps              | 1683     |\n",
            "|    time_elapsed     | 67       |\n",
            "|    total_timesteps  | 113668   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.331    |\n",
            "|    n_updates        | 15916    |\n",
            "----------------------------------\n",
            "Num timesteps: 114000\n",
            "Best mean reward: -85.94 - Last mean reward per episode: -85.82\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 140      |\n",
            "|    ep_rew_mean      | -83      |\n",
            "|    exploration_rate | 0.638    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1096     |\n",
            "|    fps              | 1679     |\n",
            "|    time_elapsed     | 67       |\n",
            "|    total_timesteps  | 114181   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.51     |\n",
            "|    n_updates        | 16045    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 141      |\n",
            "|    ep_rew_mean      | -84.5    |\n",
            "|    exploration_rate | 0.637    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1100     |\n",
            "|    fps              | 1676     |\n",
            "|    time_elapsed     | 68       |\n",
            "|    total_timesteps  | 114725   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.943    |\n",
            "|    n_updates        | 16181    |\n",
            "----------------------------------\n",
            "Num timesteps: 115000\n",
            "Best mean reward: -85.82 - Last mean reward per episode: -84.50\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 142      |\n",
            "|    ep_rew_mean      | -82.9    |\n",
            "|    exploration_rate | 0.635    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1104     |\n",
            "|    fps              | 1672     |\n",
            "|    time_elapsed     | 68       |\n",
            "|    total_timesteps  | 115243   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.55     |\n",
            "|    n_updates        | 16310    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 142      |\n",
            "|    ep_rew_mean      | -81.4    |\n",
            "|    exploration_rate | 0.633    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1108     |\n",
            "|    fps              | 1668     |\n",
            "|    time_elapsed     | 69       |\n",
            "|    total_timesteps  | 115771   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.13     |\n",
            "|    n_updates        | 16442    |\n",
            "----------------------------------\n",
            "Num timesteps: 116000\n",
            "Best mean reward: -84.50 - Last mean reward per episode: -81.81\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 143      |\n",
            "|    ep_rew_mean      | -81.4    |\n",
            "|    exploration_rate | 0.632    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1112     |\n",
            "|    fps              | 1664     |\n",
            "|    time_elapsed     | 69       |\n",
            "|    total_timesteps  | 116351   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.432    |\n",
            "|    n_updates        | 16587    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 141      |\n",
            "|    ep_rew_mean      | -83.2    |\n",
            "|    exploration_rate | 0.63     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1116     |\n",
            "|    fps              | 1660     |\n",
            "|    time_elapsed     | 70       |\n",
            "|    total_timesteps  | 116926   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.98     |\n",
            "|    n_updates        | 16731    |\n",
            "----------------------------------\n",
            "Num timesteps: 117000\n",
            "Best mean reward: -81.81 - Last mean reward per episode: -83.23\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 141      |\n",
            "|    ep_rew_mean      | -82.8    |\n",
            "|    exploration_rate | 0.628    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1120     |\n",
            "|    fps              | 1657     |\n",
            "|    time_elapsed     | 70       |\n",
            "|    total_timesteps  | 117435   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.93     |\n",
            "|    n_updates        | 16858    |\n",
            "----------------------------------\n",
            "Num timesteps: 118000\n",
            "Best mean reward: -81.81 - Last mean reward per episode: -83.18\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 142      |\n",
            "|    ep_rew_mean      | -83.1    |\n",
            "|    exploration_rate | 0.626    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1124     |\n",
            "|    fps              | 1652     |\n",
            "|    time_elapsed     | 71       |\n",
            "|    total_timesteps  | 118077   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.54     |\n",
            "|    n_updates        | 17019    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 142      |\n",
            "|    ep_rew_mean      | -84.8    |\n",
            "|    exploration_rate | 0.624    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1128     |\n",
            "|    fps              | 1649     |\n",
            "|    time_elapsed     | 71       |\n",
            "|    total_timesteps  | 118646   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.332    |\n",
            "|    n_updates        | 17161    |\n",
            "----------------------------------\n",
            "Num timesteps: 119000\n",
            "Best mean reward: -81.81 - Last mean reward per episode: -84.17\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 143      |\n",
            "|    ep_rew_mean      | -82.4    |\n",
            "|    exploration_rate | 0.622    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1132     |\n",
            "|    fps              | 1644     |\n",
            "|    time_elapsed     | 72       |\n",
            "|    total_timesteps  | 119302   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.1      |\n",
            "|    n_updates        | 17325    |\n",
            "----------------------------------\n",
            "Num timesteps: 120000\n",
            "Best mean reward: -81.81 - Last mean reward per episode: -80.70\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 146      |\n",
            "|    ep_rew_mean      | -80.2    |\n",
            "|    exploration_rate | 0.62     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1136     |\n",
            "|    fps              | 1639     |\n",
            "|    time_elapsed     | 73       |\n",
            "|    total_timesteps  | 120024   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.95     |\n",
            "|    n_updates        | 17505    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 147      |\n",
            "|    ep_rew_mean      | -82      |\n",
            "|    exploration_rate | 0.618    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1140     |\n",
            "|    fps              | 1635     |\n",
            "|    time_elapsed     | 73       |\n",
            "|    total_timesteps  | 120693   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.579    |\n",
            "|    n_updates        | 17673    |\n",
            "----------------------------------\n",
            "Num timesteps: 121000\n",
            "Best mean reward: -80.70 - Last mean reward per episode: -81.50\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 147      |\n",
            "|    ep_rew_mean      | -80.9    |\n",
            "|    exploration_rate | 0.616    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1144     |\n",
            "|    fps              | 1631     |\n",
            "|    time_elapsed     | 74       |\n",
            "|    total_timesteps  | 121239   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.64     |\n",
            "|    n_updates        | 17809    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 147      |\n",
            "|    ep_rew_mean      | -80.8    |\n",
            "|    exploration_rate | 0.615    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1148     |\n",
            "|    fps              | 1628     |\n",
            "|    time_elapsed     | 74       |\n",
            "|    total_timesteps  | 121730   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.773    |\n",
            "|    n_updates        | 17932    |\n",
            "----------------------------------\n",
            "Num timesteps: 122000\n",
            "Best mean reward: -80.70 - Last mean reward per episode: -79.28\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 149      |\n",
            "|    ep_rew_mean      | -81      |\n",
            "|    exploration_rate | 0.612    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1152     |\n",
            "|    fps              | 1624     |\n",
            "|    time_elapsed     | 75       |\n",
            "|    total_timesteps  | 122383   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.519    |\n",
            "|    n_updates        | 18095    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 150      |\n",
            "|    ep_rew_mean      | -81      |\n",
            "|    exploration_rate | 0.611    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1156     |\n",
            "|    fps              | 1621     |\n",
            "|    time_elapsed     | 75       |\n",
            "|    total_timesteps  | 122920   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.322    |\n",
            "|    n_updates        | 18229    |\n",
            "----------------------------------\n",
            "Num timesteps: 123000\n",
            "Best mean reward: -79.28 - Last mean reward per episode: -81.00\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 150      |\n",
            "|    ep_rew_mean      | -81.4    |\n",
            "|    exploration_rate | 0.609    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1160     |\n",
            "|    fps              | 1617     |\n",
            "|    time_elapsed     | 76       |\n",
            "|    total_timesteps  | 123507   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.07     |\n",
            "|    n_updates        | 18376    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 149      |\n",
            "|    ep_rew_mean      | -81.3    |\n",
            "|    exploration_rate | 0.608    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1164     |\n",
            "|    fps              | 1615     |\n",
            "|    time_elapsed     | 76       |\n",
            "|    total_timesteps  | 123945   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.415    |\n",
            "|    n_updates        | 18486    |\n",
            "----------------------------------\n",
            "Num timesteps: 124000\n",
            "Best mean reward: -79.28 - Last mean reward per episode: -81.28\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 150      |\n",
            "|    ep_rew_mean      | -79.4    |\n",
            "|    exploration_rate | 0.606    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1168     |\n",
            "|    fps              | 1611     |\n",
            "|    time_elapsed     | 77       |\n",
            "|    total_timesteps  | 124550   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.509    |\n",
            "|    n_updates        | 18637    |\n",
            "----------------------------------\n",
            "Num timesteps: 125000\n",
            "Best mean reward: -79.28 - Last mean reward per episode: -78.32\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 149      |\n",
            "|    ep_rew_mean      | -77.7    |\n",
            "|    exploration_rate | 0.604    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1172     |\n",
            "|    fps              | 1608     |\n",
            "|    time_elapsed     | 77       |\n",
            "|    total_timesteps  | 125090   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.478    |\n",
            "|    n_updates        | 18772    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 142      |\n",
            "|    ep_rew_mean      | -76.8    |\n",
            "|    exploration_rate | 0.602    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1176     |\n",
            "|    fps              | 1604     |\n",
            "|    time_elapsed     | 78       |\n",
            "|    total_timesteps  | 125768   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.274    |\n",
            "|    n_updates        | 18941    |\n",
            "----------------------------------\n",
            "Num timesteps: 126000\n",
            "Best mean reward: -78.32 - Last mean reward per episode: -76.30\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 143      |\n",
            "|    ep_rew_mean      | -75.1    |\n",
            "|    exploration_rate | 0.6      |\n",
            "| time/               |          |\n",
            "|    episodes         | 1180     |\n",
            "|    fps              | 1600     |\n",
            "|    time_elapsed     | 78       |\n",
            "|    total_timesteps  | 126392   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.403    |\n",
            "|    n_updates        | 19097    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 144      |\n",
            "|    ep_rew_mean      | -75.9    |\n",
            "|    exploration_rate | 0.598    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1184     |\n",
            "|    fps              | 1598     |\n",
            "|    time_elapsed     | 79       |\n",
            "|    total_timesteps  | 126803   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.796    |\n",
            "|    n_updates        | 19200    |\n",
            "----------------------------------\n",
            "Num timesteps: 127000\n",
            "Best mean reward: -76.30 - Last mean reward per episode: -74.84\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 142      |\n",
            "|    ep_rew_mean      | -75.2    |\n",
            "|    exploration_rate | 0.597    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1188     |\n",
            "|    fps              | 1595     |\n",
            "|    time_elapsed     | 79       |\n",
            "|    total_timesteps  | 127228   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.267    |\n",
            "|    n_updates        | 19306    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 140      |\n",
            "|    ep_rew_mean      | -73.8    |\n",
            "|    exploration_rate | 0.596    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1192     |\n",
            "|    fps              | 1593     |\n",
            "|    time_elapsed     | 80       |\n",
            "|    total_timesteps  | 127687   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.27     |\n",
            "|    n_updates        | 19421    |\n",
            "----------------------------------\n",
            "Num timesteps: 128000\n",
            "Best mean reward: -74.84 - Last mean reward per episode: -73.12\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 142      |\n",
            "|    ep_rew_mean      | -72.8    |\n",
            "|    exploration_rate | 0.594    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1196     |\n",
            "|    fps              | 1589     |\n",
            "|    time_elapsed     | 80       |\n",
            "|    total_timesteps  | 128362   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.23     |\n",
            "|    n_updates        | 19590    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 143      |\n",
            "|    ep_rew_mean      | -69.6    |\n",
            "|    exploration_rate | 0.592    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1200     |\n",
            "|    fps              | 1585     |\n",
            "|    time_elapsed     | 81       |\n",
            "|    total_timesteps  | 128996   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.677    |\n",
            "|    n_updates        | 19748    |\n",
            "----------------------------------\n",
            "Num timesteps: 129000\n",
            "Best mean reward: -73.12 - Last mean reward per episode: -69.62\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 144      |\n",
            "|    ep_rew_mean      | -70.6    |\n",
            "|    exploration_rate | 0.589    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1204     |\n",
            "|    fps              | 1581     |\n",
            "|    time_elapsed     | 81       |\n",
            "|    total_timesteps  | 129638   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.92     |\n",
            "|    n_updates        | 19909    |\n",
            "----------------------------------\n",
            "Num timesteps: 130000\n",
            "Best mean reward: -69.62 - Last mean reward per episode: -70.43\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 144      |\n",
            "|    ep_rew_mean      | -69.8    |\n",
            "|    exploration_rate | 0.588    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1208     |\n",
            "|    fps              | 1579     |\n",
            "|    time_elapsed     | 82       |\n",
            "|    total_timesteps  | 130132   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.09     |\n",
            "|    n_updates        | 20032    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 146      |\n",
            "|    ep_rew_mean      | -69.3    |\n",
            "|    exploration_rate | 0.585    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1212     |\n",
            "|    fps              | 1574     |\n",
            "|    time_elapsed     | 83       |\n",
            "|    total_timesteps  | 130923   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.7      |\n",
            "|    n_updates        | 20230    |\n",
            "----------------------------------\n",
            "Num timesteps: 131000\n",
            "Best mean reward: -69.62 - Last mean reward per episode: -69.31\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 145      |\n",
            "|    ep_rew_mean      | -67.7    |\n",
            "|    exploration_rate | 0.584    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1216     |\n",
            "|    fps              | 1571     |\n",
            "|    time_elapsed     | 83       |\n",
            "|    total_timesteps  | 131456   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.455    |\n",
            "|    n_updates        | 20363    |\n",
            "----------------------------------\n",
            "Num timesteps: 132000\n",
            "Best mean reward: -69.31 - Last mean reward per episode: -67.07\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 146      |\n",
            "|    ep_rew_mean      | -67.9    |\n",
            "|    exploration_rate | 0.582    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1220     |\n",
            "|    fps              | 1568     |\n",
            "|    time_elapsed     | 84       |\n",
            "|    total_timesteps  | 132010   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.326    |\n",
            "|    n_updates        | 20502    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 147      |\n",
            "|    ep_rew_mean      | -67.4    |\n",
            "|    exploration_rate | 0.58     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1224     |\n",
            "|    fps              | 1564     |\n",
            "|    time_elapsed     | 84       |\n",
            "|    total_timesteps  | 132743   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.739    |\n",
            "|    n_updates        | 20685    |\n",
            "----------------------------------\n",
            "Num timesteps: 133000\n",
            "Best mean reward: -67.07 - Last mean reward per episode: -67.44\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 148      |\n",
            "|    ep_rew_mean      | -65.6    |\n",
            "|    exploration_rate | 0.577    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1228     |\n",
            "|    fps              | 1560     |\n",
            "|    time_elapsed     | 85       |\n",
            "|    total_timesteps  | 133428   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.563    |\n",
            "|    n_updates        | 20856    |\n",
            "----------------------------------\n",
            "Num timesteps: 134000\n",
            "Best mean reward: -67.07 - Last mean reward per episode: -65.43\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 147      |\n",
            "|    ep_rew_mean      | -66.5    |\n",
            "|    exploration_rate | 0.576    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1232     |\n",
            "|    fps              | 1557     |\n",
            "|    time_elapsed     | 86       |\n",
            "|    total_timesteps  | 134014   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.41     |\n",
            "|    n_updates        | 21003    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 145      |\n",
            "|    ep_rew_mean      | -66.9    |\n",
            "|    exploration_rate | 0.574    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1236     |\n",
            "|    fps              | 1554     |\n",
            "|    time_elapsed     | 86       |\n",
            "|    total_timesteps  | 134549   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.25     |\n",
            "|    n_updates        | 21137    |\n",
            "----------------------------------\n",
            "Num timesteps: 135000\n",
            "Best mean reward: -65.43 - Last mean reward per episode: -67.15\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 144      |\n",
            "|    ep_rew_mean      | -66.7    |\n",
            "|    exploration_rate | 0.572    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1240     |\n",
            "|    fps              | 1551     |\n",
            "|    time_elapsed     | 87       |\n",
            "|    total_timesteps  | 135135   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.2      |\n",
            "|    n_updates        | 21283    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 146      |\n",
            "|    ep_rew_mean      | -66      |\n",
            "|    exploration_rate | 0.57     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1244     |\n",
            "|    fps              | 1547     |\n",
            "|    time_elapsed     | 87       |\n",
            "|    total_timesteps  | 135853   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.17     |\n",
            "|    n_updates        | 21463    |\n",
            "----------------------------------\n",
            "Num timesteps: 136000\n",
            "Best mean reward: -65.43 - Last mean reward per episode: -66.04\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 148      |\n",
            "|    ep_rew_mean      | -63.5    |\n",
            "|    exploration_rate | 0.568    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1248     |\n",
            "|    fps              | 1543     |\n",
            "|    time_elapsed     | 88       |\n",
            "|    total_timesteps  | 136514   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.67     |\n",
            "|    n_updates        | 21628    |\n",
            "----------------------------------\n",
            "Num timesteps: 137000\n",
            "Best mean reward: -65.43 - Last mean reward per episode: -62.94\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 149      |\n",
            "|    ep_rew_mean      | -61.7    |\n",
            "|    exploration_rate | 0.565    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1252     |\n",
            "|    fps              | 1539     |\n",
            "|    time_elapsed     | 89       |\n",
            "|    total_timesteps  | 137256   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.793    |\n",
            "|    n_updates        | 21813    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 150      |\n",
            "|    ep_rew_mean      | -65.9    |\n",
            "|    exploration_rate | 0.563    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1256     |\n",
            "|    fps              | 1536     |\n",
            "|    time_elapsed     | 89       |\n",
            "|    total_timesteps  | 137944   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.58     |\n",
            "|    n_updates        | 21985    |\n",
            "----------------------------------\n",
            "Num timesteps: 138000\n",
            "Best mean reward: -62.94 - Last mean reward per episode: -65.87\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 152      |\n",
            "|    ep_rew_mean      | -64.6    |\n",
            "|    exploration_rate | 0.561    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1260     |\n",
            "|    fps              | 1532     |\n",
            "|    time_elapsed     | 90       |\n",
            "|    total_timesteps  | 138703   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.315    |\n",
            "|    n_updates        | 22175    |\n",
            "----------------------------------\n",
            "Num timesteps: 139000\n",
            "Best mean reward: -62.94 - Last mean reward per episode: -63.69\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 155      |\n",
            "|    ep_rew_mean      | -63      |\n",
            "|    exploration_rate | 0.558    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1264     |\n",
            "|    fps              | 1528     |\n",
            "|    time_elapsed     | 91       |\n",
            "|    total_timesteps  | 139483   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.317    |\n",
            "|    n_updates        | 22370    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 154      |\n",
            "|    ep_rew_mean      | -62.6    |\n",
            "|    exploration_rate | 0.557    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1268     |\n",
            "|    fps              | 1526     |\n",
            "|    time_elapsed     | 91       |\n",
            "|    total_timesteps  | 139999   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.301    |\n",
            "|    n_updates        | 22499    |\n",
            "----------------------------------\n",
            "Num timesteps: 140000\n",
            "Best mean reward: -62.94 - Last mean reward per episode: -62.64\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 156      |\n",
            "|    ep_rew_mean      | -63.2    |\n",
            "|    exploration_rate | 0.555    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1272     |\n",
            "|    fps              | 1523     |\n",
            "|    time_elapsed     | 92       |\n",
            "|    total_timesteps  | 140651   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.76     |\n",
            "|    n_updates        | 22662    |\n",
            "----------------------------------\n",
            "Num timesteps: 141000\n",
            "Best mean reward: -62.64 - Last mean reward per episode: -62.64\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 154      |\n",
            "|    ep_rew_mean      | -62.9    |\n",
            "|    exploration_rate | 0.553    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1276     |\n",
            "|    fps              | 1520     |\n",
            "|    time_elapsed     | 92       |\n",
            "|    total_timesteps  | 141176   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.22     |\n",
            "|    n_updates        | 22793    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 154      |\n",
            "|    ep_rew_mean      | -63      |\n",
            "|    exploration_rate | 0.551    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1280     |\n",
            "|    fps              | 1517     |\n",
            "|    time_elapsed     | 93       |\n",
            "|    total_timesteps  | 141838   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.339    |\n",
            "|    n_updates        | 22959    |\n",
            "----------------------------------\n",
            "Num timesteps: 142000\n",
            "Best mean reward: -62.64 - Last mean reward per episode: -63.00\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 160      |\n",
            "|    ep_rew_mean      | -60.6    |\n",
            "|    exploration_rate | 0.548    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1284     |\n",
            "|    fps              | 1512     |\n",
            "|    time_elapsed     | 94       |\n",
            "|    total_timesteps  | 142789   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.211    |\n",
            "|    n_updates        | 23197    |\n",
            "----------------------------------\n",
            "Num timesteps: 143000\n",
            "Best mean reward: -62.64 - Last mean reward per episode: -60.68\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 162      |\n",
            "|    ep_rew_mean      | -61.1    |\n",
            "|    exploration_rate | 0.546    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1288     |\n",
            "|    fps              | 1508     |\n",
            "|    time_elapsed     | 95       |\n",
            "|    total_timesteps  | 143477   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.504    |\n",
            "|    n_updates        | 23369    |\n",
            "----------------------------------\n",
            "Num timesteps: 144000\n",
            "Best mean reward: -60.68 - Last mean reward per episode: -59.65\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 164      |\n",
            "|    ep_rew_mean      | -60      |\n",
            "|    exploration_rate | 0.544    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1292     |\n",
            "|    fps              | 1505     |\n",
            "|    time_elapsed     | 95       |\n",
            "|    total_timesteps  | 144089   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.288    |\n",
            "|    n_updates        | 23522    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 165      |\n",
            "|    ep_rew_mean      | -60.6    |\n",
            "|    exploration_rate | 0.541    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1296     |\n",
            "|    fps              | 1502     |\n",
            "|    time_elapsed     | 96       |\n",
            "|    total_timesteps  | 144836   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.373    |\n",
            "|    n_updates        | 23708    |\n",
            "----------------------------------\n",
            "Num timesteps: 145000\n",
            "Best mean reward: -59.65 - Last mean reward per episode: -59.79\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 165      |\n",
            "|    ep_rew_mean      | -61.2    |\n",
            "|    exploration_rate | 0.539    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1300     |\n",
            "|    fps              | 1499     |\n",
            "|    time_elapsed     | 97       |\n",
            "|    total_timesteps  | 145490   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.189    |\n",
            "|    n_updates        | 23872    |\n",
            "----------------------------------\n",
            "Num timesteps: 146000\n",
            "Best mean reward: -59.65 - Last mean reward per episode: -61.20\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 164      |\n",
            "|    ep_rew_mean      | -60.2    |\n",
            "|    exploration_rate | 0.537    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1304     |\n",
            "|    fps              | 1497     |\n",
            "|    time_elapsed     | 97       |\n",
            "|    total_timesteps  | 146053   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.505    |\n",
            "|    n_updates        | 24013    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 167      |\n",
            "|    ep_rew_mean      | -60.7    |\n",
            "|    exploration_rate | 0.535    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1308     |\n",
            "|    fps              | 1493     |\n",
            "|    time_elapsed     | 98       |\n",
            "|    total_timesteps  | 146867   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.563    |\n",
            "|    n_updates        | 24216    |\n",
            "----------------------------------\n",
            "Num timesteps: 147000\n",
            "Best mean reward: -59.65 - Last mean reward per episode: -59.14\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 164      |\n",
            "|    ep_rew_mean      | -59.6    |\n",
            "|    exploration_rate | 0.533    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1312     |\n",
            "|    fps              | 1491     |\n",
            "|    time_elapsed     | 98       |\n",
            "|    total_timesteps  | 147361   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.308    |\n",
            "|    n_updates        | 24340    |\n",
            "----------------------------------\n",
            "Num timesteps: 148000\n",
            "Best mean reward: -59.14 - Last mean reward per episode: -57.49\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 167      |\n",
            "|    ep_rew_mean      | -57.5    |\n",
            "|    exploration_rate | 0.531    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1316     |\n",
            "|    fps              | 1487     |\n",
            "|    time_elapsed     | 99       |\n",
            "|    total_timesteps  | 148199   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.508    |\n",
            "|    n_updates        | 24549    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 168      |\n",
            "|    ep_rew_mean      | -57.3    |\n",
            "|    exploration_rate | 0.529    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1320     |\n",
            "|    fps              | 1484     |\n",
            "|    time_elapsed     | 100      |\n",
            "|    total_timesteps  | 148770   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.332    |\n",
            "|    n_updates        | 24692    |\n",
            "----------------------------------\n",
            "Num timesteps: 149000\n",
            "Best mean reward: -57.49 - Last mean reward per episode: -56.87\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 166      |\n",
            "|    ep_rew_mean      | -56.3    |\n",
            "|    exploration_rate | 0.527    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1324     |\n",
            "|    fps              | 1482     |\n",
            "|    time_elapsed     | 100      |\n",
            "|    total_timesteps  | 149310   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.61     |\n",
            "|    n_updates        | 24827    |\n",
            "----------------------------------\n",
            "Num timesteps: 150000\n",
            "Best mean reward: -56.87 - Last mean reward per episode: -56.35\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 175      |\n",
            "|    ep_rew_mean      | -53.8    |\n",
            "|    exploration_rate | 0.522    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1328     |\n",
            "|    fps              | 1465     |\n",
            "|    time_elapsed     | 103      |\n",
            "|    total_timesteps  | 150945   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.492    |\n",
            "|    n_updates        | 25236    |\n",
            "----------------------------------\n",
            "Num timesteps: 151000\n",
            "Best mean reward: -56.35 - Last mean reward per episode: -53.75\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 177      |\n",
            "|    ep_rew_mean      | -51.8    |\n",
            "|    exploration_rate | 0.52     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1332     |\n",
            "|    fps              | 1460     |\n",
            "|    time_elapsed     | 103      |\n",
            "|    total_timesteps  | 151689   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.334    |\n",
            "|    n_updates        | 25422    |\n",
            "----------------------------------\n",
            "Num timesteps: 152000\n",
            "Best mean reward: -53.75 - Last mean reward per episode: -51.84\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 179      |\n",
            "|    ep_rew_mean      | -50.7    |\n",
            "|    exploration_rate | 0.517    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1336     |\n",
            "|    fps              | 1457     |\n",
            "|    time_elapsed     | 104      |\n",
            "|    total_timesteps  | 152424   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.728    |\n",
            "|    n_updates        | 25605    |\n",
            "----------------------------------\n",
            "Num timesteps: 153000\n",
            "Best mean reward: -51.84 - Last mean reward per episode: -48.63\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 179      |\n",
            "|    ep_rew_mean      | -48      |\n",
            "|    exploration_rate | 0.515    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1340     |\n",
            "|    fps              | 1455     |\n",
            "|    time_elapsed     | 105      |\n",
            "|    total_timesteps  | 153073   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.6      |\n",
            "|    n_updates        | 25768    |\n",
            "----------------------------------\n",
            "Num timesteps: 154000\n",
            "Best mean reward: -48.63 - Last mean reward per episode: -48.16\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 182      |\n",
            "|    ep_rew_mean      | -47.9    |\n",
            "|    exploration_rate | 0.512    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1344     |\n",
            "|    fps              | 1451     |\n",
            "|    time_elapsed     | 106      |\n",
            "|    total_timesteps  | 154014   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.522    |\n",
            "|    n_updates        | 26003    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 184      |\n",
            "|    ep_rew_mean      | -49.7    |\n",
            "|    exploration_rate | 0.509    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1348     |\n",
            "|    fps              | 1446     |\n",
            "|    time_elapsed     | 107      |\n",
            "|    total_timesteps  | 154930   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4        |\n",
            "|    n_updates        | 26232    |\n",
            "----------------------------------\n",
            "Num timesteps: 155000\n",
            "Best mean reward: -48.16 - Last mean reward per episode: -49.66\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 184      |\n",
            "|    ep_rew_mean      | -48.6    |\n",
            "|    exploration_rate | 0.507    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1352     |\n",
            "|    fps              | 1444     |\n",
            "|    time_elapsed     | 107      |\n",
            "|    total_timesteps  | 155622   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.86     |\n",
            "|    n_updates        | 26405    |\n",
            "----------------------------------\n",
            "Num timesteps: 156000\n",
            "Best mean reward: -48.16 - Last mean reward per episode: -44.05\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 157000\n",
            "Best mean reward: -44.05 - Last mean reward per episode: -43.46\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 193      |\n",
            "|    ep_rew_mean      | -41.8    |\n",
            "|    exploration_rate | 0.502    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1356     |\n",
            "|    fps              | 1431     |\n",
            "|    time_elapsed     | 109      |\n",
            "|    total_timesteps  | 157258   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.31     |\n",
            "|    n_updates        | 26814    |\n",
            "----------------------------------\n",
            "Num timesteps: 158000\n",
            "Best mean reward: -43.46 - Last mean reward per episode: -41.98\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 159000\n",
            "Best mean reward: -41.98 - Last mean reward per episode: -42.83\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 203      |\n",
            "|    ep_rew_mean      | -42.5    |\n",
            "|    exploration_rate | 0.496    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1360     |\n",
            "|    fps              | 1412     |\n",
            "|    time_elapsed     | 112      |\n",
            "|    total_timesteps  | 159051   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.64     |\n",
            "|    n_updates        | 27262    |\n",
            "----------------------------------\n",
            "Num timesteps: 160000\n",
            "Best mean reward: -41.98 - Last mean reward per episode: -43.08\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 215      |\n",
            "|    ep_rew_mean      | -44.2    |\n",
            "|    exploration_rate | 0.49     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1364     |\n",
            "|    fps              | 1396     |\n",
            "|    time_elapsed     | 115      |\n",
            "|    total_timesteps  | 160947   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.575    |\n",
            "|    n_updates        | 27736    |\n",
            "----------------------------------\n",
            "Num timesteps: 161000\n",
            "Best mean reward: -41.98 - Last mean reward per episode: -44.21\n",
            "Num timesteps: 162000\n",
            "Best mean reward: -41.98 - Last mean reward per episode: -45.22\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 223      |\n",
            "|    ep_rew_mean      | -42.9    |\n",
            "|    exploration_rate | 0.486    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1368     |\n",
            "|    fps              | 1380     |\n",
            "|    time_elapsed     | 117      |\n",
            "|    total_timesteps  | 162299   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.633    |\n",
            "|    n_updates        | 28074    |\n",
            "----------------------------------\n",
            "Num timesteps: 163000\n",
            "Best mean reward: -41.98 - Last mean reward per episode: -41.62\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 226      |\n",
            "|    ep_rew_mean      | -41.7    |\n",
            "|    exploration_rate | 0.483    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1372     |\n",
            "|    fps              | 1376     |\n",
            "|    time_elapsed     | 118      |\n",
            "|    total_timesteps  | 163213   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.509    |\n",
            "|    n_updates        | 28303    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 226      |\n",
            "|    ep_rew_mean      | -43.4    |\n",
            "|    exploration_rate | 0.481    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1376     |\n",
            "|    fps              | 1375     |\n",
            "|    time_elapsed     | 119      |\n",
            "|    total_timesteps  | 163807   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.442    |\n",
            "|    n_updates        | 28451    |\n",
            "----------------------------------\n",
            "Num timesteps: 164000\n",
            "Best mean reward: -41.62 - Last mean reward per episode: -43.35\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 228      |\n",
            "|    ep_rew_mean      | -44.3    |\n",
            "|    exploration_rate | 0.479    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1380     |\n",
            "|    fps              | 1372     |\n",
            "|    time_elapsed     | 119      |\n",
            "|    total_timesteps  | 164641   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.41     |\n",
            "|    n_updates        | 28660    |\n",
            "----------------------------------\n",
            "Num timesteps: 165000\n",
            "Best mean reward: -41.62 - Last mean reward per episode: -44.33\n",
            "Num timesteps: 166000\n",
            "Best mean reward: -41.62 - Last mean reward per episode: -45.73\n",
            "Num timesteps: 167000\n",
            "Best mean reward: -41.62 - Last mean reward per episode: -45.67\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 249      |\n",
            "|    ep_rew_mean      | -44.5    |\n",
            "|    exploration_rate | 0.469    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1384     |\n",
            "|    fps              | 1340     |\n",
            "|    time_elapsed     | 125      |\n",
            "|    total_timesteps  | 167703   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.267    |\n",
            "|    n_updates        | 29425    |\n",
            "----------------------------------\n",
            "Num timesteps: 168000\n",
            "Best mean reward: -41.62 - Last mean reward per episode: -44.29\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 251      |\n",
            "|    ep_rew_mean      | -42.3    |\n",
            "|    exploration_rate | 0.466    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1388     |\n",
            "|    fps              | 1337     |\n",
            "|    time_elapsed     | 126      |\n",
            "|    total_timesteps  | 168595   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.78     |\n",
            "|    n_updates        | 29648    |\n",
            "----------------------------------\n",
            "Num timesteps: 169000\n",
            "Best mean reward: -41.62 - Last mean reward per episode: -42.28\n",
            "Num timesteps: 170000\n",
            "Best mean reward: -41.62 - Last mean reward per episode: -43.45\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 265      |\n",
            "|    ep_rew_mean      | -44.5    |\n",
            "|    exploration_rate | 0.46     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1392     |\n",
            "|    fps              | 1324     |\n",
            "|    time_elapsed     | 128      |\n",
            "|    total_timesteps  | 170554   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.477    |\n",
            "|    n_updates        | 30138    |\n",
            "----------------------------------\n",
            "Num timesteps: 171000\n",
            "Best mean reward: -41.62 - Last mean reward per episode: -44.04\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 272      |\n",
            "|    ep_rew_mean      | -42.2    |\n",
            "|    exploration_rate | 0.455    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1396     |\n",
            "|    fps              | 1315     |\n",
            "|    time_elapsed     | 130      |\n",
            "|    total_timesteps  | 171992   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.28     |\n",
            "|    n_updates        | 30497    |\n",
            "----------------------------------\n",
            "Num timesteps: 172000\n",
            "Best mean reward: -41.62 - Last mean reward per episode: -42.22\n",
            "Num timesteps: 173000\n",
            "Best mean reward: -41.62 - Last mean reward per episode: -41.80\n",
            "Num timesteps: 174000\n",
            "Best mean reward: -41.62 - Last mean reward per episode: -40.16\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 285      |\n",
            "|    ep_rew_mean      | -39.4    |\n",
            "|    exploration_rate | 0.449    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1400     |\n",
            "|    fps              | 1299     |\n",
            "|    time_elapsed     | 133      |\n",
            "|    total_timesteps  | 174039   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.388    |\n",
            "|    n_updates        | 31009    |\n",
            "----------------------------------\n",
            "Num timesteps: 175000\n",
            "Best mean reward: -40.16 - Last mean reward per episode: -36.93\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 292      |\n",
            "|    ep_rew_mean      | -37.6    |\n",
            "|    exploration_rate | 0.445    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1404     |\n",
            "|    fps              | 1295     |\n",
            "|    time_elapsed     | 135      |\n",
            "|    total_timesteps  | 175209   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.349    |\n",
            "|    n_updates        | 31302    |\n",
            "----------------------------------\n",
            "Num timesteps: 176000\n",
            "Best mean reward: -36.93 - Last mean reward per episode: -36.72\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 177000\n",
            "Best mean reward: -36.72 - Last mean reward per episode: -36.64\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 304      |\n",
            "|    ep_rew_mean      | -34.3    |\n",
            "|    exploration_rate | 0.439    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1408     |\n",
            "|    fps              | 1284     |\n",
            "|    time_elapsed     | 138      |\n",
            "|    total_timesteps  | 177280   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.405    |\n",
            "|    n_updates        | 31819    |\n",
            "----------------------------------\n",
            "Num timesteps: 178000\n",
            "Best mean reward: -36.64 - Last mean reward per episode: -34.51\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 310      |\n",
            "|    ep_rew_mean      | -31.9    |\n",
            "|    exploration_rate | 0.435    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1412     |\n",
            "|    fps              | 1280     |\n",
            "|    time_elapsed     | 139      |\n",
            "|    total_timesteps  | 178403   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.351    |\n",
            "|    n_updates        | 32100    |\n",
            "----------------------------------\n",
            "Num timesteps: 179000\n",
            "Best mean reward: -34.51 - Last mean reward per episode: -31.94\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 180000\n",
            "Best mean reward: -31.94 - Last mean reward per episode: -32.55\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 323      |\n",
            "|    ep_rew_mean      | -32.9    |\n",
            "|    exploration_rate | 0.428    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1416     |\n",
            "|    fps              | 1268     |\n",
            "|    time_elapsed     | 142      |\n",
            "|    total_timesteps  | 180475   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.735    |\n",
            "|    n_updates        | 32618    |\n",
            "----------------------------------\n",
            "Num timesteps: 181000\n",
            "Best mean reward: -31.94 - Last mean reward per episode: -33.14\n",
            "Num timesteps: 182000\n",
            "Best mean reward: -31.94 - Last mean reward per episode: -32.04\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 333      |\n",
            "|    ep_rew_mean      | -31      |\n",
            "|    exploration_rate | 0.423    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1420     |\n",
            "|    fps              | 1261     |\n",
            "|    time_elapsed     | 144      |\n",
            "|    total_timesteps  | 182056   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.381    |\n",
            "|    n_updates        | 33013    |\n",
            "----------------------------------\n",
            "Num timesteps: 183000\n",
            "Best mean reward: -31.94 - Last mean reward per episode: -30.60\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 184000\n",
            "Best mean reward: -30.60 - Last mean reward per episode: -30.79\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 353      |\n",
            "|    ep_rew_mean      | -30.1    |\n",
            "|    exploration_rate | 0.415    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1424     |\n",
            "|    fps              | 1239     |\n",
            "|    time_elapsed     | 148      |\n",
            "|    total_timesteps  | 184593   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.422    |\n",
            "|    n_updates        | 33648    |\n",
            "----------------------------------\n",
            "Num timesteps: 185000\n",
            "Best mean reward: -30.60 - Last mean reward per episode: -30.09\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 186000\n",
            "Best mean reward: -30.09 - Last mean reward per episode: -29.08\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 187000\n",
            "Best mean reward: -29.08 - Last mean reward per episode: -28.78\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 362      |\n",
            "|    ep_rew_mean      | -28.8    |\n",
            "|    exploration_rate | 0.407    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1428     |\n",
            "|    fps              | 1219     |\n",
            "|    time_elapsed     | 153      |\n",
            "|    total_timesteps  | 187149   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.444    |\n",
            "|    n_updates        | 34287    |\n",
            "----------------------------------\n",
            "Num timesteps: 188000\n",
            "Best mean reward: -28.78 - Last mean reward per episode: -28.82\n",
            "Num timesteps: 189000\n",
            "Best mean reward: -28.78 - Last mean reward per episode: -29.39\n",
            "Num timesteps: 190000\n",
            "Best mean reward: -28.78 - Last mean reward per episode: -30.25\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 388      |\n",
            "|    ep_rew_mean      | -30.2    |\n",
            "|    exploration_rate | 0.397    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1432     |\n",
            "|    fps              | 1199     |\n",
            "|    time_elapsed     | 158      |\n",
            "|    total_timesteps  | 190502   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.6      |\n",
            "|    n_updates        | 35125    |\n",
            "----------------------------------\n",
            "Num timesteps: 191000\n",
            "Best mean reward: -28.78 - Last mean reward per episode: -30.22\n",
            "Num timesteps: 192000\n",
            "Best mean reward: -28.78 - Last mean reward per episode: -30.93\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 404      |\n",
            "|    ep_rew_mean      | -30      |\n",
            "|    exploration_rate | 0.389    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1436     |\n",
            "|    fps              | 1181     |\n",
            "|    time_elapsed     | 163      |\n",
            "|    total_timesteps  | 192806   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.21     |\n",
            "|    n_updates        | 35701    |\n",
            "----------------------------------\n",
            "Num timesteps: 193000\n",
            "Best mean reward: -28.78 - Last mean reward per episode: -30.04\n",
            "Num timesteps: 194000\n",
            "Best mean reward: -28.78 - Last mean reward per episode: -29.18\n",
            "Num timesteps: 195000\n",
            "Best mean reward: -28.78 - Last mean reward per episode: -28.85\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 427      |\n",
            "|    ep_rew_mean      | -29.3    |\n",
            "|    exploration_rate | 0.38     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1440     |\n",
            "|    fps              | 1167     |\n",
            "|    time_elapsed     | 167      |\n",
            "|    total_timesteps  | 195817   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.591    |\n",
            "|    n_updates        | 36454    |\n",
            "----------------------------------\n",
            "Num timesteps: 196000\n",
            "Best mean reward: -28.78 - Last mean reward per episode: -29.31\n",
            "Num timesteps: 197000\n",
            "Best mean reward: -28.78 - Last mean reward per episode: -28.41\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 198000\n",
            "Best mean reward: -28.41 - Last mean reward per episode: -27.56\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 199000\n",
            "Best mean reward: -27.56 - Last mean reward per episode: -27.02\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 458      |\n",
            "|    ep_rew_mean      | -27      |\n",
            "|    exploration_rate | 0.367    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1444     |\n",
            "|    fps              | 1137     |\n",
            "|    time_elapsed     | 175      |\n",
            "|    total_timesteps  | 199817   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.25     |\n",
            "|    n_updates        | 37454    |\n",
            "----------------------------------\n",
            "Num timesteps: 200000\n",
            "Best mean reward: -27.02 - Last mean reward per episode: -26.95\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 201000\n",
            "Best mean reward: -26.95 - Last mean reward per episode: -26.88\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 202000\n",
            "Best mean reward: -26.88 - Last mean reward per episode: -25.81\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 203000\n",
            "Best mean reward: -25.81 - Last mean reward per episode: -25.55\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 486      |\n",
            "|    ep_rew_mean      | -25.2    |\n",
            "|    exploration_rate | 0.356    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1448     |\n",
            "|    fps              | 1111     |\n",
            "|    time_elapsed     | 183      |\n",
            "|    total_timesteps  | 203502   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.22     |\n",
            "|    n_updates        | 38375    |\n",
            "----------------------------------\n",
            "Num timesteps: 204000\n",
            "Best mean reward: -25.55 - Last mean reward per episode: -25.23\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 205000\n",
            "Best mean reward: -25.23 - Last mean reward per episode: -24.52\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 206000\n",
            "Best mean reward: -24.52 - Last mean reward per episode: -23.83\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 504      |\n",
            "|    ep_rew_mean      | -23.9    |\n",
            "|    exploration_rate | 0.348    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1452     |\n",
            "|    fps              | 1102     |\n",
            "|    time_elapsed     | 186      |\n",
            "|    total_timesteps  | 206013   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.34     |\n",
            "|    n_updates        | 39003    |\n",
            "----------------------------------\n",
            "Num timesteps: 207000\n",
            "Best mean reward: -23.83 - Last mean reward per episode: -23.94\n",
            "Num timesteps: 208000\n",
            "Best mean reward: -23.83 - Last mean reward per episode: -24.24\n",
            "Num timesteps: 209000\n",
            "Best mean reward: -23.83 - Last mean reward per episode: -24.58\n",
            "Num timesteps: 210000\n",
            "Best mean reward: -23.83 - Last mean reward per episode: -24.93\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 528      |\n",
            "|    ep_rew_mean      | -25.9    |\n",
            "|    exploration_rate | 0.335    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1456     |\n",
            "|    fps              | 1080     |\n",
            "|    time_elapsed     | 194      |\n",
            "|    total_timesteps  | 210013   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.71     |\n",
            "|    n_updates        | 40003    |\n",
            "----------------------------------\n",
            "Num timesteps: 211000\n",
            "Best mean reward: -23.83 - Last mean reward per episode: -25.90\n",
            "Num timesteps: 212000\n",
            "Best mean reward: -23.83 - Last mean reward per episode: -24.63\n",
            "Num timesteps: 213000\n",
            "Best mean reward: -23.83 - Last mean reward per episode: -23.29\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 541      |\n",
            "|    ep_rew_mean      | -21.8    |\n",
            "|    exploration_rate | 0.325    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1460     |\n",
            "|    fps              | 1063     |\n",
            "|    time_elapsed     | 200      |\n",
            "|    total_timesteps  | 213180   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.17     |\n",
            "|    n_updates        | 40794    |\n",
            "----------------------------------\n",
            "Num timesteps: 214000\n",
            "Best mean reward: -23.29 - Last mean reward per episode: -21.81\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 215000\n",
            "Best mean reward: -21.81 - Last mean reward per episode: -21.31\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 216000\n",
            "Best mean reward: -21.31 - Last mean reward per episode: -20.95\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 217000\n",
            "Best mean reward: -20.95 - Last mean reward per episode: -21.30\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 562      |\n",
            "|    ep_rew_mean      | -21.3    |\n",
            "|    exploration_rate | 0.312    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1464     |\n",
            "|    fps              | 1044     |\n",
            "|    time_elapsed     | 207      |\n",
            "|    total_timesteps  | 217180   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.48     |\n",
            "|    n_updates        | 41794    |\n",
            "----------------------------------\n",
            "Num timesteps: 218000\n",
            "Best mean reward: -20.95 - Last mean reward per episode: -21.31\n",
            "Num timesteps: 219000\n",
            "Best mean reward: -20.95 - Last mean reward per episode: -19.98\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 220000\n",
            "Best mean reward: -19.98 - Last mean reward per episode: -20.43\n",
            "Num timesteps: 221000\n",
            "Best mean reward: -19.98 - Last mean reward per episode: -19.81\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 589      |\n",
            "|    ep_rew_mean      | -20.1    |\n",
            "|    exploration_rate | 0.3      |\n",
            "| time/               |          |\n",
            "|    episodes         | 1468     |\n",
            "|    fps              | 1026     |\n",
            "|    time_elapsed     | 215      |\n",
            "|    total_timesteps  | 221180   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.311    |\n",
            "|    n_updates        | 42794    |\n",
            "----------------------------------\n",
            "Num timesteps: 222000\n",
            "Best mean reward: -19.81 - Last mean reward per episode: -20.15\n",
            "Num timesteps: 223000\n",
            "Best mean reward: -19.81 - Last mean reward per episode: -20.39\n",
            "Num timesteps: 224000\n",
            "Best mean reward: -19.81 - Last mean reward per episode: -18.66\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 611      |\n",
            "|    ep_rew_mean      | -18.5    |\n",
            "|    exploration_rate | 0.29     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1472     |\n",
            "|    fps              | 1014     |\n",
            "|    time_elapsed     | 221      |\n",
            "|    total_timesteps  | 224293   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.802    |\n",
            "|    n_updates        | 43573    |\n",
            "----------------------------------\n",
            "Num timesteps: 225000\n",
            "Best mean reward: -18.66 - Last mean reward per episode: -18.55\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 226000\n",
            "Best mean reward: -18.55 - Last mean reward per episode: -16.19\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 227000\n",
            "Best mean reward: -16.19 - Last mean reward per episode: -15.09\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 228000\n",
            "Best mean reward: -15.09 - Last mean reward per episode: -15.40\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 645      |\n",
            "|    ep_rew_mean      | -14.9    |\n",
            "|    exploration_rate | 0.277    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1476     |\n",
            "|    fps              | 995      |\n",
            "|    time_elapsed     | 229      |\n",
            "|    total_timesteps  | 228293   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.489    |\n",
            "|    n_updates        | 44573    |\n",
            "----------------------------------\n",
            "Num timesteps: 229000\n",
            "Best mean reward: -15.09 - Last mean reward per episode: -14.88\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 230000\n",
            "Best mean reward: -14.88 - Last mean reward per episode: -12.27\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 231000\n",
            "Best mean reward: -12.27 - Last mean reward per episode: -12.93\n",
            "Num timesteps: 232000\n",
            "Best mean reward: -12.27 - Last mean reward per episode: -13.23\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 677      |\n",
            "|    ep_rew_mean      | -13.5    |\n",
            "|    exploration_rate | 0.264    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1480     |\n",
            "|    fps              | 978      |\n",
            "|    time_elapsed     | 237      |\n",
            "|    total_timesteps  | 232293   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 6.33     |\n",
            "|    n_updates        | 45573    |\n",
            "----------------------------------\n",
            "Num timesteps: 233000\n",
            "Best mean reward: -12.27 - Last mean reward per episode: -13.52\n",
            "Num timesteps: 234000\n",
            "Best mean reward: -12.27 - Last mean reward per episode: -11.43\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 235000\n",
            "Best mean reward: -11.43 - Last mean reward per episode: -12.22\n",
            "Num timesteps: 236000\n",
            "Best mean reward: -11.43 - Last mean reward per episode: -12.08\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 686      |\n",
            "|    ep_rew_mean      | -13      |\n",
            "|    exploration_rate | 0.252    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1484     |\n",
            "|    fps              | 967      |\n",
            "|    time_elapsed     | 244      |\n",
            "|    total_timesteps  | 236293   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.2      |\n",
            "|    n_updates        | 46573    |\n",
            "----------------------------------\n",
            "Num timesteps: 237000\n",
            "Best mean reward: -11.43 - Last mean reward per episode: -13.00\n",
            "Num timesteps: 238000\n",
            "Best mean reward: -11.43 - Last mean reward per episode: -13.89\n",
            "Num timesteps: 239000\n",
            "Best mean reward: -11.43 - Last mean reward per episode: -14.93\n",
            "Num timesteps: 240000\n",
            "Best mean reward: -11.43 - Last mean reward per episode: -15.66\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 717      |\n",
            "|    ep_rew_mean      | -15      |\n",
            "|    exploration_rate | 0.239    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1488     |\n",
            "|    fps              | 953      |\n",
            "|    time_elapsed     | 252      |\n",
            "|    total_timesteps  | 240293   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.588    |\n",
            "|    n_updates        | 47573    |\n",
            "----------------------------------\n",
            "Num timesteps: 241000\n",
            "Best mean reward: -11.43 - Last mean reward per episode: -14.99\n",
            "Num timesteps: 242000\n",
            "Best mean reward: -11.43 - Last mean reward per episode: -14.46\n",
            "Num timesteps: 243000\n",
            "Best mean reward: -11.43 - Last mean reward per episode: -12.51\n",
            "Num timesteps: 244000\n",
            "Best mean reward: -11.43 - Last mean reward per episode: -11.36\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 736      |\n",
            "|    ep_rew_mean      | -9.57    |\n",
            "|    exploration_rate | 0.227    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1492     |\n",
            "|    fps              | 940      |\n",
            "|    time_elapsed     | 259      |\n",
            "|    total_timesteps  | 244185   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.78     |\n",
            "|    n_updates        | 48546    |\n",
            "----------------------------------\n",
            "Num timesteps: 245000\n",
            "Best mean reward: -11.36 - Last mean reward per episode: -9.57\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 246000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -10.18\n",
            "Num timesteps: 247000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -10.10\n",
            "Num timesteps: 248000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -11.02\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 762      |\n",
            "|    ep_rew_mean      | -12.3    |\n",
            "|    exploration_rate | 0.214    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1496     |\n",
            "|    fps              | 927      |\n",
            "|    time_elapsed     | 267      |\n",
            "|    total_timesteps  | 248185   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.58     |\n",
            "|    n_updates        | 49546    |\n",
            "----------------------------------\n",
            "Num timesteps: 249000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -12.28\n",
            "Num timesteps: 250000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -12.69\n",
            "Num timesteps: 251000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -12.64\n",
            "Num timesteps: 252000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -13.20\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 781      |\n",
            "|    ep_rew_mean      | -14.6    |\n",
            "|    exploration_rate | 0.201    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1500     |\n",
            "|    fps              | 914      |\n",
            "|    time_elapsed     | 275      |\n",
            "|    total_timesteps  | 252185   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.503    |\n",
            "|    n_updates        | 50546    |\n",
            "----------------------------------\n",
            "Num timesteps: 253000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -14.64\n",
            "Num timesteps: 254000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -14.20\n",
            "Num timesteps: 255000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -14.23\n",
            "Num timesteps: 256000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -15.12\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 810      |\n",
            "|    ep_rew_mean      | -14.6    |\n",
            "|    exploration_rate | 0.189    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1504     |\n",
            "|    fps              | 903      |\n",
            "|    time_elapsed     | 283      |\n",
            "|    total_timesteps  | 256185   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.671    |\n",
            "|    n_updates        | 51546    |\n",
            "----------------------------------\n",
            "Num timesteps: 257000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -14.56\n",
            "Num timesteps: 258000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -13.90\n",
            "Num timesteps: 259000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -14.80\n",
            "Num timesteps: 260000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -15.21\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 829      |\n",
            "|    ep_rew_mean      | -16.6    |\n",
            "|    exploration_rate | 0.176    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1508     |\n",
            "|    fps              | 893      |\n",
            "|    time_elapsed     | 291      |\n",
            "|    total_timesteps  | 260185   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.556    |\n",
            "|    n_updates        | 52546    |\n",
            "----------------------------------\n",
            "Num timesteps: 261000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -16.61\n",
            "Num timesteps: 262000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -16.23\n",
            "Num timesteps: 263000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -17.03\n",
            "Num timesteps: 264000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -16.06\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 857      |\n",
            "|    ep_rew_mean      | -14.2    |\n",
            "|    exploration_rate | 0.164    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1512     |\n",
            "|    fps              | 885      |\n",
            "|    time_elapsed     | 298      |\n",
            "|    total_timesteps  | 264070   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.345    |\n",
            "|    n_updates        | 53517    |\n",
            "----------------------------------\n",
            "Num timesteps: 265000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -14.21\n",
            "Num timesteps: 266000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -14.88\n",
            "Num timesteps: 267000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -14.18\n",
            "Num timesteps: 268000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -13.93\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 876      |\n",
            "|    ep_rew_mean      | -14.2    |\n",
            "|    exploration_rate | 0.151    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1516     |\n",
            "|    fps              | 877      |\n",
            "|    time_elapsed     | 305      |\n",
            "|    total_timesteps  | 268070   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.36     |\n",
            "|    n_updates        | 54517    |\n",
            "----------------------------------\n",
            "Num timesteps: 269000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -14.25\n",
            "Num timesteps: 270000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -13.18\n",
            "Num timesteps: 271000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -11.99\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 899      |\n",
            "|    ep_rew_mean      | -13.3    |\n",
            "|    exploration_rate | 0.139    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1520     |\n",
            "|    fps              | 868      |\n",
            "|    time_elapsed     | 313      |\n",
            "|    total_timesteps  | 271992   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.471    |\n",
            "|    n_updates        | 55497    |\n",
            "----------------------------------\n",
            "Num timesteps: 272000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -13.26\n",
            "Num timesteps: 273000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -14.07\n",
            "Num timesteps: 274000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -15.11\n",
            "Num timesteps: 275000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -14.91\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 914      |\n",
            "|    ep_rew_mean      | -15.2    |\n",
            "|    exploration_rate | 0.126    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1524     |\n",
            "|    fps              | 859      |\n",
            "|    time_elapsed     | 321      |\n",
            "|    total_timesteps  | 275992   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.283    |\n",
            "|    n_updates        | 56497    |\n",
            "----------------------------------\n",
            "Num timesteps: 276000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -15.15\n",
            "Num timesteps: 277000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -16.24\n",
            "Num timesteps: 278000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -16.72\n",
            "Num timesteps: 279000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -17.16\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 925      |\n",
            "|    ep_rew_mean      | -14.2    |\n",
            "|    exploration_rate | 0.114    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1528     |\n",
            "|    fps              | 849      |\n",
            "|    time_elapsed     | 329      |\n",
            "|    total_timesteps  | 279633   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.77     |\n",
            "|    n_updates        | 57408    |\n",
            "----------------------------------\n",
            "Num timesteps: 280000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -14.15\n",
            "Num timesteps: 281000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -14.28\n",
            "Num timesteps: 282000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -14.99\n",
            "Num timesteps: 283000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -13.25\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 931      |\n",
            "|    ep_rew_mean      | -14      |\n",
            "|    exploration_rate | 0.102    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1532     |\n",
            "|    fps              | 842      |\n",
            "|    time_elapsed     | 336      |\n",
            "|    total_timesteps  | 283633   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.599    |\n",
            "|    n_updates        | 58408    |\n",
            "----------------------------------\n",
            "Num timesteps: 284000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -13.99\n",
            "Num timesteps: 285000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -10.80\n",
            "Num timesteps: 286000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -11.52\n",
            "Num timesteps: 287000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -12.09\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 947      |\n",
            "|    ep_rew_mean      | -11.8    |\n",
            "|    exploration_rate | 0.0896   |\n",
            "| time/               |          |\n",
            "|    episodes         | 1536     |\n",
            "|    fps              | 835      |\n",
            "|    time_elapsed     | 344      |\n",
            "|    total_timesteps  | 287479   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.3      |\n",
            "|    n_updates        | 59369    |\n",
            "----------------------------------\n",
            "Num timesteps: 288000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -11.82\n",
            "Num timesteps: 289000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -10.10\n",
            "Num timesteps: 290000\n",
            "Best mean reward: -9.57 - Last mean reward per episode: -8.43\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 950      |\n",
            "|    ep_rew_mean      | -8.27    |\n",
            "|    exploration_rate | 0.079    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1540     |\n",
            "|    fps              | 829      |\n",
            "|    time_elapsed     | 350      |\n",
            "|    total_timesteps  | 290839   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.24     |\n",
            "|    n_updates        | 60209    |\n",
            "----------------------------------\n",
            "Num timesteps: 291000\n",
            "Best mean reward: -8.43 - Last mean reward per episode: -8.27\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 292000\n",
            "Best mean reward: -8.27 - Last mean reward per episode: -8.63\n",
            "Num timesteps: 293000\n",
            "Best mean reward: -8.27 - Last mean reward per episode: -9.25\n",
            "Num timesteps: 294000\n",
            "Best mean reward: -8.27 - Last mean reward per episode: -8.43\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 950      |\n",
            "|    ep_rew_mean      | -8.84    |\n",
            "|    exploration_rate | 0.0663   |\n",
            "| time/               |          |\n",
            "|    episodes         | 1544     |\n",
            "|    fps              | 823      |\n",
            "|    time_elapsed     | 358      |\n",
            "|    total_timesteps  | 294839   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.58     |\n",
            "|    n_updates        | 61209    |\n",
            "----------------------------------\n",
            "Num timesteps: 295000\n",
            "Best mean reward: -8.27 - Last mean reward per episode: -8.84\n",
            "Num timesteps: 296000\n",
            "Best mean reward: -8.27 - Last mean reward per episode: -6.14\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 297000\n",
            "Best mean reward: -6.14 - Last mean reward per episode: -4.79\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 944      |\n",
            "|    ep_rew_mean      | -0.201   |\n",
            "|    exploration_rate | 0.0567   |\n",
            "| time/               |          |\n",
            "|    episodes         | 1548     |\n",
            "|    fps              | 819      |\n",
            "|    time_elapsed     | 363      |\n",
            "|    total_timesteps  | 297900   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.25     |\n",
            "|    n_updates        | 61974    |\n",
            "----------------------------------\n",
            "Num timesteps: 298000\n",
            "Best mean reward: -4.79 - Last mean reward per episode: -0.20\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 299000\n",
            "Best mean reward: -0.20 - Last mean reward per episode: -1.80\n",
            "Num timesteps: 300000\n",
            "Best mean reward: -0.20 - Last mean reward per episode: -1.28\n",
            "Num timesteps: 301000\n",
            "Best mean reward: -0.20 - Last mean reward per episode: -1.91\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 959      |\n",
            "|    ep_rew_mean      | -2.34    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1552     |\n",
            "|    fps              | 813      |\n",
            "|    time_elapsed     | 371      |\n",
            "|    total_timesteps  | 301900   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.394    |\n",
            "|    n_updates        | 62974    |\n",
            "----------------------------------\n",
            "Num timesteps: 302000\n",
            "Best mean reward: -0.20 - Last mean reward per episode: -2.34\n",
            "Num timesteps: 303000\n",
            "Best mean reward: -0.20 - Last mean reward per episode: -2.38\n",
            "Num timesteps: 304000\n",
            "Best mean reward: -0.20 - Last mean reward per episode: -2.02\n",
            "Num timesteps: 305000\n",
            "Best mean reward: -0.20 - Last mean reward per episode: -1.72\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 954      |\n",
            "|    ep_rew_mean      | 1.56     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1556     |\n",
            "|    fps              | 808      |\n",
            "|    time_elapsed     | 377      |\n",
            "|    total_timesteps  | 305450   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.55     |\n",
            "|    n_updates        | 63862    |\n",
            "----------------------------------\n",
            "Num timesteps: 306000\n",
            "Best mean reward: -0.20 - Last mean reward per episode: 1.56\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 307000\n",
            "Best mean reward: 1.56 - Last mean reward per episode: 2.37\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 308000\n",
            "Best mean reward: 2.37 - Last mean reward per episode: 4.80\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 952      |\n",
            "|    ep_rew_mean      | 3.42     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1560     |\n",
            "|    fps              | 806      |\n",
            "|    time_elapsed     | 382      |\n",
            "|    total_timesteps  | 308391   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.658    |\n",
            "|    n_updates        | 64597    |\n",
            "----------------------------------\n",
            "Num timesteps: 309000\n",
            "Best mean reward: 4.80 - Last mean reward per episode: 3.42\n",
            "Num timesteps: 310000\n",
            "Best mean reward: 4.80 - Last mean reward per episode: 5.96\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 311000\n",
            "Best mean reward: 5.96 - Last mean reward per episode: 9.78\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 943      |\n",
            "|    ep_rew_mean      | 10.1     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1564     |\n",
            "|    fps              | 803      |\n",
            "|    time_elapsed     | 387      |\n",
            "|    total_timesteps  | 311505   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.484    |\n",
            "|    n_updates        | 65376    |\n",
            "----------------------------------\n",
            "Num timesteps: 312000\n",
            "Best mean reward: 9.78 - Last mean reward per episode: 10.12\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 313000\n",
            "Best mean reward: 10.12 - Last mean reward per episode: 14.96\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 314000\n",
            "Best mean reward: 14.96 - Last mean reward per episode: 13.78\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 937      |\n",
            "|    ep_rew_mean      | 14.4     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1568     |\n",
            "|    fps              | 801      |\n",
            "|    time_elapsed     | 393      |\n",
            "|    total_timesteps  | 314899   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.299    |\n",
            "|    n_updates        | 66224    |\n",
            "----------------------------------\n",
            "Num timesteps: 315000\n",
            "Best mean reward: 14.96 - Last mean reward per episode: 14.35\n",
            "Num timesteps: 316000\n",
            "Best mean reward: 14.96 - Last mean reward per episode: 13.77\n",
            "Num timesteps: 317000\n",
            "Best mean reward: 14.96 - Last mean reward per episode: 12.83\n",
            "Num timesteps: 318000\n",
            "Best mean reward: 14.96 - Last mean reward per episode: 11.64\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 946      |\n",
            "|    ep_rew_mean      | 11.3     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1572     |\n",
            "|    fps              | 796      |\n",
            "|    time_elapsed     | 400      |\n",
            "|    total_timesteps  | 318899   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.298    |\n",
            "|    n_updates        | 67224    |\n",
            "----------------------------------\n",
            "Num timesteps: 319000\n",
            "Best mean reward: 14.96 - Last mean reward per episode: 11.29\n",
            "Num timesteps: 320000\n",
            "Best mean reward: 14.96 - Last mean reward per episode: 10.33\n",
            "Num timesteps: 321000\n",
            "Best mean reward: 14.96 - Last mean reward per episode: 9.32\n",
            "Num timesteps: 322000\n",
            "Best mean reward: 14.96 - Last mean reward per episode: 12.00\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 940      |\n",
            "|    ep_rew_mean      | 14.6     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1576     |\n",
            "|    fps              | 794      |\n",
            "|    time_elapsed     | 405      |\n",
            "|    total_timesteps  | 322316   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 8.43     |\n",
            "|    n_updates        | 68078    |\n",
            "----------------------------------\n",
            "Num timesteps: 323000\n",
            "Best mean reward: 14.96 - Last mean reward per episode: 14.64\n",
            "Num timesteps: 324000\n",
            "Best mean reward: 14.96 - Last mean reward per episode: 13.44\n",
            "Num timesteps: 325000\n",
            "Best mean reward: 14.96 - Last mean reward per episode: 13.15\n",
            "Num timesteps: 326000\n",
            "Best mean reward: 14.96 - Last mean reward per episode: 13.06\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 940      |\n",
            "|    ep_rew_mean      | 13.4     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1580     |\n",
            "|    fps              | 789      |\n",
            "|    time_elapsed     | 413      |\n",
            "|    total_timesteps  | 326316   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.277    |\n",
            "|    n_updates        | 69078    |\n",
            "----------------------------------\n",
            "Num timesteps: 327000\n",
            "Best mean reward: 14.96 - Last mean reward per episode: 13.42\n",
            "Num timesteps: 328000\n",
            "Best mean reward: 14.96 - Last mean reward per episode: 13.33\n",
            "Num timesteps: 329000\n",
            "Best mean reward: 14.96 - Last mean reward per episode: 15.72\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 330000\n",
            "Best mean reward: 15.72 - Last mean reward per episode: 15.50\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 939      |\n",
            "|    ep_rew_mean      | 15.3     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1584     |\n",
            "|    fps              | 786      |\n",
            "|    time_elapsed     | 420      |\n",
            "|    total_timesteps  | 330197   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.09     |\n",
            "|    n_updates        | 70049    |\n",
            "----------------------------------\n",
            "Num timesteps: 331000\n",
            "Best mean reward: 15.72 - Last mean reward per episode: 18.64\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 332000\n",
            "Best mean reward: 18.64 - Last mean reward per episode: 21.14\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 333000\n",
            "Best mean reward: 21.14 - Last mean reward per episode: 21.60\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 933      |\n",
            "|    ep_rew_mean      | 21.3     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1588     |\n",
            "|    fps              | 783      |\n",
            "|    time_elapsed     | 425      |\n",
            "|    total_timesteps  | 333615   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.16     |\n",
            "|    n_updates        | 70903    |\n",
            "----------------------------------\n",
            "Num timesteps: 334000\n",
            "Best mean reward: 21.60 - Last mean reward per episode: 21.25\n",
            "Num timesteps: 335000\n",
            "Best mean reward: 21.60 - Last mean reward per episode: 24.58\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 336000\n",
            "Best mean reward: 24.58 - Last mean reward per episode: 23.44\n",
            "Num timesteps: 337000\n",
            "Best mean reward: 24.58 - Last mean reward per episode: 22.82\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 929      |\n",
            "|    ep_rew_mean      | 23.3     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1592     |\n",
            "|    fps              | 780      |\n",
            "|    time_elapsed     | 431      |\n",
            "|    total_timesteps  | 337038   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.41     |\n",
            "|    n_updates        | 71759    |\n",
            "----------------------------------\n",
            "Num timesteps: 338000\n",
            "Best mean reward: 24.58 - Last mean reward per episode: 23.29\n",
            "Num timesteps: 339000\n",
            "Best mean reward: 24.58 - Last mean reward per episode: 27.59\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 340000\n",
            "Best mean reward: 27.59 - Last mean reward per episode: 28.25\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 921      |\n",
            "|    ep_rew_mean      | 31.3     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1596     |\n",
            "|    fps              | 777      |\n",
            "|    time_elapsed     | 437      |\n",
            "|    total_timesteps  | 340245   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.431    |\n",
            "|    n_updates        | 72561    |\n",
            "----------------------------------\n",
            "Num timesteps: 341000\n",
            "Best mean reward: 28.25 - Last mean reward per episode: 31.32\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 342000\n",
            "Best mean reward: 31.32 - Last mean reward per episode: 31.60\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 343000\n",
            "Best mean reward: 31.60 - Last mean reward per episode: 33.21\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 344000\n",
            "Best mean reward: 33.21 - Last mean reward per episode: 34.04\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 921      |\n",
            "|    ep_rew_mean      | 34.4     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1600     |\n",
            "|    fps              | 773      |\n",
            "|    time_elapsed     | 444      |\n",
            "|    total_timesteps  | 344238   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.344    |\n",
            "|    n_updates        | 73559    |\n",
            "----------------------------------\n",
            "Num timesteps: 345000\n",
            "Best mean reward: 34.04 - Last mean reward per episode: 34.40\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 346000\n",
            "Best mean reward: 34.40 - Last mean reward per episode: 34.03\n",
            "Num timesteps: 347000\n",
            "Best mean reward: 34.40 - Last mean reward per episode: 33.58\n",
            "Num timesteps: 348000\n",
            "Best mean reward: 34.40 - Last mean reward per episode: 34.97\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 921      |\n",
            "|    ep_rew_mean      | 35       |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1604     |\n",
            "|    fps              | 769      |\n",
            "|    time_elapsed     | 452      |\n",
            "|    total_timesteps  | 348238   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.89     |\n",
            "|    n_updates        | 74559    |\n",
            "----------------------------------\n",
            "Num timesteps: 349000\n",
            "Best mean reward: 34.97 - Last mean reward per episode: 37.22\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 350000\n",
            "Best mean reward: 37.22 - Last mean reward per episode: 39.68\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 907      |\n",
            "|    ep_rew_mean      | 43.5     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1608     |\n",
            "|    fps              | 768      |\n",
            "|    time_elapsed     | 456      |\n",
            "|    total_timesteps  | 350931   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.41     |\n",
            "|    n_updates        | 75232    |\n",
            "----------------------------------\n",
            "Num timesteps: 351000\n",
            "Best mean reward: 39.68 - Last mean reward per episode: 43.49\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 352000\n",
            "Best mean reward: 43.49 - Last mean reward per episode: 43.77\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 353000\n",
            "Best mean reward: 43.77 - Last mean reward per episode: 47.11\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 354000\n",
            "Best mean reward: 47.11 - Last mean reward per episode: 45.79\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 901      |\n",
            "|    ep_rew_mean      | 45.8     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1612     |\n",
            "|    fps              | 765      |\n",
            "|    time_elapsed     | 462      |\n",
            "|    total_timesteps  | 354194   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.448    |\n",
            "|    n_updates        | 76048    |\n",
            "----------------------------------\n",
            "Num timesteps: 355000\n",
            "Best mean reward: 47.11 - Last mean reward per episode: 45.81\n",
            "Num timesteps: 356000\n",
            "Best mean reward: 47.11 - Last mean reward per episode: 45.72\n",
            "Num timesteps: 357000\n",
            "Best mean reward: 47.11 - Last mean reward per episode: 45.84\n",
            "Num timesteps: 358000\n",
            "Best mean reward: 47.11 - Last mean reward per episode: 46.29\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 900      |\n",
            "|    ep_rew_mean      | 48.7     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1616     |\n",
            "|    fps              | 762      |\n",
            "|    time_elapsed     | 469      |\n",
            "|    total_timesteps  | 358108   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.876    |\n",
            "|    n_updates        | 77026    |\n",
            "----------------------------------\n",
            "Num timesteps: 359000\n",
            "Best mean reward: 47.11 - Last mean reward per episode: 48.71\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 360000\n",
            "Best mean reward: 48.71 - Last mean reward per episode: 50.45\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 361000\n",
            "Best mean reward: 50.45 - Last mean reward per episode: 51.56\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 891      |\n",
            "|    ep_rew_mean      | 51.8     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1620     |\n",
            "|    fps              | 760      |\n",
            "|    time_elapsed     | 474      |\n",
            "|    total_timesteps  | 361052   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.81     |\n",
            "|    n_updates        | 77762    |\n",
            "----------------------------------\n",
            "Num timesteps: 362000\n",
            "Best mean reward: 51.56 - Last mean reward per episode: 51.83\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 363000\n",
            "Best mean reward: 51.83 - Last mean reward per episode: 56.34\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 364000\n",
            "Best mean reward: 56.34 - Last mean reward per episode: 56.38\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 887      |\n",
            "|    ep_rew_mean      | 55.6     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1624     |\n",
            "|    fps              | 757      |\n",
            "|    time_elapsed     | 481      |\n",
            "|    total_timesteps  | 364711   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.697    |\n",
            "|    n_updates        | 78677    |\n",
            "----------------------------------\n",
            "Num timesteps: 365000\n",
            "Best mean reward: 56.38 - Last mean reward per episode: 55.58\n",
            "Num timesteps: 366000\n",
            "Best mean reward: 56.38 - Last mean reward per episode: 58.40\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 367000\n",
            "Best mean reward: 58.40 - Last mean reward per episode: 58.95\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 368000\n",
            "Best mean reward: 58.95 - Last mean reward per episode: 59.31\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 889      |\n",
            "|    ep_rew_mean      | 57.1     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1628     |\n",
            "|    fps              | 754      |\n",
            "|    time_elapsed     | 488      |\n",
            "|    total_timesteps  | 368525   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.781    |\n",
            "|    n_updates        | 79631    |\n",
            "----------------------------------\n",
            "Num timesteps: 369000\n",
            "Best mean reward: 59.31 - Last mean reward per episode: 57.14\n",
            "Num timesteps: 370000\n",
            "Best mean reward: 59.31 - Last mean reward per episode: 60.19\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 371000\n",
            "Best mean reward: 60.19 - Last mean reward per episode: 61.18\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 372000\n",
            "Best mean reward: 61.18 - Last mean reward per episode: 61.31\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 885      |\n",
            "|    ep_rew_mean      | 62.7     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1632     |\n",
            "|    fps              | 751      |\n",
            "|    time_elapsed     | 495      |\n",
            "|    total_timesteps  | 372110   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.3      |\n",
            "|    n_updates        | 80527    |\n",
            "----------------------------------\n",
            "Num timesteps: 373000\n",
            "Best mean reward: 61.31 - Last mean reward per episode: 62.72\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 374000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 60.84\n",
            "Num timesteps: 375000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 61.77\n",
            "Num timesteps: 376000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 61.59\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 886      |\n",
            "|    ep_rew_mean      | 61.9     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1636     |\n",
            "|    fps              | 746      |\n",
            "|    time_elapsed     | 503      |\n",
            "|    total_timesteps  | 376110   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.314    |\n",
            "|    n_updates        | 81527    |\n",
            "----------------------------------\n",
            "Num timesteps: 377000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 61.86\n",
            "Num timesteps: 378000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 62.50\n",
            "Num timesteps: 379000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 60.17\n",
            "Num timesteps: 380000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 58.98\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 893      |\n",
            "|    ep_rew_mean      | 59.8     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1640     |\n",
            "|    fps              | 743      |\n",
            "|    time_elapsed     | 511      |\n",
            "|    total_timesteps  | 380110   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.901    |\n",
            "|    n_updates        | 82527    |\n",
            "----------------------------------\n",
            "Num timesteps: 381000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 61.99\n",
            "Num timesteps: 382000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 61.88\n",
            "Num timesteps: 383000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 61.28\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 890      |\n",
            "|    ep_rew_mean      | 61.5     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1644     |\n",
            "|    fps              | 740      |\n",
            "|    time_elapsed     | 518      |\n",
            "|    total_timesteps  | 383840   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.21     |\n",
            "|    n_updates        | 83459    |\n",
            "----------------------------------\n",
            "Num timesteps: 384000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 61.54\n",
            "Num timesteps: 385000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 59.86\n",
            "Num timesteps: 386000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 57.91\n",
            "Num timesteps: 387000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 55.53\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 899      |\n",
            "|    ep_rew_mean      | 53.3     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1648     |\n",
            "|    fps              | 737      |\n",
            "|    time_elapsed     | 525      |\n",
            "|    total_timesteps  | 387840   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.313    |\n",
            "|    n_updates        | 84459    |\n",
            "----------------------------------\n",
            "Num timesteps: 388000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 53.28\n",
            "Num timesteps: 389000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 53.47\n",
            "Num timesteps: 390000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 52.99\n",
            "Num timesteps: 391000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 52.92\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 899      |\n",
            "|    ep_rew_mean      | 54       |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1652     |\n",
            "|    fps              | 734      |\n",
            "|    time_elapsed     | 533      |\n",
            "|    total_timesteps  | 391840   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.475    |\n",
            "|    n_updates        | 85459    |\n",
            "----------------------------------\n",
            "Num timesteps: 392000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 53.98\n",
            "Num timesteps: 393000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 54.35\n",
            "Num timesteps: 394000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 53.84\n",
            "Num timesteps: 395000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 53.82\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 897      |\n",
            "|    ep_rew_mean      | 50.7     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1656     |\n",
            "|    fps              | 732      |\n",
            "|    time_elapsed     | 539      |\n",
            "|    total_timesteps  | 395118   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.481    |\n",
            "|    n_updates        | 86279    |\n",
            "----------------------------------\n",
            "Num timesteps: 396000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 50.72\n",
            "Num timesteps: 397000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 49.81\n",
            "Num timesteps: 398000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 49.69\n",
            "Num timesteps: 399000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 47.20\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 907      |\n",
            "|    ep_rew_mean      | 47.6     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1660     |\n",
            "|    fps              | 729      |\n",
            "|    time_elapsed     | 547      |\n",
            "|    total_timesteps  | 399118   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.73     |\n",
            "|    n_updates        | 87279    |\n",
            "----------------------------------\n",
            "Num timesteps: 400000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 47.57\n",
            "Num timesteps: 401000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 45.41\n",
            "Num timesteps: 402000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 45.81\n",
            "Num timesteps: 403000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 43.15\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 916      |\n",
            "|    ep_rew_mean      | 43.9     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1664     |\n",
            "|    fps              | 727      |\n",
            "|    time_elapsed     | 554      |\n",
            "|    total_timesteps  | 403118   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.407    |\n",
            "|    n_updates        | 88279    |\n",
            "----------------------------------\n",
            "Num timesteps: 404000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 43.87\n",
            "Num timesteps: 405000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 41.29\n",
            "Num timesteps: 406000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 38.58\n",
            "Num timesteps: 407000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 38.70\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 922      |\n",
            "|    ep_rew_mean      | 37.9     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1668     |\n",
            "|    fps              | 724      |\n",
            "|    time_elapsed     | 561      |\n",
            "|    total_timesteps  | 407118   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.385    |\n",
            "|    n_updates        | 89279    |\n",
            "----------------------------------\n",
            "Num timesteps: 408000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 37.87\n",
            "Num timesteps: 409000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 40.42\n",
            "Num timesteps: 410000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 40.72\n",
            "Num timesteps: 411000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 41.10\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 921      |\n",
            "|    ep_rew_mean      | 41.6     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1672     |\n",
            "|    fps              | 722      |\n",
            "|    time_elapsed     | 568      |\n",
            "|    total_timesteps  | 411040   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.375    |\n",
            "|    n_updates        | 90259    |\n",
            "----------------------------------\n",
            "Num timesteps: 412000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 41.63\n",
            "Num timesteps: 413000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 41.74\n",
            "Num timesteps: 414000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 42.03\n",
            "Num timesteps: 415000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 40.00\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 927      |\n",
            "|    ep_rew_mean      | 37.5     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1676     |\n",
            "|    fps              | 720      |\n",
            "|    time_elapsed     | 576      |\n",
            "|    total_timesteps  | 415040   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.238    |\n",
            "|    n_updates        | 91259    |\n",
            "----------------------------------\n",
            "Num timesteps: 416000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 37.45\n",
            "Num timesteps: 417000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 37.32\n",
            "Num timesteps: 418000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 38.30\n",
            "Num timesteps: 419000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 38.71\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 927      |\n",
            "|    ep_rew_mean      | 39.4     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1680     |\n",
            "|    fps              | 717      |\n",
            "|    time_elapsed     | 584      |\n",
            "|    total_timesteps  | 419040   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.326    |\n",
            "|    n_updates        | 92259    |\n",
            "----------------------------------\n",
            "Num timesteps: 420000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 39.42\n",
            "Num timesteps: 421000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 39.35\n",
            "Num timesteps: 422000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 38.07\n",
            "Num timesteps: 423000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 38.68\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 928      |\n",
            "|    ep_rew_mean      | 39       |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1684     |\n",
            "|    fps              | 713      |\n",
            "|    time_elapsed     | 592      |\n",
            "|    total_timesteps  | 423040   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.378    |\n",
            "|    n_updates        | 93259    |\n",
            "----------------------------------\n",
            "Num timesteps: 424000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 39.01\n",
            "Num timesteps: 425000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 36.43\n",
            "Num timesteps: 426000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 35.20\n",
            "Num timesteps: 427000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 35.61\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 934      |\n",
            "|    ep_rew_mean      | 34.7     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1688     |\n",
            "|    fps              | 711      |\n",
            "|    time_elapsed     | 600      |\n",
            "|    total_timesteps  | 427040   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.551    |\n",
            "|    n_updates        | 94259    |\n",
            "----------------------------------\n",
            "Num timesteps: 428000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 34.74\n",
            "Num timesteps: 429000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 31.83\n",
            "Num timesteps: 430000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 31.20\n",
            "Num timesteps: 431000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 30.67\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 940      |\n",
            "|    ep_rew_mean      | 30.1     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1692     |\n",
            "|    fps              | 708      |\n",
            "|    time_elapsed     | 608      |\n",
            "|    total_timesteps  | 431040   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.707    |\n",
            "|    n_updates        | 95259    |\n",
            "----------------------------------\n",
            "Num timesteps: 432000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 30.06\n",
            "Num timesteps: 433000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 29.39\n",
            "Num timesteps: 434000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 26.88\n",
            "Num timesteps: 435000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 26.71\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 948      |\n",
            "|    ep_rew_mean      | 24.8     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1696     |\n",
            "|    fps              | 707      |\n",
            "|    time_elapsed     | 614      |\n",
            "|    total_timesteps  | 435040   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.38     |\n",
            "|    n_updates        | 96259    |\n",
            "----------------------------------\n",
            "Num timesteps: 436000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 24.81\n",
            "Num timesteps: 437000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 24.82\n",
            "Num timesteps: 438000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 23.14\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 943      |\n",
            "|    ep_rew_mean      | 25.8     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1700     |\n",
            "|    fps              | 706      |\n",
            "|    time_elapsed     | 620      |\n",
            "|    total_timesteps  | 438576   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.623    |\n",
            "|    n_updates        | 97143    |\n",
            "----------------------------------\n",
            "Num timesteps: 439000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 25.77\n",
            "Num timesteps: 440000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 26.13\n",
            "Num timesteps: 441000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 26.18\n",
            "Num timesteps: 442000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 26.08\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 943      |\n",
            "|    ep_rew_mean      | 25.9     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1704     |\n",
            "|    fps              | 704      |\n",
            "|    time_elapsed     | 627      |\n",
            "|    total_timesteps  | 442576   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.633    |\n",
            "|    n_updates        | 98143    |\n",
            "----------------------------------\n",
            "Num timesteps: 443000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 25.91\n",
            "Num timesteps: 444000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 23.64\n",
            "Num timesteps: 445000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 21.79\n",
            "Num timesteps: 446000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 21.97\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 956      |\n",
            "|    ep_rew_mean      | 19       |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1708     |\n",
            "|    fps              | 701      |\n",
            "|    time_elapsed     | 636      |\n",
            "|    total_timesteps  | 446576   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.349    |\n",
            "|    n_updates        | 99143    |\n",
            "----------------------------------\n",
            "Num timesteps: 447000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 18.96\n",
            "Num timesteps: 448000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 19.02\n",
            "Num timesteps: 449000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 16.09\n",
            "Num timesteps: 450000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 15.84\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 964      |\n",
            "|    ep_rew_mean      | 13.7     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1712     |\n",
            "|    fps              | 698      |\n",
            "|    time_elapsed     | 645      |\n",
            "|    total_timesteps  | 450576   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.05     |\n",
            "|    n_updates        | 100143   |\n",
            "----------------------------------\n",
            "Num timesteps: 451000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 13.67\n",
            "Num timesteps: 452000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 13.98\n",
            "Num timesteps: 453000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 13.31\n",
            "Num timesteps: 454000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 13.14\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 965      |\n",
            "|    ep_rew_mean      | 11.2     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1716     |\n",
            "|    fps              | 696      |\n",
            "|    time_elapsed     | 652      |\n",
            "|    total_timesteps  | 454576   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.41     |\n",
            "|    n_updates        | 101143   |\n",
            "----------------------------------\n",
            "Num timesteps: 455000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 11.23\n",
            "Num timesteps: 456000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 11.25\n",
            "Num timesteps: 457000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 8.78\n",
            "Num timesteps: 458000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 5.51\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 975      |\n",
            "|    ep_rew_mean      | 5.59     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1720     |\n",
            "|    fps              | 694      |\n",
            "|    time_elapsed     | 660      |\n",
            "|    total_timesteps  | 458576   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.45     |\n",
            "|    n_updates        | 102143   |\n",
            "----------------------------------\n",
            "Num timesteps: 459000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 5.59\n",
            "Num timesteps: 460000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 5.27\n",
            "Num timesteps: 461000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.71\n",
            "Num timesteps: 462000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 3.04\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 979      |\n",
            "|    ep_rew_mean      | 3.57     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1724     |\n",
            "|    fps              | 693      |\n",
            "|    time_elapsed     | 667      |\n",
            "|    total_timesteps  | 462576   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.997    |\n",
            "|    n_updates        | 103143   |\n",
            "----------------------------------\n",
            "Num timesteps: 463000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 3.57\n",
            "Num timesteps: 464000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 1.40\n",
            "Num timesteps: 465000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.87\n",
            "Num timesteps: 466000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.17\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 981      |\n",
            "|    ep_rew_mean      | -0.204   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1728     |\n",
            "|    fps              | 691      |\n",
            "|    time_elapsed     | 675      |\n",
            "|    total_timesteps  | 466576   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.12     |\n",
            "|    n_updates        | 104143   |\n",
            "----------------------------------\n",
            "Num timesteps: 467000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.20\n",
            "Num timesteps: 468000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.66\n",
            "Num timesteps: 469000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.78\n",
            "Num timesteps: 470000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.82\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 985      |\n",
            "|    ep_rew_mean      | -4.74    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1732     |\n",
            "|    fps              | 689      |\n",
            "|    time_elapsed     | 682      |\n",
            "|    total_timesteps  | 470576   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.03     |\n",
            "|    n_updates        | 105143   |\n",
            "----------------------------------\n",
            "Num timesteps: 471000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.74\n",
            "Num timesteps: 472000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.89\n",
            "Num timesteps: 473000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.58\n",
            "Num timesteps: 474000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.69\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 985      |\n",
            "|    ep_rew_mean      | -5.98    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1736     |\n",
            "|    fps              | 687      |\n",
            "|    time_elapsed     | 690      |\n",
            "|    total_timesteps  | 474576   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.3      |\n",
            "|    n_updates        | 106143   |\n",
            "----------------------------------\n",
            "Num timesteps: 475000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.98\n",
            "Num timesteps: 476000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.19\n",
            "Num timesteps: 477000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.79\n",
            "Num timesteps: 478000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -7.44\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 985      |\n",
            "|    ep_rew_mean      | -7.52    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1740     |\n",
            "|    fps              | 685      |\n",
            "|    time_elapsed     | 698      |\n",
            "|    total_timesteps  | 478576   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.267    |\n",
            "|    n_updates        | 107143   |\n",
            "----------------------------------\n",
            "Num timesteps: 479000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -7.52\n",
            "Num timesteps: 480000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.12\n",
            "Num timesteps: 481000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -9.83\n",
            "Num timesteps: 482000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.23\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 987      |\n",
            "|    ep_rew_mean      | -10.2    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1744     |\n",
            "|    fps              | 683      |\n",
            "|    time_elapsed     | 706      |\n",
            "|    total_timesteps  | 482576   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.07     |\n",
            "|    n_updates        | 108143   |\n",
            "----------------------------------\n",
            "Num timesteps: 483000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.18\n",
            "Num timesteps: 484000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -9.85\n",
            "Num timesteps: 485000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -9.93\n",
            "Num timesteps: 486000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.01\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 987      |\n",
            "|    ep_rew_mean      | -10.3    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1748     |\n",
            "|    fps              | 680      |\n",
            "|    time_elapsed     | 715      |\n",
            "|    total_timesteps  | 486576   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.509    |\n",
            "|    n_updates        | 109143   |\n",
            "----------------------------------\n",
            "Num timesteps: 487000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.33\n",
            "Num timesteps: 488000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.29\n",
            "Num timesteps: 489000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.59\n",
            "Num timesteps: 490000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.33\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 987      |\n",
            "|    ep_rew_mean      | -10.7    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1752     |\n",
            "|    fps              | 678      |\n",
            "|    time_elapsed     | 723      |\n",
            "|    total_timesteps  | 490576   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.295    |\n",
            "|    n_updates        | 110143   |\n",
            "----------------------------------\n",
            "Num timesteps: 491000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.72\n",
            "Num timesteps: 492000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.65\n",
            "Num timesteps: 493000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -11.22\n",
            "Num timesteps: 494000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -11.52\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 995      |\n",
            "|    ep_rew_mean      | -11.3    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1756     |\n",
            "|    fps              | 677      |\n",
            "|    time_elapsed     | 730      |\n",
            "|    total_timesteps  | 494576   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.805    |\n",
            "|    n_updates        | 111143   |\n",
            "----------------------------------\n",
            "Num timesteps: 495000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -11.26\n",
            "Num timesteps: 496000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -11.96\n",
            "Num timesteps: 497000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -11.56\n",
            "Num timesteps: 498000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -11.40\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 995      |\n",
            "|    ep_rew_mean      | -12.1    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1760     |\n",
            "|    fps              | 675      |\n",
            "|    time_elapsed     | 738      |\n",
            "|    total_timesteps  | 498576   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.263    |\n",
            "|    n_updates        | 112143   |\n",
            "----------------------------------\n",
            "Num timesteps: 499000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -13.39\n",
            "Num timesteps: 500000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -14.29\n",
            "Num timesteps: 501000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -13.99\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 989      |\n",
            "|    ep_rew_mean      | -14.3    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1764     |\n",
            "|    fps              | 674      |\n",
            "|    time_elapsed     | 744      |\n",
            "|    total_timesteps  | 501995   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.465    |\n",
            "|    n_updates        | 112998   |\n",
            "----------------------------------\n",
            "Num timesteps: 502000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -14.35\n",
            "Num timesteps: 503000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -14.64\n",
            "Num timesteps: 504000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -14.27\n",
            "Num timesteps: 505000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -13.92\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 989      |\n",
            "|    ep_rew_mean      | -13.2    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1768     |\n",
            "|    fps              | 673      |\n",
            "|    time_elapsed     | 751      |\n",
            "|    total_timesteps  | 505995   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.835    |\n",
            "|    n_updates        | 113998   |\n",
            "----------------------------------\n",
            "Num timesteps: 506000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -13.21\n",
            "Num timesteps: 507000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -15.23\n",
            "Num timesteps: 508000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -15.16\n",
            "Num timesteps: 509000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -14.75\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 983      |\n",
            "|    ep_rew_mean      | -15.7    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1772     |\n",
            "|    fps              | 672      |\n",
            "|    time_elapsed     | 757      |\n",
            "|    total_timesteps  | 509294   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.306    |\n",
            "|    n_updates        | 114823   |\n",
            "----------------------------------\n",
            "Num timesteps: 510000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -15.70\n",
            "Num timesteps: 511000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -15.74\n",
            "Num timesteps: 512000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -15.59\n",
            "Num timesteps: 513000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -15.30\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 983      |\n",
            "|    ep_rew_mean      | -15.4    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1776     |\n",
            "|    fps              | 670      |\n",
            "|    time_elapsed     | 765      |\n",
            "|    total_timesteps  | 513294   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.996    |\n",
            "|    n_updates        | 115823   |\n",
            "----------------------------------\n",
            "Num timesteps: 514000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -15.93\n",
            "Num timesteps: 515000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -16.10\n",
            "Num timesteps: 516000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -15.95\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 974      |\n",
            "|    ep_rew_mean      | -16.4    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1780     |\n",
            "|    fps              | 669      |\n",
            "|    time_elapsed     | 771      |\n",
            "|    total_timesteps  | 516439   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.46     |\n",
            "|    n_updates        | 116609   |\n",
            "----------------------------------\n",
            "Num timesteps: 517000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -16.44\n",
            "Num timesteps: 518000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -16.48\n",
            "Num timesteps: 519000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -18.60\n",
            "Num timesteps: 520000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -18.89\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 971      |\n",
            "|    ep_rew_mean      | -17.9    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1784     |\n",
            "|    fps              | 668      |\n",
            "|    time_elapsed     | 778      |\n",
            "|    total_timesteps  | 520115   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.332    |\n",
            "|    n_updates        | 117528   |\n",
            "----------------------------------\n",
            "Num timesteps: 521000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -17.94\n",
            "Num timesteps: 522000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -17.85\n",
            "Num timesteps: 523000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -19.16\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 963      |\n",
            "|    ep_rew_mean      | -18.1    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1788     |\n",
            "|    fps              | 667      |\n",
            "|    time_elapsed     | 784      |\n",
            "|    total_timesteps  | 523303   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.17     |\n",
            "|    n_updates        | 118325   |\n",
            "----------------------------------\n",
            "Num timesteps: 524000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -18.06\n",
            "Num timesteps: 525000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -17.59\n",
            "Num timesteps: 526000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -17.74\n",
            "Num timesteps: 527000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -16.99\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 963      |\n",
            "|    ep_rew_mean      | -17      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1792     |\n",
            "|    fps              | 665      |\n",
            "|    time_elapsed     | 792      |\n",
            "|    total_timesteps  | 527303   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.396    |\n",
            "|    n_updates        | 119325   |\n",
            "----------------------------------\n",
            "Num timesteps: 528000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -17.02\n",
            "Num timesteps: 529000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -18.10\n",
            "Num timesteps: 530000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -18.55\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 954      |\n",
            "|    ep_rew_mean      | -19.2    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1796     |\n",
            "|    fps              | 664      |\n",
            "|    time_elapsed     | 798      |\n",
            "|    total_timesteps  | 530432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.979    |\n",
            "|    n_updates        | 120107   |\n",
            "----------------------------------\n",
            "Num timesteps: 531000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -19.18\n",
            "Num timesteps: 532000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -18.78\n",
            "Num timesteps: 533000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -18.51\n",
            "Num timesteps: 534000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -19.22\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 959      |\n",
            "|    ep_rew_mean      | -21.8    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1800     |\n",
            "|    fps              | 662      |\n",
            "|    time_elapsed     | 807      |\n",
            "|    total_timesteps  | 534432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.465    |\n",
            "|    n_updates        | 121107   |\n",
            "----------------------------------\n",
            "Num timesteps: 535000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -21.85\n",
            "Num timesteps: 536000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -22.08\n",
            "Num timesteps: 537000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -21.48\n",
            "Num timesteps: 538000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -22.04\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 959      |\n",
            "|    ep_rew_mean      | -21.8    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1804     |\n",
            "|    fps              | 661      |\n",
            "|    time_elapsed     | 814      |\n",
            "|    total_timesteps  | 538432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.326    |\n",
            "|    n_updates        | 122107   |\n",
            "----------------------------------\n",
            "Num timesteps: 539000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -21.77\n",
            "Num timesteps: 540000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -21.46\n",
            "Num timesteps: 541000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -22.07\n",
            "Num timesteps: 542000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -22.35\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 959      |\n",
            "|    ep_rew_mean      | -21.8    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1808     |\n",
            "|    fps              | 660      |\n",
            "|    time_elapsed     | 821      |\n",
            "|    total_timesteps  | 542432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.385    |\n",
            "|    n_updates        | 123107   |\n",
            "----------------------------------\n",
            "Num timesteps: 543000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -21.83\n",
            "Num timesteps: 544000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -22.34\n",
            "Num timesteps: 545000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -21.89\n",
            "Num timesteps: 546000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -21.75\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 959      |\n",
            "|    ep_rew_mean      | -21.9    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1812     |\n",
            "|    fps              | 658      |\n",
            "|    time_elapsed     | 829      |\n",
            "|    total_timesteps  | 546432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.414    |\n",
            "|    n_updates        | 124107   |\n",
            "----------------------------------\n",
            "Num timesteps: 547000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -21.89\n",
            "Num timesteps: 548000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -21.96\n",
            "Num timesteps: 549000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -21.82\n",
            "Num timesteps: 550000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -21.43\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 959      |\n",
            "|    ep_rew_mean      | -21.3    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1816     |\n",
            "|    fps              | 658      |\n",
            "|    time_elapsed     | 836      |\n",
            "|    total_timesteps  | 550432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.931    |\n",
            "|    n_updates        | 125107   |\n",
            "----------------------------------\n",
            "Num timesteps: 551000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -21.28\n",
            "Num timesteps: 552000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -20.45\n",
            "Num timesteps: 553000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -20.60\n",
            "Num timesteps: 554000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -20.36\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 959      |\n",
            "|    ep_rew_mean      | -20.3    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1820     |\n",
            "|    fps              | 657      |\n",
            "|    time_elapsed     | 843      |\n",
            "|    total_timesteps  | 554432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.457    |\n",
            "|    n_updates        | 126107   |\n",
            "----------------------------------\n",
            "Num timesteps: 555000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -20.29\n",
            "Num timesteps: 556000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -19.68\n",
            "Num timesteps: 557000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -19.43\n",
            "Num timesteps: 558000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -19.46\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 959      |\n",
            "|    ep_rew_mean      | -19.5    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1824     |\n",
            "|    fps              | 655      |\n",
            "|    time_elapsed     | 851      |\n",
            "|    total_timesteps  | 558432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.262    |\n",
            "|    n_updates        | 127107   |\n",
            "----------------------------------\n",
            "Num timesteps: 559000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -19.48\n",
            "Num timesteps: 560000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -19.36\n",
            "Num timesteps: 561000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -19.20\n",
            "Num timesteps: 562000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -18.95\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 959      |\n",
            "|    ep_rew_mean      | -18.9    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1828     |\n",
            "|    fps              | 654      |\n",
            "|    time_elapsed     | 859      |\n",
            "|    total_timesteps  | 562432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.385    |\n",
            "|    n_updates        | 128107   |\n",
            "----------------------------------\n",
            "Num timesteps: 563000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -18.88\n",
            "Num timesteps: 564000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -18.58\n",
            "Num timesteps: 565000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -18.41\n",
            "Num timesteps: 566000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -18.24\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 959      |\n",
            "|    ep_rew_mean      | -18.3    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1832     |\n",
            "|    fps              | 652      |\n",
            "|    time_elapsed     | 867      |\n",
            "|    total_timesteps  | 566432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.01     |\n",
            "|    n_updates        | 129107   |\n",
            "----------------------------------\n",
            "Num timesteps: 567000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -18.32\n",
            "Num timesteps: 568000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -18.07\n",
            "Num timesteps: 569000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -17.59\n",
            "Num timesteps: 570000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -17.41\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 959      |\n",
            "|    ep_rew_mean      | -17.4    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1836     |\n",
            "|    fps              | 652      |\n",
            "|    time_elapsed     | 874      |\n",
            "|    total_timesteps  | 570432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.629    |\n",
            "|    n_updates        | 130107   |\n",
            "----------------------------------\n",
            "Num timesteps: 571000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -17.41\n",
            "Num timesteps: 572000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -17.26\n",
            "Num timesteps: 573000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -16.95\n",
            "Num timesteps: 574000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -17.11\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 959      |\n",
            "|    ep_rew_mean      | -17.2    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1840     |\n",
            "|    fps              | 650      |\n",
            "|    time_elapsed     | 882      |\n",
            "|    total_timesteps  | 574432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.514    |\n",
            "|    n_updates        | 131107   |\n",
            "----------------------------------\n",
            "Num timesteps: 575000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -17.18\n",
            "Num timesteps: 576000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -16.79\n",
            "Num timesteps: 577000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -16.91\n",
            "Num timesteps: 578000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -16.54\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 959      |\n",
            "|    ep_rew_mean      | -16.4    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1844     |\n",
            "|    fps              | 649      |\n",
            "|    time_elapsed     | 890      |\n",
            "|    total_timesteps  | 578432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.374    |\n",
            "|    n_updates        | 132107   |\n",
            "----------------------------------\n",
            "Num timesteps: 579000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -16.45\n",
            "Num timesteps: 580000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -16.84\n",
            "Num timesteps: 581000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -16.59\n",
            "Num timesteps: 582000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -16.44\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 959      |\n",
            "|    ep_rew_mean      | -16      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1848     |\n",
            "|    fps              | 648      |\n",
            "|    time_elapsed     | 898      |\n",
            "|    total_timesteps  | 582432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.98     |\n",
            "|    n_updates        | 133107   |\n",
            "----------------------------------\n",
            "Num timesteps: 583000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -16.02\n",
            "Num timesteps: 584000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -15.93\n",
            "Num timesteps: 585000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -15.30\n",
            "Num timesteps: 586000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -15.17\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 959      |\n",
            "|    ep_rew_mean      | -14.7    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1852     |\n",
            "|    fps              | 647      |\n",
            "|    time_elapsed     | 906      |\n",
            "|    total_timesteps  | 586432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.329    |\n",
            "|    n_updates        | 134107   |\n",
            "----------------------------------\n",
            "Num timesteps: 587000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -14.74\n",
            "Num timesteps: 588000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -15.05\n",
            "Num timesteps: 589000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -13.83\n",
            "Num timesteps: 590000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -13.54\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 959      |\n",
            "|    ep_rew_mean      | -13.6    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1856     |\n",
            "|    fps              | 646      |\n",
            "|    time_elapsed     | 913      |\n",
            "|    total_timesteps  | 590432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.255    |\n",
            "|    n_updates        | 135107   |\n",
            "----------------------------------\n",
            "Num timesteps: 591000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -13.61\n",
            "Num timesteps: 592000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -13.35\n",
            "Num timesteps: 593000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -13.02\n",
            "Num timesteps: 594000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -12.75\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 959      |\n",
            "|    ep_rew_mean      | -12.2    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1860     |\n",
            "|    fps              | 645      |\n",
            "|    time_elapsed     | 921      |\n",
            "|    total_timesteps  | 594432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.38     |\n",
            "|    n_updates        | 136107   |\n",
            "----------------------------------\n",
            "Num timesteps: 595000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -12.21\n",
            "Num timesteps: 596000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -11.00\n",
            "Num timesteps: 597000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.30\n",
            "Num timesteps: 598000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.06\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 964      |\n",
            "|    ep_rew_mean      | -10.1    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1864     |\n",
            "|    fps              | 644      |\n",
            "|    time_elapsed     | 928      |\n",
            "|    total_timesteps  | 598432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.732    |\n",
            "|    n_updates        | 137107   |\n",
            "----------------------------------\n",
            "Num timesteps: 599000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.14\n",
            "Num timesteps: 600000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -9.82\n",
            "Num timesteps: 601000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -9.94\n",
            "Num timesteps: 602000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -9.76\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 964      |\n",
            "|    ep_rew_mean      | -9.96    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1868     |\n",
            "|    fps              | 643      |\n",
            "|    time_elapsed     | 936      |\n",
            "|    total_timesteps  | 602432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.266    |\n",
            "|    n_updates        | 138107   |\n",
            "----------------------------------\n",
            "Num timesteps: 603000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -9.96\n",
            "Num timesteps: 604000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.48\n",
            "Num timesteps: 605000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.69\n",
            "Num timesteps: 606000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.35\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 971      |\n",
            "|    ep_rew_mean      | -9.55    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1872     |\n",
            "|    fps              | 641      |\n",
            "|    time_elapsed     | 945      |\n",
            "|    total_timesteps  | 606432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.506    |\n",
            "|    n_updates        | 139107   |\n",
            "----------------------------------\n",
            "Num timesteps: 607000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -9.55\n",
            "Num timesteps: 608000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -9.58\n",
            "Num timesteps: 609000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -9.92\n",
            "Num timesteps: 610000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -9.89\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 971      |\n",
            "|    ep_rew_mean      | -9.63    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1876     |\n",
            "|    fps              | 639      |\n",
            "|    time_elapsed     | 953      |\n",
            "|    total_timesteps  | 610432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.413    |\n",
            "|    n_updates        | 140107   |\n",
            "----------------------------------\n",
            "Num timesteps: 611000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -9.63\n",
            "Num timesteps: 612000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -8.59\n",
            "Num timesteps: 613000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -8.22\n",
            "Num timesteps: 614000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -8.38\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 980      |\n",
            "|    ep_rew_mean      | -8.32    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1880     |\n",
            "|    fps              | 638      |\n",
            "|    time_elapsed     | 961      |\n",
            "|    total_timesteps  | 614432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.685    |\n",
            "|    n_updates        | 141107   |\n",
            "----------------------------------\n",
            "Num timesteps: 615000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -8.32\n",
            "Num timesteps: 616000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -8.14\n",
            "Num timesteps: 617000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.99\n",
            "Num timesteps: 618000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.02\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 983      |\n",
            "|    ep_rew_mean      | -6.37    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1884     |\n",
            "|    fps              | 637      |\n",
            "|    time_elapsed     | 970      |\n",
            "|    total_timesteps  | 618432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.619    |\n",
            "|    n_updates        | 142107   |\n",
            "----------------------------------\n",
            "Num timesteps: 619000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.37\n",
            "Num timesteps: 620000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.08\n",
            "Num timesteps: 621000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.82\n",
            "Num timesteps: 622000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.86\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 991      |\n",
            "|    ep_rew_mean      | -4.63    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1888     |\n",
            "|    fps              | 635      |\n",
            "|    time_elapsed     | 979      |\n",
            "|    total_timesteps  | 622432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.354    |\n",
            "|    n_updates        | 143107   |\n",
            "----------------------------------\n",
            "Num timesteps: 623000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.63\n",
            "Num timesteps: 624000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.39\n",
            "Num timesteps: 625000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.94\n",
            "Num timesteps: 626000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.09\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 991      |\n",
            "|    ep_rew_mean      | -3.86    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1892     |\n",
            "|    fps              | 635      |\n",
            "|    time_elapsed     | 986      |\n",
            "|    total_timesteps  | 626432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.29     |\n",
            "|    n_updates        | 144107   |\n",
            "----------------------------------\n",
            "Num timesteps: 627000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.86\n",
            "Num timesteps: 628000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.09\n",
            "Num timesteps: 629000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.32\n",
            "Num timesteps: 630000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.04\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -2.49    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1896     |\n",
            "|    fps              | 634      |\n",
            "|    time_elapsed     | 994      |\n",
            "|    total_timesteps  | 630432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 6.9      |\n",
            "|    n_updates        | 145107   |\n",
            "----------------------------------\n",
            "Num timesteps: 631000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.49\n",
            "Num timesteps: 632000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.36\n",
            "Num timesteps: 633000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.81\n",
            "Num timesteps: 634000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.01\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -1.6     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1900     |\n",
            "|    fps              | 633      |\n",
            "|    time_elapsed     | 1002     |\n",
            "|    total_timesteps  | 634432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.24     |\n",
            "|    n_updates        | 146107   |\n",
            "----------------------------------\n",
            "Num timesteps: 635000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.60\n",
            "Num timesteps: 636000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.51\n",
            "Num timesteps: 637000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.92\n",
            "Num timesteps: 638000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.88\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -1.73    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1904     |\n",
            "|    fps              | 631      |\n",
            "|    time_elapsed     | 1010     |\n",
            "|    total_timesteps  | 638432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.355    |\n",
            "|    n_updates        | 147107   |\n",
            "----------------------------------\n",
            "Num timesteps: 639000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.73\n",
            "Num timesteps: 640000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.20\n",
            "Num timesteps: 641000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.92\n",
            "Num timesteps: 642000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.48\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -1.6     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1908     |\n",
            "|    fps              | 630      |\n",
            "|    time_elapsed     | 1019     |\n",
            "|    total_timesteps  | 642432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.28     |\n",
            "|    n_updates        | 148107   |\n",
            "----------------------------------\n",
            "Num timesteps: 643000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.60\n",
            "Num timesteps: 644000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.93\n",
            "Num timesteps: 645000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.99\n",
            "Num timesteps: 646000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.59\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -0.422   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1912     |\n",
            "|    fps              | 628      |\n",
            "|    time_elapsed     | 1028     |\n",
            "|    total_timesteps  | 646432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 9.4      |\n",
            "|    n_updates        | 149107   |\n",
            "----------------------------------\n",
            "Num timesteps: 647000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.42\n",
            "Num timesteps: 648000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.03\n",
            "Num timesteps: 649000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.18\n",
            "Num timesteps: 650000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.56\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -0.434   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1916     |\n",
            "|    fps              | 627      |\n",
            "|    time_elapsed     | 1036     |\n",
            "|    total_timesteps  | 650432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.17     |\n",
            "|    n_updates        | 150107   |\n",
            "----------------------------------\n",
            "Num timesteps: 651000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.43\n",
            "Num timesteps: 652000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.42\n",
            "Num timesteps: 653000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.73\n",
            "Num timesteps: 654000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.48\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -0.41    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1920     |\n",
            "|    fps              | 626      |\n",
            "|    time_elapsed     | 1044     |\n",
            "|    total_timesteps  | 654432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 9.37     |\n",
            "|    n_updates        | 151107   |\n",
            "----------------------------------\n",
            "Num timesteps: 655000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.41\n",
            "Num timesteps: 656000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.71\n",
            "Num timesteps: 657000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.67\n",
            "Num timesteps: 658000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.68\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -0.714   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1924     |\n",
            "|    fps              | 625      |\n",
            "|    time_elapsed     | 1052     |\n",
            "|    total_timesteps  | 658432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.591    |\n",
            "|    n_updates        | 152107   |\n",
            "----------------------------------\n",
            "Num timesteps: 659000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.71\n",
            "Num timesteps: 660000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.75\n",
            "Num timesteps: 661000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.85\n",
            "Num timesteps: 662000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.86\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -0.904   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1928     |\n",
            "|    fps              | 625      |\n",
            "|    time_elapsed     | 1059     |\n",
            "|    total_timesteps  | 662432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 15.3     |\n",
            "|    n_updates        | 153107   |\n",
            "----------------------------------\n",
            "Num timesteps: 663000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.90\n",
            "Num timesteps: 664000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.01\n",
            "Num timesteps: 665000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.77\n",
            "Num timesteps: 666000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.58\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -0.291   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1932     |\n",
            "|    fps              | 623      |\n",
            "|    time_elapsed     | 1068     |\n",
            "|    total_timesteps  | 666432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.26     |\n",
            "|    n_updates        | 154107   |\n",
            "----------------------------------\n",
            "Num timesteps: 667000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.29\n",
            "Num timesteps: 668000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.08\n",
            "Num timesteps: 669000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.07\n",
            "Num timesteps: 670000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.12\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -0.0448  |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1936     |\n",
            "|    fps              | 622      |\n",
            "|    time_elapsed     | 1076     |\n",
            "|    total_timesteps  | 670432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.517    |\n",
            "|    n_updates        | 155107   |\n",
            "----------------------------------\n",
            "Num timesteps: 671000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.04\n",
            "Num timesteps: 672000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.04\n",
            "Num timesteps: 673000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.40\n",
            "Num timesteps: 674000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.12\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | 0.0406   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1940     |\n",
            "|    fps              | 621      |\n",
            "|    time_elapsed     | 1084     |\n",
            "|    total_timesteps  | 674432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.02     |\n",
            "|    n_updates        | 156107   |\n",
            "----------------------------------\n",
            "Num timesteps: 675000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.04\n",
            "Num timesteps: 676000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.02\n",
            "Num timesteps: 677000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.06\n",
            "Num timesteps: 678000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.15\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | 0.131    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1944     |\n",
            "|    fps              | 620      |\n",
            "|    time_elapsed     | 1092     |\n",
            "|    total_timesteps  | 678432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.944    |\n",
            "|    n_updates        | 157107   |\n",
            "----------------------------------\n",
            "Num timesteps: 679000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.13\n",
            "Num timesteps: 680000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.58\n",
            "Num timesteps: 681000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.67\n",
            "Num timesteps: 682000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.81\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | 0.503    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1948     |\n",
            "|    fps              | 619      |\n",
            "|    time_elapsed     | 1101     |\n",
            "|    total_timesteps  | 682432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.494    |\n",
            "|    n_updates        | 158107   |\n",
            "----------------------------------\n",
            "Num timesteps: 683000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.50\n",
            "Num timesteps: 684000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.55\n",
            "Num timesteps: 685000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.40\n",
            "Num timesteps: 686000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.41\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -0.165   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1952     |\n",
            "|    fps              | 618      |\n",
            "|    time_elapsed     | 1109     |\n",
            "|    total_timesteps  | 686432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.379    |\n",
            "|    n_updates        | 159107   |\n",
            "----------------------------------\n",
            "Num timesteps: 687000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.16\n",
            "Num timesteps: 688000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.16\n",
            "Num timesteps: 689000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.07\n",
            "Num timesteps: 690000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.02\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | 0.133    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1956     |\n",
            "|    fps              | 617      |\n",
            "|    time_elapsed     | 1117     |\n",
            "|    total_timesteps  | 690432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.24     |\n",
            "|    n_updates        | 160107   |\n",
            "----------------------------------\n",
            "Num timesteps: 691000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.13\n",
            "Num timesteps: 692000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.02\n",
            "Num timesteps: 693000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.40\n",
            "Num timesteps: 694000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.83\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -1.29    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1960     |\n",
            "|    fps              | 616      |\n",
            "|    time_elapsed     | 1126     |\n",
            "|    total_timesteps  | 694432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.369    |\n",
            "|    n_updates        | 161107   |\n",
            "----------------------------------\n",
            "Num timesteps: 695000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.29\n",
            "Num timesteps: 696000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.39\n",
            "Num timesteps: 697000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.30\n",
            "Num timesteps: 698000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.58\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -1.2     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1964     |\n",
            "|    fps              | 615      |\n",
            "|    time_elapsed     | 1134     |\n",
            "|    total_timesteps  | 698432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.29     |\n",
            "|    n_updates        | 162107   |\n",
            "----------------------------------\n",
            "Num timesteps: 699000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.20\n",
            "Num timesteps: 700000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.82\n",
            "Num timesteps: 701000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.60\n",
            "Num timesteps: 702000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.99\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -0.901   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1968     |\n",
            "|    fps              | 615      |\n",
            "|    time_elapsed     | 1141     |\n",
            "|    total_timesteps  | 702432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.654    |\n",
            "|    n_updates        | 163107   |\n",
            "----------------------------------\n",
            "Num timesteps: 703000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.90\n",
            "Num timesteps: 704000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.35\n",
            "Num timesteps: 705000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.23\n",
            "Num timesteps: 706000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.71\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -0.509   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1972     |\n",
            "|    fps              | 614      |\n",
            "|    time_elapsed     | 1150     |\n",
            "|    total_timesteps  | 706432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.371    |\n",
            "|    n_updates        | 164107   |\n",
            "----------------------------------\n",
            "Num timesteps: 707000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.51\n",
            "Num timesteps: 708000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.29\n",
            "Num timesteps: 709000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.00\n",
            "Num timesteps: 710000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.03\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -0.00646 |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1976     |\n",
            "|    fps              | 613      |\n",
            "|    time_elapsed     | 1158     |\n",
            "|    total_timesteps  | 710432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.89     |\n",
            "|    n_updates        | 165107   |\n",
            "----------------------------------\n",
            "Num timesteps: 711000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.01\n",
            "Num timesteps: 712000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.07\n",
            "Num timesteps: 713000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.14\n",
            "Num timesteps: 714000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.03\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -0.349   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1980     |\n",
            "|    fps              | 612      |\n",
            "|    time_elapsed     | 1165     |\n",
            "|    total_timesteps  | 714432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.238    |\n",
            "|    n_updates        | 166107   |\n",
            "----------------------------------\n",
            "Num timesteps: 715000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.35\n",
            "Num timesteps: 716000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.01\n",
            "Num timesteps: 717000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.05\n",
            "Num timesteps: 718000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.35\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | 0.196    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1984     |\n",
            "|    fps              | 612      |\n",
            "|    time_elapsed     | 1173     |\n",
            "|    total_timesteps  | 718432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.962    |\n",
            "|    n_updates        | 167107   |\n",
            "----------------------------------\n",
            "Num timesteps: 719000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.20\n",
            "Num timesteps: 720000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.03\n",
            "Num timesteps: 721000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.20\n",
            "Num timesteps: 722000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.19\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -0.441   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1988     |\n",
            "|    fps              | 611      |\n",
            "|    time_elapsed     | 1180     |\n",
            "|    total_timesteps  | 722432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.431    |\n",
            "|    n_updates        | 168107   |\n",
            "----------------------------------\n",
            "Num timesteps: 723000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.44\n",
            "Num timesteps: 724000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.13\n",
            "Num timesteps: 725000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.43\n",
            "Num timesteps: 726000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.53\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -1.58    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1992     |\n",
            "|    fps              | 610      |\n",
            "|    time_elapsed     | 1189     |\n",
            "|    total_timesteps  | 726432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.94     |\n",
            "|    n_updates        | 169107   |\n",
            "----------------------------------\n",
            "Num timesteps: 727000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.58\n",
            "Num timesteps: 728000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.47\n",
            "Num timesteps: 729000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.07\n",
            "Num timesteps: 730000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.67\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -0.663   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1996     |\n",
            "|    fps              | 610      |\n",
            "|    time_elapsed     | 1197     |\n",
            "|    total_timesteps  | 730432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.246    |\n",
            "|    n_updates        | 170107   |\n",
            "----------------------------------\n",
            "Num timesteps: 731000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.66\n",
            "Num timesteps: 732000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.58\n",
            "Num timesteps: 733000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.23\n",
            "Num timesteps: 734000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.15\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -0.339   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2000     |\n",
            "|    fps              | 609      |\n",
            "|    time_elapsed     | 1204     |\n",
            "|    total_timesteps  | 734432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.46     |\n",
            "|    n_updates        | 171107   |\n",
            "----------------------------------\n",
            "Num timesteps: 735000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.34\n",
            "Num timesteps: 736000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.23\n",
            "Num timesteps: 737000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.51\n",
            "Num timesteps: 738000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.74\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | 0.623    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2004     |\n",
            "|    fps              | 609      |\n",
            "|    time_elapsed     | 1212     |\n",
            "|    total_timesteps  | 738432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.397    |\n",
            "|    n_updates        | 172107   |\n",
            "----------------------------------\n",
            "Num timesteps: 739000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.62\n",
            "Num timesteps: 740000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.93\n",
            "Num timesteps: 741000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 1.22\n",
            "Num timesteps: 742000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.65\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | 0.654    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2008     |\n",
            "|    fps              | 608      |\n",
            "|    time_elapsed     | 1220     |\n",
            "|    total_timesteps  | 742432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.389    |\n",
            "|    n_updates        | 173107   |\n",
            "----------------------------------\n",
            "Num timesteps: 743000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.65\n",
            "Num timesteps: 744000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.06\n",
            "Num timesteps: 745000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.14\n",
            "Num timesteps: 746000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.05\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | 0.374    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2012     |\n",
            "|    fps              | 607      |\n",
            "|    time_elapsed     | 1228     |\n",
            "|    total_timesteps  | 746432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.622    |\n",
            "|    n_updates        | 174107   |\n",
            "----------------------------------\n",
            "Num timesteps: 747000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.37\n",
            "Num timesteps: 748000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.41\n",
            "Num timesteps: 749000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.47\n",
            "Num timesteps: 750000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.89\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | 0.836    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2016     |\n",
            "|    fps              | 607      |\n",
            "|    time_elapsed     | 1235     |\n",
            "|    total_timesteps  | 750432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 18.6     |\n",
            "|    n_updates        | 175107   |\n",
            "----------------------------------\n",
            "Num timesteps: 751000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.84\n",
            "Num timesteps: 752000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.96\n",
            "Num timesteps: 753000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 1.59\n",
            "Num timesteps: 754000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 1.82\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | 1.77     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2020     |\n",
            "|    fps              | 606      |\n",
            "|    time_elapsed     | 1243     |\n",
            "|    total_timesteps  | 754432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.269    |\n",
            "|    n_updates        | 176107   |\n",
            "----------------------------------\n",
            "Num timesteps: 755000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 1.77\n",
            "Num timesteps: 756000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.07\n",
            "Num timesteps: 757000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.04\n",
            "Num timesteps: 758000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 1.68\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | 1.9      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2024     |\n",
            "|    fps              | 605      |\n",
            "|    time_elapsed     | 1251     |\n",
            "|    total_timesteps  | 758432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.307    |\n",
            "|    n_updates        | 177107   |\n",
            "----------------------------------\n",
            "Num timesteps: 759000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 1.90\n",
            "Num timesteps: 760000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 1.99\n",
            "Num timesteps: 761000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.38\n",
            "Num timesteps: 762000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.50\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | 2.53     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2028     |\n",
            "|    fps              | 604      |\n",
            "|    time_elapsed     | 1260     |\n",
            "|    total_timesteps  | 762432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.311    |\n",
            "|    n_updates        | 178107   |\n",
            "----------------------------------\n",
            "Num timesteps: 763000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.53\n",
            "Num timesteps: 764000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.23\n",
            "Num timesteps: 765000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.44\n",
            "Num timesteps: 766000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.28\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | 2.26     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2032     |\n",
            "|    fps              | 603      |\n",
            "|    time_elapsed     | 1269     |\n",
            "|    total_timesteps  | 766432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.3      |\n",
            "|    n_updates        | 179107   |\n",
            "----------------------------------\n",
            "Num timesteps: 767000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.26\n",
            "Num timesteps: 768000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 1.90\n",
            "Num timesteps: 769000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 1.81\n",
            "Num timesteps: 770000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.24\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | 2.56     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2036     |\n",
            "|    fps              | 603      |\n",
            "|    time_elapsed     | 1277     |\n",
            "|    total_timesteps  | 770432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.12     |\n",
            "|    n_updates        | 180107   |\n",
            "----------------------------------\n",
            "Num timesteps: 771000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.56\n",
            "Num timesteps: 772000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.71\n",
            "Num timesteps: 773000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.86\n",
            "Num timesteps: 774000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.87\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | 3.22     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2040     |\n",
            "|    fps              | 602      |\n",
            "|    time_elapsed     | 1286     |\n",
            "|    total_timesteps  | 774432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.921    |\n",
            "|    n_updates        | 181107   |\n",
            "----------------------------------\n",
            "Num timesteps: 775000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 3.22\n",
            "Num timesteps: 776000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 3.27\n",
            "Num timesteps: 777000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 3.89\n",
            "Num timesteps: 778000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 3.37\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | 3.84     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2044     |\n",
            "|    fps              | 601      |\n",
            "|    time_elapsed     | 1293     |\n",
            "|    total_timesteps  | 778432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.296    |\n",
            "|    n_updates        | 182107   |\n",
            "----------------------------------\n",
            "Num timesteps: 779000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 3.84\n",
            "Num timesteps: 780000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 3.64\n",
            "Num timesteps: 781000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 3.61\n",
            "Num timesteps: 782000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.94\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | 2.99     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2048     |\n",
            "|    fps              | 600      |\n",
            "|    time_elapsed     | 1302     |\n",
            "|    total_timesteps  | 782432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 9.66     |\n",
            "|    n_updates        | 183107   |\n",
            "----------------------------------\n",
            "Num timesteps: 783000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.99\n",
            "Num timesteps: 784000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.93\n",
            "Num timesteps: 785000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.94\n",
            "Num timesteps: 786000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.63\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | 3.06     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2052     |\n",
            "|    fps              | 600      |\n",
            "|    time_elapsed     | 1309     |\n",
            "|    total_timesteps  | 786432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.376    |\n",
            "|    n_updates        | 184107   |\n",
            "----------------------------------\n",
            "Num timesteps: 787000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 3.06\n",
            "Num timesteps: 788000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.78\n",
            "Num timesteps: 789000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.92\n",
            "Num timesteps: 790000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 3.27\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | 3.4      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2056     |\n",
            "|    fps              | 599      |\n",
            "|    time_elapsed     | 1317     |\n",
            "|    total_timesteps  | 790432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.53     |\n",
            "|    n_updates        | 185107   |\n",
            "----------------------------------\n",
            "Num timesteps: 791000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 3.40\n",
            "Num timesteps: 792000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 3.76\n",
            "Num timesteps: 793000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 4.20\n",
            "Num timesteps: 794000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 4.35\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | 4.89     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2060     |\n",
            "|    fps              | 599      |\n",
            "|    time_elapsed     | 1325     |\n",
            "|    total_timesteps  | 794432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.5      |\n",
            "|    n_updates        | 186107   |\n",
            "----------------------------------\n",
            "Num timesteps: 795000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 4.89\n",
            "Num timesteps: 796000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 4.79\n",
            "Num timesteps: 797000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 4.43\n",
            "Num timesteps: 798000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 4.12\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | 3.82     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2064     |\n",
            "|    fps              | 598      |\n",
            "|    time_elapsed     | 1333     |\n",
            "|    total_timesteps  | 798432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.27     |\n",
            "|    n_updates        | 187107   |\n",
            "----------------------------------\n",
            "Num timesteps: 799000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 3.82\n",
            "Num timesteps: 800000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 3.56\n",
            "Num timesteps: 801000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 3.76\n",
            "Num timesteps: 802000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 4.12\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | 4.03     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2068     |\n",
            "|    fps              | 598      |\n",
            "|    time_elapsed     | 1341     |\n",
            "|    total_timesteps  | 802432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.243    |\n",
            "|    n_updates        | 188107   |\n",
            "----------------------------------\n",
            "Num timesteps: 803000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 4.03\n",
            "Num timesteps: 804000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 3.81\n",
            "Num timesteps: 805000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 3.51\n",
            "Num timesteps: 806000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 4.27\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | 4.14     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2072     |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 1349     |\n",
            "|    total_timesteps  | 806432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.247    |\n",
            "|    n_updates        | 189107   |\n",
            "----------------------------------\n",
            "Num timesteps: 807000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 4.14\n",
            "Num timesteps: 808000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 4.05\n",
            "Num timesteps: 809000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 4.10\n",
            "Num timesteps: 810000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 4.06\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | 4.02     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2076     |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 1358     |\n",
            "|    total_timesteps  | 810432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.05     |\n",
            "|    n_updates        | 190107   |\n",
            "----------------------------------\n",
            "Num timesteps: 811000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 4.02\n",
            "Num timesteps: 812000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 3.88\n",
            "Num timesteps: 813000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 3.78\n",
            "Num timesteps: 814000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 3.58\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | 4.02     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2080     |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 1366     |\n",
            "|    total_timesteps  | 814432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.511    |\n",
            "|    n_updates        | 191107   |\n",
            "----------------------------------\n",
            "Num timesteps: 815000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 4.02\n",
            "Num timesteps: 816000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 3.63\n",
            "Num timesteps: 817000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 3.57\n",
            "Num timesteps: 818000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 3.57\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | 3.99     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2084     |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 1373     |\n",
            "|    total_timesteps  | 818432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.256    |\n",
            "|    n_updates        | 192107   |\n",
            "----------------------------------\n",
            "Num timesteps: 819000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 3.99\n",
            "Num timesteps: 820000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 4.21\n",
            "Num timesteps: 821000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 3.57\n",
            "Num timesteps: 822000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 3.51\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | 3.35     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2088     |\n",
            "|    fps              | 594      |\n",
            "|    time_elapsed     | 1382     |\n",
            "|    total_timesteps  | 822432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.677    |\n",
            "|    n_updates        | 193107   |\n",
            "----------------------------------\n",
            "Num timesteps: 823000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 3.35\n",
            "Num timesteps: 824000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 3.65\n",
            "Num timesteps: 825000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 4.12\n",
            "Num timesteps: 826000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 4.34\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | 4.38     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2092     |\n",
            "|    fps              | 594      |\n",
            "|    time_elapsed     | 1391     |\n",
            "|    total_timesteps  | 826432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.268    |\n",
            "|    n_updates        | 194107   |\n",
            "----------------------------------\n",
            "Num timesteps: 827000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 4.38\n",
            "Num timesteps: 828000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 4.47\n",
            "Num timesteps: 829000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 4.64\n",
            "Num timesteps: 830000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 4.39\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | 3.93     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2096     |\n",
            "|    fps              | 593      |\n",
            "|    time_elapsed     | 1398     |\n",
            "|    total_timesteps  | 830432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.409    |\n",
            "|    n_updates        | 195107   |\n",
            "----------------------------------\n",
            "Num timesteps: 831000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 3.93\n",
            "Num timesteps: 832000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 3.71\n",
            "Num timesteps: 833000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 4.09\n",
            "Num timesteps: 834000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 3.78\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | 3.56     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2100     |\n",
            "|    fps              | 593      |\n",
            "|    time_elapsed     | 1406     |\n",
            "|    total_timesteps  | 834432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.245    |\n",
            "|    n_updates        | 196107   |\n",
            "----------------------------------\n",
            "Num timesteps: 835000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 3.56\n",
            "Num timesteps: 836000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 3.36\n",
            "Num timesteps: 837000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 3.08\n",
            "Num timesteps: 838000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.99\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | 2.55     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2104     |\n",
            "|    fps              | 592      |\n",
            "|    time_elapsed     | 1414     |\n",
            "|    total_timesteps  | 838432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.406    |\n",
            "|    n_updates        | 197107   |\n",
            "----------------------------------\n",
            "Num timesteps: 839000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.55\n",
            "Num timesteps: 840000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.45\n",
            "Num timesteps: 841000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.13\n",
            "Num timesteps: 842000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.73\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | 2.73     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2108     |\n",
            "|    fps              | 591      |\n",
            "|    time_elapsed     | 1423     |\n",
            "|    total_timesteps  | 842432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.71     |\n",
            "|    n_updates        | 198107   |\n",
            "----------------------------------\n",
            "Num timesteps: 843000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.73\n",
            "Num timesteps: 844000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 3.18\n",
            "Num timesteps: 845000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 3.23\n",
            "Num timesteps: 846000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 3.62\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | 2.87     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2112     |\n",
            "|    fps              | 591      |\n",
            "|    time_elapsed     | 1431     |\n",
            "|    total_timesteps  | 846432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.47     |\n",
            "|    n_updates        | 199107   |\n",
            "----------------------------------\n",
            "Num timesteps: 847000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.87\n",
            "Num timesteps: 848000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.54\n",
            "Num timesteps: 849000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.60\n",
            "Num timesteps: 850000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.54\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | 2.63     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2116     |\n",
            "|    fps              | 590      |\n",
            "|    time_elapsed     | 1438     |\n",
            "|    total_timesteps  | 850432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.527    |\n",
            "|    n_updates        | 200107   |\n",
            "----------------------------------\n",
            "Num timesteps: 851000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.63\n",
            "Num timesteps: 852000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.69\n",
            "Num timesteps: 853000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.62\n",
            "Num timesteps: 854000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.31\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | 2.51     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2120     |\n",
            "|    fps              | 590      |\n",
            "|    time_elapsed     | 1446     |\n",
            "|    total_timesteps  | 854432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.207    |\n",
            "|    n_updates        | 201107   |\n",
            "----------------------------------\n",
            "Num timesteps: 855000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.51\n",
            "Num timesteps: 856000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.40\n",
            "Num timesteps: 857000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.28\n",
            "Num timesteps: 858000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.56\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | 2.25     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2124     |\n",
            "|    fps              | 589      |\n",
            "|    time_elapsed     | 1455     |\n",
            "|    total_timesteps  | 858432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.422    |\n",
            "|    n_updates        | 202107   |\n",
            "----------------------------------\n",
            "Num timesteps: 859000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.25\n",
            "Num timesteps: 860000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.25\n",
            "Num timesteps: 861000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.07\n",
            "Num timesteps: 862000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 1.80\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | 1.77     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2128     |\n",
            "|    fps              | 589      |\n",
            "|    time_elapsed     | 1462     |\n",
            "|    total_timesteps  | 862432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.47     |\n",
            "|    n_updates        | 203107   |\n",
            "----------------------------------\n",
            "Num timesteps: 863000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 1.77\n",
            "Num timesteps: 864000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 1.47\n",
            "Num timesteps: 865000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 1.20\n",
            "Num timesteps: 866000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 1.61\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | 1.96     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2132     |\n",
            "|    fps              | 588      |\n",
            "|    time_elapsed     | 1471     |\n",
            "|    total_timesteps  | 866432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.478    |\n",
            "|    n_updates        | 204107   |\n",
            "----------------------------------\n",
            "Num timesteps: 867000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 1.96\n",
            "Num timesteps: 868000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.34\n",
            "Num timesteps: 869000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.33\n",
            "Num timesteps: 870000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.22\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | 1.54     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2136     |\n",
            "|    fps              | 588      |\n",
            "|    time_elapsed     | 1478     |\n",
            "|    total_timesteps  | 870432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.485    |\n",
            "|    n_updates        | 205107   |\n",
            "----------------------------------\n",
            "Num timesteps: 871000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 1.54\n",
            "Num timesteps: 872000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 1.62\n",
            "Num timesteps: 873000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 1.39\n",
            "Num timesteps: 874000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.89\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | 0.731    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2140     |\n",
            "|    fps              | 588      |\n",
            "|    time_elapsed     | 1486     |\n",
            "|    total_timesteps  | 874432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.227    |\n",
            "|    n_updates        | 206107   |\n",
            "----------------------------------\n",
            "Num timesteps: 875000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.73\n",
            "Num timesteps: 876000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.21\n",
            "Num timesteps: 877000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.05\n",
            "Num timesteps: 878000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.12\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -0.167   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2144     |\n",
            "|    fps              | 588      |\n",
            "|    time_elapsed     | 1493     |\n",
            "|    total_timesteps  | 878432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.284    |\n",
            "|    n_updates        | 207107   |\n",
            "----------------------------------\n",
            "Num timesteps: 879000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.17\n",
            "Num timesteps: 880000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.24\n",
            "Num timesteps: 881000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.16\n",
            "Num timesteps: 882000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.35\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | 0.273    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2148     |\n",
            "|    fps              | 587      |\n",
            "|    time_elapsed     | 1502     |\n",
            "|    total_timesteps  | 882432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.295    |\n",
            "|    n_updates        | 208107   |\n",
            "----------------------------------\n",
            "Num timesteps: 883000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.27\n",
            "Num timesteps: 884000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.43\n",
            "Num timesteps: 885000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.43\n",
            "Num timesteps: 886000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.37\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -0.0144  |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2152     |\n",
            "|    fps              | 586      |\n",
            "|    time_elapsed     | 1511     |\n",
            "|    total_timesteps  | 886432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.315    |\n",
            "|    n_updates        | 209107   |\n",
            "----------------------------------\n",
            "Num timesteps: 887000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.01\n",
            "Num timesteps: 888000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.32\n",
            "Num timesteps: 889000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.26\n",
            "Num timesteps: 890000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.02\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -0.599   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2156     |\n",
            "|    fps              | 586      |\n",
            "|    time_elapsed     | 1519     |\n",
            "|    total_timesteps  | 890432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.207    |\n",
            "|    n_updates        | 210107   |\n",
            "----------------------------------\n",
            "Num timesteps: 891000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.60\n",
            "Num timesteps: 892000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.97\n",
            "Num timesteps: 893000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.48\n",
            "Num timesteps: 894000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.18\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -1.47    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2160     |\n",
            "|    fps              | 585      |\n",
            "|    time_elapsed     | 1527     |\n",
            "|    total_timesteps  | 894432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.15     |\n",
            "|    n_updates        | 211107   |\n",
            "----------------------------------\n",
            "Num timesteps: 895000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.47\n",
            "Num timesteps: 896000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.48\n",
            "Num timesteps: 897000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.80\n",
            "Num timesteps: 898000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.61\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -1.29    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2164     |\n",
            "|    fps              | 584      |\n",
            "|    time_elapsed     | 1536     |\n",
            "|    total_timesteps  | 898432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.801    |\n",
            "|    n_updates        | 212107   |\n",
            "----------------------------------\n",
            "Num timesteps: 899000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.29\n",
            "Num timesteps: 900000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.45\n",
            "Num timesteps: 901000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.08\n",
            "Num timesteps: 902000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.19\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -1.82    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2168     |\n",
            "|    fps              | 584      |\n",
            "|    time_elapsed     | 1544     |\n",
            "|    total_timesteps  | 902432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.04     |\n",
            "|    n_updates        | 213107   |\n",
            "----------------------------------\n",
            "Num timesteps: 903000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.82\n",
            "Num timesteps: 904000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.76\n",
            "Num timesteps: 905000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.36\n",
            "Num timesteps: 906000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.95\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -1.6     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2172     |\n",
            "|    fps              | 583      |\n",
            "|    time_elapsed     | 1553     |\n",
            "|    total_timesteps  | 906432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.12     |\n",
            "|    n_updates        | 214107   |\n",
            "----------------------------------\n",
            "Num timesteps: 907000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.60\n",
            "Num timesteps: 908000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.68\n",
            "Num timesteps: 909000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.82\n",
            "Num timesteps: 910000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.79\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -1.98    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2176     |\n",
            "|    fps              | 583      |\n",
            "|    time_elapsed     | 1561     |\n",
            "|    total_timesteps  | 910432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.448    |\n",
            "|    n_updates        | 215107   |\n",
            "----------------------------------\n",
            "Num timesteps: 911000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.98\n",
            "Num timesteps: 912000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.92\n",
            "Num timesteps: 913000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.31\n",
            "Num timesteps: 914000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.69\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -1.92    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2180     |\n",
            "|    fps              | 582      |\n",
            "|    time_elapsed     | 1569     |\n",
            "|    total_timesteps  | 914432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.263    |\n",
            "|    n_updates        | 216107   |\n",
            "----------------------------------\n",
            "Num timesteps: 915000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.92\n",
            "Num timesteps: 916000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.06\n",
            "Num timesteps: 917000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.43\n",
            "Num timesteps: 918000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.61\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -3.6     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2184     |\n",
            "|    fps              | 581      |\n",
            "|    time_elapsed     | 1578     |\n",
            "|    total_timesteps  | 918432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.74     |\n",
            "|    n_updates        | 217107   |\n",
            "----------------------------------\n",
            "Num timesteps: 919000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.60\n",
            "Num timesteps: 920000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.13\n",
            "Num timesteps: 921000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.82\n",
            "Num timesteps: 922000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.39\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -3.42    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2188     |\n",
            "|    fps              | 581      |\n",
            "|    time_elapsed     | 1587     |\n",
            "|    total_timesteps  | 922432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.457    |\n",
            "|    n_updates        | 218107   |\n",
            "----------------------------------\n",
            "Num timesteps: 923000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.42\n",
            "Num timesteps: 924000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.50\n",
            "Num timesteps: 925000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.08\n",
            "Num timesteps: 926000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.41\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -4.36    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2192     |\n",
            "|    fps              | 580      |\n",
            "|    time_elapsed     | 1595     |\n",
            "|    total_timesteps  | 926432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.204    |\n",
            "|    n_updates        | 219107   |\n",
            "----------------------------------\n",
            "Num timesteps: 927000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.36\n",
            "Num timesteps: 928000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.00\n",
            "Num timesteps: 929000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.44\n",
            "Num timesteps: 930000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.75\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -4.66    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2196     |\n",
            "|    fps              | 580      |\n",
            "|    time_elapsed     | 1603     |\n",
            "|    total_timesteps  | 930432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.408    |\n",
            "|    n_updates        | 220107   |\n",
            "----------------------------------\n",
            "Num timesteps: 931000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.66\n",
            "Num timesteps: 932000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.03\n",
            "Num timesteps: 933000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.16\n",
            "Num timesteps: 934000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.25\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5.36    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2200     |\n",
            "|    fps              | 580      |\n",
            "|    time_elapsed     | 1610     |\n",
            "|    total_timesteps  | 934432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.625    |\n",
            "|    n_updates        | 221107   |\n",
            "----------------------------------\n",
            "Num timesteps: 935000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.36\n",
            "Num timesteps: 936000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.47\n",
            "Num timesteps: 937000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.18\n",
            "Num timesteps: 938000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.90\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -4.27    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2204     |\n",
            "|    fps              | 579      |\n",
            "|    time_elapsed     | 1618     |\n",
            "|    total_timesteps  | 938432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.68     |\n",
            "|    n_updates        | 222107   |\n",
            "----------------------------------\n",
            "Num timesteps: 939000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.27\n",
            "Num timesteps: 940000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.00\n",
            "Num timesteps: 941000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.16\n",
            "Num timesteps: 942000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.61\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -4.71    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2208     |\n",
            "|    fps              | 579      |\n",
            "|    time_elapsed     | 1626     |\n",
            "|    total_timesteps  | 942432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.12     |\n",
            "|    n_updates        | 223107   |\n",
            "----------------------------------\n",
            "Num timesteps: 943000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.71\n",
            "Num timesteps: 944000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.78\n",
            "Num timesteps: 945000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.08\n",
            "Num timesteps: 946000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.40\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5       |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2212     |\n",
            "|    fps              | 579      |\n",
            "|    time_elapsed     | 1634     |\n",
            "|    total_timesteps  | 946432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.293    |\n",
            "|    n_updates        | 224107   |\n",
            "----------------------------------\n",
            "Num timesteps: 947000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.00\n",
            "Num timesteps: 948000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.67\n",
            "Num timesteps: 949000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.72\n",
            "Num timesteps: 950000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.83\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -4.92    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2216     |\n",
            "|    fps              | 578      |\n",
            "|    time_elapsed     | 1642     |\n",
            "|    total_timesteps  | 950432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.193    |\n",
            "|    n_updates        | 225107   |\n",
            "----------------------------------\n",
            "Num timesteps: 951000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.92\n",
            "Num timesteps: 952000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.59\n",
            "Num timesteps: 953000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.28\n",
            "Num timesteps: 954000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.51\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -6.14    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2220     |\n",
            "|    fps              | 578      |\n",
            "|    time_elapsed     | 1650     |\n",
            "|    total_timesteps  | 954432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.193    |\n",
            "|    n_updates        | 226107   |\n",
            "----------------------------------\n",
            "Num timesteps: 955000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.14\n",
            "Num timesteps: 956000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.98\n",
            "Num timesteps: 957000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.08\n",
            "Num timesteps: 958000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.95\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5.92    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2224     |\n",
            "|    fps              | 577      |\n",
            "|    time_elapsed     | 1658     |\n",
            "|    total_timesteps  | 958432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.959    |\n",
            "|    n_updates        | 227107   |\n",
            "----------------------------------\n",
            "Num timesteps: 959000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.92\n",
            "Num timesteps: 960000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.85\n",
            "Num timesteps: 961000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.83\n",
            "Num timesteps: 962000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.43\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5.5     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2228     |\n",
            "|    fps              | 577      |\n",
            "|    time_elapsed     | 1667     |\n",
            "|    total_timesteps  | 962432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.203    |\n",
            "|    n_updates        | 228107   |\n",
            "----------------------------------\n",
            "Num timesteps: 963000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.50\n",
            "Num timesteps: 964000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.05\n",
            "Num timesteps: 965000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.37\n",
            "Num timesteps: 966000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.84\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -6.58    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2232     |\n",
            "|    fps              | 576      |\n",
            "|    time_elapsed     | 1675     |\n",
            "|    total_timesteps  | 966432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.258    |\n",
            "|    n_updates        | 229107   |\n",
            "----------------------------------\n",
            "Num timesteps: 967000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.58\n",
            "Num timesteps: 968000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -7.02\n",
            "Num timesteps: 969000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -7.30\n",
            "Num timesteps: 970000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -7.39\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -6.96    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2236     |\n",
            "|    fps              | 576      |\n",
            "|    time_elapsed     | 1684     |\n",
            "|    total_timesteps  | 970432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.54     |\n",
            "|    n_updates        | 230107   |\n",
            "----------------------------------\n",
            "Num timesteps: 971000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.96\n",
            "Num timesteps: 972000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -7.45\n",
            "Num timesteps: 973000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -7.35\n",
            "Num timesteps: 974000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -7.27\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -7.67    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2240     |\n",
            "|    fps              | 575      |\n",
            "|    time_elapsed     | 1692     |\n",
            "|    total_timesteps  | 974432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.293    |\n",
            "|    n_updates        | 231107   |\n",
            "----------------------------------\n",
            "Num timesteps: 975000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -7.67\n",
            "Num timesteps: 976000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -7.61\n",
            "Num timesteps: 977000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -7.84\n",
            "Num timesteps: 978000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -7.99\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -8.07    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2244     |\n",
            "|    fps              | 575      |\n",
            "|    time_elapsed     | 1700     |\n",
            "|    total_timesteps  | 978432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.482    |\n",
            "|    n_updates        | 232107   |\n",
            "----------------------------------\n",
            "Num timesteps: 979000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -8.07\n",
            "Num timesteps: 980000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -8.49\n",
            "Num timesteps: 981000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -8.56\n",
            "Num timesteps: 982000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -8.77\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -8.75    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2248     |\n",
            "|    fps              | 574      |\n",
            "|    time_elapsed     | 1709     |\n",
            "|    total_timesteps  | 982432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.22     |\n",
            "|    n_updates        | 233107   |\n",
            "----------------------------------\n",
            "Num timesteps: 983000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -8.75\n",
            "Num timesteps: 984000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -8.92\n",
            "Num timesteps: 985000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -9.08\n",
            "Num timesteps: 986000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -8.95\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -8.58    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2252     |\n",
            "|    fps              | 574      |\n",
            "|    time_elapsed     | 1717     |\n",
            "|    total_timesteps  | 986432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.365    |\n",
            "|    n_updates        | 234107   |\n",
            "----------------------------------\n",
            "Num timesteps: 987000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -8.58\n",
            "Num timesteps: 988000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -8.61\n",
            "Num timesteps: 989000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -8.59\n",
            "Num timesteps: 990000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -8.44\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -7.84    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2256     |\n",
            "|    fps              | 573      |\n",
            "|    time_elapsed     | 1725     |\n",
            "|    total_timesteps  | 990432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.265    |\n",
            "|    n_updates        | 235107   |\n",
            "----------------------------------\n",
            "Num timesteps: 991000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -7.84\n",
            "Num timesteps: 992000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -7.68\n",
            "Num timesteps: 993000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -7.04\n",
            "Num timesteps: 994000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -7.77\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -7.43    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2260     |\n",
            "|    fps              | 573      |\n",
            "|    time_elapsed     | 1733     |\n",
            "|    total_timesteps  | 994432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.604    |\n",
            "|    n_updates        | 236107   |\n",
            "----------------------------------\n",
            "Num timesteps: 995000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -7.43\n",
            "Num timesteps: 996000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -7.35\n",
            "Num timesteps: 997000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.78\n",
            "Num timesteps: 998000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.67\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -6.85    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2264     |\n",
            "|    fps              | 573      |\n",
            "|    time_elapsed     | 1742     |\n",
            "|    total_timesteps  | 998432   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.541    |\n",
            "|    n_updates        | 237107   |\n",
            "----------------------------------\n",
            "Num timesteps: 999000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.85\n",
            "Num timesteps: 1000000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.56\n",
            "Num timesteps: 1001000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.36\n",
            "Num timesteps: 1002000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.33\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -6.77    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2268     |\n",
            "|    fps              | 572      |\n",
            "|    time_elapsed     | 1749     |\n",
            "|    total_timesteps  | 1002432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.208    |\n",
            "|    n_updates        | 238107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1003000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.77\n",
            "Num timesteps: 1004000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.82\n",
            "Num timesteps: 1005000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.57\n",
            "Num timesteps: 1006000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.45\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -6.59    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2272     |\n",
            "|    fps              | 572      |\n",
            "|    time_elapsed     | 1757     |\n",
            "|    total_timesteps  | 1006432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.418    |\n",
            "|    n_updates        | 239107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1007000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.59\n",
            "Num timesteps: 1008000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.64\n",
            "Num timesteps: 1009000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.74\n",
            "Num timesteps: 1010000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.91\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -7.16    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2276     |\n",
            "|    fps              | 572      |\n",
            "|    time_elapsed     | 1765     |\n",
            "|    total_timesteps  | 1010432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.272    |\n",
            "|    n_updates        | 240107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1011000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -7.16\n",
            "Num timesteps: 1012000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -7.06\n",
            "Num timesteps: 1013000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.36\n",
            "Num timesteps: 1014000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.50\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5.89    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2280     |\n",
            "|    fps              | 572      |\n",
            "|    time_elapsed     | 1773     |\n",
            "|    total_timesteps  | 1014432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.246    |\n",
            "|    n_updates        | 241107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1015000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.89\n",
            "Num timesteps: 1016000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.38\n",
            "Num timesteps: 1017000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.42\n",
            "Num timesteps: 1018000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.29\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -4.45    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2284     |\n",
            "|    fps              | 571      |\n",
            "|    time_elapsed     | 1781     |\n",
            "|    total_timesteps  | 1018432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.305    |\n",
            "|    n_updates        | 242107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1019000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.45\n",
            "Num timesteps: 1020000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.14\n",
            "Num timesteps: 1021000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.28\n",
            "Num timesteps: 1022000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.23\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -4.39    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2288     |\n",
            "|    fps              | 571      |\n",
            "|    time_elapsed     | 1789     |\n",
            "|    total_timesteps  | 1022432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.141    |\n",
            "|    n_updates        | 243107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1023000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.39\n",
            "Num timesteps: 1024000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.51\n",
            "Num timesteps: 1025000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.74\n",
            "Num timesteps: 1026000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.64\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -3.75    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2292     |\n",
            "|    fps              | 570      |\n",
            "|    time_elapsed     | 1797     |\n",
            "|    total_timesteps  | 1026432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.373    |\n",
            "|    n_updates        | 244107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1027000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.75\n",
            "Num timesteps: 1028000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.95\n",
            "Num timesteps: 1029000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.21\n",
            "Num timesteps: 1030000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.99\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -3.81    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2296     |\n",
            "|    fps              | 570      |\n",
            "|    time_elapsed     | 1805     |\n",
            "|    total_timesteps  | 1030432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.05     |\n",
            "|    n_updates        | 245107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1031000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.81\n",
            "Num timesteps: 1032000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.33\n",
            "Num timesteps: 1033000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.62\n",
            "Num timesteps: 1034000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.57\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -2.84    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2300     |\n",
            "|    fps              | 570      |\n",
            "|    time_elapsed     | 1813     |\n",
            "|    total_timesteps  | 1034432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.281    |\n",
            "|    n_updates        | 246107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1035000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.84\n",
            "Num timesteps: 1036000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.96\n",
            "Num timesteps: 1037000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.46\n",
            "Num timesteps: 1038000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.47\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -3.87    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2304     |\n",
            "|    fps              | 569      |\n",
            "|    time_elapsed     | 1821     |\n",
            "|    total_timesteps  | 1038432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.357    |\n",
            "|    n_updates        | 247107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1039000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.87\n",
            "Num timesteps: 1040000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.92\n",
            "Num timesteps: 1041000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.72\n",
            "Num timesteps: 1042000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.76\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -3.61    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2308     |\n",
            "|    fps              | 569      |\n",
            "|    time_elapsed     | 1831     |\n",
            "|    total_timesteps  | 1042432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.331    |\n",
            "|    n_updates        | 248107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1043000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.61\n",
            "Num timesteps: 1044000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.57\n",
            "Num timesteps: 1045000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.09\n",
            "Num timesteps: 1046000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.31\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -3.23    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2312     |\n",
            "|    fps              | 568      |\n",
            "|    time_elapsed     | 1839     |\n",
            "|    total_timesteps  | 1046432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.281    |\n",
            "|    n_updates        | 249107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1047000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.23\n",
            "Num timesteps: 1048000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.66\n",
            "Num timesteps: 1049000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.71\n",
            "Num timesteps: 1050000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.37\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -3.17    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2316     |\n",
            "|    fps              | 568      |\n",
            "|    time_elapsed     | 1847     |\n",
            "|    total_timesteps  | 1050432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.199    |\n",
            "|    n_updates        | 250107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1051000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.17\n",
            "Num timesteps: 1052000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.93\n",
            "Num timesteps: 1053000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.23\n",
            "Num timesteps: 1054000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.80\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -2.53    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2320     |\n",
            "|    fps              | 568      |\n",
            "|    time_elapsed     | 1855     |\n",
            "|    total_timesteps  | 1054432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.427    |\n",
            "|    n_updates        | 251107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1055000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.53\n",
            "Num timesteps: 1056000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.22\n",
            "Num timesteps: 1057000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.37\n",
            "Num timesteps: 1058000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.57\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -3.42    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2324     |\n",
            "|    fps              | 568      |\n",
            "|    time_elapsed     | 1863     |\n",
            "|    total_timesteps  | 1058432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.299    |\n",
            "|    n_updates        | 252107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1059000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.42\n",
            "Num timesteps: 1060000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.74\n",
            "Num timesteps: 1061000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.12\n",
            "Num timesteps: 1062000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.27\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -3.8     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2328     |\n",
            "|    fps              | 567      |\n",
            "|    time_elapsed     | 1871     |\n",
            "|    total_timesteps  | 1062432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.298    |\n",
            "|    n_updates        | 253107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1063000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.80\n",
            "Num timesteps: 1064000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.72\n",
            "Num timesteps: 1065000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.63\n",
            "Num timesteps: 1066000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.00\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -3.46    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2332     |\n",
            "|    fps              | 567      |\n",
            "|    time_elapsed     | 1879     |\n",
            "|    total_timesteps  | 1066432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.3      |\n",
            "|    n_updates        | 254107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1067000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.46\n",
            "Num timesteps: 1068000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.51\n",
            "Num timesteps: 1069000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.29\n",
            "Num timesteps: 1070000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.13\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -3.59    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2336     |\n",
            "|    fps              | 567      |\n",
            "|    time_elapsed     | 1887     |\n",
            "|    total_timesteps  | 1070432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.119    |\n",
            "|    n_updates        | 255107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1071000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.59\n",
            "Num timesteps: 1072000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.82\n",
            "Num timesteps: 1073000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.59\n",
            "Num timesteps: 1074000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.81\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -3.66    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2340     |\n",
            "|    fps              | 566      |\n",
            "|    time_elapsed     | 1896     |\n",
            "|    total_timesteps  | 1074432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.17     |\n",
            "|    n_updates        | 256107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1075000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.66\n",
            "Num timesteps: 1076000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.43\n",
            "Num timesteps: 1077000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.24\n",
            "Num timesteps: 1078000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.88\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -2.85    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2344     |\n",
            "|    fps              | 566      |\n",
            "|    time_elapsed     | 1904     |\n",
            "|    total_timesteps  | 1078432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.418    |\n",
            "|    n_updates        | 257107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1079000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.85\n",
            "Num timesteps: 1080000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.46\n",
            "Num timesteps: 1081000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.09\n",
            "Num timesteps: 1082000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.51\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -1.59    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2348     |\n",
            "|    fps              | 566      |\n",
            "|    time_elapsed     | 1911     |\n",
            "|    total_timesteps  | 1082432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.74     |\n",
            "|    n_updates        | 258107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1083000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.59\n",
            "Num timesteps: 1084000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.43\n",
            "Num timesteps: 1085000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.63\n",
            "Num timesteps: 1086000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.18\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -2.59    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2352     |\n",
            "|    fps              | 565      |\n",
            "|    time_elapsed     | 1919     |\n",
            "|    total_timesteps  | 1086432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.202    |\n",
            "|    n_updates        | 259107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1087000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.59\n",
            "Num timesteps: 1088000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.77\n",
            "Num timesteps: 1089000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.16\n",
            "Num timesteps: 1090000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.76\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -4.14    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2356     |\n",
            "|    fps              | 565      |\n",
            "|    time_elapsed     | 1928     |\n",
            "|    total_timesteps  | 1090432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.222    |\n",
            "|    n_updates        | 260107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1091000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.14\n",
            "Num timesteps: 1092000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.54\n",
            "Num timesteps: 1093000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.76\n",
            "Num timesteps: 1094000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.51\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5.05    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2360     |\n",
            "|    fps              | 565      |\n",
            "|    time_elapsed     | 1936     |\n",
            "|    total_timesteps  | 1094432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.367    |\n",
            "|    n_updates        | 261107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1095000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.05\n",
            "Num timesteps: 1096000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.76\n",
            "Num timesteps: 1097000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.39\n",
            "Num timesteps: 1098000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.60\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -5.82    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2364     |\n",
            "|    fps              | 564      |\n",
            "|    time_elapsed     | 1944     |\n",
            "|    total_timesteps  | 1098432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.339    |\n",
            "|    n_updates        | 262107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1099000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.82\n",
            "Num timesteps: 1100000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.18\n",
            "Num timesteps: 1101000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.18\n",
            "Num timesteps: 1102000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -7.02\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -6.61    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2368     |\n",
            "|    fps              | 564      |\n",
            "|    time_elapsed     | 1953     |\n",
            "|    total_timesteps  | 1102432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.233    |\n",
            "|    n_updates        | 263107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1103000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.61\n",
            "Num timesteps: 1104000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.49\n",
            "Num timesteps: 1105000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -7.04\n",
            "Num timesteps: 1106000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -7.06\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -7.35    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2372     |\n",
            "|    fps              | 563      |\n",
            "|    time_elapsed     | 1961     |\n",
            "|    total_timesteps  | 1106432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.278    |\n",
            "|    n_updates        | 264107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1107000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -7.35\n",
            "Num timesteps: 1108000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -7.04\n",
            "Num timesteps: 1109000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.84\n",
            "Num timesteps: 1110000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.83\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -6.31    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2376     |\n",
            "|    fps              | 563      |\n",
            "|    time_elapsed     | 1969     |\n",
            "|    total_timesteps  | 1110432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.112    |\n",
            "|    n_updates        | 265107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1111000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.31\n",
            "Num timesteps: 1112000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.25\n",
            "Num timesteps: 1113000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.85\n",
            "Num timesteps: 1114000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -7.29\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -7.69    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2380     |\n",
            "|    fps              | 563      |\n",
            "|    time_elapsed     | 1977     |\n",
            "|    total_timesteps  | 1114432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.233    |\n",
            "|    n_updates        | 266107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1115000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -7.69\n",
            "Num timesteps: 1116000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -7.92\n",
            "Num timesteps: 1117000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -8.00\n",
            "Num timesteps: 1118000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -7.85\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -8.96    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2384     |\n",
            "|    fps              | 563      |\n",
            "|    time_elapsed     | 1985     |\n",
            "|    total_timesteps  | 1118432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.141    |\n",
            "|    n_updates        | 267107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1119000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -8.96\n",
            "Num timesteps: 1120000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -9.00\n",
            "Num timesteps: 1121000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -8.71\n",
            "Num timesteps: 1122000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -8.71\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -8.5     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2388     |\n",
            "|    fps              | 562      |\n",
            "|    time_elapsed     | 1993     |\n",
            "|    total_timesteps  | 1122432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.563    |\n",
            "|    n_updates        | 268107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1123000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -8.50\n",
            "Num timesteps: 1124000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -8.64\n",
            "Num timesteps: 1125000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -8.87\n",
            "Num timesteps: 1126000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -8.92\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -9.11    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2392     |\n",
            "|    fps              | 562      |\n",
            "|    time_elapsed     | 2001     |\n",
            "|    total_timesteps  | 1126432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.264    |\n",
            "|    n_updates        | 269107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1127000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -9.11\n",
            "Num timesteps: 1128000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -9.06\n",
            "Num timesteps: 1129000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -8.92\n",
            "Num timesteps: 1130000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -8.88\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -8.42    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2396     |\n",
            "|    fps              | 562      |\n",
            "|    time_elapsed     | 2009     |\n",
            "|    total_timesteps  | 1130432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.208    |\n",
            "|    n_updates        | 270107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1131000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -8.42\n",
            "Num timesteps: 1132000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -8.84\n",
            "Num timesteps: 1133000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -9.06\n",
            "Num timesteps: 1134000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -9.11\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -9.33    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2400     |\n",
            "|    fps              | 562      |\n",
            "|    time_elapsed     | 2017     |\n",
            "|    total_timesteps  | 1134432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.33     |\n",
            "|    n_updates        | 271107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1135000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -9.33\n",
            "Num timesteps: 1136000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -9.23\n",
            "Num timesteps: 1137000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -9.11\n",
            "Num timesteps: 1138000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -9.60\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -9.73    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2404     |\n",
            "|    fps              | 561      |\n",
            "|    time_elapsed     | 2026     |\n",
            "|    total_timesteps  | 1138432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.247    |\n",
            "|    n_updates        | 272107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1139000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -9.73\n",
            "Num timesteps: 1140000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.00\n",
            "Num timesteps: 1141000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.08\n",
            "Num timesteps: 1142000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -9.71\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -9.82    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2408     |\n",
            "|    fps              | 561      |\n",
            "|    time_elapsed     | 2035     |\n",
            "|    total_timesteps  | 1142432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.332    |\n",
            "|    n_updates        | 273107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1143000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -9.82\n",
            "Num timesteps: 1144000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -9.90\n",
            "Num timesteps: 1145000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.68\n",
            "Num timesteps: 1146000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.44\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -10.5    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2412     |\n",
            "|    fps              | 560      |\n",
            "|    time_elapsed     | 2044     |\n",
            "|    total_timesteps  | 1146432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.305    |\n",
            "|    n_updates        | 274107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1147000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.47\n",
            "Num timesteps: 1148000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.27\n",
            "Num timesteps: 1149000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.11\n",
            "Num timesteps: 1150000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.50\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -10.5    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2416     |\n",
            "|    fps              | 560      |\n",
            "|    time_elapsed     | 2051     |\n",
            "|    total_timesteps  | 1150432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.212    |\n",
            "|    n_updates        | 275107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1151000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.48\n",
            "Num timesteps: 1152000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.50\n",
            "Num timesteps: 1153000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.64\n",
            "Num timesteps: 1154000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.73\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -10.7    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2420     |\n",
            "|    fps              | 560      |\n",
            "|    time_elapsed     | 2060     |\n",
            "|    total_timesteps  | 1154432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.32     |\n",
            "|    n_updates        | 276107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1155000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.70\n",
            "Num timesteps: 1156000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.33\n",
            "Num timesteps: 1157000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.49\n",
            "Num timesteps: 1158000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.53\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -11.3    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2424     |\n",
            "|    fps              | 560      |\n",
            "|    time_elapsed     | 2067     |\n",
            "|    total_timesteps  | 1158432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.259    |\n",
            "|    n_updates        | 277107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1159000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -11.28\n",
            "Num timesteps: 1160000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -11.36\n",
            "Num timesteps: 1161000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.76\n",
            "Num timesteps: 1162000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.70\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -11.1    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2428     |\n",
            "|    fps              | 560      |\n",
            "|    time_elapsed     | 2075     |\n",
            "|    total_timesteps  | 1162432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.467    |\n",
            "|    n_updates        | 278107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1163000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -11.09\n",
            "Num timesteps: 1164000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -11.59\n",
            "Num timesteps: 1165000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -11.43\n",
            "Num timesteps: 1166000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -11.09\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -11.4    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2432     |\n",
            "|    fps              | 560      |\n",
            "|    time_elapsed     | 2082     |\n",
            "|    total_timesteps  | 1166432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.238    |\n",
            "|    n_updates        | 279107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1167000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -11.37\n",
            "Num timesteps: 1168000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -11.46\n",
            "Num timesteps: 1169000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -11.30\n",
            "Num timesteps: 1170000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -11.40\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -11.1    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2436     |\n",
            "|    fps              | 560      |\n",
            "|    time_elapsed     | 2089     |\n",
            "|    total_timesteps  | 1170432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.268    |\n",
            "|    n_updates        | 280107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1171000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -11.06\n",
            "Num timesteps: 1172000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.67\n",
            "Num timesteps: 1173000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.51\n",
            "Num timesteps: 1174000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.32\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -10.5    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2440     |\n",
            "|    fps              | 559      |\n",
            "|    time_elapsed     | 2097     |\n",
            "|    total_timesteps  | 1174432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.225    |\n",
            "|    n_updates        | 281107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1175000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.47\n",
            "Num timesteps: 1176000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.68\n",
            "Num timesteps: 1177000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -11.13\n",
            "Num timesteps: 1178000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -11.42\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -11.4    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2444     |\n",
            "|    fps              | 559      |\n",
            "|    time_elapsed     | 2106     |\n",
            "|    total_timesteps  | 1178432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.82     |\n",
            "|    n_updates        | 282107   |\n",
            "----------------------------------\n",
            "Num timesteps: 1179000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -11.36\n",
            "Num timesteps: 1180000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -11.54\n",
            "Num timesteps: 1181000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -11.83\n",
            "Num timesteps: 1182000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -11.90\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 998      |\n",
            "|    ep_rew_mean      | -10.2    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2448     |\n",
            "|    fps              | 559      |\n",
            "|    time_elapsed     | 2114     |\n",
            "|    total_timesteps  | 1182203  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.203    |\n",
            "|    n_updates        | 283050   |\n",
            "----------------------------------\n",
            "Num timesteps: 1183000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.23\n",
            "Num timesteps: 1184000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.37\n",
            "Num timesteps: 1185000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.24\n",
            "Num timesteps: 1186000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -9.65\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 998      |\n",
            "|    ep_rew_mean      | -9.78    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2452     |\n",
            "|    fps              | 559      |\n",
            "|    time_elapsed     | 2121     |\n",
            "|    total_timesteps  | 1186203  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.22     |\n",
            "|    n_updates        | 284050   |\n",
            "----------------------------------\n",
            "Num timesteps: 1187000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -9.78\n",
            "Num timesteps: 1188000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -10.07\n",
            "Num timesteps: 1189000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -9.86\n",
            "Num timesteps: 1190000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -9.55\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 998      |\n",
            "|    ep_rew_mean      | -9.17    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2456     |\n",
            "|    fps              | 558      |\n",
            "|    time_elapsed     | 2129     |\n",
            "|    total_timesteps  | 1190203  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.141    |\n",
            "|    n_updates        | 285050   |\n",
            "----------------------------------\n",
            "Num timesteps: 1191000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -9.17\n",
            "Num timesteps: 1192000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -9.13\n",
            "Num timesteps: 1193000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -9.15\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 995      |\n",
            "|    ep_rew_mean      | -7.06    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2460     |\n",
            "|    fps              | 558      |\n",
            "|    time_elapsed     | 2137     |\n",
            "|    total_timesteps  | 1193935  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.21     |\n",
            "|    n_updates        | 285983   |\n",
            "----------------------------------\n",
            "Num timesteps: 1194000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -7.06\n",
            "Num timesteps: 1195000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -7.82\n",
            "Num timesteps: 1196000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -7.47\n",
            "Num timesteps: 1197000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -7.67\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 995      |\n",
            "|    ep_rew_mean      | -7.54    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2464     |\n",
            "|    fps              | 558      |\n",
            "|    time_elapsed     | 2146     |\n",
            "|    total_timesteps  | 1197935  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.217    |\n",
            "|    n_updates        | 286983   |\n",
            "----------------------------------\n",
            "Num timesteps: 1198000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -7.54\n",
            "Num timesteps: 1199000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -7.29\n",
            "Num timesteps: 1200000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -7.39\n",
            "Num timesteps: 1201000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.70\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 995      |\n",
            "|    ep_rew_mean      | -7.12    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2468     |\n",
            "|    fps              | 557      |\n",
            "|    time_elapsed     | 2154     |\n",
            "|    total_timesteps  | 1201935  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.127    |\n",
            "|    n_updates        | 287983   |\n",
            "----------------------------------\n",
            "Num timesteps: 1202000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -7.12\n",
            "Num timesteps: 1203000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.84\n",
            "Num timesteps: 1204000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.60\n",
            "Num timesteps: 1205000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.78\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 995      |\n",
            "|    ep_rew_mean      | -6.52    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2472     |\n",
            "|    fps              | 557      |\n",
            "|    time_elapsed     | 2162     |\n",
            "|    total_timesteps  | 1205935  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.197    |\n",
            "|    n_updates        | 288983   |\n",
            "----------------------------------\n",
            "Num timesteps: 1206000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.52\n",
            "Num timesteps: 1207000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.86\n",
            "Num timesteps: 1208000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.94\n",
            "Num timesteps: 1209000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -7.08\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 995      |\n",
            "|    ep_rew_mean      | -6.85    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2476     |\n",
            "|    fps              | 557      |\n",
            "|    time_elapsed     | 2170     |\n",
            "|    total_timesteps  | 1209935  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.259    |\n",
            "|    n_updates        | 289983   |\n",
            "----------------------------------\n",
            "Num timesteps: 1210000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.85\n",
            "Num timesteps: 1211000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -7.06\n",
            "Num timesteps: 1212000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -7.02\n",
            "Num timesteps: 1213000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.84\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 992      |\n",
            "|    ep_rew_mean      | -5.21    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2480     |\n",
            "|    fps              | 557      |\n",
            "|    time_elapsed     | 2178     |\n",
            "|    total_timesteps  | 1213640  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.197    |\n",
            "|    n_updates        | 290909   |\n",
            "----------------------------------\n",
            "Num timesteps: 1214000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.21\n",
            "Num timesteps: 1215000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.03\n",
            "Num timesteps: 1216000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.06\n",
            "Num timesteps: 1217000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.77\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 992      |\n",
            "|    ep_rew_mean      | -4.93    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2484     |\n",
            "|    fps              | 556      |\n",
            "|    time_elapsed     | 2186     |\n",
            "|    total_timesteps  | 1217640  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.313    |\n",
            "|    n_updates        | 291909   |\n",
            "----------------------------------\n",
            "Num timesteps: 1218000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.93\n",
            "Num timesteps: 1219000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.91\n",
            "Num timesteps: 1220000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.18\n",
            "Num timesteps: 1221000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.30\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 992      |\n",
            "|    ep_rew_mean      | -5.49    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2488     |\n",
            "|    fps              | 556      |\n",
            "|    time_elapsed     | 2194     |\n",
            "|    total_timesteps  | 1221640  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.306    |\n",
            "|    n_updates        | 292909   |\n",
            "----------------------------------\n",
            "Num timesteps: 1222000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.49\n",
            "Num timesteps: 1223000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.23\n",
            "Num timesteps: 1224000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.52\n",
            "Num timesteps: 1225000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.70\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 992      |\n",
            "|    ep_rew_mean      | -5.46    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2492     |\n",
            "|    fps              | 556      |\n",
            "|    time_elapsed     | 2203     |\n",
            "|    total_timesteps  | 1225640  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.197    |\n",
            "|    n_updates        | 293909   |\n",
            "----------------------------------\n",
            "Num timesteps: 1226000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.46\n",
            "Num timesteps: 1227000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.63\n",
            "Num timesteps: 1228000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.78\n",
            "Num timesteps: 1229000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.15\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 992      |\n",
            "|    ep_rew_mean      | -6.96    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2496     |\n",
            "|    fps              | 555      |\n",
            "|    time_elapsed     | 2213     |\n",
            "|    total_timesteps  | 1229640  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.149    |\n",
            "|    n_updates        | 294909   |\n",
            "----------------------------------\n",
            "Num timesteps: 1230000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.96\n",
            "Num timesteps: 1231000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -7.02\n",
            "Num timesteps: 1232000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.17\n",
            "Num timesteps: 1233000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.42\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 991      |\n",
            "|    ep_rew_mean      | -4.13    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2500     |\n",
            "|    fps              | 555      |\n",
            "|    time_elapsed     | 2220     |\n",
            "|    total_timesteps  | 1233514  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.205    |\n",
            "|    n_updates        | 295878   |\n",
            "----------------------------------\n",
            "Num timesteps: 1234000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.13\n",
            "Num timesteps: 1235000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.04\n",
            "Num timesteps: 1236000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.69\n",
            "Num timesteps: 1237000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.95\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 990      |\n",
            "|    ep_rew_mean      | -4.03    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2504     |\n",
            "|    fps              | 555      |\n",
            "|    time_elapsed     | 2228     |\n",
            "|    total_timesteps  | 1237461  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.195    |\n",
            "|    n_updates        | 296865   |\n",
            "----------------------------------\n",
            "Num timesteps: 1238000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.03\n",
            "Num timesteps: 1239000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.29\n",
            "Num timesteps: 1240000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.88\n",
            "Num timesteps: 1241000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.35\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 987      |\n",
            "|    ep_rew_mean      | -4.54    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2508     |\n",
            "|    fps              | 555      |\n",
            "|    time_elapsed     | 2236     |\n",
            "|    total_timesteps  | 1241154  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.212    |\n",
            "|    n_updates        | 297788   |\n",
            "----------------------------------\n",
            "Num timesteps: 1242000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.54\n",
            "Num timesteps: 1243000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.74\n",
            "Num timesteps: 1244000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.36\n",
            "Num timesteps: 1245000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.31\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 987      |\n",
            "|    ep_rew_mean      | -4.35    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2512     |\n",
            "|    fps              | 554      |\n",
            "|    time_elapsed     | 2244     |\n",
            "|    total_timesteps  | 1245154  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.283    |\n",
            "|    n_updates        | 298788   |\n",
            "----------------------------------\n",
            "Num timesteps: 1246000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.35\n",
            "Num timesteps: 1247000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.28\n",
            "Num timesteps: 1248000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.58\n",
            "Num timesteps: 1249000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.26\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 987      |\n",
            "|    ep_rew_mean      | -5.68    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2516     |\n",
            "|    fps              | 554      |\n",
            "|    time_elapsed     | 2252     |\n",
            "|    total_timesteps  | 1249154  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.168    |\n",
            "|    n_updates        | 299788   |\n",
            "----------------------------------\n",
            "Num timesteps: 1250000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.68\n",
            "Num timesteps: 1251000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.89\n",
            "Num timesteps: 1252000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.19\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 984      |\n",
            "|    ep_rew_mean      | -4.52    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2520     |\n",
            "|    fps              | 554      |\n",
            "|    time_elapsed     | 2259     |\n",
            "|    total_timesteps  | 1252857  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.193    |\n",
            "|    n_updates        | 300714   |\n",
            "----------------------------------\n",
            "Num timesteps: 1253000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.52\n",
            "Num timesteps: 1254000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.62\n",
            "Num timesteps: 1255000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.56\n",
            "Num timesteps: 1256000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.72\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 984      |\n",
            "|    ep_rew_mean      | -3.76    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2524     |\n",
            "|    fps              | 554      |\n",
            "|    time_elapsed     | 2267     |\n",
            "|    total_timesteps  | 1256857  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.255    |\n",
            "|    n_updates        | 301714   |\n",
            "----------------------------------\n",
            "Num timesteps: 1257000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.76\n",
            "Num timesteps: 1258000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.35\n",
            "Num timesteps: 1259000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.62\n",
            "Num timesteps: 1260000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.88\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 984      |\n",
            "|    ep_rew_mean      | -3.77    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2528     |\n",
            "|    fps              | 553      |\n",
            "|    time_elapsed     | 2276     |\n",
            "|    total_timesteps  | 1260857  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.222    |\n",
            "|    n_updates        | 302714   |\n",
            "----------------------------------\n",
            "Num timesteps: 1261000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.77\n",
            "Num timesteps: 1262000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.41\n",
            "Num timesteps: 1263000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.54\n",
            "Num timesteps: 1264000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.45\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 984      |\n",
            "|    ep_rew_mean      | -3.03    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2532     |\n",
            "|    fps              | 553      |\n",
            "|    time_elapsed     | 2284     |\n",
            "|    total_timesteps  | 1264857  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.249    |\n",
            "|    n_updates        | 303714   |\n",
            "----------------------------------\n",
            "Num timesteps: 1265000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.03\n",
            "Num timesteps: 1266000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.96\n",
            "Num timesteps: 1267000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.95\n",
            "Num timesteps: 1268000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.17\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 984      |\n",
            "|    ep_rew_mean      | -2.86    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2536     |\n",
            "|    fps              | 553      |\n",
            "|    time_elapsed     | 2293     |\n",
            "|    total_timesteps  | 1268857  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.65     |\n",
            "|    n_updates        | 304714   |\n",
            "----------------------------------\n",
            "Num timesteps: 1269000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.86\n",
            "Num timesteps: 1270000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.64\n",
            "Num timesteps: 1271000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.92\n",
            "Num timesteps: 1272000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.59\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 984      |\n",
            "|    ep_rew_mean      | -1.88    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2540     |\n",
            "|    fps              | 553      |\n",
            "|    time_elapsed     | 2301     |\n",
            "|    total_timesteps  | 1272857  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.18     |\n",
            "|    n_updates        | 305714   |\n",
            "----------------------------------\n",
            "Num timesteps: 1273000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.88\n",
            "Num timesteps: 1274000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.91\n",
            "Num timesteps: 1275000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.68\n",
            "Num timesteps: 1276000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.46\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 984      |\n",
            "|    ep_rew_mean      | -1.29    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2544     |\n",
            "|    fps              | 552      |\n",
            "|    time_elapsed     | 2310     |\n",
            "|    total_timesteps  | 1276857  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.211    |\n",
            "|    n_updates        | 306714   |\n",
            "----------------------------------\n",
            "Num timesteps: 1277000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.29\n",
            "Num timesteps: 1278000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.40\n",
            "Num timesteps: 1279000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -1.44\n",
            "Num timesteps: 1280000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.09\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 987      |\n",
            "|    ep_rew_mean      | -3.82    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2548     |\n",
            "|    fps              | 552      |\n",
            "|    time_elapsed     | 2319     |\n",
            "|    total_timesteps  | 1280857  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.222    |\n",
            "|    n_updates        | 307714   |\n",
            "----------------------------------\n",
            "Num timesteps: 1281000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.82\n",
            "Num timesteps: 1282000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.72\n",
            "Num timesteps: 1283000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.64\n",
            "Num timesteps: 1284000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.32\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 987      |\n",
            "|    ep_rew_mean      | -2.8     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2552     |\n",
            "|    fps              | 552      |\n",
            "|    time_elapsed     | 2327     |\n",
            "|    total_timesteps  | 1284857  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.116    |\n",
            "|    n_updates        | 308714   |\n",
            "----------------------------------\n",
            "Num timesteps: 1285000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.80\n",
            "Num timesteps: 1286000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.41\n",
            "Num timesteps: 1287000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.71\n",
            "Num timesteps: 1288000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.49\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 987      |\n",
            "|    ep_rew_mean      | -2.63    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2556     |\n",
            "|    fps              | 552      |\n",
            "|    time_elapsed     | 2334     |\n",
            "|    total_timesteps  | 1288857  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.175    |\n",
            "|    n_updates        | 309714   |\n",
            "----------------------------------\n",
            "Num timesteps: 1289000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.63\n",
            "Num timesteps: 1290000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.34\n",
            "Num timesteps: 1291000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.45\n",
            "Num timesteps: 1292000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.46\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 989      |\n",
            "|    ep_rew_mean      | -4.86    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2560     |\n",
            "|    fps              | 551      |\n",
            "|    time_elapsed     | 2342     |\n",
            "|    total_timesteps  | 1292857  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.125    |\n",
            "|    n_updates        | 310714   |\n",
            "----------------------------------\n",
            "Num timesteps: 1293000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.86\n",
            "Num timesteps: 1294000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.32\n",
            "Num timesteps: 1295000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.04\n",
            "Num timesteps: 1296000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.11\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 989      |\n",
            "|    ep_rew_mean      | -4.34    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2564     |\n",
            "|    fps              | 551      |\n",
            "|    time_elapsed     | 2350     |\n",
            "|    total_timesteps  | 1296857  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.165    |\n",
            "|    n_updates        | 311714   |\n",
            "----------------------------------\n",
            "Num timesteps: 1297000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.34\n",
            "Num timesteps: 1298000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.31\n",
            "Num timesteps: 1299000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.20\n",
            "Num timesteps: 1300000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.58\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 989      |\n",
            "|    ep_rew_mean      | -4.34    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2568     |\n",
            "|    fps              | 551      |\n",
            "|    time_elapsed     | 2358     |\n",
            "|    total_timesteps  | 1300857  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0854   |\n",
            "|    n_updates        | 312714   |\n",
            "----------------------------------\n",
            "Num timesteps: 1301000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.34\n",
            "Num timesteps: 1302000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.63\n",
            "Num timesteps: 1303000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.48\n",
            "Num timesteps: 1304000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.33\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 989      |\n",
            "|    ep_rew_mean      | -4.21    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2572     |\n",
            "|    fps              | 551      |\n",
            "|    time_elapsed     | 2367     |\n",
            "|    total_timesteps  | 1304857  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.157    |\n",
            "|    n_updates        | 313714   |\n",
            "----------------------------------\n",
            "Num timesteps: 1305000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.21\n",
            "Num timesteps: 1306000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.29\n",
            "Num timesteps: 1307000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.39\n",
            "Num timesteps: 1308000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.60\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 989      |\n",
            "|    ep_rew_mean      | -5.18    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2576     |\n",
            "|    fps              | 550      |\n",
            "|    time_elapsed     | 2375     |\n",
            "|    total_timesteps  | 1308857  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.185    |\n",
            "|    n_updates        | 314714   |\n",
            "----------------------------------\n",
            "Num timesteps: 1309000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.18\n",
            "Num timesteps: 1310000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.02\n",
            "Num timesteps: 1311000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.85\n",
            "Num timesteps: 1312000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.84\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 992      |\n",
            "|    ep_rew_mean      | -6.31    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2580     |\n",
            "|    fps              | 551      |\n",
            "|    time_elapsed     | 2382     |\n",
            "|    total_timesteps  | 1312857  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.182    |\n",
            "|    n_updates        | 315714   |\n",
            "----------------------------------\n",
            "Num timesteps: 1313000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.31\n",
            "Num timesteps: 1314000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.42\n",
            "Num timesteps: 1315000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.17\n",
            "Num timesteps: 1316000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.25\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 992      |\n",
            "|    ep_rew_mean      | -5.48    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2584     |\n",
            "|    fps              | 550      |\n",
            "|    time_elapsed     | 2390     |\n",
            "|    total_timesteps  | 1316857  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0822   |\n",
            "|    n_updates        | 316714   |\n",
            "----------------------------------\n",
            "Num timesteps: 1317000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.48\n",
            "Num timesteps: 1318000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.87\n",
            "Num timesteps: 1319000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.66\n",
            "Num timesteps: 1320000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.58\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 992      |\n",
            "|    ep_rew_mean      | -5.74    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2588     |\n",
            "|    fps              | 550      |\n",
            "|    time_elapsed     | 2398     |\n",
            "|    total_timesteps  | 1320857  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.147    |\n",
            "|    n_updates        | 317714   |\n",
            "----------------------------------\n",
            "Num timesteps: 1321000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.74\n",
            "Num timesteps: 1322000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.77\n",
            "Num timesteps: 1323000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.32\n",
            "Num timesteps: 1324000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.20\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 992      |\n",
            "|    ep_rew_mean      | -5.14    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2592     |\n",
            "|    fps              | 550      |\n",
            "|    time_elapsed     | 2406     |\n",
            "|    total_timesteps  | 1324857  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.256    |\n",
            "|    n_updates        | 318714   |\n",
            "----------------------------------\n",
            "Num timesteps: 1325000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.14\n",
            "Num timesteps: 1326000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.05\n",
            "Num timesteps: 1327000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.16\n",
            "Num timesteps: 1328000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.72\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 992      |\n",
            "|    ep_rew_mean      | -4.06    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2596     |\n",
            "|    fps              | 550      |\n",
            "|    time_elapsed     | 2414     |\n",
            "|    total_timesteps  | 1328857  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.241    |\n",
            "|    n_updates        | 319714   |\n",
            "----------------------------------\n",
            "Num timesteps: 1329000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.06\n",
            "Num timesteps: 1330000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.66\n",
            "Num timesteps: 1331000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.97\n",
            "Num timesteps: 1332000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.50\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 993      |\n",
            "|    ep_rew_mean      | -5.65    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2600     |\n",
            "|    fps              | 550      |\n",
            "|    time_elapsed     | 2422     |\n",
            "|    total_timesteps  | 1332857  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.289    |\n",
            "|    n_updates        | 320714   |\n",
            "----------------------------------\n",
            "Num timesteps: 1333000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.65\n",
            "Num timesteps: 1334000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.71\n",
            "Num timesteps: 1335000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.77\n",
            "Num timesteps: 1336000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.64\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 994      |\n",
            "|    ep_rew_mean      | -5.69    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2604     |\n",
            "|    fps              | 549      |\n",
            "|    time_elapsed     | 2431     |\n",
            "|    total_timesteps  | 1336857  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.152    |\n",
            "|    n_updates        | 321714   |\n",
            "----------------------------------\n",
            "Num timesteps: 1337000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -5.69\n",
            "Num timesteps: 1338000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.91\n",
            "Num timesteps: 1339000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -6.19\n",
            "Num timesteps: 1340000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.81\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 997      |\n",
            "|    ep_rew_mean      | -4.92    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2608     |\n",
            "|    fps              | 549      |\n",
            "|    time_elapsed     | 2439     |\n",
            "|    total_timesteps  | 1340857  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.219    |\n",
            "|    n_updates        | 322714   |\n",
            "----------------------------------\n",
            "Num timesteps: 1341000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.92\n",
            "Num timesteps: 1342000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.61\n",
            "Num timesteps: 1343000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.33\n",
            "Num timesteps: 1344000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -4.33\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 997      |\n",
            "|    ep_rew_mean      | -3.78    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2612     |\n",
            "|    fps              | 549      |\n",
            "|    time_elapsed     | 2447     |\n",
            "|    total_timesteps  | 1344857  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.228    |\n",
            "|    n_updates        | 323714   |\n",
            "----------------------------------\n",
            "Num timesteps: 1345000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.78\n",
            "Num timesteps: 1346000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.81\n",
            "Num timesteps: 1347000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.49\n",
            "Num timesteps: 1348000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.87\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 997      |\n",
            "|    ep_rew_mean      | -2.65    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2616     |\n",
            "|    fps              | 549      |\n",
            "|    time_elapsed     | 2456     |\n",
            "|    total_timesteps  | 1348857  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.163    |\n",
            "|    n_updates        | 324714   |\n",
            "----------------------------------\n",
            "Num timesteps: 1349000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.65\n",
            "Num timesteps: 1350000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.29\n",
            "Num timesteps: 1351000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.85\n",
            "Num timesteps: 1352000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.64\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1e+03    |\n",
            "|    ep_rew_mean      | -3.18    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2620     |\n",
            "|    fps              | 548      |\n",
            "|    time_elapsed     | 2464     |\n",
            "|    total_timesteps  | 1352857  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.106    |\n",
            "|    n_updates        | 325714   |\n",
            "----------------------------------\n",
            "Num timesteps: 1353000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -3.18\n",
            "Num timesteps: 1354000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -2.84\n",
            "Num timesteps: 1355000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.47\n",
            "Num timesteps: 1356000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.12\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 997      |\n",
            "|    ep_rew_mean      | -0.511   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2624     |\n",
            "|    fps              | 548      |\n",
            "|    time_elapsed     | 2471     |\n",
            "|    total_timesteps  | 1356535  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.163    |\n",
            "|    n_updates        | 326633   |\n",
            "----------------------------------\n",
            "Num timesteps: 1357000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.51\n",
            "Num timesteps: 1358000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.39\n",
            "Num timesteps: 1359000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.25\n",
            "Num timesteps: 1360000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.20\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 997      |\n",
            "|    ep_rew_mean      | 0.335    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2628     |\n",
            "|    fps              | 548      |\n",
            "|    time_elapsed     | 2480     |\n",
            "|    total_timesteps  | 1360535  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.182    |\n",
            "|    n_updates        | 327633   |\n",
            "----------------------------------\n",
            "Num timesteps: 1361000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.34\n",
            "Num timesteps: 1362000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.35\n",
            "Num timesteps: 1363000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.34\n",
            "Num timesteps: 1364000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.35\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 997      |\n",
            "|    ep_rew_mean      | -0.122   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2632     |\n",
            "|    fps              | 548      |\n",
            "|    time_elapsed     | 2488     |\n",
            "|    total_timesteps  | 1364535  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.157    |\n",
            "|    n_updates        | 328633   |\n",
            "----------------------------------\n",
            "Num timesteps: 1365000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.12\n",
            "Num timesteps: 1366000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.06\n",
            "Num timesteps: 1367000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.18\n",
            "Num timesteps: 1368000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.10\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 997      |\n",
            "|    ep_rew_mean      | -0.531   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2636     |\n",
            "|    fps              | 548      |\n",
            "|    time_elapsed     | 2496     |\n",
            "|    total_timesteps  | 1368535  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.208    |\n",
            "|    n_updates        | 329633   |\n",
            "----------------------------------\n",
            "Num timesteps: 1369000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.53\n",
            "Num timesteps: 1370000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.43\n",
            "Num timesteps: 1371000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.67\n",
            "Num timesteps: 1372000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.71\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 997      |\n",
            "|    ep_rew_mean      | -0.738   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2640     |\n",
            "|    fps              | 547      |\n",
            "|    time_elapsed     | 2504     |\n",
            "|    total_timesteps  | 1372535  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.127    |\n",
            "|    n_updates        | 330633   |\n",
            "----------------------------------\n",
            "Num timesteps: 1373000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.74\n",
            "Num timesteps: 1374000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.52\n",
            "Num timesteps: 1375000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.51\n",
            "Num timesteps: 1376000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.38\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 997      |\n",
            "|    ep_rew_mean      | -0.438   |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2644     |\n",
            "|    fps              | 548      |\n",
            "|    time_elapsed     | 2511     |\n",
            "|    total_timesteps  | 1376535  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0797   |\n",
            "|    n_updates        | 331633   |\n",
            "----------------------------------\n",
            "Num timesteps: 1377000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.44\n",
            "Num timesteps: 1378000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.45\n",
            "Num timesteps: 1379000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.44\n",
            "Num timesteps: 1380000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.16\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 997      |\n",
            "|    ep_rew_mean      | 0.598    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2648     |\n",
            "|    fps              | 548      |\n",
            "|    time_elapsed     | 2519     |\n",
            "|    total_timesteps  | 1380535  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.207    |\n",
            "|    n_updates        | 332633   |\n",
            "----------------------------------\n",
            "Num timesteps: 1381000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.60\n",
            "Num timesteps: 1382000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.29\n",
            "Num timesteps: 1383000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.57\n",
            "Num timesteps: 1384000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.26\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 997      |\n",
            "|    ep_rew_mean      | 0.122    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2652     |\n",
            "|    fps              | 547      |\n",
            "|    time_elapsed     | 2527     |\n",
            "|    total_timesteps  | 1384535  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.158    |\n",
            "|    n_updates        | 333633   |\n",
            "----------------------------------\n",
            "Num timesteps: 1385000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.12\n",
            "Num timesteps: 1386000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.29\n",
            "Num timesteps: 1387000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.51\n",
            "Num timesteps: 1388000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.06\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 997      |\n",
            "|    ep_rew_mean      | -0.0495  |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2656     |\n",
            "|    fps              | 547      |\n",
            "|    time_elapsed     | 2535     |\n",
            "|    total_timesteps  | 1388535  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.184    |\n",
            "|    n_updates        | 334633   |\n",
            "----------------------------------\n",
            "Num timesteps: 1389000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.05\n",
            "Num timesteps: 1390000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: -0.16\n",
            "Num timesteps: 1391000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.21\n",
            "Num timesteps: 1392000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.58\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 997      |\n",
            "|    ep_rew_mean      | 1.13     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2660     |\n",
            "|    fps              | 547      |\n",
            "|    time_elapsed     | 2544     |\n",
            "|    total_timesteps  | 1392535  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.147    |\n",
            "|    n_updates        | 335633   |\n",
            "----------------------------------\n",
            "Num timesteps: 1393000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 1.13\n",
            "Num timesteps: 1394000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.71\n",
            "Num timesteps: 1395000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.45\n",
            "Num timesteps: 1396000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.69\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 997      |\n",
            "|    ep_rew_mean      | 1.15     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2664     |\n",
            "|    fps              | 547      |\n",
            "|    time_elapsed     | 2552     |\n",
            "|    total_timesteps  | 1396535  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.128    |\n",
            "|    n_updates        | 336633   |\n",
            "----------------------------------\n",
            "Num timesteps: 1397000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 1.15\n",
            "Num timesteps: 1398000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.95\n",
            "Num timesteps: 1399000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 1.11\n",
            "Num timesteps: 1400000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 1.15\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 997      |\n",
            "|    ep_rew_mean      | 0.961    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2668     |\n",
            "|    fps              | 546      |\n",
            "|    time_elapsed     | 2561     |\n",
            "|    total_timesteps  | 1400535  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.233    |\n",
            "|    n_updates        | 337633   |\n",
            "----------------------------------\n",
            "Num timesteps: 1401000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 0.96\n",
            "Num timesteps: 1402000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 1.36\n",
            "Num timesteps: 1403000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 1.20\n",
            "Num timesteps: 1404000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 1.22\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 997      |\n",
            "|    ep_rew_mean      | 1.28     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2672     |\n",
            "|    fps              | 546      |\n",
            "|    time_elapsed     | 2569     |\n",
            "|    total_timesteps  | 1404535  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.137    |\n",
            "|    n_updates        | 338633   |\n",
            "----------------------------------\n",
            "Num timesteps: 1405000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 1.28\n",
            "Num timesteps: 1406000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 1.63\n",
            "Num timesteps: 1407000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 1.73\n",
            "Num timesteps: 1408000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 2.33\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 992      |\n",
            "|    ep_rew_mean      | 4.86     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2676     |\n",
            "|    fps              | 546      |\n",
            "|    time_elapsed     | 2575     |\n",
            "|    total_timesteps  | 1408046  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.164    |\n",
            "|    n_updates        | 339511   |\n",
            "----------------------------------\n",
            "Num timesteps: 1409000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 6.55\n",
            "Num timesteps: 1410000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 6.34\n",
            "Num timesteps: 1411000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 6.48\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 990      |\n",
            "|    ep_rew_mean      | 6.41     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2680     |\n",
            "|    fps              | 546      |\n",
            "|    time_elapsed     | 2583     |\n",
            "|    total_timesteps  | 1411875  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.148    |\n",
            "|    n_updates        | 340468   |\n",
            "----------------------------------\n",
            "Num timesteps: 1412000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 6.41\n",
            "Num timesteps: 1413000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 6.22\n",
            "Num timesteps: 1414000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 5.71\n",
            "Num timesteps: 1415000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 5.33\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 990      |\n",
            "|    ep_rew_mean      | 5.53     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2684     |\n",
            "|    fps              | 546      |\n",
            "|    time_elapsed     | 2590     |\n",
            "|    total_timesteps  | 1415875  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0824   |\n",
            "|    n_updates        | 341468   |\n",
            "----------------------------------\n",
            "Num timesteps: 1416000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 5.53\n",
            "Num timesteps: 1417000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 5.92\n",
            "Num timesteps: 1418000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 5.65\n",
            "Num timesteps: 1419000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 5.87\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 990      |\n",
            "|    ep_rew_mean      | 6.18     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2688     |\n",
            "|    fps              | 546      |\n",
            "|    time_elapsed     | 2598     |\n",
            "|    total_timesteps  | 1419875  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.16     |\n",
            "|    n_updates        | 342468   |\n",
            "----------------------------------\n",
            "Num timesteps: 1420000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 6.18\n",
            "Num timesteps: 1421000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 6.17\n",
            "Num timesteps: 1422000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 5.96\n",
            "Num timesteps: 1423000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 6.28\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 990      |\n",
            "|    ep_rew_mean      | 6.3      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2692     |\n",
            "|    fps              | 546      |\n",
            "|    time_elapsed     | 2606     |\n",
            "|    total_timesteps  | 1423875  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.174    |\n",
            "|    n_updates        | 343468   |\n",
            "----------------------------------\n",
            "Num timesteps: 1424000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 6.30\n",
            "Num timesteps: 1425000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 6.25\n",
            "Num timesteps: 1426000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 7.40\n",
            "Num timesteps: 1427000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 9.56\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 987      |\n",
            "|    ep_rew_mean      | 9.24     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2696     |\n",
            "|    fps              | 546      |\n",
            "|    time_elapsed     | 2613     |\n",
            "|    total_timesteps  | 1427549  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.209    |\n",
            "|    n_updates        | 344387   |\n",
            "----------------------------------\n",
            "Num timesteps: 1428000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 9.24\n",
            "Num timesteps: 1429000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 9.50\n",
            "Num timesteps: 1430000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 9.15\n",
            "Num timesteps: 1431000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 9.27\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 987      |\n",
            "|    ep_rew_mean      | 9.08     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2700     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 2622     |\n",
            "|    total_timesteps  | 1431549  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.125    |\n",
            "|    n_updates        | 345387   |\n",
            "----------------------------------\n",
            "Num timesteps: 1432000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 9.08\n",
            "Num timesteps: 1433000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 9.52\n",
            "Num timesteps: 1434000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 9.16\n",
            "Num timesteps: 1435000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 9.43\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 987      |\n",
            "|    ep_rew_mean      | 10.7     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2704     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 2629     |\n",
            "|    total_timesteps  | 1435549  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.16     |\n",
            "|    n_updates        | 346387   |\n",
            "----------------------------------\n",
            "Num timesteps: 1436000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 10.73\n",
            "Num timesteps: 1437000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 10.67\n",
            "Num timesteps: 1438000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 11.34\n",
            "Num timesteps: 1439000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 11.23\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 987      |\n",
            "|    ep_rew_mean      | 11.2     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2708     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 2637     |\n",
            "|    total_timesteps  | 1439549  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.177    |\n",
            "|    n_updates        | 347387   |\n",
            "----------------------------------\n",
            "Num timesteps: 1440000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 11.24\n",
            "Num timesteps: 1441000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 12.76\n",
            "Num timesteps: 1442000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 15.87\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 980      |\n",
            "|    ep_rew_mean      | 15.3     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2712     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 2644     |\n",
            "|    total_timesteps  | 1442879  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.152    |\n",
            "|    n_updates        | 348219   |\n",
            "----------------------------------\n",
            "Num timesteps: 1443000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 15.35\n",
            "Num timesteps: 1444000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 17.71\n",
            "Num timesteps: 1445000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 17.44\n",
            "Num timesteps: 1446000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 17.53\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 972      |\n",
            "|    ep_rew_mean      | 19.7     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2716     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 2650     |\n",
            "|    total_timesteps  | 1446017  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.129    |\n",
            "|    n_updates        | 349004   |\n",
            "----------------------------------\n",
            "Num timesteps: 1447000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 21.45\n",
            "Num timesteps: 1448000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 21.25\n",
            "Num timesteps: 1449000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 23.19\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 964      |\n",
            "|    ep_rew_mean      | 25       |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2720     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 2656     |\n",
            "|    total_timesteps  | 1449291  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0897   |\n",
            "|    n_updates        | 349822   |\n",
            "----------------------------------\n",
            "Num timesteps: 1450000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 27.25\n",
            "Num timesteps: 1451000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 27.38\n",
            "Num timesteps: 1452000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 26.77\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 960      |\n",
            "|    ep_rew_mean      | 27.1     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2724     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 2662     |\n",
            "|    total_timesteps  | 1452506  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.218    |\n",
            "|    n_updates        | 350626   |\n",
            "----------------------------------\n",
            "Num timesteps: 1453000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 27.10\n",
            "Num timesteps: 1454000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 26.77\n",
            "Num timesteps: 1455000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 30.70\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 951      |\n",
            "|    ep_rew_mean      | 32.1     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2728     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 2668     |\n",
            "|    total_timesteps  | 1455671  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.76     |\n",
            "|    n_updates        | 351417   |\n",
            "----------------------------------\n",
            "Num timesteps: 1456000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 32.10\n",
            "Num timesteps: 1457000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 36.09\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 934      |\n",
            "|    ep_rew_mean      | 41       |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2732     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 2671     |\n",
            "|    total_timesteps  | 1457984  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.01     |\n",
            "|    n_updates        | 351995   |\n",
            "----------------------------------\n",
            "Num timesteps: 1458000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 40.98\n",
            "Num timesteps: 1459000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 43.11\n",
            "Num timesteps: 1460000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 42.48\n",
            "Num timesteps: 1461000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 44.22\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 931      |\n",
            "|    ep_rew_mean      | 44.2     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2736     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 2678     |\n",
            "|    total_timesteps  | 1461621  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.111    |\n",
            "|    n_updates        | 352905   |\n",
            "----------------------------------\n",
            "Num timesteps: 1462000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 44.15\n",
            "Num timesteps: 1463000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 47.69\n",
            "Num timesteps: 1464000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 47.62\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 925      |\n",
            "|    ep_rew_mean      | 47.4     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2740     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 2684     |\n",
            "|    total_timesteps  | 1464993  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.148    |\n",
            "|    n_updates        | 353748   |\n",
            "----------------------------------\n",
            "Num timesteps: 1465000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 47.38\n",
            "Num timesteps: 1466000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 47.19\n",
            "Num timesteps: 1467000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 49.10\n",
            "Num timesteps: 1468000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 50.53\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 922      |\n",
            "|    ep_rew_mean      | 50.3     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2744     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 2691     |\n",
            "|    total_timesteps  | 1468702  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.117    |\n",
            "|    n_updates        | 354675   |\n",
            "----------------------------------\n",
            "Num timesteps: 1469000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 50.26\n",
            "Num timesteps: 1470000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 50.21\n",
            "Num timesteps: 1471000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 52.57\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 912      |\n",
            "|    ep_rew_mean      | 54.7     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2748     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 2696     |\n",
            "|    total_timesteps  | 1471700  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.12     |\n",
            "|    n_updates        | 355424   |\n",
            "----------------------------------\n",
            "Num timesteps: 1472000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 54.74\n",
            "Num timesteps: 1473000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 55.27\n",
            "Num timesteps: 1474000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 55.39\n",
            "Num timesteps: 1475000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 55.05\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 910      |\n",
            "|    ep_rew_mean      | 54.1     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2752     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 2703     |\n",
            "|    total_timesteps  | 1475487  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.123    |\n",
            "|    n_updates        | 356371   |\n",
            "----------------------------------\n",
            "Num timesteps: 1476000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 54.10\n",
            "Num timesteps: 1477000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 55.95\n",
            "Num timesteps: 1478000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 59.64\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 899      |\n",
            "|    ep_rew_mean      | 61.5     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2756     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 2708     |\n",
            "|    total_timesteps  | 1478390  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.241    |\n",
            "|    n_updates        | 357097   |\n",
            "----------------------------------\n",
            "Num timesteps: 1479000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 61.48\n",
            "Num timesteps: 1480000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 61.60\n",
            "Num timesteps: 1481000\n",
            "Best mean reward: 62.72 - Last mean reward per episode: 65.41\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 889      |\n",
            "|    ep_rew_mean      | 67.8     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2760     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 2714     |\n",
            "|    total_timesteps  | 1481465  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.216    |\n",
            "|    n_updates        | 357866   |\n",
            "----------------------------------\n",
            "Num timesteps: 1482000\n",
            "Best mean reward: 65.41 - Last mean reward per episode: 67.79\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1483000\n",
            "Best mean reward: 67.79 - Last mean reward per episode: 68.14\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1484000\n",
            "Best mean reward: 68.14 - Last mean reward per episode: 73.01\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 882      |\n",
            "|    ep_rew_mean      | 72.7     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2764     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 2720     |\n",
            "|    total_timesteps  | 1484776  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.202    |\n",
            "|    n_updates        | 358693   |\n",
            "----------------------------------\n",
            "Num timesteps: 1485000\n",
            "Best mean reward: 73.01 - Last mean reward per episode: 72.67\n",
            "Num timesteps: 1486000\n",
            "Best mean reward: 73.01 - Last mean reward per episode: 77.90\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1487000\n",
            "Best mean reward: 77.90 - Last mean reward per episode: 77.51\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 871      |\n",
            "|    ep_rew_mean      | 77.7     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2768     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 2725     |\n",
            "|    total_timesteps  | 1487666  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.157    |\n",
            "|    n_updates        | 359416   |\n",
            "----------------------------------\n",
            "Num timesteps: 1488000\n",
            "Best mean reward: 77.90 - Last mean reward per episode: 77.74\n",
            "Num timesteps: 1489000\n",
            "Best mean reward: 77.90 - Last mean reward per episode: 79.54\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1490000\n",
            "Best mean reward: 79.54 - Last mean reward per episode: 83.60\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 857      |\n",
            "|    ep_rew_mean      | 86.1     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2772     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 2729     |\n",
            "|    total_timesteps  | 1490219  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.128    |\n",
            "|    n_updates        | 360054   |\n",
            "----------------------------------\n",
            "Num timesteps: 1491000\n",
            "Best mean reward: 83.60 - Last mean reward per episode: 86.12\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1492000\n",
            "Best mean reward: 86.12 - Last mean reward per episode: 87.76\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1493000\n",
            "Best mean reward: 87.76 - Last mean reward per episode: 89.90\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 851      |\n",
            "|    ep_rew_mean      | 87.8     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2776     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 2735     |\n",
            "|    total_timesteps  | 1493118  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.656    |\n",
            "|    n_updates        | 360779   |\n",
            "----------------------------------\n",
            "Num timesteps: 1494000\n",
            "Best mean reward: 89.90 - Last mean reward per episode: 87.81\n",
            "Num timesteps: 1495000\n",
            "Best mean reward: 89.90 - Last mean reward per episode: 86.08\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 840      |\n",
            "|    ep_rew_mean      | 93.2     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2780     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 2740     |\n",
            "|    total_timesteps  | 1495835  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.111    |\n",
            "|    n_updates        | 361458   |\n",
            "----------------------------------\n",
            "Num timesteps: 1496000\n",
            "Best mean reward: 89.90 - Last mean reward per episode: 93.15\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1497000\n",
            "Best mean reward: 93.15 - Last mean reward per episode: 93.30\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1498000\n",
            "Best mean reward: 93.30 - Last mean reward per episode: 95.86\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1499000\n",
            "Best mean reward: 95.86 - Last mean reward per episode: 98.10\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 833      |\n",
            "|    ep_rew_mean      | 97.9     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2784     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 2746     |\n",
            "|    total_timesteps  | 1499172  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.179    |\n",
            "|    n_updates        | 362292   |\n",
            "----------------------------------\n",
            "Num timesteps: 1500000\n",
            "Best mean reward: 98.10 - Last mean reward per episode: 97.89\n",
            "Num timesteps: 1501000\n",
            "Best mean reward: 98.10 - Last mean reward per episode: 99.45\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1502000\n",
            "Best mean reward: 99.45 - Last mean reward per episode: 101.19\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 831      |\n",
            "|    ep_rew_mean      | 101      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2788     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 2754     |\n",
            "|    total_timesteps  | 1502999  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.114    |\n",
            "|    n_updates        | 363249   |\n",
            "----------------------------------\n",
            "Num timesteps: 1503000\n",
            "Best mean reward: 101.19 - Last mean reward per episode: 101.00\n",
            "Num timesteps: 1504000\n",
            "Best mean reward: 101.19 - Last mean reward per episode: 103.66\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1505000\n",
            "Best mean reward: 103.66 - Last mean reward per episode: 103.29\n",
            "Num timesteps: 1506000\n",
            "Best mean reward: 103.66 - Last mean reward per episode: 103.12\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 822      |\n",
            "|    ep_rew_mean      | 105      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2792     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 2760     |\n",
            "|    total_timesteps  | 1506026  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.173    |\n",
            "|    n_updates        | 364006   |\n",
            "----------------------------------\n",
            "Num timesteps: 1507000\n",
            "Best mean reward: 103.66 - Last mean reward per episode: 105.13\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1508000\n",
            "Best mean reward: 105.13 - Last mean reward per episode: 105.39\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1509000\n",
            "Best mean reward: 105.39 - Last mean reward per episode: 104.36\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 819      |\n",
            "|    ep_rew_mean      | 105      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2796     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 2767     |\n",
            "|    total_timesteps  | 1509489  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.152    |\n",
            "|    n_updates        | 364872   |\n",
            "----------------------------------\n",
            "Num timesteps: 1510000\n",
            "Best mean reward: 105.39 - Last mean reward per episode: 104.70\n",
            "Num timesteps: 1511000\n",
            "Best mean reward: 105.39 - Last mean reward per episode: 108.43\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1512000\n",
            "Best mean reward: 108.43 - Last mean reward per episode: 108.37\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 812      |\n",
            "|    ep_rew_mean      | 110      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2800     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 2773     |\n",
            "|    total_timesteps  | 1512706  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.673    |\n",
            "|    n_updates        | 365676   |\n",
            "----------------------------------\n",
            "Num timesteps: 1513000\n",
            "Best mean reward: 108.43 - Last mean reward per episode: 110.02\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1514000\n",
            "Best mean reward: 110.02 - Last mean reward per episode: 109.56\n",
            "Num timesteps: 1515000\n",
            "Best mean reward: 110.02 - Last mean reward per episode: 110.30\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1516000\n",
            "Best mean reward: 110.30 - Last mean reward per episode: 112.12\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 808      |\n",
            "|    ep_rew_mean      | 113      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2804     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 2780     |\n",
            "|    total_timesteps  | 1516379  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0773   |\n",
            "|    n_updates        | 366594   |\n",
            "----------------------------------\n",
            "Num timesteps: 1517000\n",
            "Best mean reward: 112.12 - Last mean reward per episode: 112.68\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1518000\n",
            "Best mean reward: 112.68 - Last mean reward per episode: 112.19\n",
            "Num timesteps: 1519000\n",
            "Best mean reward: 112.68 - Last mean reward per episode: 111.81\n",
            "Num timesteps: 1520000\n",
            "Best mean reward: 112.68 - Last mean reward per episode: 113.28\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 808      |\n",
            "|    ep_rew_mean      | 114      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2808     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 2788     |\n",
            "|    total_timesteps  | 1520346  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.16     |\n",
            "|    n_updates        | 367586   |\n",
            "----------------------------------\n",
            "Num timesteps: 1521000\n",
            "Best mean reward: 113.28 - Last mean reward per episode: 113.81\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1522000\n",
            "Best mean reward: 113.81 - Last mean reward per episode: 115.44\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 796      |\n",
            "|    ep_rew_mean      | 118      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2812     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 2791     |\n",
            "|    total_timesteps  | 1522468  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.534    |\n",
            "|    n_updates        | 368116   |\n",
            "----------------------------------\n",
            "Num timesteps: 1523000\n",
            "Best mean reward: 115.44 - Last mean reward per episode: 117.61\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1524000\n",
            "Best mean reward: 117.61 - Last mean reward per episode: 117.77\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 790      |\n",
            "|    ep_rew_mean      | 120      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2816     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 2796     |\n",
            "|    total_timesteps  | 1524993  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.103    |\n",
            "|    n_updates        | 368748   |\n",
            "----------------------------------\n",
            "Num timesteps: 1525000\n",
            "Best mean reward: 117.77 - Last mean reward per episode: 120.47\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1526000\n",
            "Best mean reward: 120.47 - Last mean reward per episode: 120.18\n",
            "Num timesteps: 1527000\n",
            "Best mean reward: 120.47 - Last mean reward per episode: 122.01\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1528000\n",
            "Best mean reward: 122.01 - Last mean reward per episode: 119.98\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 789      |\n",
            "|    ep_rew_mean      | 120      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2820     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 2801     |\n",
            "|    total_timesteps  | 1528192  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0928   |\n",
            "|    n_updates        | 369547   |\n",
            "----------------------------------\n",
            "Num timesteps: 1529000\n",
            "Best mean reward: 122.01 - Last mean reward per episode: 120.47\n",
            "Num timesteps: 1530000\n",
            "Best mean reward: 122.01 - Last mean reward per episode: 119.34\n",
            "Num timesteps: 1531000\n",
            "Best mean reward: 122.01 - Last mean reward per episode: 119.04\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 793      |\n",
            "|    ep_rew_mean      | 121      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2824     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 2809     |\n",
            "|    total_timesteps  | 1531758  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.55     |\n",
            "|    n_updates        | 370439   |\n",
            "----------------------------------\n",
            "Num timesteps: 1532000\n",
            "Best mean reward: 122.01 - Last mean reward per episode: 120.96\n",
            "Num timesteps: 1533000\n",
            "Best mean reward: 122.01 - Last mean reward per episode: 123.52\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1534000\n",
            "Best mean reward: 123.52 - Last mean reward per episode: 123.42\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 789      |\n",
            "|    ep_rew_mean      | 122      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2828     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 2814     |\n",
            "|    total_timesteps  | 1534540  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.191    |\n",
            "|    n_updates        | 371134   |\n",
            "----------------------------------\n",
            "Num timesteps: 1535000\n",
            "Best mean reward: 123.52 - Last mean reward per episode: 121.78\n",
            "Num timesteps: 1536000\n",
            "Best mean reward: 123.52 - Last mean reward per episode: 120.39\n",
            "Num timesteps: 1537000\n",
            "Best mean reward: 123.52 - Last mean reward per episode: 118.13\n",
            "Num timesteps: 1538000\n",
            "Best mean reward: 123.52 - Last mean reward per episode: 115.31\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 804      |\n",
            "|    ep_rew_mean      | 115      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2832     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 2822     |\n",
            "|    total_timesteps  | 1538390  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.542    |\n",
            "|    n_updates        | 372097   |\n",
            "----------------------------------\n",
            "Num timesteps: 1539000\n",
            "Best mean reward: 123.52 - Last mean reward per episode: 114.76\n",
            "Num timesteps: 1540000\n",
            "Best mean reward: 123.52 - Last mean reward per episode: 116.85\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 793      |\n",
            "|    ep_rew_mean      | 120      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2836     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 2827     |\n",
            "|    total_timesteps  | 1540966  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.135    |\n",
            "|    n_updates        | 372741   |\n",
            "----------------------------------\n",
            "Num timesteps: 1541000\n",
            "Best mean reward: 123.52 - Last mean reward per episode: 119.71\n",
            "Num timesteps: 1542000\n",
            "Best mean reward: 123.52 - Last mean reward per episode: 119.97\n",
            "Num timesteps: 1543000\n",
            "Best mean reward: 123.52 - Last mean reward per episode: 120.04\n",
            "Num timesteps: 1544000\n",
            "Best mean reward: 123.52 - Last mean reward per episode: 119.94\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 793      |\n",
            "|    ep_rew_mean      | 120      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2840     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 2834     |\n",
            "|    total_timesteps  | 1544338  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.148    |\n",
            "|    n_updates        | 373584   |\n",
            "----------------------------------\n",
            "Num timesteps: 1545000\n",
            "Best mean reward: 123.52 - Last mean reward per episode: 120.13\n",
            "Num timesteps: 1546000\n",
            "Best mean reward: 123.52 - Last mean reward per episode: 121.94\n",
            "Num timesteps: 1547000\n",
            "Best mean reward: 123.52 - Last mean reward per episode: 122.91\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 785      |\n",
            "|    ep_rew_mean      | 125      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2844     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 2839     |\n",
            "|    total_timesteps  | 1547224  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.172    |\n",
            "|    n_updates        | 374305   |\n",
            "----------------------------------\n",
            "Num timesteps: 1548000\n",
            "Best mean reward: 123.52 - Last mean reward per episode: 124.79\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1549000\n",
            "Best mean reward: 124.79 - Last mean reward per episode: 125.55\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1550000\n",
            "Best mean reward: 125.55 - Last mean reward per episode: 125.54\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 787      |\n",
            "|    ep_rew_mean      | 125      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2848     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 2846     |\n",
            "|    total_timesteps  | 1550433  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.314    |\n",
            "|    n_updates        | 375108   |\n",
            "----------------------------------\n",
            "Num timesteps: 1551000\n",
            "Best mean reward: 125.55 - Last mean reward per episode: 124.93\n",
            "Num timesteps: 1552000\n",
            "Best mean reward: 125.55 - Last mean reward per episode: 128.55\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1553000\n",
            "Best mean reward: 128.55 - Last mean reward per episode: 130.50\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 779      |\n",
            "|    ep_rew_mean      | 133      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2852     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 2851     |\n",
            "|    total_timesteps  | 1553403  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.083    |\n",
            "|    n_updates        | 375850   |\n",
            "----------------------------------\n",
            "Num timesteps: 1554000\n",
            "Best mean reward: 130.50 - Last mean reward per episode: 133.28\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1555000\n",
            "Best mean reward: 133.28 - Last mean reward per episode: 132.82\n",
            "Num timesteps: 1556000\n",
            "Best mean reward: 133.28 - Last mean reward per episode: 131.32\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 782      |\n",
            "|    ep_rew_mean      | 131      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2856     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 2857     |\n",
            "|    total_timesteps  | 1556543  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.603    |\n",
            "|    n_updates        | 376635   |\n",
            "----------------------------------\n",
            "Num timesteps: 1557000\n",
            "Best mean reward: 133.28 - Last mean reward per episode: 131.31\n",
            "Num timesteps: 1558000\n",
            "Best mean reward: 133.28 - Last mean reward per episode: 133.57\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1559000\n",
            "Best mean reward: 133.57 - Last mean reward per episode: 133.13\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 781      |\n",
            "|    ep_rew_mean      | 133      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2860     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 2862     |\n",
            "|    total_timesteps  | 1559552  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.103    |\n",
            "|    n_updates        | 377387   |\n",
            "----------------------------------\n",
            "Num timesteps: 1560000\n",
            "Best mean reward: 133.57 - Last mean reward per episode: 132.75\n",
            "Num timesteps: 1561000\n",
            "Best mean reward: 133.57 - Last mean reward per episode: 134.60\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1562000\n",
            "Best mean reward: 134.60 - Last mean reward per episode: 132.59\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 782      |\n",
            "|    ep_rew_mean      | 132      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2864     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 2869     |\n",
            "|    total_timesteps  | 1562962  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.151    |\n",
            "|    n_updates        | 378240   |\n",
            "----------------------------------\n",
            "Num timesteps: 1563000\n",
            "Best mean reward: 134.60 - Last mean reward per episode: 132.00\n",
            "Num timesteps: 1564000\n",
            "Best mean reward: 134.60 - Last mean reward per episode: 132.53\n",
            "Num timesteps: 1565000\n",
            "Best mean reward: 134.60 - Last mean reward per episode: 131.42\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 779      |\n",
            "|    ep_rew_mean      | 134      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2868     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 2873     |\n",
            "|    total_timesteps  | 1565551  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0912   |\n",
            "|    n_updates        | 378887   |\n",
            "----------------------------------\n",
            "Num timesteps: 1566000\n",
            "Best mean reward: 134.60 - Last mean reward per episode: 134.47\n",
            "Num timesteps: 1567000\n",
            "Best mean reward: 134.60 - Last mean reward per episode: 132.05\n",
            "Num timesteps: 1568000\n",
            "Best mean reward: 134.60 - Last mean reward per episode: 130.72\n",
            "Num timesteps: 1569000\n",
            "Best mean reward: 134.60 - Last mean reward per episode: 129.70\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 791      |\n",
            "|    ep_rew_mean      | 129      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2872     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 2881     |\n",
            "|    total_timesteps  | 1569327  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.198    |\n",
            "|    n_updates        | 379831   |\n",
            "----------------------------------\n",
            "Num timesteps: 1570000\n",
            "Best mean reward: 134.60 - Last mean reward per episode: 128.74\n",
            "Num timesteps: 1571000\n",
            "Best mean reward: 134.60 - Last mean reward per episode: 129.19\n",
            "Num timesteps: 1572000\n",
            "Best mean reward: 134.60 - Last mean reward per episode: 127.24\n",
            "Num timesteps: 1573000\n",
            "Best mean reward: 134.60 - Last mean reward per episode: 126.96\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 801      |\n",
            "|    ep_rew_mean      | 127      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2876     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 2888     |\n",
            "|    total_timesteps  | 1573216  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0907   |\n",
            "|    n_updates        | 380803   |\n",
            "----------------------------------\n",
            "Num timesteps: 1574000\n",
            "Best mean reward: 134.60 - Last mean reward per episode: 128.64\n",
            "Num timesteps: 1575000\n",
            "Best mean reward: 134.60 - Last mean reward per episode: 128.52\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 801      |\n",
            "|    ep_rew_mean      | 126      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2880     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 2893     |\n",
            "|    total_timesteps  | 1575887  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.12     |\n",
            "|    n_updates        | 381471   |\n",
            "----------------------------------\n",
            "Num timesteps: 1576000\n",
            "Best mean reward: 134.60 - Last mean reward per episode: 125.89\n",
            "Num timesteps: 1577000\n",
            "Best mean reward: 134.60 - Last mean reward per episode: 128.08\n",
            "Num timesteps: 1578000\n",
            "Best mean reward: 134.60 - Last mean reward per episode: 127.60\n",
            "Num timesteps: 1579000\n",
            "Best mean reward: 134.60 - Last mean reward per episode: 125.21\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 801      |\n",
            "|    ep_rew_mean      | 127      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2884     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 2900     |\n",
            "|    total_timesteps  | 1579295  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.108    |\n",
            "|    n_updates        | 382323   |\n",
            "----------------------------------\n",
            "Num timesteps: 1580000\n",
            "Best mean reward: 134.60 - Last mean reward per episode: 127.24\n",
            "Num timesteps: 1581000\n",
            "Best mean reward: 134.60 - Last mean reward per episode: 128.86\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 786      |\n",
            "|    ep_rew_mean      | 132      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2888     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 2904     |\n",
            "|    total_timesteps  | 1581621  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.233    |\n",
            "|    n_updates        | 382905   |\n",
            "----------------------------------\n",
            "Num timesteps: 1582000\n",
            "Best mean reward: 134.60 - Last mean reward per episode: 132.25\n",
            "Num timesteps: 1583000\n",
            "Best mean reward: 134.60 - Last mean reward per episode: 129.93\n",
            "Num timesteps: 1584000\n",
            "Best mean reward: 134.60 - Last mean reward per episode: 132.77\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 787      |\n",
            "|    ep_rew_mean      | 133      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2892     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 2909     |\n",
            "|    total_timesteps  | 1584769  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.164    |\n",
            "|    n_updates        | 383692   |\n",
            "----------------------------------\n",
            "Num timesteps: 1585000\n",
            "Best mean reward: 134.60 - Last mean reward per episode: 132.54\n",
            "Num timesteps: 1586000\n",
            "Best mean reward: 134.60 - Last mean reward per episode: 134.14\n",
            "Num timesteps: 1587000\n",
            "Best mean reward: 134.60 - Last mean reward per episode: 135.78\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 783      |\n",
            "|    ep_rew_mean      | 137      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2896     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 2914     |\n",
            "|    total_timesteps  | 1587790  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.106    |\n",
            "|    n_updates        | 384447   |\n",
            "----------------------------------\n",
            "Num timesteps: 1588000\n",
            "Best mean reward: 135.78 - Last mean reward per episode: 137.29\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1589000\n",
            "Best mean reward: 137.29 - Last mean reward per episode: 137.72\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1590000\n",
            "Best mean reward: 137.72 - Last mean reward per episode: 137.72\n",
            "Num timesteps: 1591000\n",
            "Best mean reward: 137.72 - Last mean reward per episode: 137.49\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 784      |\n",
            "|    ep_rew_mean      | 136      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2900     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 2920     |\n",
            "|    total_timesteps  | 1591091  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.155    |\n",
            "|    n_updates        | 385272   |\n",
            "----------------------------------\n",
            "Num timesteps: 1592000\n",
            "Best mean reward: 137.72 - Last mean reward per episode: 138.28\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1593000\n",
            "Best mean reward: 138.28 - Last mean reward per episode: 137.79\n",
            "Num timesteps: 1594000\n",
            "Best mean reward: 138.28 - Last mean reward per episode: 136.18\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 784      |\n",
            "|    ep_rew_mean      | 135      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2904     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 2928     |\n",
            "|    total_timesteps  | 1594765  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.12     |\n",
            "|    n_updates        | 386191   |\n",
            "----------------------------------\n",
            "Num timesteps: 1595000\n",
            "Best mean reward: 138.28 - Last mean reward per episode: 134.75\n",
            "Num timesteps: 1596000\n",
            "Best mean reward: 138.28 - Last mean reward per episode: 135.22\n",
            "Num timesteps: 1597000\n",
            "Best mean reward: 138.28 - Last mean reward per episode: 134.97\n",
            "Num timesteps: 1598000\n",
            "Best mean reward: 138.28 - Last mean reward per episode: 133.36\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 784      |\n",
            "|    ep_rew_mean      | 133      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2908     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 2935     |\n",
            "|    total_timesteps  | 1598765  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.138    |\n",
            "|    n_updates        | 387191   |\n",
            "----------------------------------\n",
            "Num timesteps: 1599000\n",
            "Best mean reward: 138.28 - Last mean reward per episode: 132.88\n",
            "Num timesteps: 1600000\n",
            "Best mean reward: 138.28 - Last mean reward per episode: 132.51\n",
            "Num timesteps: 1601000\n",
            "Best mean reward: 138.28 - Last mean reward per episode: 130.06\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 794      |\n",
            "|    ep_rew_mean      | 130      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2912     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 2941     |\n",
            "|    total_timesteps  | 1601878  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.118    |\n",
            "|    n_updates        | 387969   |\n",
            "----------------------------------\n",
            "Num timesteps: 1602000\n",
            "Best mean reward: 138.28 - Last mean reward per episode: 129.66\n",
            "Num timesteps: 1603000\n",
            "Best mean reward: 138.28 - Last mean reward per episode: 129.73\n",
            "Num timesteps: 1604000\n",
            "Best mean reward: 138.28 - Last mean reward per episode: 129.66\n",
            "Num timesteps: 1605000\n",
            "Best mean reward: 138.28 - Last mean reward per episode: 129.36\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 801      |\n",
            "|    ep_rew_mean      | 127      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2916     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 2946     |\n",
            "|    total_timesteps  | 1605076  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.247    |\n",
            "|    n_updates        | 388768   |\n",
            "----------------------------------\n",
            "Num timesteps: 1606000\n",
            "Best mean reward: 138.28 - Last mean reward per episode: 127.58\n",
            "Num timesteps: 1607000\n",
            "Best mean reward: 138.28 - Last mean reward per episode: 125.83\n",
            "Num timesteps: 1608000\n",
            "Best mean reward: 138.28 - Last mean reward per episode: 127.89\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 801      |\n",
            "|    ep_rew_mean      | 127      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2920     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 2953     |\n",
            "|    total_timesteps  | 1608277  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0639   |\n",
            "|    n_updates        | 389569   |\n",
            "----------------------------------\n",
            "Num timesteps: 1609000\n",
            "Best mean reward: 138.28 - Last mean reward per episode: 126.84\n",
            "Num timesteps: 1610000\n",
            "Best mean reward: 138.28 - Last mean reward per episode: 129.23\n",
            "Num timesteps: 1611000\n",
            "Best mean reward: 138.28 - Last mean reward per episode: 129.21\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 794      |\n",
            "|    ep_rew_mean      | 129      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2924     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 2958     |\n",
            "|    total_timesteps  | 1611179  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.126    |\n",
            "|    n_updates        | 390294   |\n",
            "----------------------------------\n",
            "Num timesteps: 1612000\n",
            "Best mean reward: 138.28 - Last mean reward per episode: 128.25\n",
            "Num timesteps: 1613000\n",
            "Best mean reward: 138.28 - Last mean reward per episode: 128.10\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 793      |\n",
            "|    ep_rew_mean      | 130      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2928     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 2963     |\n",
            "|    total_timesteps  | 1613808  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.146    |\n",
            "|    n_updates        | 390951   |\n",
            "----------------------------------\n",
            "Num timesteps: 1614000\n",
            "Best mean reward: 138.28 - Last mean reward per episode: 130.46\n",
            "Num timesteps: 1615000\n",
            "Best mean reward: 138.28 - Last mean reward per episode: 132.57\n",
            "Num timesteps: 1616000\n",
            "Best mean reward: 138.28 - Last mean reward per episode: 136.75\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 778      |\n",
            "|    ep_rew_mean      | 138      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2932     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 2966     |\n",
            "|    total_timesteps  | 1616210  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.102    |\n",
            "|    n_updates        | 391552   |\n",
            "----------------------------------\n",
            "Num timesteps: 1617000\n",
            "Best mean reward: 138.28 - Last mean reward per episode: 138.01\n",
            "Num timesteps: 1618000\n",
            "Best mean reward: 138.28 - Last mean reward per episode: 137.31\n",
            "Num timesteps: 1619000\n",
            "Best mean reward: 138.28 - Last mean reward per episode: 138.37\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 782      |\n",
            "|    ep_rew_mean      | 137      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2936     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 2972     |\n",
            "|    total_timesteps  | 1619148  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.11     |\n",
            "|    n_updates        | 392286   |\n",
            "----------------------------------\n",
            "Num timesteps: 1620000\n",
            "Best mean reward: 138.37 - Last mean reward per episode: 137.46\n",
            "Num timesteps: 1621000\n",
            "Best mean reward: 138.37 - Last mean reward per episode: 135.84\n",
            "Num timesteps: 1622000\n",
            "Best mean reward: 138.37 - Last mean reward per episode: 136.11\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 780      |\n",
            "|    ep_rew_mean      | 138      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2940     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 2978     |\n",
            "|    total_timesteps  | 1622328  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.142    |\n",
            "|    n_updates        | 393081   |\n",
            "----------------------------------\n",
            "Num timesteps: 1623000\n",
            "Best mean reward: 138.37 - Last mean reward per episode: 138.34\n",
            "Num timesteps: 1624000\n",
            "Best mean reward: 138.37 - Last mean reward per episode: 138.67\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 775      |\n",
            "|    ep_rew_mean      | 139      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2944     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 2982     |\n",
            "|    total_timesteps  | 1624675  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.192    |\n",
            "|    n_updates        | 393668   |\n",
            "----------------------------------\n",
            "Num timesteps: 1625000\n",
            "Best mean reward: 138.67 - Last mean reward per episode: 139.18\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1626000\n",
            "Best mean reward: 139.18 - Last mean reward per episode: 140.78\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 764      |\n",
            "|    ep_rew_mean      | 143      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2948     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 2985     |\n",
            "|    total_timesteps  | 1626830  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.127    |\n",
            "|    n_updates        | 394207   |\n",
            "----------------------------------\n",
            "Num timesteps: 1627000\n",
            "Best mean reward: 140.78 - Last mean reward per episode: 143.14\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1628000\n",
            "Best mean reward: 143.14 - Last mean reward per episode: 143.12\n",
            "Num timesteps: 1629000\n",
            "Best mean reward: 143.14 - Last mean reward per episode: 143.99\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 757      |\n",
            "|    ep_rew_mean      | 145      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2952     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 2989     |\n",
            "|    total_timesteps  | 1629083  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0821   |\n",
            "|    n_updates        | 394770   |\n",
            "----------------------------------\n",
            "Num timesteps: 1630000\n",
            "Best mean reward: 143.99 - Last mean reward per episode: 145.48\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1631000\n",
            "Best mean reward: 145.48 - Last mean reward per episode: 147.33\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1632000\n",
            "Best mean reward: 147.33 - Last mean reward per episode: 147.73\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 756      |\n",
            "|    ep_rew_mean      | 146      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2956     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 2994     |\n",
            "|    total_timesteps  | 1632103  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.117    |\n",
            "|    n_updates        | 395525   |\n",
            "----------------------------------\n",
            "Num timesteps: 1633000\n",
            "Best mean reward: 147.73 - Last mean reward per episode: 146.05\n",
            "Num timesteps: 1634000\n",
            "Best mean reward: 147.73 - Last mean reward per episode: 145.33\n",
            "Num timesteps: 1635000\n",
            "Best mean reward: 147.73 - Last mean reward per episode: 145.03\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 759      |\n",
            "|    ep_rew_mean      | 143      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2960     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 3001     |\n",
            "|    total_timesteps  | 1635445  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.13     |\n",
            "|    n_updates        | 396361   |\n",
            "----------------------------------\n",
            "Num timesteps: 1636000\n",
            "Best mean reward: 147.73 - Last mean reward per episode: 143.01\n",
            "Num timesteps: 1637000\n",
            "Best mean reward: 147.73 - Last mean reward per episode: 141.56\n",
            "Num timesteps: 1638000\n",
            "Best mean reward: 147.73 - Last mean reward per episode: 141.04\n",
            "Num timesteps: 1639000\n",
            "Best mean reward: 147.73 - Last mean reward per episode: 143.00\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 761      |\n",
            "|    ep_rew_mean      | 141      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2964     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 3008     |\n",
            "|    total_timesteps  | 1639111  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.176    |\n",
            "|    n_updates        | 397277   |\n",
            "----------------------------------\n",
            "Num timesteps: 1640000\n",
            "Best mean reward: 147.73 - Last mean reward per episode: 140.43\n",
            "Num timesteps: 1641000\n",
            "Best mean reward: 147.73 - Last mean reward per episode: 140.58\n",
            "Num timesteps: 1642000\n",
            "Best mean reward: 147.73 - Last mean reward per episode: 143.41\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 765      |\n",
            "|    ep_rew_mean      | 143      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2968     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 3013     |\n",
            "|    total_timesteps  | 1642014  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.123    |\n",
            "|    n_updates        | 398003   |\n",
            "----------------------------------\n",
            "Num timesteps: 1643000\n",
            "Best mean reward: 147.73 - Last mean reward per episode: 144.65\n",
            "Num timesteps: 1644000\n",
            "Best mean reward: 147.73 - Last mean reward per episode: 145.83\n",
            "Num timesteps: 1645000\n",
            "Best mean reward: 147.73 - Last mean reward per episode: 145.74\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 760      |\n",
            "|    ep_rew_mean      | 147      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2972     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 3019     |\n",
            "|    total_timesteps  | 1645329  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.272    |\n",
            "|    n_updates        | 398832   |\n",
            "----------------------------------\n",
            "Num timesteps: 1646000\n",
            "Best mean reward: 147.73 - Last mean reward per episode: 146.66\n",
            "Num timesteps: 1647000\n",
            "Best mean reward: 147.73 - Last mean reward per episode: 146.43\n",
            "Num timesteps: 1648000\n",
            "Best mean reward: 147.73 - Last mean reward per episode: 148.14\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1649000\n",
            "Best mean reward: 148.14 - Last mean reward per episode: 146.23\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 759      |\n",
            "|    ep_rew_mean      | 146      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2976     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 3027     |\n",
            "|    total_timesteps  | 1649092  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.165    |\n",
            "|    n_updates        | 399772   |\n",
            "----------------------------------\n",
            "Num timesteps: 1650000\n",
            "Best mean reward: 148.14 - Last mean reward per episode: 146.43\n",
            "Num timesteps: 1651000\n",
            "Best mean reward: 148.14 - Last mean reward per episode: 144.38\n",
            "Num timesteps: 1652000\n",
            "Best mean reward: 148.14 - Last mean reward per episode: 144.56\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 764      |\n",
            "|    ep_rew_mean      | 147      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2980     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 3033     |\n",
            "|    total_timesteps  | 1652294  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.13     |\n",
            "|    n_updates        | 400573   |\n",
            "----------------------------------\n",
            "Num timesteps: 1653000\n",
            "Best mean reward: 148.14 - Last mean reward per episode: 146.91\n",
            "Num timesteps: 1654000\n",
            "Best mean reward: 148.14 - Last mean reward per episode: 145.68\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 756      |\n",
            "|    ep_rew_mean      | 148      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2984     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 3037     |\n",
            "|    total_timesteps  | 1654930  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.147    |\n",
            "|    n_updates        | 401232   |\n",
            "----------------------------------\n",
            "Num timesteps: 1655000\n",
            "Best mean reward: 148.14 - Last mean reward per episode: 148.49\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1656000\n",
            "Best mean reward: 148.49 - Last mean reward per episode: 150.26\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1657000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 149.09\n",
            "Num timesteps: 1658000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 146.68\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 765      |\n",
            "|    ep_rew_mean      | 144      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2988     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 3044     |\n",
            "|    total_timesteps  | 1658164  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.13     |\n",
            "|    n_updates        | 402040   |\n",
            "----------------------------------\n",
            "Num timesteps: 1659000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 144.21\n",
            "Num timesteps: 1660000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 143.63\n",
            "Num timesteps: 1661000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 140.75\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 768      |\n",
            "|    ep_rew_mean      | 142      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2992     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 3051     |\n",
            "|    total_timesteps  | 1661609  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.13     |\n",
            "|    n_updates        | 402902   |\n",
            "----------------------------------\n",
            "Num timesteps: 1662000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 141.71\n",
            "Num timesteps: 1663000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 141.69\n",
            "Num timesteps: 1664000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 140.70\n",
            "Num timesteps: 1665000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 140.46\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 775      |\n",
            "|    ep_rew_mean      | 139      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2996     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 3059     |\n",
            "|    total_timesteps  | 1665313  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.63     |\n",
            "|    n_updates        | 403828   |\n",
            "----------------------------------\n",
            "Num timesteps: 1666000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 138.87\n",
            "Num timesteps: 1667000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 136.97\n",
            "Num timesteps: 1668000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 135.36\n",
            "Num timesteps: 1669000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 135.37\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 779      |\n",
            "|    ep_rew_mean      | 137      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3000     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 3067     |\n",
            "|    total_timesteps  | 1669003  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.35     |\n",
            "|    n_updates        | 404750   |\n",
            "----------------------------------\n",
            "Num timesteps: 1670000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 137.07\n",
            "Num timesteps: 1671000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 135.29\n",
            "Num timesteps: 1672000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 135.55\n",
            "Num timesteps: 1673000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 135.41\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 782      |\n",
            "|    ep_rew_mean      | 135      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3004     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 3075     |\n",
            "|    total_timesteps  | 1673003  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.197    |\n",
            "|    n_updates        | 405750   |\n",
            "----------------------------------\n",
            "Num timesteps: 1674000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 135.35\n",
            "Num timesteps: 1675000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 135.14\n",
            "Num timesteps: 1676000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 135.13\n",
            "Num timesteps: 1677000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 135.48\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 782      |\n",
            "|    ep_rew_mean      | 136      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3008     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3083     |\n",
            "|    total_timesteps  | 1677003  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.197    |\n",
            "|    n_updates        | 406750   |\n",
            "----------------------------------\n",
            "Num timesteps: 1678000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 136.63\n",
            "Num timesteps: 1679000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 135.90\n",
            "Num timesteps: 1680000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 134.13\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 788      |\n",
            "|    ep_rew_mean      | 132      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3012     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3090     |\n",
            "|    total_timesteps  | 1680643  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0764   |\n",
            "|    n_updates        | 407660   |\n",
            "----------------------------------\n",
            "Num timesteps: 1681000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 132.45\n",
            "Num timesteps: 1682000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 130.07\n",
            "Num timesteps: 1683000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 130.40\n",
            "Num timesteps: 1684000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 129.79\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 792      |\n",
            "|    ep_rew_mean      | 130      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3016     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3097     |\n",
            "|    total_timesteps  | 1684268  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0744   |\n",
            "|    n_updates        | 408566   |\n",
            "----------------------------------\n",
            "Num timesteps: 1685000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 129.93\n",
            "Num timesteps: 1686000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 128.29\n",
            "Num timesteps: 1687000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 129.01\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 794      |\n",
            "|    ep_rew_mean      | 128      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3020     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3103     |\n",
            "|    total_timesteps  | 1687676  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.216    |\n",
            "|    n_updates        | 409418   |\n",
            "----------------------------------\n",
            "Num timesteps: 1688000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 127.69\n",
            "Num timesteps: 1689000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 125.89\n",
            "Num timesteps: 1690000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 124.51\n",
            "Num timesteps: 1691000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 122.43\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 802      |\n",
            "|    ep_rew_mean      | 123      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3024     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3111     |\n",
            "|    total_timesteps  | 1691410  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0901   |\n",
            "|    n_updates        | 410352   |\n",
            "----------------------------------\n",
            "Num timesteps: 1692000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 122.58\n",
            "Num timesteps: 1693000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 121.08\n",
            "Num timesteps: 1694000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 119.47\n",
            "Num timesteps: 1695000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 117.07\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 816      |\n",
            "|    ep_rew_mean      | 115      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3028     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3119     |\n",
            "|    total_timesteps  | 1695410  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.116    |\n",
            "|    n_updates        | 411352   |\n",
            "----------------------------------\n",
            "Num timesteps: 1696000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 114.67\n",
            "Num timesteps: 1697000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 111.96\n",
            "Num timesteps: 1698000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 110.47\n",
            "Num timesteps: 1699000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 108.30\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 832      |\n",
            "|    ep_rew_mean      | 106      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3032     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3127     |\n",
            "|    total_timesteps  | 1699410  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0717   |\n",
            "|    n_updates        | 412352   |\n",
            "----------------------------------\n",
            "Num timesteps: 1700000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 105.86\n",
            "Num timesteps: 1701000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 104.06\n",
            "Num timesteps: 1702000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 102.91\n",
            "Num timesteps: 1703000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 100.43\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 843      |\n",
            "|    ep_rew_mean      | 98.9     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3036     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3135     |\n",
            "|    total_timesteps  | 1703410  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0934   |\n",
            "|    n_updates        | 413352   |\n",
            "----------------------------------\n",
            "Num timesteps: 1704000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 98.94\n",
            "Num timesteps: 1705000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 99.10\n",
            "Num timesteps: 1706000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 100.57\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 837      |\n",
            "|    ep_rew_mean      | 101      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3040     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3140     |\n",
            "|    total_timesteps  | 1706013  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0828   |\n",
            "|    n_updates        | 414003   |\n",
            "----------------------------------\n",
            "Num timesteps: 1707000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 100.98\n",
            "Num timesteps: 1708000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 99.54\n",
            "Num timesteps: 1709000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 96.10\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 852      |\n",
            "|    ep_rew_mean      | 93.4     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3044     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3147     |\n",
            "|    total_timesteps  | 1709831  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.119    |\n",
            "|    n_updates        | 414957   |\n",
            "----------------------------------\n",
            "Num timesteps: 1710000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 93.43\n",
            "Num timesteps: 1711000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 92.99\n",
            "Num timesteps: 1712000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 93.31\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 856      |\n",
            "|    ep_rew_mean      | 91.2     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3048     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3152     |\n",
            "|    total_timesteps  | 1712451  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.14     |\n",
            "|    n_updates        | 415612   |\n",
            "----------------------------------\n",
            "Num timesteps: 1713000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 91.17\n",
            "Num timesteps: 1714000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 90.92\n",
            "Num timesteps: 1715000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 89.45\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 866      |\n",
            "|    ep_rew_mean      | 87.2     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3052     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3158     |\n",
            "|    total_timesteps  | 1715658  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.102    |\n",
            "|    n_updates        | 416414   |\n",
            "----------------------------------\n",
            "Num timesteps: 1716000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 87.24\n",
            "Num timesteps: 1717000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 85.28\n",
            "Num timesteps: 1718000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 83.64\n",
            "Num timesteps: 1719000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 82.78\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 870      |\n",
            "|    ep_rew_mean      | 84.7     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3056     |\n",
            "|    fps              | 542      |\n",
            "|    time_elapsed     | 3166     |\n",
            "|    total_timesteps  | 1719138  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0664   |\n",
            "|    n_updates        | 417284   |\n",
            "----------------------------------\n",
            "Num timesteps: 1720000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 85.17\n",
            "Num timesteps: 1721000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 83.71\n",
            "Num timesteps: 1722000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 84.06\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 869      |\n",
            "|    ep_rew_mean      | 85.5     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3060     |\n",
            "|    fps              | 542      |\n",
            "|    time_elapsed     | 3172     |\n",
            "|    total_timesteps  | 1722323  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.75     |\n",
            "|    n_updates        | 418080   |\n",
            "----------------------------------\n",
            "Num timesteps: 1723000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 85.49\n",
            "Num timesteps: 1724000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 85.13\n",
            "Num timesteps: 1725000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 85.57\n",
            "Num timesteps: 1726000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 83.88\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 872      |\n",
            "|    ep_rew_mean      | 83.7     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3064     |\n",
            "|    fps              | 542      |\n",
            "|    time_elapsed     | 3181     |\n",
            "|    total_timesteps  | 1726323  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0988   |\n",
            "|    n_updates        | 419080   |\n",
            "----------------------------------\n",
            "Num timesteps: 1727000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 83.66\n",
            "Num timesteps: 1728000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 82.10\n",
            "Num timesteps: 1729000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 81.70\n",
            "Num timesteps: 1730000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 79.00\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 882      |\n",
            "|    ep_rew_mean      | 77.2     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3068     |\n",
            "|    fps              | 542      |\n",
            "|    time_elapsed     | 3188     |\n",
            "|    total_timesteps  | 1730224  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.61     |\n",
            "|    n_updates        | 420055   |\n",
            "----------------------------------\n",
            "Num timesteps: 1731000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 77.56\n",
            "Num timesteps: 1732000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 76.25\n",
            "Num timesteps: 1733000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 76.30\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 880      |\n",
            "|    ep_rew_mean      | 75.8     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3072     |\n",
            "|    fps              | 542      |\n",
            "|    time_elapsed     | 3194     |\n",
            "|    total_timesteps  | 1733282  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.123    |\n",
            "|    n_updates        | 420820   |\n",
            "----------------------------------\n",
            "Num timesteps: 1734000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 78.50\n",
            "Num timesteps: 1735000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 79.39\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 864      |\n",
            "|    ep_rew_mean      | 81.8     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3076     |\n",
            "|    fps              | 542      |\n",
            "|    time_elapsed     | 3198     |\n",
            "|    total_timesteps  | 1735526  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.134    |\n",
            "|    n_updates        | 421381   |\n",
            "----------------------------------\n",
            "Num timesteps: 1736000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 81.79\n",
            "Num timesteps: 1737000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 83.37\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 854      |\n",
            "|    ep_rew_mean      | 83.8     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3080     |\n",
            "|    fps              | 542      |\n",
            "|    time_elapsed     | 3201     |\n",
            "|    total_timesteps  | 1737697  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.115    |\n",
            "|    n_updates        | 421924   |\n",
            "----------------------------------\n",
            "Num timesteps: 1738000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 83.77\n",
            "Num timesteps: 1739000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 85.76\n",
            "Num timesteps: 1740000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 85.96\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 853      |\n",
            "|    ep_rew_mean      | 83       |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3084     |\n",
            "|    fps              | 542      |\n",
            "|    time_elapsed     | 3205     |\n",
            "|    total_timesteps  | 1740228  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.113    |\n",
            "|    n_updates        | 422556   |\n",
            "----------------------------------\n",
            "Num timesteps: 1741000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 83.15\n",
            "Num timesteps: 1742000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 83.10\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 846      |\n",
            "|    ep_rew_mean      | 87.6     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3088     |\n",
            "|    fps              | 542      |\n",
            "|    time_elapsed     | 3209     |\n",
            "|    total_timesteps  | 1742793  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.4      |\n",
            "|    n_updates        | 423198   |\n",
            "----------------------------------\n",
            "Num timesteps: 1743000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 87.62\n",
            "Num timesteps: 1744000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 92.30\n",
            "Num timesteps: 1745000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 93.58\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 836      |\n",
            "|    ep_rew_mean      | 92.7     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3092     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3213     |\n",
            "|    total_timesteps  | 1745258  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.252    |\n",
            "|    n_updates        | 423814   |\n",
            "----------------------------------\n",
            "Num timesteps: 1746000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 92.75\n",
            "Num timesteps: 1747000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 90.78\n",
            "Num timesteps: 1748000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 90.98\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 827      |\n",
            "|    ep_rew_mean      | 93.8     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3096     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3219     |\n",
            "|    total_timesteps  | 1748031  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.13     |\n",
            "|    n_updates        | 424507   |\n",
            "----------------------------------\n",
            "Num timesteps: 1749000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 93.77\n",
            "Num timesteps: 1750000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 94.50\n",
            "Num timesteps: 1751000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 97.00\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 822      |\n",
            "|    ep_rew_mean      | 95.2     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3100     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3224     |\n",
            "|    total_timesteps  | 1751154  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0983   |\n",
            "|    n_updates        | 425288   |\n",
            "----------------------------------\n",
            "Num timesteps: 1752000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 95.16\n",
            "Num timesteps: 1753000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 97.69\n",
            "Num timesteps: 1754000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 97.35\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 818      |\n",
            "|    ep_rew_mean      | 97.4     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3104     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3231     |\n",
            "|    total_timesteps  | 1754848  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.135    |\n",
            "|    n_updates        | 426211   |\n",
            "----------------------------------\n",
            "Num timesteps: 1755000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 97.39\n",
            "Num timesteps: 1756000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 97.49\n",
            "Num timesteps: 1757000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 100.30\n",
            "Num timesteps: 1758000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 100.07\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 812      |\n",
            "|    ep_rew_mean      | 101      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3108     |\n",
            "|    fps              | 542      |\n",
            "|    time_elapsed     | 3238     |\n",
            "|    total_timesteps  | 1758227  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.059    |\n",
            "|    n_updates        | 427056   |\n",
            "----------------------------------\n",
            "Num timesteps: 1759000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 101.59\n",
            "Num timesteps: 1760000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 104.77\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 804      |\n",
            "|    ep_rew_mean      | 105      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3112     |\n",
            "|    fps              | 542      |\n",
            "|    time_elapsed     | 3243     |\n",
            "|    total_timesteps  | 1760997  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.287    |\n",
            "|    n_updates        | 427749   |\n",
            "----------------------------------\n",
            "Num timesteps: 1761000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 104.50\n",
            "Num timesteps: 1762000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 106.07\n",
            "Num timesteps: 1763000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 105.78\n",
            "Num timesteps: 1764000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 104.23\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 807      |\n",
            "|    ep_rew_mean      | 104      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3116     |\n",
            "|    fps              | 542      |\n",
            "|    time_elapsed     | 3252     |\n",
            "|    total_timesteps  | 1764926  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.12     |\n",
            "|    n_updates        | 428731   |\n",
            "----------------------------------\n",
            "Num timesteps: 1765000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 104.02\n",
            "Num timesteps: 1766000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 104.29\n",
            "Num timesteps: 1767000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 103.95\n",
            "Num timesteps: 1768000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 101.23\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 812      |\n",
            "|    ep_rew_mean      | 101      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3120     |\n",
            "|    fps              | 542      |\n",
            "|    time_elapsed     | 3261     |\n",
            "|    total_timesteps  | 1768926  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.148    |\n",
            "|    n_updates        | 429731   |\n",
            "----------------------------------\n",
            "Num timesteps: 1769000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 100.86\n",
            "Num timesteps: 1770000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 100.71\n",
            "Num timesteps: 1771000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 102.20\n",
            "Num timesteps: 1772000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 104.08\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 811      |\n",
            "|    ep_rew_mean      | 102      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3124     |\n",
            "|    fps              | 542      |\n",
            "|    time_elapsed     | 3267     |\n",
            "|    total_timesteps  | 1772559  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.137    |\n",
            "|    n_updates        | 430639   |\n",
            "----------------------------------\n",
            "Num timesteps: 1773000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 102.25\n",
            "Num timesteps: 1774000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 102.24\n",
            "Num timesteps: 1775000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 102.32\n",
            "Num timesteps: 1776000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 103.90\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 809      |\n",
            "|    ep_rew_mean      | 104      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3128     |\n",
            "|    fps              | 542      |\n",
            "|    time_elapsed     | 3275     |\n",
            "|    total_timesteps  | 1776323  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.187    |\n",
            "|    n_updates        | 431580   |\n",
            "----------------------------------\n",
            "Num timesteps: 1777000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 106.93\n",
            "Num timesteps: 1778000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 106.23\n",
            "Num timesteps: 1779000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 106.71\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 801      |\n",
            "|    ep_rew_mean      | 108      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3132     |\n",
            "|    fps              | 542      |\n",
            "|    time_elapsed     | 3281     |\n",
            "|    total_timesteps  | 1779482  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.348    |\n",
            "|    n_updates        | 432370   |\n",
            "----------------------------------\n",
            "Num timesteps: 1780000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 108.24\n",
            "Num timesteps: 1781000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 112.40\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 782      |\n",
            "|    ep_rew_mean      | 116      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3136     |\n",
            "|    fps              | 542      |\n",
            "|    time_elapsed     | 3284     |\n",
            "|    total_timesteps  | 1781564  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.152    |\n",
            "|    n_updates        | 432890   |\n",
            "----------------------------------\n",
            "Num timesteps: 1782000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 118.83\n",
            "Num timesteps: 1783000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 118.17\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 778      |\n",
            "|    ep_rew_mean      | 118      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3140     |\n",
            "|    fps              | 542      |\n",
            "|    time_elapsed     | 3287     |\n",
            "|    total_timesteps  | 1783832  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0711   |\n",
            "|    n_updates        | 433457   |\n",
            "----------------------------------\n",
            "Num timesteps: 1784000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 118.44\n",
            "Num timesteps: 1785000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 120.34\n",
            "Num timesteps: 1786000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 122.05\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 768      |\n",
            "|    ep_rew_mean      | 122      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3144     |\n",
            "|    fps              | 542      |\n",
            "|    time_elapsed     | 3292     |\n",
            "|    total_timesteps  | 1786643  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.253    |\n",
            "|    n_updates        | 434160   |\n",
            "----------------------------------\n",
            "Num timesteps: 1787000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 122.22\n",
            "Num timesteps: 1788000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 123.03\n",
            "Num timesteps: 1789000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 123.31\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 768      |\n",
            "|    ep_rew_mean      | 123      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3148     |\n",
            "|    fps              | 542      |\n",
            "|    time_elapsed     | 3296     |\n",
            "|    total_timesteps  | 1789253  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.111    |\n",
            "|    n_updates        | 434813   |\n",
            "----------------------------------\n",
            "Num timesteps: 1790000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 123.92\n",
            "Num timesteps: 1791000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 124.51\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 762      |\n",
            "|    ep_rew_mean      | 125      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3152     |\n",
            "|    fps              | 542      |\n",
            "|    time_elapsed     | 3301     |\n",
            "|    total_timesteps  | 1791847  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.458    |\n",
            "|    n_updates        | 435461   |\n",
            "----------------------------------\n",
            "Num timesteps: 1792000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 125.19\n",
            "Num timesteps: 1793000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 125.34\n",
            "Num timesteps: 1794000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 127.47\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 757      |\n",
            "|    ep_rew_mean      | 127      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3156     |\n",
            "|    fps              | 542      |\n",
            "|    time_elapsed     | 3307     |\n",
            "|    total_timesteps  | 1794825  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0766   |\n",
            "|    n_updates        | 436206   |\n",
            "----------------------------------\n",
            "Num timesteps: 1795000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 126.58\n",
            "Num timesteps: 1796000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 124.69\n",
            "Num timesteps: 1797000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 126.54\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 755      |\n",
            "|    ep_rew_mean      | 126      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3160     |\n",
            "|    fps              | 542      |\n",
            "|    time_elapsed     | 3312     |\n",
            "|    total_timesteps  | 1797845  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0859   |\n",
            "|    n_updates        | 436961   |\n",
            "----------------------------------\n",
            "Num timesteps: 1798000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 125.83\n",
            "Num timesteps: 1799000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 127.30\n",
            "Num timesteps: 1800000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 131.43\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 746      |\n",
            "|    ep_rew_mean      | 131      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3164     |\n",
            "|    fps              | 542      |\n",
            "|    time_elapsed     | 3318     |\n",
            "|    total_timesteps  | 1800881  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.166    |\n",
            "|    n_updates        | 437720   |\n",
            "----------------------------------\n",
            "Num timesteps: 1801000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 131.45\n",
            "Num timesteps: 1802000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 134.67\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 728      |\n",
            "|    ep_rew_mean      | 139      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3168     |\n",
            "|    fps              | 542      |\n",
            "|    time_elapsed     | 3321     |\n",
            "|    total_timesteps  | 1802984  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0802   |\n",
            "|    n_updates        | 438245   |\n",
            "----------------------------------\n",
            "Num timesteps: 1803000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 138.84\n",
            "Num timesteps: 1804000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 138.39\n",
            "Num timesteps: 1805000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 138.17\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 724      |\n",
            "|    ep_rew_mean      | 139      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3172     |\n",
            "|    fps              | 542      |\n",
            "|    time_elapsed     | 3326     |\n",
            "|    total_timesteps  | 1805648  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.435    |\n",
            "|    n_updates        | 438911   |\n",
            "----------------------------------\n",
            "Num timesteps: 1806000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 138.98\n",
            "Num timesteps: 1807000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 138.78\n",
            "Num timesteps: 1808000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 140.70\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 728      |\n",
            "|    ep_rew_mean      | 138      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3176     |\n",
            "|    fps              | 542      |\n",
            "|    time_elapsed     | 3331     |\n",
            "|    total_timesteps  | 1808376  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.095    |\n",
            "|    n_updates        | 439593   |\n",
            "----------------------------------\n",
            "Num timesteps: 1809000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 139.04\n",
            "Num timesteps: 1810000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 139.77\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 726      |\n",
            "|    ep_rew_mean      | 140      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3180     |\n",
            "|    fps              | 542      |\n",
            "|    time_elapsed     | 3333     |\n",
            "|    total_timesteps  | 1810306  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0948   |\n",
            "|    n_updates        | 440076   |\n",
            "----------------------------------\n",
            "Num timesteps: 1811000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 139.77\n",
            "Num timesteps: 1812000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 140.22\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 723      |\n",
            "|    ep_rew_mean      | 143      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3184     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3337     |\n",
            "|    total_timesteps  | 1812537  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.146    |\n",
            "|    n_updates        | 440634   |\n",
            "----------------------------------\n",
            "Num timesteps: 1813000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 142.71\n",
            "Num timesteps: 1814000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 144.31\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 721      |\n",
            "|    ep_rew_mean      | 141      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3188     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3341     |\n",
            "|    total_timesteps  | 1814892  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0861   |\n",
            "|    n_updates        | 441222   |\n",
            "----------------------------------\n",
            "Num timesteps: 1815000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 141.48\n",
            "Num timesteps: 1816000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 142.12\n",
            "Num timesteps: 1817000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 142.70\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 719      |\n",
            "|    ep_rew_mean      | 143      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3192     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3345     |\n",
            "|    total_timesteps  | 1817148  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.126    |\n",
            "|    n_updates        | 441786   |\n",
            "----------------------------------\n",
            "Num timesteps: 1818000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 145.65\n",
            "Num timesteps: 1819000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 146.58\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 712      |\n",
            "|    ep_rew_mean      | 146      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3196     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3348     |\n",
            "|    total_timesteps  | 1819273  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0943   |\n",
            "|    n_updates        | 442318   |\n",
            "----------------------------------\n",
            "Num timesteps: 1820000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 148.73\n",
            "Num timesteps: 1821000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 149.73\n",
            "Num timesteps: 1822000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 148.36\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 709      |\n",
            "|    ep_rew_mean      | 148      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3200     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3352     |\n",
            "|    total_timesteps  | 1822033  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.203    |\n",
            "|    n_updates        | 443008   |\n",
            "----------------------------------\n",
            "Num timesteps: 1823000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 148.72\n",
            "Num timesteps: 1824000\n",
            "Best mean reward: 150.26 - Last mean reward per episode: 152.01\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 697      |\n",
            "|    ep_rew_mean      | 152      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3204     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3357     |\n",
            "|    total_timesteps  | 1824554  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.07     |\n",
            "|    n_updates        | 443638   |\n",
            "----------------------------------\n",
            "Num timesteps: 1825000\n",
            "Best mean reward: 152.01 - Last mean reward per episode: 151.83\n",
            "Num timesteps: 1826000\n",
            "Best mean reward: 152.01 - Last mean reward per episode: 151.65\n",
            "Num timesteps: 1827000\n",
            "Best mean reward: 152.01 - Last mean reward per episode: 148.71\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 696      |\n",
            "|    ep_rew_mean      | 152      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3208     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3363     |\n",
            "|    total_timesteps  | 1827793  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0879   |\n",
            "|    n_updates        | 444448   |\n",
            "----------------------------------\n",
            "Num timesteps: 1828000\n",
            "Best mean reward: 152.01 - Last mean reward per episode: 152.03\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1829000\n",
            "Best mean reward: 152.03 - Last mean reward per episode: 149.15\n",
            "Num timesteps: 1830000\n",
            "Best mean reward: 152.03 - Last mean reward per episode: 151.78\n",
            "Num timesteps: 1831000\n",
            "Best mean reward: 152.03 - Last mean reward per episode: 149.13\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 701      |\n",
            "|    ep_rew_mean      | 149      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3212     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3370     |\n",
            "|    total_timesteps  | 1831095  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.129    |\n",
            "|    n_updates        | 445273   |\n",
            "----------------------------------\n",
            "Num timesteps: 1832000\n",
            "Best mean reward: 152.03 - Last mean reward per episode: 148.93\n",
            "Num timesteps: 1833000\n",
            "Best mean reward: 152.03 - Last mean reward per episode: 151.18\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 687      |\n",
            "|    ep_rew_mean      | 155      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3216     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3374     |\n",
            "|    total_timesteps  | 1833633  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0815   |\n",
            "|    n_updates        | 445908   |\n",
            "----------------------------------\n",
            "Num timesteps: 1834000\n",
            "Best mean reward: 152.03 - Last mean reward per episode: 157.68\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1835000\n",
            "Best mean reward: 157.68 - Last mean reward per episode: 157.94\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1836000\n",
            "Best mean reward: 157.94 - Last mean reward per episode: 160.03\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 677      |\n",
            "|    ep_rew_mean      | 160      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3220     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3380     |\n",
            "|    total_timesteps  | 1836663  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.369    |\n",
            "|    n_updates        | 446665   |\n",
            "----------------------------------\n",
            "Num timesteps: 1837000\n",
            "Best mean reward: 160.03 - Last mean reward per episode: 160.10\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1838000\n",
            "Best mean reward: 160.10 - Last mean reward per episode: 160.85\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1839000\n",
            "Best mean reward: 160.85 - Last mean reward per episode: 159.01\n",
            "Num timesteps: 1840000\n",
            "Best mean reward: 160.85 - Last mean reward per episode: 157.52\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 676      |\n",
            "|    ep_rew_mean      | 159      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3224     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3386     |\n",
            "|    total_timesteps  | 1840202  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.158    |\n",
            "|    n_updates        | 447550   |\n",
            "----------------------------------\n",
            "Num timesteps: 1841000\n",
            "Best mean reward: 160.85 - Last mean reward per episode: 159.45\n",
            "Num timesteps: 1842000\n",
            "Best mean reward: 160.85 - Last mean reward per episode: 161.47\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1843000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 159.48\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 674      |\n",
            "|    ep_rew_mean      | 161      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3228     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3393     |\n",
            "|    total_timesteps  | 1843691  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.18     |\n",
            "|    n_updates        | 448422   |\n",
            "----------------------------------\n",
            "Num timesteps: 1844000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 161.24\n",
            "Num timesteps: 1845000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 158.69\n",
            "Num timesteps: 1846000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 158.64\n",
            "Num timesteps: 1847000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 158.73\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 679      |\n",
            "|    ep_rew_mean      | 159      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3232     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3400     |\n",
            "|    total_timesteps  | 1847335  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.138    |\n",
            "|    n_updates        | 449333   |\n",
            "----------------------------------\n",
            "Num timesteps: 1848000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 159.98\n",
            "Num timesteps: 1849000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 159.31\n",
            "Num timesteps: 1850000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 156.94\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 684      |\n",
            "|    ep_rew_mean      | 157      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3236     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3405     |\n",
            "|    total_timesteps  | 1850004  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.266    |\n",
            "|    n_updates        | 450000   |\n",
            "----------------------------------\n",
            "Num timesteps: 1851000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 157.26\n",
            "Num timesteps: 1852000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 156.51\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 689      |\n",
            "|    ep_rew_mean      | 155      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3240     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3410     |\n",
            "|    total_timesteps  | 1852752  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0892   |\n",
            "|    n_updates        | 450687   |\n",
            "----------------------------------\n",
            "Num timesteps: 1853000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 155.24\n",
            "Num timesteps: 1854000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 155.31\n",
            "Num timesteps: 1855000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 154.45\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 686      |\n",
            "|    ep_rew_mean      | 156      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3244     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3415     |\n",
            "|    total_timesteps  | 1855201  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.113    |\n",
            "|    n_updates        | 451300   |\n",
            "----------------------------------\n",
            "Num timesteps: 1856000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 156.22\n",
            "Num timesteps: 1857000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 153.80\n",
            "Num timesteps: 1858000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 151.35\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 688      |\n",
            "|    ep_rew_mean      | 154      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3248     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3420     |\n",
            "|    total_timesteps  | 1858026  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0568   |\n",
            "|    n_updates        | 452006   |\n",
            "----------------------------------\n",
            "Num timesteps: 1859000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 153.85\n",
            "Num timesteps: 1860000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 151.43\n",
            "Num timesteps: 1861000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 150.16\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 692      |\n",
            "|    ep_rew_mean      | 151      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3252     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3425     |\n",
            "|    total_timesteps  | 1861076  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0731   |\n",
            "|    n_updates        | 452768   |\n",
            "----------------------------------\n",
            "Num timesteps: 1862000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 151.12\n",
            "Num timesteps: 1863000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 150.36\n",
            "Num timesteps: 1864000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 153.17\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 693      |\n",
            "|    ep_rew_mean      | 151      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3256     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3431     |\n",
            "|    total_timesteps  | 1864159  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.122    |\n",
            "|    n_updates        | 453539   |\n",
            "----------------------------------\n",
            "Num timesteps: 1865000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 152.90\n",
            "Num timesteps: 1866000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 152.38\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 685      |\n",
            "|    ep_rew_mean      | 152      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3260     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3435     |\n",
            "|    total_timesteps  | 1866368  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.229    |\n",
            "|    n_updates        | 454091   |\n",
            "----------------------------------\n",
            "Num timesteps: 1867000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 152.44\n",
            "Num timesteps: 1868000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 153.65\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 681      |\n",
            "|    ep_rew_mean      | 154      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3264     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3439     |\n",
            "|    total_timesteps  | 1868952  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.167    |\n",
            "|    n_updates        | 454737   |\n",
            "----------------------------------\n",
            "Num timesteps: 1869000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 154.34\n",
            "Num timesteps: 1870000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 154.54\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 679      |\n",
            "|    ep_rew_mean      | 154      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3268     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3442     |\n",
            "|    total_timesteps  | 1870928  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.184    |\n",
            "|    n_updates        | 455231   |\n",
            "----------------------------------\n",
            "Num timesteps: 1871000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 153.92\n",
            "Num timesteps: 1872000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 152.19\n",
            "Num timesteps: 1873000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 154.52\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 676      |\n",
            "|    ep_rew_mean      | 154      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3272     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3445     |\n",
            "|    total_timesteps  | 1873216  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0968   |\n",
            "|    n_updates        | 455803   |\n",
            "----------------------------------\n",
            "Num timesteps: 1874000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 153.39\n",
            "Num timesteps: 1875000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 153.71\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 674      |\n",
            "|    ep_rew_mean      | 154      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3276     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3450     |\n",
            "|    total_timesteps  | 1875730  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.113    |\n",
            "|    n_updates        | 456432   |\n",
            "----------------------------------\n",
            "Num timesteps: 1876000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 154.32\n",
            "Num timesteps: 1877000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 153.83\n",
            "Num timesteps: 1878000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 153.94\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 678      |\n",
            "|    ep_rew_mean      | 151      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3280     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3453     |\n",
            "|    total_timesteps  | 1878096  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.195    |\n",
            "|    n_updates        | 457023   |\n",
            "----------------------------------\n",
            "Num timesteps: 1879000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 151.25\n",
            "Num timesteps: 1880000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 150.97\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 680      |\n",
            "|    ep_rew_mean      | 151      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3284     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3457     |\n",
            "|    total_timesteps  | 1880491  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.56     |\n",
            "|    n_updates        | 457622   |\n",
            "----------------------------------\n",
            "Num timesteps: 1881000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 150.63\n",
            "Num timesteps: 1882000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 148.12\n",
            "Num timesteps: 1883000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 147.32\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 689      |\n",
            "|    ep_rew_mean      | 147      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3288     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3463     |\n",
            "|    total_timesteps  | 1883804  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.129    |\n",
            "|    n_updates        | 458450   |\n",
            "----------------------------------\n",
            "Num timesteps: 1884000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 147.15\n",
            "Num timesteps: 1885000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 146.60\n",
            "Num timesteps: 1886000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 146.87\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 691      |\n",
            "|    ep_rew_mean      | 145      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3292     |\n",
            "|    fps              | 543      |\n",
            "|    time_elapsed     | 3467     |\n",
            "|    total_timesteps  | 1886243  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0905   |\n",
            "|    n_updates        | 459060   |\n",
            "----------------------------------\n",
            "Num timesteps: 1887000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 144.46\n",
            "Num timesteps: 1888000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 145.70\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 694      |\n",
            "|    ep_rew_mean      | 145      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3296     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 3471     |\n",
            "|    total_timesteps  | 1888630  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0744   |\n",
            "|    n_updates        | 459657   |\n",
            "----------------------------------\n",
            "Num timesteps: 1889000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 144.41\n",
            "Num timesteps: 1890000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 143.43\n",
            "Num timesteps: 1891000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 142.53\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 698      |\n",
            "|    ep_rew_mean      | 142      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3300     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 3477     |\n",
            "|    total_timesteps  | 1891850  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.066    |\n",
            "|    n_updates        | 460462   |\n",
            "----------------------------------\n",
            "Num timesteps: 1892000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 142.41\n",
            "Num timesteps: 1893000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 142.99\n",
            "Num timesteps: 1894000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 140.59\n",
            "Num timesteps: 1895000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 139.66\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 706      |\n",
            "|    ep_rew_mean      | 140      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3304     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 3483     |\n",
            "|    total_timesteps  | 1895167  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.137    |\n",
            "|    n_updates        | 461291   |\n",
            "----------------------------------\n",
            "Num timesteps: 1896000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 142.79\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 692      |\n",
            "|    ep_rew_mean      | 145      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3308     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 3486     |\n",
            "|    total_timesteps  | 1896993  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.105    |\n",
            "|    n_updates        | 461748   |\n",
            "----------------------------------\n",
            "Num timesteps: 1897000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 144.87\n",
            "Num timesteps: 1898000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 144.90\n",
            "Num timesteps: 1899000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 142.39\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 686      |\n",
            "|    ep_rew_mean      | 148      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3312     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 3491     |\n",
            "|    total_timesteps  | 1899700  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.091    |\n",
            "|    n_updates        | 462424   |\n",
            "----------------------------------\n",
            "Num timesteps: 1900000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 148.11\n",
            "Num timesteps: 1901000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 147.94\n",
            "Num timesteps: 1902000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 145.37\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 686      |\n",
            "|    ep_rew_mean      | 145      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3316     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 3496     |\n",
            "|    total_timesteps  | 1902249  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.45     |\n",
            "|    n_updates        | 463062   |\n",
            "----------------------------------\n",
            "Num timesteps: 1903000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 145.08\n",
            "Num timesteps: 1904000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 145.03\n",
            "Num timesteps: 1905000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 143.17\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 685      |\n",
            "|    ep_rew_mean      | 146      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3320     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 3501     |\n",
            "|    total_timesteps  | 1905117  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.127    |\n",
            "|    n_updates        | 463779   |\n",
            "----------------------------------\n",
            "Num timesteps: 1906000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 149.69\n",
            "Num timesteps: 1907000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 151.61\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 676      |\n",
            "|    ep_rew_mean      | 150      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3324     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 3505     |\n",
            "|    total_timesteps  | 1907763  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0692   |\n",
            "|    n_updates        | 464440   |\n",
            "----------------------------------\n",
            "Num timesteps: 1908000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 149.67\n",
            "Num timesteps: 1909000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 155.39\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 656      |\n",
            "|    ep_rew_mean      | 156      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3328     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 3507     |\n",
            "|    total_timesteps  | 1909320  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0944   |\n",
            "|    n_updates        | 464829   |\n",
            "----------------------------------\n",
            "Num timesteps: 1910000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 158.75\n",
            "Num timesteps: 1911000\n",
            "Best mean reward: 161.47 - Last mean reward per episode: 163.39\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 642      |\n",
            "|    ep_rew_mean      | 163      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3332     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 3511     |\n",
            "|    total_timesteps  | 1911495  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.71     |\n",
            "|    n_updates        | 465373   |\n",
            "----------------------------------\n",
            "Num timesteps: 1912000\n",
            "Best mean reward: 163.39 - Last mean reward per episode: 163.50\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1913000\n",
            "Best mean reward: 163.50 - Last mean reward per episode: 166.57\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 634      |\n",
            "|    ep_rew_mean      | 167      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3336     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 3514     |\n",
            "|    total_timesteps  | 1913395  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.132    |\n",
            "|    n_updates        | 465848   |\n",
            "----------------------------------\n",
            "Num timesteps: 1914000\n",
            "Best mean reward: 166.57 - Last mean reward per episode: 166.67\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1915000\n",
            "Best mean reward: 166.67 - Last mean reward per episode: 168.91\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 627      |\n",
            "|    ep_rew_mean      | 170      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3340     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 3517     |\n",
            "|    total_timesteps  | 1915439  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0552   |\n",
            "|    n_updates        | 466359   |\n",
            "----------------------------------\n",
            "Num timesteps: 1916000\n",
            "Best mean reward: 168.91 - Last mean reward per episode: 169.56\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1917000\n",
            "Best mean reward: 169.56 - Last mean reward per episode: 168.97\n",
            "Num timesteps: 1918000\n",
            "Best mean reward: 169.56 - Last mean reward per episode: 169.85\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 629      |\n",
            "|    ep_rew_mean      | 170      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3344     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 3521     |\n",
            "|    total_timesteps  | 1918052  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.52     |\n",
            "|    n_updates        | 467012   |\n",
            "----------------------------------\n",
            "Num timesteps: 1919000\n",
            "Best mean reward: 169.85 - Last mean reward per episode: 172.23\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1920000\n",
            "Best mean reward: 172.23 - Last mean reward per episode: 172.55\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 627      |\n",
            "|    ep_rew_mean      | 172      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3348     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 3526     |\n",
            "|    total_timesteps  | 1920712  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0799   |\n",
            "|    n_updates        | 467677   |\n",
            "----------------------------------\n",
            "Num timesteps: 1921000\n",
            "Best mean reward: 172.55 - Last mean reward per episode: 171.69\n",
            "Num timesteps: 1922000\n",
            "Best mean reward: 172.55 - Last mean reward per episode: 175.23\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1923000\n",
            "Best mean reward: 175.23 - Last mean reward per episode: 175.14\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 621      |\n",
            "|    ep_rew_mean      | 175      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3352     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 3530     |\n",
            "|    total_timesteps  | 1923221  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.316    |\n",
            "|    n_updates        | 468305   |\n",
            "----------------------------------\n",
            "Num timesteps: 1924000\n",
            "Best mean reward: 175.23 - Last mean reward per episode: 175.13\n",
            "Num timesteps: 1925000\n",
            "Best mean reward: 175.23 - Last mean reward per episode: 176.93\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 614      |\n",
            "|    ep_rew_mean      | 178      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3356     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 3534     |\n",
            "|    total_timesteps  | 1925594  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.177    |\n",
            "|    n_updates        | 468898   |\n",
            "----------------------------------\n",
            "Num timesteps: 1926000\n",
            "Best mean reward: 176.93 - Last mean reward per episode: 178.31\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1927000\n",
            "Best mean reward: 178.31 - Last mean reward per episode: 180.11\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 611      |\n",
            "|    ep_rew_mean      | 180      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3360     |\n",
            "|    fps              | 544      |\n",
            "|    time_elapsed     | 3537     |\n",
            "|    total_timesteps  | 1927504  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0538   |\n",
            "|    n_updates        | 469375   |\n",
            "----------------------------------\n",
            "Num timesteps: 1928000\n",
            "Best mean reward: 180.11 - Last mean reward per episode: 180.24\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1929000\n",
            "Best mean reward: 180.24 - Last mean reward per episode: 179.53\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 604      |\n",
            "|    ep_rew_mean      | 181      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3364     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 3539     |\n",
            "|    total_timesteps  | 1929336  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.77     |\n",
            "|    n_updates        | 469833   |\n",
            "----------------------------------\n",
            "Num timesteps: 1930000\n",
            "Best mean reward: 180.24 - Last mean reward per episode: 181.17\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 599      |\n",
            "|    ep_rew_mean      | 182      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3368     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 3541     |\n",
            "|    total_timesteps  | 1930799  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.64     |\n",
            "|    n_updates        | 470199   |\n",
            "----------------------------------\n",
            "Num timesteps: 1931000\n",
            "Best mean reward: 181.17 - Last mean reward per episode: 182.30\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1932000\n",
            "Best mean reward: 182.30 - Last mean reward per episode: 185.31\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 594      |\n",
            "|    ep_rew_mean      | 186      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3372     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 3544     |\n",
            "|    total_timesteps  | 1932653  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.1      |\n",
            "|    n_updates        | 470663   |\n",
            "----------------------------------\n",
            "Num timesteps: 1933000\n",
            "Best mean reward: 185.31 - Last mean reward per episode: 187.16\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1934000\n",
            "Best mean reward: 187.16 - Last mean reward per episode: 188.70\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 587      |\n",
            "|    ep_rew_mean      | 188      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3376     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 3546     |\n",
            "|    total_timesteps  | 1934391  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.69     |\n",
            "|    n_updates        | 471097   |\n",
            "----------------------------------\n",
            "Num timesteps: 1935000\n",
            "Best mean reward: 188.70 - Last mean reward per episode: 188.93\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 576      |\n",
            "|    ep_rew_mean      | 192      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3380     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 3548     |\n",
            "|    total_timesteps  | 1935730  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.493    |\n",
            "|    n_updates        | 471432   |\n",
            "----------------------------------\n",
            "Num timesteps: 1936000\n",
            "Best mean reward: 188.93 - Last mean reward per episode: 191.75\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1937000\n",
            "Best mean reward: 191.75 - Last mean reward per episode: 191.43\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 571      |\n",
            "|    ep_rew_mean      | 192      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3384     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 3551     |\n",
            "|    total_timesteps  | 1937542  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.301    |\n",
            "|    n_updates        | 471885   |\n",
            "----------------------------------\n",
            "Num timesteps: 1938000\n",
            "Best mean reward: 191.75 - Last mean reward per episode: 191.75\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1939000\n",
            "Best mean reward: 191.75 - Last mean reward per episode: 194.91\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 553      |\n",
            "|    ep_rew_mean      | 197      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3388     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 3553     |\n",
            "|    total_timesteps  | 1939063  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.59     |\n",
            "|    n_updates        | 472265   |\n",
            "----------------------------------\n",
            "Num timesteps: 1940000\n",
            "Best mean reward: 194.91 - Last mean reward per episode: 196.44\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1941000\n",
            "Best mean reward: 196.44 - Last mean reward per episode: 197.10\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 549      |\n",
            "|    ep_rew_mean      | 200      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3392     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 3556     |\n",
            "|    total_timesteps  | 1941139  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0711   |\n",
            "|    n_updates        | 472784   |\n",
            "----------------------------------\n",
            "Num timesteps: 1942000\n",
            "Best mean reward: 197.10 - Last mean reward per episode: 199.22\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1943000\n",
            "Best mean reward: 199.22 - Last mean reward per episode: 199.62\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 544      |\n",
            "|    ep_rew_mean      | 200      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3396     |\n",
            "|    fps              | 545      |\n",
            "|    time_elapsed     | 3558     |\n",
            "|    total_timesteps  | 1943031  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0668   |\n",
            "|    n_updates        | 473257   |\n",
            "----------------------------------\n",
            "Num timesteps: 1944000\n",
            "Best mean reward: 199.62 - Last mean reward per episode: 201.17\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 527      |\n",
            "|    ep_rew_mean      | 206      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3400     |\n",
            "|    fps              | 546      |\n",
            "|    time_elapsed     | 3560     |\n",
            "|    total_timesteps  | 1944532  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.124    |\n",
            "|    n_updates        | 473632   |\n",
            "----------------------------------\n",
            "Num timesteps: 1945000\n",
            "Best mean reward: 201.17 - Last mean reward per episode: 206.26\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1946000\n",
            "Best mean reward: 206.26 - Last mean reward per episode: 209.09\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 516      |\n",
            "|    ep_rew_mean      | 210      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3404     |\n",
            "|    fps              | 546      |\n",
            "|    time_elapsed     | 3564     |\n",
            "|    total_timesteps  | 1946754  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.145    |\n",
            "|    n_updates        | 474188   |\n",
            "----------------------------------\n",
            "Num timesteps: 1947000\n",
            "Best mean reward: 209.09 - Last mean reward per episode: 209.65\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1948000\n",
            "Best mean reward: 209.65 - Last mean reward per episode: 210.53\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 511      |\n",
            "|    ep_rew_mean      | 211      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3408     |\n",
            "|    fps              | 546      |\n",
            "|    time_elapsed     | 3566     |\n",
            "|    total_timesteps  | 1948107  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.141    |\n",
            "|    n_updates        | 474526   |\n",
            "----------------------------------\n",
            "Num timesteps: 1949000\n",
            "Best mean reward: 210.53 - Last mean reward per episode: 213.02\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 502      |\n",
            "|    ep_rew_mean      | 215      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3412     |\n",
            "|    fps              | 546      |\n",
            "|    time_elapsed     | 3568     |\n",
            "|    total_timesteps  | 1949918  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.375    |\n",
            "|    n_updates        | 474979   |\n",
            "----------------------------------\n",
            "Num timesteps: 1950000\n",
            "Best mean reward: 213.02 - Last mean reward per episode: 214.58\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1951000\n",
            "Best mean reward: 214.58 - Last mean reward per episode: 219.02\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 490      |\n",
            "|    ep_rew_mean      | 220      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3416     |\n",
            "|    fps              | 546      |\n",
            "|    time_elapsed     | 3570     |\n",
            "|    total_timesteps  | 1951199  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0669   |\n",
            "|    n_updates        | 475299   |\n",
            "----------------------------------\n",
            "Num timesteps: 1952000\n",
            "Best mean reward: 219.02 - Last mean reward per episode: 220.15\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 477      |\n",
            "|    ep_rew_mean      | 225      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3420     |\n",
            "|    fps              | 546      |\n",
            "|    time_elapsed     | 3572     |\n",
            "|    total_timesteps  | 1952773  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.154    |\n",
            "|    n_updates        | 475693   |\n",
            "----------------------------------\n",
            "Num timesteps: 1953000\n",
            "Best mean reward: 220.15 - Last mean reward per episode: 225.34\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1954000\n",
            "Best mean reward: 225.34 - Last mean reward per episode: 224.72\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 472      |\n",
            "|    ep_rew_mean      | 228      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3424     |\n",
            "|    fps              | 546      |\n",
            "|    time_elapsed     | 3575     |\n",
            "|    total_timesteps  | 1954934  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.124    |\n",
            "|    n_updates        | 476233   |\n",
            "----------------------------------\n",
            "Num timesteps: 1955000\n",
            "Best mean reward: 225.34 - Last mean reward per episode: 228.38\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1956000\n",
            "Best mean reward: 228.38 - Last mean reward per episode: 228.51\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 473      |\n",
            "|    ep_rew_mean      | 228      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3428     |\n",
            "|    fps              | 546      |\n",
            "|    time_elapsed     | 3577     |\n",
            "|    total_timesteps  | 1956594  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0801   |\n",
            "|    n_updates        | 476648   |\n",
            "----------------------------------\n",
            "Num timesteps: 1957000\n",
            "Best mean reward: 228.51 - Last mean reward per episode: 228.35\n",
            "Num timesteps: 1958000\n",
            "Best mean reward: 228.51 - Last mean reward per episode: 228.30\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 472      |\n",
            "|    ep_rew_mean      | 227      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3432     |\n",
            "|    fps              | 546      |\n",
            "|    time_elapsed     | 3581     |\n",
            "|    total_timesteps  | 1958684  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.135    |\n",
            "|    n_updates        | 477170   |\n",
            "----------------------------------\n",
            "Num timesteps: 1959000\n",
            "Best mean reward: 228.51 - Last mean reward per episode: 227.26\n",
            "Num timesteps: 1960000\n",
            "Best mean reward: 228.51 - Last mean reward per episode: 226.27\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 473      |\n",
            "|    ep_rew_mean      | 226      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3436     |\n",
            "|    fps              | 547      |\n",
            "|    time_elapsed     | 3583     |\n",
            "|    total_timesteps  | 1960647  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.137    |\n",
            "|    n_updates        | 477661   |\n",
            "----------------------------------\n",
            "Num timesteps: 1961000\n",
            "Best mean reward: 228.51 - Last mean reward per episode: 226.09\n",
            "Num timesteps: 1962000\n",
            "Best mean reward: 228.51 - Last mean reward per episode: 226.05\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 467      |\n",
            "|    ep_rew_mean      | 226      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3440     |\n",
            "|    fps              | 547      |\n",
            "|    time_elapsed     | 3585     |\n",
            "|    total_timesteps  | 1962104  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0828   |\n",
            "|    n_updates        | 478025   |\n",
            "----------------------------------\n",
            "Num timesteps: 1963000\n",
            "Best mean reward: 228.51 - Last mean reward per episode: 226.37\n",
            "Num timesteps: 1964000\n",
            "Best mean reward: 228.51 - Last mean reward per episode: 228.72\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 462      |\n",
            "|    ep_rew_mean      | 228      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3444     |\n",
            "|    fps              | 547      |\n",
            "|    time_elapsed     | 3588     |\n",
            "|    total_timesteps  | 1964292  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0392   |\n",
            "|    n_updates        | 478572   |\n",
            "----------------------------------\n",
            "Num timesteps: 1965000\n",
            "Best mean reward: 228.72 - Last mean reward per episode: 228.00\n",
            "Num timesteps: 1966000\n",
            "Best mean reward: 228.72 - Last mean reward per episode: 230.61\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 454      |\n",
            "|    ep_rew_mean      | 232      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3448     |\n",
            "|    fps              | 547      |\n",
            "|    time_elapsed     | 3591     |\n",
            "|    total_timesteps  | 1966132  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0902   |\n",
            "|    n_updates        | 479032   |\n",
            "----------------------------------\n",
            "Num timesteps: 1967000\n",
            "Best mean reward: 230.61 - Last mean reward per episode: 232.62\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 445      |\n",
            "|    ep_rew_mean      | 235      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3452     |\n",
            "|    fps              | 547      |\n",
            "|    time_elapsed     | 3593     |\n",
            "|    total_timesteps  | 1967720  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.184    |\n",
            "|    n_updates        | 479429   |\n",
            "----------------------------------\n",
            "Num timesteps: 1968000\n",
            "Best mean reward: 232.62 - Last mean reward per episode: 234.51\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1969000\n",
            "Best mean reward: 234.51 - Last mean reward per episode: 235.22\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 438      |\n",
            "|    ep_rew_mean      | 236      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3456     |\n",
            "|    fps              | 547      |\n",
            "|    time_elapsed     | 3595     |\n",
            "|    total_timesteps  | 1969435  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.15     |\n",
            "|    n_updates        | 479858   |\n",
            "----------------------------------\n",
            "Num timesteps: 1970000\n",
            "Best mean reward: 235.22 - Last mean reward per episode: 237.18\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 431      |\n",
            "|    ep_rew_mean      | 237      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3460     |\n",
            "|    fps              | 547      |\n",
            "|    time_elapsed     | 3597     |\n",
            "|    total_timesteps  | 1970637  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.135    |\n",
            "|    n_updates        | 480159   |\n",
            "----------------------------------\n",
            "Num timesteps: 1971000\n",
            "Best mean reward: 237.18 - Last mean reward per episode: 237.40\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 1972000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 237.33\n",
            "Num timesteps: 1973000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 237.11\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 439      |\n",
            "|    ep_rew_mean      | 236      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3464     |\n",
            "|    fps              | 547      |\n",
            "|    time_elapsed     | 3601     |\n",
            "|    total_timesteps  | 1973194  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.164    |\n",
            "|    n_updates        | 480798   |\n",
            "----------------------------------\n",
            "Num timesteps: 1974000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 236.63\n",
            "Num timesteps: 1975000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 235.78\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 444      |\n",
            "|    ep_rew_mean      | 235      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3468     |\n",
            "|    fps              | 548      |\n",
            "|    time_elapsed     | 3604     |\n",
            "|    total_timesteps  | 1975195  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0579   |\n",
            "|    n_updates        | 481298   |\n",
            "----------------------------------\n",
            "Num timesteps: 1976000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 234.46\n",
            "Num timesteps: 1977000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 234.13\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 447      |\n",
            "|    ep_rew_mean      | 234      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3472     |\n",
            "|    fps              | 548      |\n",
            "|    time_elapsed     | 3607     |\n",
            "|    total_timesteps  | 1977343  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.19     |\n",
            "|    n_updates        | 481835   |\n",
            "----------------------------------\n",
            "Num timesteps: 1978000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 233.82\n",
            "Num timesteps: 1979000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 233.34\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 448      |\n",
            "|    ep_rew_mean      | 234      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3476     |\n",
            "|    fps              | 548      |\n",
            "|    time_elapsed     | 3610     |\n",
            "|    total_timesteps  | 1979224  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.109    |\n",
            "|    n_updates        | 482305   |\n",
            "----------------------------------\n",
            "Num timesteps: 1980000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 234.21\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 450      |\n",
            "|    ep_rew_mean      | 234      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3480     |\n",
            "|    fps              | 548      |\n",
            "|    time_elapsed     | 3612     |\n",
            "|    total_timesteps  | 1980696  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.57     |\n",
            "|    n_updates        | 482673   |\n",
            "----------------------------------\n",
            "Num timesteps: 1981000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 233.83\n",
            "Num timesteps: 1982000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 234.35\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 447      |\n",
            "|    ep_rew_mean      | 235      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3484     |\n",
            "|    fps              | 548      |\n",
            "|    time_elapsed     | 3614     |\n",
            "|    total_timesteps  | 1982239  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.101    |\n",
            "|    n_updates        | 483059   |\n",
            "----------------------------------\n",
            "Num timesteps: 1983000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 235.07\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 449      |\n",
            "|    ep_rew_mean      | 235      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3488     |\n",
            "|    fps              | 548      |\n",
            "|    time_elapsed     | 3616     |\n",
            "|    total_timesteps  | 1983925  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.06     |\n",
            "|    n_updates        | 483481   |\n",
            "----------------------------------\n",
            "Num timesteps: 1984000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 234.70\n",
            "Num timesteps: 1985000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 232.89\n",
            "Num timesteps: 1986000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 232.63\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 450      |\n",
            "|    ep_rew_mean      | 233      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3492     |\n",
            "|    fps              | 548      |\n",
            "|    time_elapsed     | 3620     |\n",
            "|    total_timesteps  | 1986177  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.77     |\n",
            "|    n_updates        | 484044   |\n",
            "----------------------------------\n",
            "Num timesteps: 1987000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 233.84\n",
            "Num timesteps: 1988000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 233.03\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 454      |\n",
            "|    ep_rew_mean      | 231      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3496     |\n",
            "|    fps              | 548      |\n",
            "|    time_elapsed     | 3624     |\n",
            "|    total_timesteps  | 1988436  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0816   |\n",
            "|    n_updates        | 484608   |\n",
            "----------------------------------\n",
            "Num timesteps: 1989000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 230.64\n",
            "Num timesteps: 1990000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 230.23\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 462      |\n",
            "|    ep_rew_mean      | 230      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3500     |\n",
            "|    fps              | 548      |\n",
            "|    time_elapsed     | 3627     |\n",
            "|    total_timesteps  | 1990742  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.157    |\n",
            "|    n_updates        | 485185   |\n",
            "----------------------------------\n",
            "Num timesteps: 1991000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 229.94\n",
            "Num timesteps: 1992000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 230.34\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 461      |\n",
            "|    ep_rew_mean      | 232      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3504     |\n",
            "|    fps              | 548      |\n",
            "|    time_elapsed     | 3631     |\n",
            "|    total_timesteps  | 1992841  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.266    |\n",
            "|    n_updates        | 485710   |\n",
            "----------------------------------\n",
            "Num timesteps: 1993000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 231.93\n",
            "Num timesteps: 1994000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 231.22\n",
            "Num timesteps: 1995000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 229.23\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 474      |\n",
            "|    ep_rew_mean      | 229      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3508     |\n",
            "|    fps              | 548      |\n",
            "|    time_elapsed     | 3636     |\n",
            "|    total_timesteps  | 1995495  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0971   |\n",
            "|    n_updates        | 486373   |\n",
            "----------------------------------\n",
            "Num timesteps: 1996000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 229.14\n",
            "Num timesteps: 1997000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 228.65\n",
            "Num timesteps: 1998000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 228.62\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 485      |\n",
            "|    ep_rew_mean      | 228      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3512     |\n",
            "|    fps              | 548      |\n",
            "|    time_elapsed     | 3641     |\n",
            "|    total_timesteps  | 1998396  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.294    |\n",
            "|    n_updates        | 487098   |\n",
            "----------------------------------\n",
            "Num timesteps: 1999000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 228.34\n",
            "Num timesteps: 2000000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 227.73\n",
            "Num timesteps: 2001000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 225.13\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 501      |\n",
            "|    ep_rew_mean      | 222      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3516     |\n",
            "|    fps              | 548      |\n",
            "|    time_elapsed     | 3646     |\n",
            "|    total_timesteps  | 2001323  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.135    |\n",
            "|    n_updates        | 487830   |\n",
            "----------------------------------\n",
            "Num timesteps: 2002000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 221.90\n",
            "Num timesteps: 2003000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 222.50\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 503      |\n",
            "|    ep_rew_mean      | 222      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3520     |\n",
            "|    fps              | 548      |\n",
            "|    time_elapsed     | 3649     |\n",
            "|    total_timesteps  | 2003057  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.138    |\n",
            "|    n_updates        | 488264   |\n",
            "----------------------------------\n",
            "Num timesteps: 2004000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 223.28\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 496      |\n",
            "|    ep_rew_mean      | 223      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3524     |\n",
            "|    fps              | 549      |\n",
            "|    time_elapsed     | 3650     |\n",
            "|    total_timesteps  | 2004572  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0711   |\n",
            "|    n_updates        | 488642   |\n",
            "----------------------------------\n",
            "Num timesteps: 2005000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 222.67\n",
            "Num timesteps: 2006000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 220.01\n",
            "Num timesteps: 2007000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 219.45\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 511      |\n",
            "|    ep_rew_mean      | 218      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3528     |\n",
            "|    fps              | 548      |\n",
            "|    time_elapsed     | 3657     |\n",
            "|    total_timesteps  | 2007709  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.051    |\n",
            "|    n_updates        | 489427   |\n",
            "----------------------------------\n",
            "Num timesteps: 2008000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 217.62\n",
            "Num timesteps: 2009000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 215.11\n",
            "Num timesteps: 2010000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 213.02\n",
            "Num timesteps: 2011000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 211.82\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 524      |\n",
            "|    ep_rew_mean      | 213      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3532     |\n",
            "|    fps              | 548      |\n",
            "|    time_elapsed     | 3664     |\n",
            "|    total_timesteps  | 2011084  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.61     |\n",
            "|    n_updates        | 490270   |\n",
            "----------------------------------\n",
            "Num timesteps: 2012000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 212.75\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 523      |\n",
            "|    ep_rew_mean      | 213      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3536     |\n",
            "|    fps              | 548      |\n",
            "|    time_elapsed     | 3666     |\n",
            "|    total_timesteps  | 2012968  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0652   |\n",
            "|    n_updates        | 490741   |\n",
            "----------------------------------\n",
            "Num timesteps: 2013000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 213.45\n",
            "Num timesteps: 2014000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 212.74\n",
            "Num timesteps: 2015000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 210.53\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 535      |\n",
            "|    ep_rew_mean      | 211      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3540     |\n",
            "|    fps              | 549      |\n",
            "|    time_elapsed     | 3671     |\n",
            "|    total_timesteps  | 2015575  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0401   |\n",
            "|    n_updates        | 491393   |\n",
            "----------------------------------\n",
            "Num timesteps: 2016000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 211.20\n",
            "Num timesteps: 2017000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 211.77\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 534      |\n",
            "|    ep_rew_mean      | 212      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3544     |\n",
            "|    fps              | 549      |\n",
            "|    time_elapsed     | 3674     |\n",
            "|    total_timesteps  | 2017730  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.1      |\n",
            "|    n_updates        | 491932   |\n",
            "----------------------------------\n",
            "Num timesteps: 2018000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 211.82\n",
            "Num timesteps: 2019000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 211.81\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 532      |\n",
            "|    ep_rew_mean      | 212      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3548     |\n",
            "|    fps              | 549      |\n",
            "|    time_elapsed     | 3676     |\n",
            "|    total_timesteps  | 2019328  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.318    |\n",
            "|    n_updates        | 492331   |\n",
            "----------------------------------\n",
            "Num timesteps: 2020000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 211.82\n",
            "Num timesteps: 2021000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 210.80\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 542      |\n",
            "|    ep_rew_mean      | 210      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3552     |\n",
            "|    fps              | 549      |\n",
            "|    time_elapsed     | 3680     |\n",
            "|    total_timesteps  | 2021920  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.145    |\n",
            "|    n_updates        | 492979   |\n",
            "----------------------------------\n",
            "Num timesteps: 2022000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 209.50\n",
            "Num timesteps: 2023000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 209.20\n",
            "Num timesteps: 2024000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 208.32\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 549      |\n",
            "|    ep_rew_mean      | 209      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3556     |\n",
            "|    fps              | 549      |\n",
            "|    time_elapsed     | 3684     |\n",
            "|    total_timesteps  | 2024301  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.54     |\n",
            "|    n_updates        | 493575   |\n",
            "----------------------------------\n",
            "Num timesteps: 2025000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 208.39\n",
            "Num timesteps: 2026000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 208.40\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 554      |\n",
            "|    ep_rew_mean      | 208      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3560     |\n",
            "|    fps              | 549      |\n",
            "|    time_elapsed     | 3686     |\n",
            "|    total_timesteps  | 2026073  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0926   |\n",
            "|    n_updates        | 494018   |\n",
            "----------------------------------\n",
            "Num timesteps: 2027000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 207.89\n",
            "Num timesteps: 2028000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 207.68\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 550      |\n",
            "|    ep_rew_mean      | 209      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3564     |\n",
            "|    fps              | 549      |\n",
            "|    time_elapsed     | 3690     |\n",
            "|    total_timesteps  | 2028177  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.136    |\n",
            "|    n_updates        | 494544   |\n",
            "----------------------------------\n",
            "Num timesteps: 2029000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 208.46\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 547      |\n",
            "|    ep_rew_mean      | 209      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3568     |\n",
            "|    fps              | 549      |\n",
            "|    time_elapsed     | 3692     |\n",
            "|    total_timesteps  | 2029877  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.144    |\n",
            "|    n_updates        | 494969   |\n",
            "----------------------------------\n",
            "Num timesteps: 2030000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 209.49\n",
            "Num timesteps: 2031000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 209.76\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 543      |\n",
            "|    ep_rew_mean      | 211      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3572     |\n",
            "|    fps              | 549      |\n",
            "|    time_elapsed     | 3694     |\n",
            "|    total_timesteps  | 2031652  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.21     |\n",
            "|    n_updates        | 495412   |\n",
            "----------------------------------\n",
            "Num timesteps: 2032000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 210.72\n",
            "Num timesteps: 2033000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 210.70\n",
            "Num timesteps: 2034000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 211.08\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 551      |\n",
            "|    ep_rew_mean      | 210      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3576     |\n",
            "|    fps              | 549      |\n",
            "|    time_elapsed     | 3699     |\n",
            "|    total_timesteps  | 2034317  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.14     |\n",
            "|    n_updates        | 496079   |\n",
            "----------------------------------\n",
            "Num timesteps: 2035000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 208.80\n",
            "Num timesteps: 2036000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 208.89\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 558      |\n",
            "|    ep_rew_mean      | 208      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3580     |\n",
            "|    fps              | 550      |\n",
            "|    time_elapsed     | 3702     |\n",
            "|    total_timesteps  | 2036466  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0622   |\n",
            "|    n_updates        | 496616   |\n",
            "----------------------------------\n",
            "Num timesteps: 2037000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 208.03\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 557      |\n",
            "|    ep_rew_mean      | 209      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3584     |\n",
            "|    fps              | 550      |\n",
            "|    time_elapsed     | 3703     |\n",
            "|    total_timesteps  | 2037972  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.15     |\n",
            "|    n_updates        | 496992   |\n",
            "----------------------------------\n",
            "Num timesteps: 2038000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 208.98\n",
            "Num timesteps: 2039000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 208.37\n",
            "Num timesteps: 2040000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 208.78\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 562      |\n",
            "|    ep_rew_mean      | 209      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3588     |\n",
            "|    fps              | 550      |\n",
            "|    time_elapsed     | 3707     |\n",
            "|    total_timesteps  | 2040155  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.129    |\n",
            "|    n_updates        | 497538   |\n",
            "----------------------------------\n",
            "Num timesteps: 2041000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 210.62\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 557      |\n",
            "|    ep_rew_mean      | 211      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3592     |\n",
            "|    fps              | 550      |\n",
            "|    time_elapsed     | 3709     |\n",
            "|    total_timesteps  | 2041829  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.59     |\n",
            "|    n_updates        | 497957   |\n",
            "----------------------------------\n",
            "Num timesteps: 2042000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 210.78\n",
            "Num timesteps: 2043000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 210.79\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 553      |\n",
            "|    ep_rew_mean      | 213      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3596     |\n",
            "|    fps              | 550      |\n",
            "|    time_elapsed     | 3711     |\n",
            "|    total_timesteps  | 2043729  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0521   |\n",
            "|    n_updates        | 498432   |\n",
            "----------------------------------\n",
            "Num timesteps: 2044000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 213.09\n",
            "Num timesteps: 2045000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 214.50\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 545      |\n",
            "|    ep_rew_mean      | 214      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3600     |\n",
            "|    fps              | 550      |\n",
            "|    time_elapsed     | 3713     |\n",
            "|    total_timesteps  | 2045235  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.118    |\n",
            "|    n_updates        | 498808   |\n",
            "----------------------------------\n",
            "Num timesteps: 2046000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 214.75\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 539      |\n",
            "|    ep_rew_mean      | 215      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3604     |\n",
            "|    fps              | 550      |\n",
            "|    time_elapsed     | 3715     |\n",
            "|    total_timesteps  | 2046701  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.102    |\n",
            "|    n_updates        | 499175   |\n",
            "----------------------------------\n",
            "Num timesteps: 2047000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 214.86\n",
            "Num timesteps: 2048000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 215.76\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 533      |\n",
            "|    ep_rew_mean      | 216      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3608     |\n",
            "|    fps              | 550      |\n",
            "|    time_elapsed     | 3718     |\n",
            "|    total_timesteps  | 2048747  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.167    |\n",
            "|    n_updates        | 499686   |\n",
            "----------------------------------\n",
            "Num timesteps: 2049000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 216.14\n",
            "Num timesteps: 2050000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 217.60\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 517      |\n",
            "|    ep_rew_mean      | 218      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3612     |\n",
            "|    fps              | 551      |\n",
            "|    time_elapsed     | 3720     |\n",
            "|    total_timesteps  | 2050098  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.108    |\n",
            "|    n_updates        | 500024   |\n",
            "----------------------------------\n",
            "Num timesteps: 2051000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 217.44\n",
            "Num timesteps: 2052000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 220.33\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 508      |\n",
            "|    ep_rew_mean      | 222      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3616     |\n",
            "|    fps              | 551      |\n",
            "|    time_elapsed     | 3723     |\n",
            "|    total_timesteps  | 2052170  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0686   |\n",
            "|    n_updates        | 500542   |\n",
            "----------------------------------\n",
            "Num timesteps: 2053000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 221.91\n",
            "Num timesteps: 2054000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 221.15\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 510      |\n",
            "|    ep_rew_mean      | 221      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3620     |\n",
            "|    fps              | 551      |\n",
            "|    time_elapsed     | 3726     |\n",
            "|    total_timesteps  | 2054080  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0834   |\n",
            "|    n_updates        | 501019   |\n",
            "----------------------------------\n",
            "Num timesteps: 2055000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 220.81\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 512      |\n",
            "|    ep_rew_mean      | 221      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3624     |\n",
            "|    fps              | 551      |\n",
            "|    time_elapsed     | 3728     |\n",
            "|    total_timesteps  | 2055786  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0941   |\n",
            "|    n_updates        | 501446   |\n",
            "----------------------------------\n",
            "Num timesteps: 2056000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 220.60\n",
            "Num timesteps: 2057000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 220.64\n",
            "Num timesteps: 2058000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 221.86\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 505      |\n",
            "|    ep_rew_mean      | 224      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3628     |\n",
            "|    fps              | 551      |\n",
            "|    time_elapsed     | 3732     |\n",
            "|    total_timesteps  | 2058172  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.121    |\n",
            "|    n_updates        | 502042   |\n",
            "----------------------------------\n",
            "Num timesteps: 2059000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 228.80\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 488      |\n",
            "|    ep_rew_mean      | 230      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3632     |\n",
            "|    fps              | 551      |\n",
            "|    time_elapsed     | 3734     |\n",
            "|    total_timesteps  | 2059867  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.13     |\n",
            "|    n_updates        | 502466   |\n",
            "----------------------------------\n",
            "Num timesteps: 2060000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 230.11\n",
            "Num timesteps: 2061000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 230.15\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 487      |\n",
            "|    ep_rew_mean      | 231      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3636     |\n",
            "|    fps              | 551      |\n",
            "|    time_elapsed     | 3736     |\n",
            "|    total_timesteps  | 2061691  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.126    |\n",
            "|    n_updates        | 502922   |\n",
            "----------------------------------\n",
            "Num timesteps: 2062000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 232.11\n",
            "Num timesteps: 2063000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 233.49\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 480      |\n",
            "|    ep_rew_mean      | 232      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3640     |\n",
            "|    fps              | 551      |\n",
            "|    time_elapsed     | 3739     |\n",
            "|    total_timesteps  | 2063586  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.461    |\n",
            "|    n_updates        | 503396   |\n",
            "----------------------------------\n",
            "Num timesteps: 2064000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 232.55\n",
            "Num timesteps: 2065000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 232.22\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 476      |\n",
            "|    ep_rew_mean      | 233      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3644     |\n",
            "|    fps              | 551      |\n",
            "|    time_elapsed     | 3741     |\n",
            "|    total_timesteps  | 2065355  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.108    |\n",
            "|    n_updates        | 503838   |\n",
            "----------------------------------\n",
            "Num timesteps: 2066000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 232.61\n",
            "Num timesteps: 2067000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 232.39\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 478      |\n",
            "|    ep_rew_mean      | 232      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3648     |\n",
            "|    fps              | 552      |\n",
            "|    time_elapsed     | 3744     |\n",
            "|    total_timesteps  | 2067104  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.39     |\n",
            "|    n_updates        | 504275   |\n",
            "----------------------------------\n",
            "Num timesteps: 2068000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 232.50\n",
            "Num timesteps: 2069000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 232.12\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 473      |\n",
            "|    ep_rew_mean      | 232      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3652     |\n",
            "|    fps              | 552      |\n",
            "|    time_elapsed     | 3747     |\n",
            "|    total_timesteps  | 2069172  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.18     |\n",
            "|    n_updates        | 504792   |\n",
            "----------------------------------\n",
            "Num timesteps: 2070000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 232.80\n",
            "Num timesteps: 2071000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 233.15\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 472      |\n",
            "|    ep_rew_mean      | 232      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3656     |\n",
            "|    fps              | 552      |\n",
            "|    time_elapsed     | 3751     |\n",
            "|    total_timesteps  | 2071504  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.478    |\n",
            "|    n_updates        | 505375   |\n",
            "----------------------------------\n",
            "Num timesteps: 2072000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 232.15\n",
            "Num timesteps: 2073000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 231.16\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 476      |\n",
            "|    ep_rew_mean      | 229      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3660     |\n",
            "|    fps              | 552      |\n",
            "|    time_elapsed     | 3755     |\n",
            "|    total_timesteps  | 2073699  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.39     |\n",
            "|    n_updates        | 505924   |\n",
            "----------------------------------\n",
            "Num timesteps: 2074000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 229.21\n",
            "Num timesteps: 2075000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 228.84\n",
            "Num timesteps: 2076000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 229.26\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 482      |\n",
            "|    ep_rew_mean      | 229      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3664     |\n",
            "|    fps              | 552      |\n",
            "|    time_elapsed     | 3759     |\n",
            "|    total_timesteps  | 2076328  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.12     |\n",
            "|    n_updates        | 506581   |\n",
            "----------------------------------\n",
            "Num timesteps: 2077000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 229.14\n",
            "Num timesteps: 2078000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 228.23\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 489      |\n",
            "|    ep_rew_mean      | 228      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3668     |\n",
            "|    fps              | 552      |\n",
            "|    time_elapsed     | 3764     |\n",
            "|    total_timesteps  | 2078825  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.174    |\n",
            "|    n_updates        | 507206   |\n",
            "----------------------------------\n",
            "Num timesteps: 2079000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 227.55\n",
            "Num timesteps: 2080000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 228.58\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 487      |\n",
            "|    ep_rew_mean      | 229      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3672     |\n",
            "|    fps              | 552      |\n",
            "|    time_elapsed     | 3766     |\n",
            "|    total_timesteps  | 2080349  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.128    |\n",
            "|    n_updates        | 507587   |\n",
            "----------------------------------\n",
            "Num timesteps: 2081000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 228.64\n",
            "Num timesteps: 2082000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 227.10\n",
            "Num timesteps: 2083000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 224.22\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 495      |\n",
            "|    ep_rew_mean      | 223      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3676     |\n",
            "|    fps              | 552      |\n",
            "|    time_elapsed     | 3773     |\n",
            "|    total_timesteps  | 2083795  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.122    |\n",
            "|    n_updates        | 508448   |\n",
            "----------------------------------\n",
            "Num timesteps: 2084000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 222.71\n",
            "Num timesteps: 2085000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 223.16\n",
            "Num timesteps: 2086000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 221.66\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 497      |\n",
            "|    ep_rew_mean      | 222      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3680     |\n",
            "|    fps              | 552      |\n",
            "|    time_elapsed     | 3777     |\n",
            "|    total_timesteps  | 2086166  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0916   |\n",
            "|    n_updates        | 509041   |\n",
            "----------------------------------\n",
            "Num timesteps: 2087000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 222.16\n",
            "Num timesteps: 2088000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 219.31\n",
            "Num timesteps: 2089000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 217.64\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 512      |\n",
            "|    ep_rew_mean      | 217      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3684     |\n",
            "|    fps              | 552      |\n",
            "|    time_elapsed     | 3783     |\n",
            "|    total_timesteps  | 2089136  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.121    |\n",
            "|    n_updates        | 509783   |\n",
            "----------------------------------\n",
            "Num timesteps: 2090000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 217.17\n",
            "Num timesteps: 2091000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 215.29\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 514      |\n",
            "|    ep_rew_mean      | 214      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3688     |\n",
            "|    fps              | 552      |\n",
            "|    time_elapsed     | 3787     |\n",
            "|    total_timesteps  | 2091525  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0848   |\n",
            "|    n_updates        | 510381   |\n",
            "----------------------------------\n",
            "Num timesteps: 2092000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 214.49\n",
            "Num timesteps: 2093000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 214.08\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 516      |\n",
            "|    ep_rew_mean      | 215      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3692     |\n",
            "|    fps              | 552      |\n",
            "|    time_elapsed     | 3790     |\n",
            "|    total_timesteps  | 2093397  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0786   |\n",
            "|    n_updates        | 510849   |\n",
            "----------------------------------\n",
            "Num timesteps: 2094000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 214.98\n",
            "Num timesteps: 2095000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 214.27\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 515      |\n",
            "|    ep_rew_mean      | 215      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3696     |\n",
            "|    fps              | 552      |\n",
            "|    time_elapsed     | 3792     |\n",
            "|    total_timesteps  | 2095202  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.132    |\n",
            "|    n_updates        | 511300   |\n",
            "----------------------------------\n",
            "Num timesteps: 2096000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 214.90\n",
            "Num timesteps: 2097000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 213.73\n",
            "Num timesteps: 2098000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 213.17\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 529      |\n",
            "|    ep_rew_mean      | 213      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3700     |\n",
            "|    fps              | 552      |\n",
            "|    time_elapsed     | 3797     |\n",
            "|    total_timesteps  | 2098104  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.136    |\n",
            "|    n_updates        | 512025   |\n",
            "----------------------------------\n",
            "Num timesteps: 2099000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 212.45\n",
            "Num timesteps: 2100000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 211.75\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 541      |\n",
            "|    ep_rew_mean      | 209      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3704     |\n",
            "|    fps              | 552      |\n",
            "|    time_elapsed     | 3802     |\n",
            "|    total_timesteps  | 2100762  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.71     |\n",
            "|    n_updates        | 512690   |\n",
            "----------------------------------\n",
            "Num timesteps: 2101000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 208.60\n",
            "Num timesteps: 2102000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 210.23\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 537      |\n",
            "|    ep_rew_mean      | 210      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3708     |\n",
            "|    fps              | 552      |\n",
            "|    time_elapsed     | 3804     |\n",
            "|    total_timesteps  | 2102429  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0885   |\n",
            "|    n_updates        | 513107   |\n",
            "----------------------------------\n",
            "Num timesteps: 2103000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 209.81\n",
            "Num timesteps: 2104000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 209.05\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 545      |\n",
            "|    ep_rew_mean      | 209      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3712     |\n",
            "|    fps              | 552      |\n",
            "|    time_elapsed     | 3808     |\n",
            "|    total_timesteps  | 2104615  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.218    |\n",
            "|    n_updates        | 513653   |\n",
            "----------------------------------\n",
            "Num timesteps: 2105000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 208.77\n",
            "Num timesteps: 2106000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 208.46\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 546      |\n",
            "|    ep_rew_mean      | 208      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3716     |\n",
            "|    fps              | 552      |\n",
            "|    time_elapsed     | 3811     |\n",
            "|    total_timesteps  | 2106733  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0805   |\n",
            "|    n_updates        | 514183   |\n",
            "----------------------------------\n",
            "Num timesteps: 2107000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 208.30\n",
            "Num timesteps: 2108000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 207.94\n",
            "Num timesteps: 2109000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 207.44\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 550      |\n",
            "|    ep_rew_mean      | 207      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3720     |\n",
            "|    fps              | 552      |\n",
            "|    time_elapsed     | 3815     |\n",
            "|    total_timesteps  | 2109100  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.123    |\n",
            "|    n_updates        | 514774   |\n",
            "----------------------------------\n",
            "Num timesteps: 2110000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 207.33\n",
            "Num timesteps: 2111000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 206.27\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 559      |\n",
            "|    ep_rew_mean      | 204      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3724     |\n",
            "|    fps              | 552      |\n",
            "|    time_elapsed     | 3819     |\n",
            "|    total_timesteps  | 2111671  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0373   |\n",
            "|    n_updates        | 515417   |\n",
            "----------------------------------\n",
            "Num timesteps: 2112000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 203.71\n",
            "Num timesteps: 2113000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 205.05\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 558      |\n",
            "|    ep_rew_mean      | 204      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3728     |\n",
            "|    fps              | 553      |\n",
            "|    time_elapsed     | 3822     |\n",
            "|    total_timesteps  | 2113976  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.1      |\n",
            "|    n_updates        | 515993   |\n",
            "----------------------------------\n",
            "Num timesteps: 2114000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 204.46\n",
            "Num timesteps: 2115000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 203.91\n",
            "Num timesteps: 2116000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 204.13\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 563      |\n",
            "|    ep_rew_mean      | 204      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3732     |\n",
            "|    fps              | 553      |\n",
            "|    time_elapsed     | 3826     |\n",
            "|    total_timesteps  | 2116128  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0968   |\n",
            "|    n_updates        | 516531   |\n",
            "----------------------------------\n",
            "Num timesteps: 2117000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 203.59\n",
            "Num timesteps: 2118000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 202.97\n",
            "Num timesteps: 2119000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 200.96\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 577      |\n",
            "|    ep_rew_mean      | 200      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3736     |\n",
            "|    fps              | 552      |\n",
            "|    time_elapsed     | 3832     |\n",
            "|    total_timesteps  | 2119343  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0366   |\n",
            "|    n_updates        | 517335   |\n",
            "----------------------------------\n",
            "Num timesteps: 2120000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 199.21\n",
            "Num timesteps: 2121000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 199.31\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 577      |\n",
            "|    ep_rew_mean      | 200      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3740     |\n",
            "|    fps              | 553      |\n",
            "|    time_elapsed     | 3835     |\n",
            "|    total_timesteps  | 2121279  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0922   |\n",
            "|    n_updates        | 517819   |\n",
            "----------------------------------\n",
            "Num timesteps: 2122000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 199.64\n",
            "Num timesteps: 2123000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 198.75\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 581      |\n",
            "|    ep_rew_mean      | 199      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3744     |\n",
            "|    fps              | 553      |\n",
            "|    time_elapsed     | 3838     |\n",
            "|    total_timesteps  | 2123428  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0513   |\n",
            "|    n_updates        | 518356   |\n",
            "----------------------------------\n",
            "Num timesteps: 2124000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 199.07\n",
            "Num timesteps: 2125000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 198.18\n",
            "Num timesteps: 2126000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 195.76\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 590      |\n",
            "|    ep_rew_mean      | 196      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3748     |\n",
            "|    fps              | 553      |\n",
            "|    time_elapsed     | 3843     |\n",
            "|    total_timesteps  | 2126088  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.111    |\n",
            "|    n_updates        | 519021   |\n",
            "----------------------------------\n",
            "Num timesteps: 2127000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 195.73\n",
            "Num timesteps: 2128000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 194.02\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 593      |\n",
            "|    ep_rew_mean      | 195      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3752     |\n",
            "|    fps              | 553      |\n",
            "|    time_elapsed     | 3847     |\n",
            "|    total_timesteps  | 2128461  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.313    |\n",
            "|    n_updates        | 519615   |\n",
            "----------------------------------\n",
            "Num timesteps: 2129000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 194.51\n",
            "Num timesteps: 2130000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 194.79\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 593      |\n",
            "|    ep_rew_mean      | 195      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3756     |\n",
            "|    fps              | 553      |\n",
            "|    time_elapsed     | 3851     |\n",
            "|    total_timesteps  | 2130835  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.157    |\n",
            "|    n_updates        | 520208   |\n",
            "----------------------------------\n",
            "Num timesteps: 2131000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 194.90\n",
            "Num timesteps: 2132000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 194.17\n",
            "Num timesteps: 2133000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 193.83\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 597      |\n",
            "|    ep_rew_mean      | 196      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3760     |\n",
            "|    fps              | 553      |\n",
            "|    time_elapsed     | 3855     |\n",
            "|    total_timesteps  | 2133362  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.254    |\n",
            "|    n_updates        | 520840   |\n",
            "----------------------------------\n",
            "Num timesteps: 2134000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 196.09\n",
            "Num timesteps: 2135000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 197.16\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 590      |\n",
            "|    ep_rew_mean      | 198      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3764     |\n",
            "|    fps              | 553      |\n",
            "|    time_elapsed     | 3858     |\n",
            "|    total_timesteps  | 2135368  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.157    |\n",
            "|    n_updates        | 521341   |\n",
            "----------------------------------\n",
            "Num timesteps: 2136000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 198.78\n",
            "Num timesteps: 2137000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 198.81\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 586      |\n",
            "|    ep_rew_mean      | 198      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3768     |\n",
            "|    fps              | 553      |\n",
            "|    time_elapsed     | 3862     |\n",
            "|    total_timesteps  | 2137457  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.108    |\n",
            "|    n_updates        | 521864   |\n",
            "----------------------------------\n",
            "Num timesteps: 2138000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 198.11\n",
            "Num timesteps: 2139000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 197.19\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 592      |\n",
            "|    ep_rew_mean      | 196      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3772     |\n",
            "|    fps              | 553      |\n",
            "|    time_elapsed     | 3865     |\n",
            "|    total_timesteps  | 2139524  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0994   |\n",
            "|    n_updates        | 522380   |\n",
            "----------------------------------\n",
            "Num timesteps: 2140000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 196.04\n",
            "Num timesteps: 2141000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 197.46\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 580      |\n",
            "|    ep_rew_mean      | 202      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3776     |\n",
            "|    fps              | 553      |\n",
            "|    time_elapsed     | 3868     |\n",
            "|    total_timesteps  | 2141826  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.233    |\n",
            "|    n_updates        | 522956   |\n",
            "----------------------------------\n",
            "Num timesteps: 2142000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 202.01\n",
            "Num timesteps: 2143000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 203.31\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 578      |\n",
            "|    ep_rew_mean      | 203      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3780     |\n",
            "|    fps              | 553      |\n",
            "|    time_elapsed     | 3871     |\n",
            "|    total_timesteps  | 2143959  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.217    |\n",
            "|    n_updates        | 523489   |\n",
            "----------------------------------\n",
            "Num timesteps: 2144000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 203.07\n",
            "Num timesteps: 2145000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 205.08\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 567      |\n",
            "|    ep_rew_mean      | 207      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3784     |\n",
            "|    fps              | 553      |\n",
            "|    time_elapsed     | 3874     |\n",
            "|    total_timesteps  | 2145831  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.31     |\n",
            "|    n_updates        | 523957   |\n",
            "----------------------------------\n",
            "Num timesteps: 2146000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 206.92\n",
            "Num timesteps: 2147000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 209.41\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 562      |\n",
            "|    ep_rew_mean      | 210      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3788     |\n",
            "|    fps              | 553      |\n",
            "|    time_elapsed     | 3877     |\n",
            "|    total_timesteps  | 2147753  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.1      |\n",
            "|    n_updates        | 524438   |\n",
            "----------------------------------\n",
            "Num timesteps: 2148000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 209.91\n",
            "Num timesteps: 2149000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 210.94\n",
            "Num timesteps: 2150000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 209.55\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 568      |\n",
            "|    ep_rew_mean      | 209      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3792     |\n",
            "|    fps              | 553      |\n",
            "|    time_elapsed     | 3881     |\n",
            "|    total_timesteps  | 2150179  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.231    |\n",
            "|    n_updates        | 525044   |\n",
            "----------------------------------\n",
            "Num timesteps: 2151000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 208.27\n",
            "Num timesteps: 2152000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 207.92\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 571      |\n",
            "|    ep_rew_mean      | 208      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3796     |\n",
            "|    fps              | 554      |\n",
            "|    time_elapsed     | 3884     |\n",
            "|    total_timesteps  | 2152344  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.208    |\n",
            "|    n_updates        | 525585   |\n",
            "----------------------------------\n",
            "Num timesteps: 2153000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 208.35\n",
            "Num timesteps: 2154000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 209.26\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 561      |\n",
            "|    ep_rew_mean      | 210      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3800     |\n",
            "|    fps              | 554      |\n",
            "|    time_elapsed     | 3887     |\n",
            "|    total_timesteps  | 2154243  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0808   |\n",
            "|    n_updates        | 526060   |\n",
            "----------------------------------\n",
            "Num timesteps: 2155000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 210.04\n",
            "Num timesteps: 2156000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 210.19\n",
            "Num timesteps: 2157000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 209.23\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 564      |\n",
            "|    ep_rew_mean      | 211      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3804     |\n",
            "|    fps              | 554      |\n",
            "|    time_elapsed     | 3892     |\n",
            "|    total_timesteps  | 2157154  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.146    |\n",
            "|    n_updates        | 526788   |\n",
            "----------------------------------\n",
            "Num timesteps: 2158000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 211.04\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 565      |\n",
            "|    ep_rew_mean      | 211      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3808     |\n",
            "|    fps              | 554      |\n",
            "|    time_elapsed     | 3895     |\n",
            "|    total_timesteps  | 2158955  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0913   |\n",
            "|    n_updates        | 527238   |\n",
            "----------------------------------\n",
            "Num timesteps: 2159000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 211.15\n",
            "Num timesteps: 2160000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 211.61\n",
            "Num timesteps: 2161000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 210.82\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 565      |\n",
            "|    ep_rew_mean      | 211      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3812     |\n",
            "|    fps              | 554      |\n",
            "|    time_elapsed     | 3898     |\n",
            "|    total_timesteps  | 2161103  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.163    |\n",
            "|    n_updates        | 527775   |\n",
            "----------------------------------\n",
            "Num timesteps: 2162000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 211.16\n",
            "Num timesteps: 2163000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 211.19\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 565      |\n",
            "|    ep_rew_mean      | 211      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3816     |\n",
            "|    fps              | 554      |\n",
            "|    time_elapsed     | 3901     |\n",
            "|    total_timesteps  | 2163202  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.104    |\n",
            "|    n_updates        | 528300   |\n",
            "----------------------------------\n",
            "Num timesteps: 2164000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 211.60\n",
            "Num timesteps: 2165000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 211.98\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 560      |\n",
            "|    ep_rew_mean      | 212      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3820     |\n",
            "|    fps              | 554      |\n",
            "|    time_elapsed     | 3904     |\n",
            "|    total_timesteps  | 2165131  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.14     |\n",
            "|    n_updates        | 528782   |\n",
            "----------------------------------\n",
            "Num timesteps: 2166000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 213.48\n",
            "Num timesteps: 2167000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 213.45\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 555      |\n",
            "|    ep_rew_mean      | 216      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3824     |\n",
            "|    fps              | 554      |\n",
            "|    time_elapsed     | 3907     |\n",
            "|    total_timesteps  | 2167176  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.12     |\n",
            "|    n_updates        | 529293   |\n",
            "----------------------------------\n",
            "Num timesteps: 2168000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 215.30\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 549      |\n",
            "|    ep_rew_mean      | 217      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3828     |\n",
            "|    fps              | 554      |\n",
            "|    time_elapsed     | 3909     |\n",
            "|    total_timesteps  | 2168869  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.196    |\n",
            "|    n_updates        | 529717   |\n",
            "----------------------------------\n",
            "Num timesteps: 2169000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 217.06\n",
            "Num timesteps: 2170000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 217.32\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 544      |\n",
            "|    ep_rew_mean      | 217      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3832     |\n",
            "|    fps              | 554      |\n",
            "|    time_elapsed     | 3911     |\n",
            "|    total_timesteps  | 2170504  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.254    |\n",
            "|    n_updates        | 530125   |\n",
            "----------------------------------\n",
            "Num timesteps: 2171000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 217.76\n",
            "Num timesteps: 2172000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 221.06\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 529      |\n",
            "|    ep_rew_mean      | 222      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3836     |\n",
            "|    fps              | 554      |\n",
            "|    time_elapsed     | 3914     |\n",
            "|    total_timesteps  | 2172241  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0704   |\n",
            "|    n_updates        | 530560   |\n",
            "----------------------------------\n",
            "Num timesteps: 2173000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 222.61\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 525      |\n",
            "|    ep_rew_mean      | 223      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3840     |\n",
            "|    fps              | 555      |\n",
            "|    time_elapsed     | 3916     |\n",
            "|    total_timesteps  | 2173769  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.44     |\n",
            "|    n_updates        | 530942   |\n",
            "----------------------------------\n",
            "Num timesteps: 2174000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 223.46\n",
            "Num timesteps: 2175000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 224.36\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 525      |\n",
            "|    ep_rew_mean      | 224      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3844     |\n",
            "|    fps              | 555      |\n",
            "|    time_elapsed     | 3919     |\n",
            "|    total_timesteps  | 2175922  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0869   |\n",
            "|    n_updates        | 531480   |\n",
            "----------------------------------\n",
            "Num timesteps: 2176000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 223.85\n",
            "Num timesteps: 2177000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 225.18\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 516      |\n",
            "|    ep_rew_mean      | 227      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3848     |\n",
            "|    fps              | 555      |\n",
            "|    time_elapsed     | 3922     |\n",
            "|    total_timesteps  | 2177712  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0853   |\n",
            "|    n_updates        | 531927   |\n",
            "----------------------------------\n",
            "Num timesteps: 2178000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 227.27\n",
            "Num timesteps: 2179000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 229.95\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 509      |\n",
            "|    ep_rew_mean      | 230      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3852     |\n",
            "|    fps              | 555      |\n",
            "|    time_elapsed     | 3924     |\n",
            "|    total_timesteps  | 2179385  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.06     |\n",
            "|    n_updates        | 532346   |\n",
            "----------------------------------\n",
            "Num timesteps: 2180000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 229.55\n",
            "Num timesteps: 2181000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 229.40\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 509      |\n",
            "|    ep_rew_mean      | 230      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3856     |\n",
            "|    fps              | 555      |\n",
            "|    time_elapsed     | 3928     |\n",
            "|    total_timesteps  | 2181729  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0724   |\n",
            "|    n_updates        | 532932   |\n",
            "----------------------------------\n",
            "Num timesteps: 2182000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 229.94\n",
            "Num timesteps: 2183000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 230.01\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 504      |\n",
            "|    ep_rew_mean      | 230      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3860     |\n",
            "|    fps              | 555      |\n",
            "|    time_elapsed     | 3931     |\n",
            "|    total_timesteps  | 2183750  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0946   |\n",
            "|    n_updates        | 533437   |\n",
            "----------------------------------\n",
            "Num timesteps: 2184000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 230.09\n",
            "Num timesteps: 2185000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 230.03\n",
            "Num timesteps: 2186000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 229.50\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 510      |\n",
            "|    ep_rew_mean      | 228      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3864     |\n",
            "|    fps              | 555      |\n",
            "|    time_elapsed     | 3935     |\n",
            "|    total_timesteps  | 2186368  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.175    |\n",
            "|    n_updates        | 534091   |\n",
            "----------------------------------\n",
            "Num timesteps: 2187000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 228.45\n",
            "Num timesteps: 2188000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 227.31\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 514      |\n",
            "|    ep_rew_mean      | 228      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3868     |\n",
            "|    fps              | 555      |\n",
            "|    time_elapsed     | 3939     |\n",
            "|    total_timesteps  | 2188871  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0901   |\n",
            "|    n_updates        | 534717   |\n",
            "----------------------------------\n",
            "Num timesteps: 2189000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 227.80\n",
            "Num timesteps: 2190000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 227.55\n",
            "Num timesteps: 2191000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 228.06\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 516      |\n",
            "|    ep_rew_mean      | 228      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3872     |\n",
            "|    fps              | 555      |\n",
            "|    time_elapsed     | 3943     |\n",
            "|    total_timesteps  | 2191132  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0989   |\n",
            "|    n_updates        | 535282   |\n",
            "----------------------------------\n",
            "Num timesteps: 2192000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 227.99\n",
            "Num timesteps: 2193000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 227.77\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 521      |\n",
            "|    ep_rew_mean      | 226      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3876     |\n",
            "|    fps              | 555      |\n",
            "|    time_elapsed     | 3947     |\n",
            "|    total_timesteps  | 2193951  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.112    |\n",
            "|    n_updates        | 535987   |\n",
            "----------------------------------\n",
            "Num timesteps: 2194000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 226.37\n",
            "Num timesteps: 2195000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 224.91\n",
            "Num timesteps: 2196000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 224.88\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 527      |\n",
            "|    ep_rew_mean      | 224      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3880     |\n",
            "|    fps              | 555      |\n",
            "|    time_elapsed     | 3952     |\n",
            "|    total_timesteps  | 2196694  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.06     |\n",
            "|    n_updates        | 536673   |\n",
            "----------------------------------\n",
            "Num timesteps: 2197000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 224.29\n",
            "Num timesteps: 2198000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 224.48\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 530      |\n",
            "|    ep_rew_mean      | 224      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3884     |\n",
            "|    fps              | 555      |\n",
            "|    time_elapsed     | 3955     |\n",
            "|    total_timesteps  | 2198868  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.102    |\n",
            "|    n_updates        | 537216   |\n",
            "----------------------------------\n",
            "Num timesteps: 2199000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 223.64\n",
            "Num timesteps: 2200000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 223.26\n",
            "Num timesteps: 2201000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 221.45\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 538      |\n",
            "|    ep_rew_mean      | 221      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3888     |\n",
            "|    fps              | 555      |\n",
            "|    time_elapsed     | 3959     |\n",
            "|    total_timesteps  | 2201536  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.116    |\n",
            "|    n_updates        | 537883   |\n",
            "----------------------------------\n",
            "Num timesteps: 2202000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 221.26\n",
            "Num timesteps: 2203000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 220.73\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 535      |\n",
            "|    ep_rew_mean      | 222      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3892     |\n",
            "|    fps              | 556      |\n",
            "|    time_elapsed     | 3963     |\n",
            "|    total_timesteps  | 2203656  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0835   |\n",
            "|    n_updates        | 538413   |\n",
            "----------------------------------\n",
            "Num timesteps: 2204000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 221.99\n",
            "Num timesteps: 2205000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 222.82\n",
            "Num timesteps: 2206000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 223.39\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 537      |\n",
            "|    ep_rew_mean      | 222      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3896     |\n",
            "|    fps              | 556      |\n",
            "|    time_elapsed     | 3967     |\n",
            "|    total_timesteps  | 2206055  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.158    |\n",
            "|    n_updates        | 539013   |\n",
            "----------------------------------\n",
            "Num timesteps: 2207000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 222.72\n",
            "Num timesteps: 2208000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 221.85\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 543      |\n",
            "|    ep_rew_mean      | 221      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3900     |\n",
            "|    fps              | 556      |\n",
            "|    time_elapsed     | 3971     |\n",
            "|    total_timesteps  | 2208556  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.109    |\n",
            "|    n_updates        | 539638   |\n",
            "----------------------------------\n",
            "Num timesteps: 2209000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 221.37\n",
            "Num timesteps: 2210000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 221.29\n",
            "Num timesteps: 2211000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 221.48\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 543      |\n",
            "|    ep_rew_mean      | 220      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3904     |\n",
            "|    fps              | 556      |\n",
            "|    time_elapsed     | 3976     |\n",
            "|    total_timesteps  | 2211471  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.161    |\n",
            "|    n_updates        | 540367   |\n",
            "----------------------------------\n",
            "Num timesteps: 2212000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 219.89\n",
            "Num timesteps: 2213000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 217.56\n",
            "Num timesteps: 2214000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 216.12\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 556      |\n",
            "|    ep_rew_mean      | 216      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3908     |\n",
            "|    fps              | 556      |\n",
            "|    time_elapsed     | 3981     |\n",
            "|    total_timesteps  | 2214532  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0664   |\n",
            "|    n_updates        | 541132   |\n",
            "----------------------------------\n",
            "Num timesteps: 2215000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 215.88\n",
            "Num timesteps: 2216000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 215.07\n",
            "Num timesteps: 2217000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 215.21\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 561      |\n",
            "|    ep_rew_mean      | 216      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3912     |\n",
            "|    fps              | 556      |\n",
            "|    time_elapsed     | 3986     |\n",
            "|    total_timesteps  | 2217213  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.125    |\n",
            "|    n_updates        | 541803   |\n",
            "----------------------------------\n",
            "Num timesteps: 2218000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 215.65\n",
            "Num timesteps: 2219000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 214.92\n",
            "Num timesteps: 2220000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 213.19\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 573      |\n",
            "|    ep_rew_mean      | 213      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3916     |\n",
            "|    fps              | 556      |\n",
            "|    time_elapsed     | 3992     |\n",
            "|    total_timesteps  | 2220507  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.122    |\n",
            "|    n_updates        | 542626   |\n",
            "----------------------------------\n",
            "Num timesteps: 2221000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 212.82\n",
            "Num timesteps: 2222000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 211.87\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 578      |\n",
            "|    ep_rew_mean      | 212      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3920     |\n",
            "|    fps              | 556      |\n",
            "|    time_elapsed     | 3996     |\n",
            "|    total_timesteps  | 2222965  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0603   |\n",
            "|    n_updates        | 543241   |\n",
            "----------------------------------\n",
            "Num timesteps: 2223000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 212.03\n",
            "Num timesteps: 2224000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 211.19\n",
            "Num timesteps: 2225000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 208.34\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 587      |\n",
            "|    ep_rew_mean      | 208      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3924     |\n",
            "|    fps              | 556      |\n",
            "|    time_elapsed     | 4001     |\n",
            "|    total_timesteps  | 2225887  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.134    |\n",
            "|    n_updates        | 543971   |\n",
            "----------------------------------\n",
            "Num timesteps: 2226000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 208.23\n",
            "Num timesteps: 2227000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 208.89\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 590      |\n",
            "|    ep_rew_mean      | 209      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3928     |\n",
            "|    fps              | 556      |\n",
            "|    time_elapsed     | 4004     |\n",
            "|    total_timesteps  | 2227821  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.156    |\n",
            "|    n_updates        | 544455   |\n",
            "----------------------------------\n",
            "Num timesteps: 2228000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 208.52\n",
            "Num timesteps: 2229000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 207.99\n",
            "Num timesteps: 2230000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 207.13\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 601      |\n",
            "|    ep_rew_mean      | 206      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3932     |\n",
            "|    fps              | 556      |\n",
            "|    time_elapsed     | 4009     |\n",
            "|    total_timesteps  | 2230581  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.297    |\n",
            "|    n_updates        | 545145   |\n",
            "----------------------------------\n",
            "Num timesteps: 2231000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 206.28\n",
            "Num timesteps: 2232000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 205.41\n",
            "Num timesteps: 2233000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 205.51\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 608      |\n",
            "|    ep_rew_mean      | 204      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3936     |\n",
            "|    fps              | 556      |\n",
            "|    time_elapsed     | 4013     |\n",
            "|    total_timesteps  | 2233075  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.171    |\n",
            "|    n_updates        | 545768   |\n",
            "----------------------------------\n",
            "Num timesteps: 2234000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 204.40\n",
            "Num timesteps: 2235000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 204.26\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 614      |\n",
            "|    ep_rew_mean      | 204      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3940     |\n",
            "|    fps              | 556      |\n",
            "|    time_elapsed     | 4016     |\n",
            "|    total_timesteps  | 2235212  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0742   |\n",
            "|    n_updates        | 546302   |\n",
            "----------------------------------\n",
            "Num timesteps: 2236000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 204.06\n",
            "Num timesteps: 2237000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 203.78\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 615      |\n",
            "|    ep_rew_mean      | 204      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3944     |\n",
            "|    fps              | 556      |\n",
            "|    time_elapsed     | 4019     |\n",
            "|    total_timesteps  | 2237409  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0989   |\n",
            "|    n_updates        | 546852   |\n",
            "----------------------------------\n",
            "Num timesteps: 2238000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 203.45\n",
            "Num timesteps: 2239000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 203.30\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 614      |\n",
            "|    ep_rew_mean      | 204      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3948     |\n",
            "|    fps              | 556      |\n",
            "|    time_elapsed     | 4022     |\n",
            "|    total_timesteps  | 2239154  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.26     |\n",
            "|    n_updates        | 547288   |\n",
            "----------------------------------\n",
            "Num timesteps: 2240000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 202.99\n",
            "Num timesteps: 2241000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 199.24\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 618      |\n",
            "|    ep_rew_mean      | 199      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3952     |\n",
            "|    fps              | 556      |\n",
            "|    time_elapsed     | 4025     |\n",
            "|    total_timesteps  | 2241157  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.155    |\n",
            "|    n_updates        | 547789   |\n",
            "----------------------------------\n",
            "Num timesteps: 2242000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 199.38\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 611      |\n",
            "|    ep_rew_mean      | 200      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3956     |\n",
            "|    fps              | 556      |\n",
            "|    time_elapsed     | 4027     |\n",
            "|    total_timesteps  | 2242793  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.115    |\n",
            "|    n_updates        | 548198   |\n",
            "----------------------------------\n",
            "Num timesteps: 2243000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 200.01\n",
            "Num timesteps: 2244000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 201.80\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 605      |\n",
            "|    ep_rew_mean      | 202      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3960     |\n",
            "|    fps              | 556      |\n",
            "|    time_elapsed     | 4029     |\n",
            "|    total_timesteps  | 2244274  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0659   |\n",
            "|    n_updates        | 548568   |\n",
            "----------------------------------\n",
            "Num timesteps: 2245000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 202.02\n",
            "Num timesteps: 2246000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 202.37\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 599      |\n",
            "|    ep_rew_mean      | 203      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3964     |\n",
            "|    fps              | 557      |\n",
            "|    time_elapsed     | 4032     |\n",
            "|    total_timesteps  | 2246276  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.102    |\n",
            "|    n_updates        | 549068   |\n",
            "----------------------------------\n",
            "Num timesteps: 2247000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 204.00\n",
            "Num timesteps: 2248000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 204.05\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 596      |\n",
            "|    ep_rew_mean      | 204      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3968     |\n",
            "|    fps              | 557      |\n",
            "|    time_elapsed     | 4035     |\n",
            "|    total_timesteps  | 2248421  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.267    |\n",
            "|    n_updates        | 549605   |\n",
            "----------------------------------\n",
            "Num timesteps: 2249000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 203.91\n",
            "Num timesteps: 2250000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 204.05\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 590      |\n",
            "|    ep_rew_mean      | 204      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3972     |\n",
            "|    fps              | 557      |\n",
            "|    time_elapsed     | 4037     |\n",
            "|    total_timesteps  | 2250160  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.127    |\n",
            "|    n_updates        | 550039   |\n",
            "----------------------------------\n",
            "Num timesteps: 2251000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 205.09\n",
            "Num timesteps: 2252000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 204.89\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 586      |\n",
            "|    ep_rew_mean      | 206      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3976     |\n",
            "|    fps              | 557      |\n",
            "|    time_elapsed     | 4041     |\n",
            "|    total_timesteps  | 2252572  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0554   |\n",
            "|    n_updates        | 550642   |\n",
            "----------------------------------\n",
            "Num timesteps: 2253000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 206.03\n",
            "Num timesteps: 2254000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 207.94\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 576      |\n",
            "|    ep_rew_mean      | 208      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3980     |\n",
            "|    fps              | 557      |\n",
            "|    time_elapsed     | 4044     |\n",
            "|    total_timesteps  | 2254274  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.122    |\n",
            "|    n_updates        | 551068   |\n",
            "----------------------------------\n",
            "Num timesteps: 2255000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 208.51\n",
            "Num timesteps: 2256000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 208.39\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 573      |\n",
            "|    ep_rew_mean      | 209      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3984     |\n",
            "|    fps              | 557      |\n",
            "|    time_elapsed     | 4046     |\n",
            "|    total_timesteps  | 2256142  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.329    |\n",
            "|    n_updates        | 551535   |\n",
            "----------------------------------\n",
            "Num timesteps: 2257000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 209.63\n",
            "Num timesteps: 2258000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 211.58\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 566      |\n",
            "|    ep_rew_mean      | 211      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3988     |\n",
            "|    fps              | 557      |\n",
            "|    time_elapsed     | 4049     |\n",
            "|    total_timesteps  | 2258145  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.114    |\n",
            "|    n_updates        | 552036   |\n",
            "----------------------------------\n",
            "Num timesteps: 2259000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 211.54\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 563      |\n",
            "|    ep_rew_mean      | 212      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3992     |\n",
            "|    fps              | 557      |\n",
            "|    time_elapsed     | 4052     |\n",
            "|    total_timesteps  | 2259907  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0284   |\n",
            "|    n_updates        | 552476   |\n",
            "----------------------------------\n",
            "Num timesteps: 2260000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 212.38\n",
            "Num timesteps: 2261000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 212.31\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 555      |\n",
            "|    ep_rew_mean      | 213      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3996     |\n",
            "|    fps              | 557      |\n",
            "|    time_elapsed     | 4054     |\n",
            "|    total_timesteps  | 2261583  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.069    |\n",
            "|    n_updates        | 552895   |\n",
            "----------------------------------\n",
            "Num timesteps: 2262000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 212.62\n",
            "Num timesteps: 2263000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 212.91\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 547      |\n",
            "|    ep_rew_mean      | 214      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4000     |\n",
            "|    fps              | 557      |\n",
            "|    time_elapsed     | 4056     |\n",
            "|    total_timesteps  | 2263284  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.111    |\n",
            "|    n_updates        | 553320   |\n",
            "----------------------------------\n",
            "Num timesteps: 2264000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 214.04\n",
            "Num timesteps: 2265000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 213.90\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 541      |\n",
            "|    ep_rew_mean      | 216      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4004     |\n",
            "|    fps              | 557      |\n",
            "|    time_elapsed     | 4060     |\n",
            "|    total_timesteps  | 2265575  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.78     |\n",
            "|    n_updates        | 553893   |\n",
            "----------------------------------\n",
            "Num timesteps: 2266000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 216.42\n",
            "Num timesteps: 2267000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 219.77\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 533      |\n",
            "|    ep_rew_mean      | 220      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4008     |\n",
            "|    fps              | 558      |\n",
            "|    time_elapsed     | 4063     |\n",
            "|    total_timesteps  | 2267784  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.069    |\n",
            "|    n_updates        | 554445   |\n",
            "----------------------------------\n",
            "Num timesteps: 2268000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 220.50\n",
            "Num timesteps: 2269000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 221.46\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 523      |\n",
            "|    ep_rew_mean      | 221      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4012     |\n",
            "|    fps              | 558      |\n",
            "|    time_elapsed     | 4066     |\n",
            "|    total_timesteps  | 2269537  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0959   |\n",
            "|    n_updates        | 554884   |\n",
            "----------------------------------\n",
            "Num timesteps: 2270000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 221.20\n",
            "Num timesteps: 2271000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 220.89\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 514      |\n",
            "|    ep_rew_mean      | 223      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4016     |\n",
            "|    fps              | 558      |\n",
            "|    time_elapsed     | 4069     |\n",
            "|    total_timesteps  | 2271927  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.372    |\n",
            "|    n_updates        | 555481   |\n",
            "----------------------------------\n",
            "Num timesteps: 2272000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 223.00\n",
            "Num timesteps: 2273000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 223.39\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 509      |\n",
            "|    ep_rew_mean      | 224      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4020     |\n",
            "|    fps              | 558      |\n",
            "|    time_elapsed     | 4072     |\n",
            "|    total_timesteps  | 2273877  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.204    |\n",
            "|    n_updates        | 555969   |\n",
            "----------------------------------\n",
            "Num timesteps: 2274000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 223.75\n",
            "Num timesteps: 2275000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 226.92\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 500      |\n",
            "|    ep_rew_mean      | 227      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4024     |\n",
            "|    fps              | 558      |\n",
            "|    time_elapsed     | 4074     |\n",
            "|    total_timesteps  | 2275901  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.143    |\n",
            "|    n_updates        | 556475   |\n",
            "----------------------------------\n",
            "Num timesteps: 2276000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 227.13\n",
            "Num timesteps: 2277000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 226.35\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 498      |\n",
            "|    ep_rew_mean      | 227      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4028     |\n",
            "|    fps              | 558      |\n",
            "|    time_elapsed     | 4077     |\n",
            "|    total_timesteps  | 2277626  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0724   |\n",
            "|    n_updates        | 556906   |\n",
            "----------------------------------\n",
            "Num timesteps: 2278000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 227.77\n",
            "Num timesteps: 2279000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 229.82\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 485      |\n",
            "|    ep_rew_mean      | 230      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4032     |\n",
            "|    fps              | 558      |\n",
            "|    time_elapsed     | 4078     |\n",
            "|    total_timesteps  | 2279058  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0646   |\n",
            "|    n_updates        | 557264   |\n",
            "----------------------------------\n",
            "Num timesteps: 2280000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 230.09\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 478      |\n",
            "|    ep_rew_mean      | 231      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4036     |\n",
            "|    fps              | 558      |\n",
            "|    time_elapsed     | 4081     |\n",
            "|    total_timesteps  | 2280898  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.159    |\n",
            "|    n_updates        | 557724   |\n",
            "----------------------------------\n",
            "Num timesteps: 2281000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 230.80\n",
            "Num timesteps: 2282000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 231.02\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 474      |\n",
            "|    ep_rew_mean      | 231      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4040     |\n",
            "|    fps              | 558      |\n",
            "|    time_elapsed     | 4083     |\n",
            "|    total_timesteps  | 2282563  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.245    |\n",
            "|    n_updates        | 558140   |\n",
            "----------------------------------\n",
            "Num timesteps: 2283000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 231.10\n",
            "Num timesteps: 2284000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 231.41\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 467      |\n",
            "|    ep_rew_mean      | 232      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4044     |\n",
            "|    fps              | 559      |\n",
            "|    time_elapsed     | 4085     |\n",
            "|    total_timesteps  | 2284122  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.167    |\n",
            "|    n_updates        | 558530   |\n",
            "----------------------------------\n",
            "Num timesteps: 2285000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 229.22\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 465      |\n",
            "|    ep_rew_mean      | 229      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4048     |\n",
            "|    fps              | 559      |\n",
            "|    time_elapsed     | 4087     |\n",
            "|    total_timesteps  | 2285661  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0548   |\n",
            "|    n_updates        | 558915   |\n",
            "----------------------------------\n",
            "Num timesteps: 2286000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 229.34\n",
            "Num timesteps: 2287000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 234.29\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 462      |\n",
            "|    ep_rew_mean      | 234      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4052     |\n",
            "|    fps              | 559      |\n",
            "|    time_elapsed     | 4089     |\n",
            "|    total_timesteps  | 2287325  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.266    |\n",
            "|    n_updates        | 559331   |\n",
            "----------------------------------\n",
            "Num timesteps: 2288000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 233.41\n",
            "Num timesteps: 2289000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 233.97\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 463      |\n",
            "|    ep_rew_mean      | 234      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4056     |\n",
            "|    fps              | 559      |\n",
            "|    time_elapsed     | 4092     |\n",
            "|    total_timesteps  | 2289045  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.087    |\n",
            "|    n_updates        | 559761   |\n",
            "----------------------------------\n",
            "Num timesteps: 2290000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 232.93\n",
            "Num timesteps: 2291000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 232.53\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 468      |\n",
            "|    ep_rew_mean      | 232      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4060     |\n",
            "|    fps              | 559      |\n",
            "|    time_elapsed     | 4095     |\n",
            "|    total_timesteps  | 2291065  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.186    |\n",
            "|    n_updates        | 560266   |\n",
            "----------------------------------\n",
            "Num timesteps: 2292000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 232.93\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 465      |\n",
            "|    ep_rew_mean      | 233      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4064     |\n",
            "|    fps              | 559      |\n",
            "|    time_elapsed     | 4097     |\n",
            "|    total_timesteps  | 2292790  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.102    |\n",
            "|    n_updates        | 560697   |\n",
            "----------------------------------\n",
            "Num timesteps: 2293000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 233.09\n",
            "Num timesteps: 2294000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 233.33\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 459      |\n",
            "|    ep_rew_mean      | 234      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4068     |\n",
            "|    fps              | 559      |\n",
            "|    time_elapsed     | 4099     |\n",
            "|    total_timesteps  | 2294315  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.107    |\n",
            "|    n_updates        | 561078   |\n",
            "----------------------------------\n",
            "Num timesteps: 2295000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 234.36\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 457      |\n",
            "|    ep_rew_mean      | 235      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4072     |\n",
            "|    fps              | 559      |\n",
            "|    time_elapsed     | 4101     |\n",
            "|    total_timesteps  | 2295897  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0575   |\n",
            "|    n_updates        | 561474   |\n",
            "----------------------------------\n",
            "Num timesteps: 2296000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 234.95\n",
            "Num timesteps: 2297000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 236.38\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 449      |\n",
            "|    ep_rew_mean      | 236      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4076     |\n",
            "|    fps              | 559      |\n",
            "|    time_elapsed     | 4103     |\n",
            "|    total_timesteps  | 2297498  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.187    |\n",
            "|    n_updates        | 561874   |\n",
            "----------------------------------\n",
            "Num timesteps: 2298000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 236.91\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 446      |\n",
            "|    ep_rew_mean      | 238      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4080     |\n",
            "|    fps              | 559      |\n",
            "|    time_elapsed     | 4105     |\n",
            "|    total_timesteps  | 2298862  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.153    |\n",
            "|    n_updates        | 562215   |\n",
            "----------------------------------\n",
            "Num timesteps: 2299000\n",
            "Best mean reward: 237.40 - Last mean reward per episode: 237.55\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 2300000\n",
            "Best mean reward: 237.55 - Last mean reward per episode: 237.66\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 444      |\n",
            "|    ep_rew_mean      | 237      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4084     |\n",
            "|    fps              | 560      |\n",
            "|    time_elapsed     | 4107     |\n",
            "|    total_timesteps  | 2300591  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.24     |\n",
            "|    n_updates        | 562647   |\n",
            "----------------------------------\n",
            "Num timesteps: 2301000\n",
            "Best mean reward: 237.66 - Last mean reward per episode: 237.37\n",
            "Num timesteps: 2302000\n",
            "Best mean reward: 237.66 - Last mean reward per episode: 237.76\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 238      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4088     |\n",
            "|    fps              | 560      |\n",
            "|    time_elapsed     | 4110     |\n",
            "|    total_timesteps  | 2302263  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0728   |\n",
            "|    n_updates        | 563065   |\n",
            "----------------------------------\n",
            "Num timesteps: 2303000\n",
            "Best mean reward: 237.76 - Last mean reward per episode: 238.41\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 439      |\n",
            "|    ep_rew_mean      | 238      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4092     |\n",
            "|    fps              | 560      |\n",
            "|    time_elapsed     | 4112     |\n",
            "|    total_timesteps  | 2303773  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0945   |\n",
            "|    n_updates        | 563443   |\n",
            "----------------------------------\n",
            "Num timesteps: 2304000\n",
            "Best mean reward: 238.41 - Last mean reward per episode: 237.89\n",
            "Num timesteps: 2305000\n",
            "Best mean reward: 238.41 - Last mean reward per episode: 238.12\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 440      |\n",
            "|    ep_rew_mean      | 239      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4096     |\n",
            "|    fps              | 560      |\n",
            "|    time_elapsed     | 4114     |\n",
            "|    total_timesteps  | 2305625  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.25     |\n",
            "|    n_updates        | 563906   |\n",
            "----------------------------------\n",
            "Num timesteps: 2306000\n",
            "Best mean reward: 238.41 - Last mean reward per episode: 238.50\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 2307000\n",
            "Best mean reward: 238.50 - Last mean reward per episode: 238.95\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 443      |\n",
            "|    ep_rew_mean      | 238      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4100     |\n",
            "|    fps              | 560      |\n",
            "|    time_elapsed     | 4117     |\n",
            "|    total_timesteps  | 2307625  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.109    |\n",
            "|    n_updates        | 564406   |\n",
            "----------------------------------\n",
            "Num timesteps: 2308000\n",
            "Best mean reward: 238.95 - Last mean reward per episode: 238.02\n",
            "Num timesteps: 2309000\n",
            "Best mean reward: 238.95 - Last mean reward per episode: 238.21\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 440      |\n",
            "|    ep_rew_mean      | 238      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4104     |\n",
            "|    fps              | 560      |\n",
            "|    time_elapsed     | 4120     |\n",
            "|    total_timesteps  | 2309610  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.11     |\n",
            "|    n_updates        | 564902   |\n",
            "----------------------------------\n",
            "Num timesteps: 2310000\n",
            "Best mean reward: 238.95 - Last mean reward per episode: 237.65\n",
            "Num timesteps: 2311000\n",
            "Best mean reward: 238.95 - Last mean reward per episode: 238.54\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 432      |\n",
            "|    ep_rew_mean      | 239      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4108     |\n",
            "|    fps              | 560      |\n",
            "|    time_elapsed     | 4122     |\n",
            "|    total_timesteps  | 2311005  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0602   |\n",
            "|    n_updates        | 565251   |\n",
            "----------------------------------\n",
            "Num timesteps: 2312000\n",
            "Best mean reward: 238.95 - Last mean reward per episode: 239.69\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 429      |\n",
            "|    ep_rew_mean      | 239      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4112     |\n",
            "|    fps              | 560      |\n",
            "|    time_elapsed     | 4123     |\n",
            "|    total_timesteps  | 2312393  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.73     |\n",
            "|    n_updates        | 565598   |\n",
            "----------------------------------\n",
            "Num timesteps: 2313000\n",
            "Best mean reward: 239.69 - Last mean reward per episode: 239.72\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 419      |\n",
            "|    ep_rew_mean      | 240      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4116     |\n",
            "|    fps              | 560      |\n",
            "|    time_elapsed     | 4125     |\n",
            "|    total_timesteps  | 2313790  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.147    |\n",
            "|    n_updates        | 565947   |\n",
            "----------------------------------\n",
            "Num timesteps: 2314000\n",
            "Best mean reward: 239.72 - Last mean reward per episode: 240.11\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 2315000\n",
            "Best mean reward: 240.11 - Last mean reward per episode: 240.43\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 417      |\n",
            "|    ep_rew_mean      | 240      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4120     |\n",
            "|    fps              | 560      |\n",
            "|    time_elapsed     | 4128     |\n",
            "|    total_timesteps  | 2315558  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.47     |\n",
            "|    n_updates        | 566389   |\n",
            "----------------------------------\n",
            "Num timesteps: 2316000\n",
            "Best mean reward: 240.43 - Last mean reward per episode: 240.36\n",
            "Num timesteps: 2317000\n",
            "Best mean reward: 240.43 - Last mean reward per episode: 240.20\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 413      |\n",
            "|    ep_rew_mean      | 240      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4124     |\n",
            "|    fps              | 561      |\n",
            "|    time_elapsed     | 4130     |\n",
            "|    total_timesteps  | 2317246  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0582   |\n",
            "|    n_updates        | 566811   |\n",
            "----------------------------------\n",
            "Num timesteps: 2318000\n",
            "Best mean reward: 240.43 - Last mean reward per episode: 240.73\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 408      |\n",
            "|    ep_rew_mean      | 241      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4128     |\n",
            "|    fps              | 561      |\n",
            "|    time_elapsed     | 4131     |\n",
            "|    total_timesteps  | 2318449  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.508    |\n",
            "|    n_updates        | 567112   |\n",
            "----------------------------------\n",
            "Num timesteps: 2319000\n",
            "Best mean reward: 240.73 - Last mean reward per episode: 240.58\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 409      |\n",
            "|    ep_rew_mean      | 241      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4132     |\n",
            "|    fps              | 561      |\n",
            "|    time_elapsed     | 4133     |\n",
            "|    total_timesteps  | 2319954  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.109    |\n",
            "|    n_updates        | 567488   |\n",
            "----------------------------------\n",
            "Num timesteps: 2320000\n",
            "Best mean reward: 240.73 - Last mean reward per episode: 240.66\n",
            "Num timesteps: 2321000\n",
            "Best mean reward: 240.73 - Last mean reward per episode: 241.10\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 407      |\n",
            "|    ep_rew_mean      | 241      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4136     |\n",
            "|    fps              | 561      |\n",
            "|    time_elapsed     | 4135     |\n",
            "|    total_timesteps  | 2321559  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.143    |\n",
            "|    n_updates        | 567889   |\n",
            "----------------------------------\n",
            "Num timesteps: 2322000\n",
            "Best mean reward: 241.10 - Last mean reward per episode: 240.95\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 404      |\n",
            "|    ep_rew_mean      | 242      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4140     |\n",
            "|    fps              | 561      |\n",
            "|    time_elapsed     | 4137     |\n",
            "|    total_timesteps  | 2322952  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.337    |\n",
            "|    n_updates        | 568237   |\n",
            "----------------------------------\n",
            "Num timesteps: 2323000\n",
            "Best mean reward: 241.10 - Last mean reward per episode: 241.69\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 2324000\n",
            "Best mean reward: 241.69 - Last mean reward per episode: 241.35\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 403      |\n",
            "|    ep_rew_mean      | 242      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4144     |\n",
            "|    fps              | 561      |\n",
            "|    time_elapsed     | 4139     |\n",
            "|    total_timesteps  | 2324419  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.304    |\n",
            "|    n_updates        | 568604   |\n",
            "----------------------------------\n",
            "Num timesteps: 2325000\n",
            "Best mean reward: 241.69 - Last mean reward per episode: 245.06\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 2326000\n",
            "Best mean reward: 245.06 - Last mean reward per episode: 244.37\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 403      |\n",
            "|    ep_rew_mean      | 244      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4148     |\n",
            "|    fps              | 561      |\n",
            "|    time_elapsed     | 4141     |\n",
            "|    total_timesteps  | 2326003  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.267    |\n",
            "|    n_updates        | 569000   |\n",
            "----------------------------------\n",
            "Num timesteps: 2327000\n",
            "Best mean reward: 245.06 - Last mean reward per episode: 243.88\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 402      |\n",
            "|    ep_rew_mean      | 244      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4152     |\n",
            "|    fps              | 561      |\n",
            "|    time_elapsed     | 4143     |\n",
            "|    total_timesteps  | 2327562  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0689   |\n",
            "|    n_updates        | 569390   |\n",
            "----------------------------------\n",
            "Num timesteps: 2328000\n",
            "Best mean reward: 245.06 - Last mean reward per episode: 243.97\n",
            "Num timesteps: 2329000\n",
            "Best mean reward: 245.06 - Last mean reward per episode: 244.74\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 402      |\n",
            "|    ep_rew_mean      | 245      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4156     |\n",
            "|    fps              | 561      |\n",
            "|    time_elapsed     | 4146     |\n",
            "|    total_timesteps  | 2329219  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0957   |\n",
            "|    n_updates        | 569804   |\n",
            "----------------------------------\n",
            "Num timesteps: 2330000\n",
            "Best mean reward: 245.06 - Last mean reward per episode: 245.72\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 2331000\n",
            "Best mean reward: 245.72 - Last mean reward per episode: 246.22\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 404      |\n",
            "|    ep_rew_mean      | 245      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4160     |\n",
            "|    fps              | 561      |\n",
            "|    time_elapsed     | 4149     |\n",
            "|    total_timesteps  | 2331470  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.082    |\n",
            "|    n_updates        | 570367   |\n",
            "----------------------------------\n",
            "Num timesteps: 2332000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 245.07\n",
            "Num timesteps: 2333000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 244.99\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 403      |\n",
            "|    ep_rew_mean      | 245      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4164     |\n",
            "|    fps              | 561      |\n",
            "|    time_elapsed     | 4151     |\n",
            "|    total_timesteps  | 2333101  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0652   |\n",
            "|    n_updates        | 570775   |\n",
            "----------------------------------\n",
            "Num timesteps: 2334000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 245.11\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 406      |\n",
            "|    ep_rew_mean      | 246      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4168     |\n",
            "|    fps              | 562      |\n",
            "|    time_elapsed     | 4154     |\n",
            "|    total_timesteps  | 2334946  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.29     |\n",
            "|    n_updates        | 571236   |\n",
            "----------------------------------\n",
            "Num timesteps: 2335000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 245.63\n",
            "Num timesteps: 2336000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 245.79\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 401      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4172     |\n",
            "|    fps              | 562      |\n",
            "|    time_elapsed     | 4155     |\n",
            "|    total_timesteps  | 2336006  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.178    |\n",
            "|    n_updates        | 571501   |\n",
            "----------------------------------\n",
            "Num timesteps: 2337000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 243.53\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 398      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4176     |\n",
            "|    fps              | 562      |\n",
            "|    time_elapsed     | 4156     |\n",
            "|    total_timesteps  | 2337308  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.109    |\n",
            "|    n_updates        | 571826   |\n",
            "----------------------------------\n",
            "Num timesteps: 2338000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 240.78\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 398      |\n",
            "|    ep_rew_mean      | 240      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4180     |\n",
            "|    fps              | 562      |\n",
            "|    time_elapsed     | 4158     |\n",
            "|    total_timesteps  | 2338630  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.88     |\n",
            "|    n_updates        | 572157   |\n",
            "----------------------------------\n",
            "Num timesteps: 2339000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 240.54\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 393      |\n",
            "|    ep_rew_mean      | 241      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4184     |\n",
            "|    fps              | 562      |\n",
            "|    time_elapsed     | 4160     |\n",
            "|    total_timesteps  | 2339876  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.8      |\n",
            "|    n_updates        | 572468   |\n",
            "----------------------------------\n",
            "Num timesteps: 2340000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 241.49\n",
            "Num timesteps: 2341000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 241.21\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 390      |\n",
            "|    ep_rew_mean      | 239      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4188     |\n",
            "|    fps              | 562      |\n",
            "|    time_elapsed     | 4162     |\n",
            "|    total_timesteps  | 2341283  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.35     |\n",
            "|    n_updates        | 572820   |\n",
            "----------------------------------\n",
            "Num timesteps: 2342000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 238.84\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 391      |\n",
            "|    ep_rew_mean      | 239      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4192     |\n",
            "|    fps              | 562      |\n",
            "|    time_elapsed     | 4164     |\n",
            "|    total_timesteps  | 2342919  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0818   |\n",
            "|    n_updates        | 573229   |\n",
            "----------------------------------\n",
            "Num timesteps: 2343000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 239.02\n",
            "Num timesteps: 2344000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 239.03\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 390      |\n",
            "|    ep_rew_mean      | 239      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4196     |\n",
            "|    fps              | 562      |\n",
            "|    time_elapsed     | 4166     |\n",
            "|    total_timesteps  | 2344664  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.233    |\n",
            "|    n_updates        | 573665   |\n",
            "----------------------------------\n",
            "Num timesteps: 2345000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 239.36\n",
            "Num timesteps: 2346000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 240.10\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 385      |\n",
            "|    ep_rew_mean      | 241      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4200     |\n",
            "|    fps              | 562      |\n",
            "|    time_elapsed     | 4168     |\n",
            "|    total_timesteps  | 2346096  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0906   |\n",
            "|    n_updates        | 574023   |\n",
            "----------------------------------\n",
            "Num timesteps: 2347000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 241.53\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 378      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4204     |\n",
            "|    fps              | 562      |\n",
            "|    time_elapsed     | 4169     |\n",
            "|    total_timesteps  | 2347367  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.16     |\n",
            "|    n_updates        | 574341   |\n",
            "----------------------------------\n",
            "Num timesteps: 2348000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 243.72\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 379      |\n",
            "|    ep_rew_mean      | 242      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4208     |\n",
            "|    fps              | 563      |\n",
            "|    time_elapsed     | 4171     |\n",
            "|    total_timesteps  | 2348873  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.219    |\n",
            "|    n_updates        | 574718   |\n",
            "----------------------------------\n",
            "Num timesteps: 2349000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 242.34\n",
            "Num timesteps: 2350000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 242.38\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 380      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4212     |\n",
            "|    fps              | 563      |\n",
            "|    time_elapsed     | 4173     |\n",
            "|    total_timesteps  | 2350351  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0503   |\n",
            "|    n_updates        | 575087   |\n",
            "----------------------------------\n",
            "Num timesteps: 2351000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 242.49\n",
            "Num timesteps: 2352000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 243.13\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 383      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4216     |\n",
            "|    fps              | 563      |\n",
            "|    time_elapsed     | 4175     |\n",
            "|    total_timesteps  | 2352105  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.077    |\n",
            "|    n_updates        | 575526   |\n",
            "----------------------------------\n",
            "Num timesteps: 2353000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 243.53\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 383      |\n",
            "|    ep_rew_mean      | 244      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4220     |\n",
            "|    fps              | 563      |\n",
            "|    time_elapsed     | 4178     |\n",
            "|    total_timesteps  | 2353887  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0766   |\n",
            "|    n_updates        | 575971   |\n",
            "----------------------------------\n",
            "Num timesteps: 2354000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 243.66\n",
            "Num timesteps: 2355000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 242.54\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 380      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4224     |\n",
            "|    fps              | 563      |\n",
            "|    time_elapsed     | 4180     |\n",
            "|    total_timesteps  | 2355286  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.82     |\n",
            "|    n_updates        | 576321   |\n",
            "----------------------------------\n",
            "Num timesteps: 2356000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 243.44\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 382      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4228     |\n",
            "|    fps              | 563      |\n",
            "|    time_elapsed     | 4181     |\n",
            "|    total_timesteps  | 2356622  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.97     |\n",
            "|    n_updates        | 576655   |\n",
            "----------------------------------\n",
            "Num timesteps: 2357000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 243.88\n",
            "Num timesteps: 2358000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 243.40\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 383      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4232     |\n",
            "|    fps              | 563      |\n",
            "|    time_elapsed     | 4183     |\n",
            "|    total_timesteps  | 2358249  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.145    |\n",
            "|    n_updates        | 577062   |\n",
            "----------------------------------\n",
            "Num timesteps: 2359000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 243.46\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 380      |\n",
            "|    ep_rew_mean      | 244      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4236     |\n",
            "|    fps              | 563      |\n",
            "|    time_elapsed     | 4185     |\n",
            "|    total_timesteps  | 2359590  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0618   |\n",
            "|    n_updates        | 577397   |\n",
            "----------------------------------\n",
            "Num timesteps: 2360000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 243.45\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 380      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4240     |\n",
            "|    fps              | 563      |\n",
            "|    time_elapsed     | 4187     |\n",
            "|    total_timesteps  | 2360938  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0472   |\n",
            "|    n_updates        | 577734   |\n",
            "----------------------------------\n",
            "Num timesteps: 2361000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 243.27\n",
            "Num timesteps: 2362000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 241.27\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 378      |\n",
            "|    ep_rew_mean      | 241      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4244     |\n",
            "|    fps              | 563      |\n",
            "|    time_elapsed     | 4189     |\n",
            "|    total_timesteps  | 2362261  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.116    |\n",
            "|    n_updates        | 578065   |\n",
            "----------------------------------\n",
            "Num timesteps: 2363000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 241.28\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 378      |\n",
            "|    ep_rew_mean      | 241      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4248     |\n",
            "|    fps              | 563      |\n",
            "|    time_elapsed     | 4191     |\n",
            "|    total_timesteps  | 2363799  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.181    |\n",
            "|    n_updates        | 578449   |\n",
            "----------------------------------\n",
            "Num timesteps: 2364000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 241.05\n",
            "Num timesteps: 2365000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 241.16\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 375      |\n",
            "|    ep_rew_mean      | 241      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4252     |\n",
            "|    fps              | 564      |\n",
            "|    time_elapsed     | 4192     |\n",
            "|    total_timesteps  | 2365097  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.227    |\n",
            "|    n_updates        | 578774   |\n",
            "----------------------------------\n",
            "Num timesteps: 2366000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 240.83\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 376      |\n",
            "|    ep_rew_mean      | 241      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4256     |\n",
            "|    fps              | 564      |\n",
            "|    time_elapsed     | 4195     |\n",
            "|    total_timesteps  | 2366826  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.107    |\n",
            "|    n_updates        | 579206   |\n",
            "----------------------------------\n",
            "Num timesteps: 2367000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 240.61\n",
            "Num timesteps: 2368000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 241.63\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 368      |\n",
            "|    ep_rew_mean      | 241      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4260     |\n",
            "|    fps              | 564      |\n",
            "|    time_elapsed     | 4196     |\n",
            "|    total_timesteps  | 2368308  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.142    |\n",
            "|    n_updates        | 579576   |\n",
            "----------------------------------\n",
            "Num timesteps: 2369000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 241.28\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 366      |\n",
            "|    ep_rew_mean      | 242      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4264     |\n",
            "|    fps              | 564      |\n",
            "|    time_elapsed     | 4198     |\n",
            "|    total_timesteps  | 2369746  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.123    |\n",
            "|    n_updates        | 579936   |\n",
            "----------------------------------\n",
            "Num timesteps: 2370000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 241.69\n",
            "Num timesteps: 2371000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 241.69\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 364      |\n",
            "|    ep_rew_mean      | 242      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4268     |\n",
            "|    fps              | 564      |\n",
            "|    time_elapsed     | 4200     |\n",
            "|    total_timesteps  | 2371373  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0695   |\n",
            "|    n_updates        | 580343   |\n",
            "----------------------------------\n",
            "Num timesteps: 2372000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 241.32\n",
            "Num timesteps: 2373000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 241.15\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 371      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4272     |\n",
            "|    fps              | 564      |\n",
            "|    time_elapsed     | 4203     |\n",
            "|    total_timesteps  | 2373086  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.286    |\n",
            "|    n_updates        | 580771   |\n",
            "----------------------------------\n",
            "Num timesteps: 2374000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 243.22\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 373      |\n",
            "|    ep_rew_mean      | 244      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4276     |\n",
            "|    fps              | 564      |\n",
            "|    time_elapsed     | 4205     |\n",
            "|    total_timesteps  | 2374650  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.451    |\n",
            "|    n_updates        | 581162   |\n",
            "----------------------------------\n",
            "Num timesteps: 2375000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 245.30\n",
            "Num timesteps: 2376000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 244.68\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 376      |\n",
            "|    ep_rew_mean      | 246      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4280     |\n",
            "|    fps              | 564      |\n",
            "|    time_elapsed     | 4207     |\n",
            "|    total_timesteps  | 2376200  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.96     |\n",
            "|    n_updates        | 581549   |\n",
            "----------------------------------\n",
            "Num timesteps: 2377000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 245.84\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 378      |\n",
            "|    ep_rew_mean      | 246      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4284     |\n",
            "|    fps              | 564      |\n",
            "|    time_elapsed     | 4208     |\n",
            "|    total_timesteps  | 2377646  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.113    |\n",
            "|    n_updates        | 581911   |\n",
            "----------------------------------\n",
            "Num timesteps: 2378000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 246.03\n",
            "Num timesteps: 2379000\n",
            "Best mean reward: 246.22 - Last mean reward per episode: 246.37\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 379      |\n",
            "|    ep_rew_mean      | 248      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4288     |\n",
            "|    fps              | 565      |\n",
            "|    time_elapsed     | 4210     |\n",
            "|    total_timesteps  | 2379197  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.325    |\n",
            "|    n_updates        | 582299   |\n",
            "----------------------------------\n",
            "Num timesteps: 2380000\n",
            "Best mean reward: 246.37 - Last mean reward per episode: 248.32\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 376      |\n",
            "|    ep_rew_mean      | 249      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4292     |\n",
            "|    fps              | 565      |\n",
            "|    time_elapsed     | 4212     |\n",
            "|    total_timesteps  | 2380568  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0915   |\n",
            "|    n_updates        | 582641   |\n",
            "----------------------------------\n",
            "Num timesteps: 2381000\n",
            "Best mean reward: 248.32 - Last mean reward per episode: 248.58\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 2382000\n",
            "Best mean reward: 248.58 - Last mean reward per episode: 248.18\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 374      |\n",
            "|    ep_rew_mean      | 248      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4296     |\n",
            "|    fps              | 565      |\n",
            "|    time_elapsed     | 4214     |\n",
            "|    total_timesteps  | 2382049  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.339    |\n",
            "|    n_updates        | 583012   |\n",
            "----------------------------------\n",
            "Num timesteps: 2383000\n",
            "Best mean reward: 248.58 - Last mean reward per episode: 248.32\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 373      |\n",
            "|    ep_rew_mean      | 247      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4300     |\n",
            "|    fps              | 565      |\n",
            "|    time_elapsed     | 4216     |\n",
            "|    total_timesteps  | 2383395  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.121    |\n",
            "|    n_updates        | 583348   |\n",
            "----------------------------------\n",
            "Num timesteps: 2384000\n",
            "Best mean reward: 248.58 - Last mean reward per episode: 247.62\n",
            "Num timesteps: 2385000\n",
            "Best mean reward: 248.58 - Last mean reward per episode: 247.00\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 377      |\n",
            "|    ep_rew_mean      | 247      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4304     |\n",
            "|    fps              | 565      |\n",
            "|    time_elapsed     | 4218     |\n",
            "|    total_timesteps  | 2385019  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0775   |\n",
            "|    n_updates        | 583754   |\n",
            "----------------------------------\n",
            "Num timesteps: 2386000\n",
            "Best mean reward: 248.58 - Last mean reward per episode: 246.41\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 376      |\n",
            "|    ep_rew_mean      | 247      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4308     |\n",
            "|    fps              | 565      |\n",
            "|    time_elapsed     | 4220     |\n",
            "|    total_timesteps  | 2386423  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.175    |\n",
            "|    n_updates        | 584105   |\n",
            "----------------------------------\n",
            "Num timesteps: 2387000\n",
            "Best mean reward: 248.58 - Last mean reward per episode: 247.12\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 375      |\n",
            "|    ep_rew_mean      | 248      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4312     |\n",
            "|    fps              | 565      |\n",
            "|    time_elapsed     | 4221     |\n",
            "|    total_timesteps  | 2387843  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.121    |\n",
            "|    n_updates        | 584460   |\n",
            "----------------------------------\n",
            "Num timesteps: 2388000\n",
            "Best mean reward: 248.58 - Last mean reward per episode: 247.58\n",
            "Num timesteps: 2389000\n",
            "Best mean reward: 248.58 - Last mean reward per episode: 247.61\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 372      |\n",
            "|    ep_rew_mean      | 248      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4316     |\n",
            "|    fps              | 565      |\n",
            "|    time_elapsed     | 4223     |\n",
            "|    total_timesteps  | 2389336  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.46     |\n",
            "|    n_updates        | 584833   |\n",
            "----------------------------------\n",
            "Num timesteps: 2390000\n",
            "Best mean reward: 248.58 - Last mean reward per episode: 247.48\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 369      |\n",
            "|    ep_rew_mean      | 248      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4320     |\n",
            "|    fps              | 565      |\n",
            "|    time_elapsed     | 4225     |\n",
            "|    total_timesteps  | 2390786  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.241    |\n",
            "|    n_updates        | 585196   |\n",
            "----------------------------------\n",
            "Num timesteps: 2391000\n",
            "Best mean reward: 248.58 - Last mean reward per episode: 247.96\n",
            "Num timesteps: 2392000\n",
            "Best mean reward: 248.58 - Last mean reward per episode: 249.68\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 371      |\n",
            "|    ep_rew_mean      | 249      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4324     |\n",
            "|    fps              | 565      |\n",
            "|    time_elapsed     | 4227     |\n",
            "|    total_timesteps  | 2392426  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.065    |\n",
            "|    n_updates        | 585606   |\n",
            "----------------------------------\n",
            "Num timesteps: 2393000\n",
            "Best mean reward: 249.68 - Last mean reward per episode: 248.44\n",
            "Num timesteps: 2394000\n",
            "Best mean reward: 249.68 - Last mean reward per episode: 247.41\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 375      |\n",
            "|    ep_rew_mean      | 247      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4328     |\n",
            "|    fps              | 565      |\n",
            "|    time_elapsed     | 4229     |\n",
            "|    total_timesteps  | 2394103  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.111    |\n",
            "|    n_updates        | 586025   |\n",
            "----------------------------------\n",
            "Num timesteps: 2395000\n",
            "Best mean reward: 249.68 - Last mean reward per episode: 246.76\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 371      |\n",
            "|    ep_rew_mean      | 247      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4332     |\n",
            "|    fps              | 566      |\n",
            "|    time_elapsed     | 4231     |\n",
            "|    total_timesteps  | 2395324  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.154    |\n",
            "|    n_updates        | 586330   |\n",
            "----------------------------------\n",
            "Num timesteps: 2396000\n",
            "Best mean reward: 249.68 - Last mean reward per episode: 247.53\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 374      |\n",
            "|    ep_rew_mean      | 248      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4336     |\n",
            "|    fps              | 566      |\n",
            "|    time_elapsed     | 4233     |\n",
            "|    total_timesteps  | 2396996  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0237   |\n",
            "|    n_updates        | 586748   |\n",
            "----------------------------------\n",
            "Num timesteps: 2397000\n",
            "Best mean reward: 249.68 - Last mean reward per episode: 248.11\n",
            "Num timesteps: 2398000\n",
            "Best mean reward: 249.68 - Last mean reward per episode: 247.23\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 379      |\n",
            "|    ep_rew_mean      | 248      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4340     |\n",
            "|    fps              | 566      |\n",
            "|    time_elapsed     | 4236     |\n",
            "|    total_timesteps  | 2398865  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.151    |\n",
            "|    n_updates        | 587216   |\n",
            "----------------------------------\n",
            "Num timesteps: 2399000\n",
            "Best mean reward: 249.68 - Last mean reward per episode: 247.57\n",
            "Num timesteps: 2400000\n",
            "Best mean reward: 249.68 - Last mean reward per episode: 249.76\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 381      |\n",
            "|    ep_rew_mean      | 250      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4344     |\n",
            "|    fps              | 566      |\n",
            "|    time_elapsed     | 4237     |\n",
            "|    total_timesteps  | 2400334  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0722   |\n",
            "|    n_updates        | 587583   |\n",
            "----------------------------------\n",
            "Num timesteps: 2401000\n",
            "Best mean reward: 249.76 - Last mean reward per episode: 250.29\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 379      |\n",
            "|    ep_rew_mean      | 251      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4348     |\n",
            "|    fps              | 566      |\n",
            "|    time_elapsed     | 4239     |\n",
            "|    total_timesteps  | 2401711  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.141    |\n",
            "|    n_updates        | 587927   |\n",
            "----------------------------------\n",
            "Num timesteps: 2402000\n",
            "Best mean reward: 250.29 - Last mean reward per episode: 251.00\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 2403000\n",
            "Best mean reward: 251.00 - Last mean reward per episode: 251.22\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 380      |\n",
            "|    ep_rew_mean      | 252      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4352     |\n",
            "|    fps              | 566      |\n",
            "|    time_elapsed     | 4241     |\n",
            "|    total_timesteps  | 2403086  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0511   |\n",
            "|    n_updates        | 588271   |\n",
            "----------------------------------\n",
            "Num timesteps: 2404000\n",
            "Best mean reward: 251.22 - Last mean reward per episode: 251.80\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 376      |\n",
            "|    ep_rew_mean      | 252      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4356     |\n",
            "|    fps              | 566      |\n",
            "|    time_elapsed     | 4242     |\n",
            "|    total_timesteps  | 2404377  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.106    |\n",
            "|    n_updates        | 588594   |\n",
            "----------------------------------\n",
            "Num timesteps: 2405000\n",
            "Best mean reward: 251.80 - Last mean reward per episode: 252.19\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 376      |\n",
            "|    ep_rew_mean      | 252      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4360     |\n",
            "|    fps              | 566      |\n",
            "|    time_elapsed     | 4244     |\n",
            "|    total_timesteps  | 2405920  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0946   |\n",
            "|    n_updates        | 588979   |\n",
            "----------------------------------\n",
            "Num timesteps: 2406000\n",
            "Best mean reward: 252.19 - Last mean reward per episode: 252.38\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 2407000\n",
            "Best mean reward: 252.38 - Last mean reward per episode: 252.55\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 375      |\n",
            "|    ep_rew_mean      | 252      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4364     |\n",
            "|    fps              | 566      |\n",
            "|    time_elapsed     | 4246     |\n",
            "|    total_timesteps  | 2407262  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.247    |\n",
            "|    n_updates        | 589315   |\n",
            "----------------------------------\n",
            "Num timesteps: 2408000\n",
            "Best mean reward: 252.55 - Last mean reward per episode: 252.27\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 371      |\n",
            "|    ep_rew_mean      | 253      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4368     |\n",
            "|    fps              | 566      |\n",
            "|    time_elapsed     | 4247     |\n",
            "|    total_timesteps  | 2408425  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.149    |\n",
            "|    n_updates        | 589606   |\n",
            "----------------------------------\n",
            "Num timesteps: 2409000\n",
            "Best mean reward: 252.55 - Last mean reward per episode: 252.56\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 2410000\n",
            "Best mean reward: 252.56 - Last mean reward per episode: 252.52\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 371      |\n",
            "|    ep_rew_mean      | 252      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4372     |\n",
            "|    fps              | 567      |\n",
            "|    time_elapsed     | 4249     |\n",
            "|    total_timesteps  | 2410173  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.482    |\n",
            "|    n_updates        | 590043   |\n",
            "----------------------------------\n",
            "Num timesteps: 2411000\n",
            "Best mean reward: 252.56 - Last mean reward per episode: 252.18\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 371      |\n",
            "|    ep_rew_mean      | 252      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4376     |\n",
            "|    fps              | 567      |\n",
            "|    time_elapsed     | 4252     |\n",
            "|    total_timesteps  | 2411764  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0395   |\n",
            "|    n_updates        | 590440   |\n",
            "----------------------------------\n",
            "Num timesteps: 2412000\n",
            "Best mean reward: 252.56 - Last mean reward per episode: 252.23\n",
            "Num timesteps: 2413000\n",
            "Best mean reward: 252.56 - Last mean reward per episode: 252.83\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 374      |\n",
            "|    ep_rew_mean      | 252      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4380     |\n",
            "|    fps              | 567      |\n",
            "|    time_elapsed     | 4254     |\n",
            "|    total_timesteps  | 2413567  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.037    |\n",
            "|    n_updates        | 590891   |\n",
            "----------------------------------\n",
            "Num timesteps: 2414000\n",
            "Best mean reward: 252.83 - Last mean reward per episode: 251.89\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 372      |\n",
            "|    ep_rew_mean      | 252      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4384     |\n",
            "|    fps              | 567      |\n",
            "|    time_elapsed     | 4256     |\n",
            "|    total_timesteps  | 2414864  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.37     |\n",
            "|    n_updates        | 591215   |\n",
            "----------------------------------\n",
            "Num timesteps: 2415000\n",
            "Best mean reward: 252.83 - Last mean reward per episode: 251.64\n",
            "Num timesteps: 2416000\n",
            "Best mean reward: 252.83 - Last mean reward per episode: 252.00\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 371      |\n",
            "|    ep_rew_mean      | 252      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4388     |\n",
            "|    fps              | 567      |\n",
            "|    time_elapsed     | 4257     |\n",
            "|    total_timesteps  | 2416278  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.353    |\n",
            "|    n_updates        | 591569   |\n",
            "----------------------------------\n",
            "Num timesteps: 2417000\n",
            "Best mean reward: 252.83 - Last mean reward per episode: 252.67\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 371      |\n",
            "|    ep_rew_mean      | 253      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4392     |\n",
            "|    fps              | 567      |\n",
            "|    time_elapsed     | 4259     |\n",
            "|    total_timesteps  | 2417652  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.98     |\n",
            "|    n_updates        | 591912   |\n",
            "----------------------------------\n",
            "Num timesteps: 2418000\n",
            "Best mean reward: 252.83 - Last mean reward per episode: 252.94\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 2419000\n",
            "Best mean reward: 252.94 - Last mean reward per episode: 253.07\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 373      |\n",
            "|    ep_rew_mean      | 253      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4396     |\n",
            "|    fps              | 567      |\n",
            "|    time_elapsed     | 4261     |\n",
            "|    total_timesteps  | 2419380  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.18     |\n",
            "|    n_updates        | 592344   |\n",
            "----------------------------------\n",
            "Num timesteps: 2420000\n",
            "Best mean reward: 253.07 - Last mean reward per episode: 252.62\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 374      |\n",
            "|    ep_rew_mean      | 253      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4400     |\n",
            "|    fps              | 567      |\n",
            "|    time_elapsed     | 4263     |\n",
            "|    total_timesteps  | 2420838  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.536    |\n",
            "|    n_updates        | 592709   |\n",
            "----------------------------------\n",
            "Num timesteps: 2421000\n",
            "Best mean reward: 253.07 - Last mean reward per episode: 253.12\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 2422000\n",
            "Best mean reward: 253.12 - Last mean reward per episode: 252.52\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 374      |\n",
            "|    ep_rew_mean      | 253      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4404     |\n",
            "|    fps              | 567      |\n",
            "|    time_elapsed     | 4265     |\n",
            "|    total_timesteps  | 2422458  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.104    |\n",
            "|    n_updates        | 593114   |\n",
            "----------------------------------\n",
            "Num timesteps: 2423000\n",
            "Best mean reward: 253.12 - Last mean reward per episode: 252.45\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 374      |\n",
            "|    ep_rew_mean      | 252      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4408     |\n",
            "|    fps              | 567      |\n",
            "|    time_elapsed     | 4267     |\n",
            "|    total_timesteps  | 2423857  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.41     |\n",
            "|    n_updates        | 593464   |\n",
            "----------------------------------\n",
            "Num timesteps: 2424000\n",
            "Best mean reward: 253.12 - Last mean reward per episode: 251.73\n",
            "Num timesteps: 2425000\n",
            "Best mean reward: 253.12 - Last mean reward per episode: 252.54\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 376      |\n",
            "|    ep_rew_mean      | 252      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4412     |\n",
            "|    fps              | 568      |\n",
            "|    time_elapsed     | 4269     |\n",
            "|    total_timesteps  | 2425435  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.267    |\n",
            "|    n_updates        | 593858   |\n",
            "----------------------------------\n",
            "Num timesteps: 2426000\n",
            "Best mean reward: 253.12 - Last mean reward per episode: 252.31\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 376      |\n",
            "|    ep_rew_mean      | 252      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4416     |\n",
            "|    fps              | 568      |\n",
            "|    time_elapsed     | 4271     |\n",
            "|    total_timesteps  | 2426965  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.136    |\n",
            "|    n_updates        | 594241   |\n",
            "----------------------------------\n",
            "Num timesteps: 2427000\n",
            "Best mean reward: 253.12 - Last mean reward per episode: 252.12\n",
            "Num timesteps: 2428000\n",
            "Best mean reward: 253.12 - Last mean reward per episode: 252.39\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 376      |\n",
            "|    ep_rew_mean      | 253      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4420     |\n",
            "|    fps              | 568      |\n",
            "|    time_elapsed     | 4273     |\n",
            "|    total_timesteps  | 2428433  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.59     |\n",
            "|    n_updates        | 594608   |\n",
            "----------------------------------\n",
            "Num timesteps: 2429000\n",
            "Best mean reward: 253.12 - Last mean reward per episode: 252.13\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 374      |\n",
            "|    ep_rew_mean      | 253      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4424     |\n",
            "|    fps              | 568      |\n",
            "|    time_elapsed     | 4274     |\n",
            "|    total_timesteps  | 2429782  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.175    |\n",
            "|    n_updates        | 594945   |\n",
            "----------------------------------\n",
            "Num timesteps: 2430000\n",
            "Best mean reward: 253.12 - Last mean reward per episode: 253.00\n",
            "Num timesteps: 2431000\n",
            "Best mean reward: 253.12 - Last mean reward per episode: 253.80\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 369      |\n",
            "|    ep_rew_mean      | 254      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4428     |\n",
            "|    fps              | 568      |\n",
            "|    time_elapsed     | 4276     |\n",
            "|    total_timesteps  | 2431023  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.201    |\n",
            "|    n_updates        | 595255   |\n",
            "----------------------------------\n",
            "Num timesteps: 2432000\n",
            "Best mean reward: 253.80 - Last mean reward per episode: 253.82\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 371      |\n",
            "|    ep_rew_mean      | 253      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4432     |\n",
            "|    fps              | 568      |\n",
            "|    time_elapsed     | 4278     |\n",
            "|    total_timesteps  | 2432455  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.365    |\n",
            "|    n_updates        | 595613   |\n",
            "----------------------------------\n",
            "Num timesteps: 2433000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 252.80\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 367      |\n",
            "|    ep_rew_mean      | 252      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4436     |\n",
            "|    fps              | 568      |\n",
            "|    time_elapsed     | 4279     |\n",
            "|    total_timesteps  | 2433702  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.44     |\n",
            "|    n_updates        | 595925   |\n",
            "----------------------------------\n",
            "Num timesteps: 2434000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 252.39\n",
            "Num timesteps: 2435000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 252.45\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 363      |\n",
            "|    ep_rew_mean      | 253      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4440     |\n",
            "|    fps              | 568      |\n",
            "|    time_elapsed     | 4281     |\n",
            "|    total_timesteps  | 2435185  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.219    |\n",
            "|    n_updates        | 596296   |\n",
            "----------------------------------\n",
            "Num timesteps: 2436000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 252.76\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 362      |\n",
            "|    ep_rew_mean      | 253      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4444     |\n",
            "|    fps              | 568      |\n",
            "|    time_elapsed     | 4283     |\n",
            "|    total_timesteps  | 2436565  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.139    |\n",
            "|    n_updates        | 596641   |\n",
            "----------------------------------\n",
            "Num timesteps: 2437000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 252.77\n",
            "Num timesteps: 2438000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 252.92\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 363      |\n",
            "|    ep_rew_mean      | 253      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4448     |\n",
            "|    fps              | 568      |\n",
            "|    time_elapsed     | 4284     |\n",
            "|    total_timesteps  | 2438005  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0789   |\n",
            "|    n_updates        | 597001   |\n",
            "----------------------------------\n",
            "Num timesteps: 2439000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 252.72\n",
            "Num timesteps: 2440000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 250.61\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 369      |\n",
            "|    ep_rew_mean      | 251      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4452     |\n",
            "|    fps              | 569      |\n",
            "|    time_elapsed     | 4287     |\n",
            "|    total_timesteps  | 2440010  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0787   |\n",
            "|    n_updates        | 597502   |\n",
            "----------------------------------\n",
            "Num timesteps: 2441000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 250.42\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 369      |\n",
            "|    ep_rew_mean      | 250      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4456     |\n",
            "|    fps              | 569      |\n",
            "|    time_elapsed     | 4289     |\n",
            "|    total_timesteps  | 2441301  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0395   |\n",
            "|    n_updates        | 597825   |\n",
            "----------------------------------\n",
            "Num timesteps: 2442000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 250.03\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 367      |\n",
            "|    ep_rew_mean      | 250      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4460     |\n",
            "|    fps              | 569      |\n",
            "|    time_elapsed     | 4291     |\n",
            "|    total_timesteps  | 2442587  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.331    |\n",
            "|    n_updates        | 598146   |\n",
            "----------------------------------\n",
            "Num timesteps: 2443000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 249.71\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 366      |\n",
            "|    ep_rew_mean      | 250      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4464     |\n",
            "|    fps              | 569      |\n",
            "|    time_elapsed     | 4292     |\n",
            "|    total_timesteps  | 2443902  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.031    |\n",
            "|    n_updates        | 598475   |\n",
            "----------------------------------\n",
            "Num timesteps: 2444000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 249.68\n",
            "Num timesteps: 2445000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 249.11\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 373      |\n",
            "|    ep_rew_mean      | 248      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4468     |\n",
            "|    fps              | 569      |\n",
            "|    time_elapsed     | 4295     |\n",
            "|    total_timesteps  | 2445771  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.394    |\n",
            "|    n_updates        | 598942   |\n",
            "----------------------------------\n",
            "Num timesteps: 2446000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 247.58\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 366      |\n",
            "|    ep_rew_mean      | 246      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4472     |\n",
            "|    fps              | 569      |\n",
            "|    time_elapsed     | 4296     |\n",
            "|    total_timesteps  | 2446775  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.492    |\n",
            "|    n_updates        | 599193   |\n",
            "----------------------------------\n",
            "Num timesteps: 2447000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 246.45\n",
            "Num timesteps: 2448000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 246.39\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 365      |\n",
            "|    ep_rew_mean      | 246      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4476     |\n",
            "|    fps              | 569      |\n",
            "|    time_elapsed     | 4298     |\n",
            "|    total_timesteps  | 2448286  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0855   |\n",
            "|    n_updates        | 599571   |\n",
            "----------------------------------\n",
            "Num timesteps: 2449000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 245.52\n",
            "Num timesteps: 2450000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 245.03\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 366      |\n",
            "|    ep_rew_mean      | 245      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4480     |\n",
            "|    fps              | 569      |\n",
            "|    time_elapsed     | 4300     |\n",
            "|    total_timesteps  | 2450144  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.02     |\n",
            "|    n_updates        | 600035   |\n",
            "----------------------------------\n",
            "Num timesteps: 2451000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 245.75\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 367      |\n",
            "|    ep_rew_mean      | 246      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4484     |\n",
            "|    fps              | 569      |\n",
            "|    time_elapsed     | 4302     |\n",
            "|    total_timesteps  | 2451584  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.21     |\n",
            "|    n_updates        | 600395   |\n",
            "----------------------------------\n",
            "Num timesteps: 2452000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 246.04\n",
            "Num timesteps: 2453000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 245.31\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 374      |\n",
            "|    ep_rew_mean      | 245      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4488     |\n",
            "|    fps              | 569      |\n",
            "|    time_elapsed     | 4305     |\n",
            "|    total_timesteps  | 2453701  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.119    |\n",
            "|    n_updates        | 600925   |\n",
            "----------------------------------\n",
            "Num timesteps: 2454000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 245.14\n",
            "Num timesteps: 2455000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 243.79\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 375      |\n",
            "|    ep_rew_mean      | 244      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4492     |\n",
            "|    fps              | 570      |\n",
            "|    time_elapsed     | 4307     |\n",
            "|    total_timesteps  | 2455136  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.02     |\n",
            "|    n_updates        | 601283   |\n",
            "----------------------------------\n",
            "Num timesteps: 2456000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 243.73\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 371      |\n",
            "|    ep_rew_mean      | 244      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4496     |\n",
            "|    fps              | 570      |\n",
            "|    time_elapsed     | 4308     |\n",
            "|    total_timesteps  | 2456489  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.124    |\n",
            "|    n_updates        | 601622   |\n",
            "----------------------------------\n",
            "Num timesteps: 2457000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 244.09\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 371      |\n",
            "|    ep_rew_mean      | 242      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4500     |\n",
            "|    fps              | 570      |\n",
            "|    time_elapsed     | 4310     |\n",
            "|    total_timesteps  | 2457891  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0528   |\n",
            "|    n_updates        | 601972   |\n",
            "----------------------------------\n",
            "Num timesteps: 2458000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 242.38\n",
            "Num timesteps: 2459000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 242.80\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 368      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4504     |\n",
            "|    fps              | 570      |\n",
            "|    time_elapsed     | 4312     |\n",
            "|    total_timesteps  | 2459233  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0745   |\n",
            "|    n_updates        | 602308   |\n",
            "----------------------------------\n",
            "Num timesteps: 2460000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 243.00\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 367      |\n",
            "|    ep_rew_mean      | 244      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4508     |\n",
            "|    fps              | 570      |\n",
            "|    time_elapsed     | 4314     |\n",
            "|    total_timesteps  | 2460543  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0874   |\n",
            "|    n_updates        | 602635   |\n",
            "----------------------------------\n",
            "Num timesteps: 2461000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 243.82\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 364      |\n",
            "|    ep_rew_mean      | 242      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4512     |\n",
            "|    fps              | 570      |\n",
            "|    time_elapsed     | 4315     |\n",
            "|    total_timesteps  | 2461802  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.25     |\n",
            "|    n_updates        | 602950   |\n",
            "----------------------------------\n",
            "Num timesteps: 2462000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 242.38\n",
            "Num timesteps: 2463000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 242.83\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 362      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4516     |\n",
            "|    fps              | 570      |\n",
            "|    time_elapsed     | 4317     |\n",
            "|    total_timesteps  | 2463175  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.79     |\n",
            "|    n_updates        | 603293   |\n",
            "----------------------------------\n",
            "Num timesteps: 2464000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 243.16\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 362      |\n",
            "|    ep_rew_mean      | 242      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4520     |\n",
            "|    fps              | 570      |\n",
            "|    time_elapsed     | 4319     |\n",
            "|    total_timesteps  | 2464616  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.106    |\n",
            "|    n_updates        | 603653   |\n",
            "----------------------------------\n",
            "Num timesteps: 2465000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 242.43\n",
            "Num timesteps: 2466000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 242.07\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 368      |\n",
            "|    ep_rew_mean      | 242      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4524     |\n",
            "|    fps              | 570      |\n",
            "|    time_elapsed     | 4322     |\n",
            "|    total_timesteps  | 2466621  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.109    |\n",
            "|    n_updates        | 604155   |\n",
            "----------------------------------\n",
            "Num timesteps: 2467000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 241.67\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 367      |\n",
            "|    ep_rew_mean      | 239      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4528     |\n",
            "|    fps              | 570      |\n",
            "|    time_elapsed     | 4323     |\n",
            "|    total_timesteps  | 2467709  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.109    |\n",
            "|    n_updates        | 604427   |\n",
            "----------------------------------\n",
            "Num timesteps: 2468000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 240.11\n",
            "Num timesteps: 2469000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 239.78\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 366      |\n",
            "|    ep_rew_mean      | 240      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4532     |\n",
            "|    fps              | 570      |\n",
            "|    time_elapsed     | 4325     |\n",
            "|    total_timesteps  | 2469037  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0868   |\n",
            "|    n_updates        | 604759   |\n",
            "----------------------------------\n",
            "Num timesteps: 2470000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 240.70\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 369      |\n",
            "|    ep_rew_mean      | 241      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4536     |\n",
            "|    fps              | 570      |\n",
            "|    time_elapsed     | 4326     |\n",
            "|    total_timesteps  | 2470596  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.101    |\n",
            "|    n_updates        | 605148   |\n",
            "----------------------------------\n",
            "Num timesteps: 2471000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 239.19\n",
            "Num timesteps: 2472000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 239.46\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 370      |\n",
            "|    ep_rew_mean      | 239      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4540     |\n",
            "|    fps              | 571      |\n",
            "|    time_elapsed     | 4329     |\n",
            "|    total_timesteps  | 2472162  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.101    |\n",
            "|    n_updates        | 605540   |\n",
            "----------------------------------\n",
            "Num timesteps: 2473000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 239.06\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 374      |\n",
            "|    ep_rew_mean      | 239      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4544     |\n",
            "|    fps              | 571      |\n",
            "|    time_elapsed     | 4331     |\n",
            "|    total_timesteps  | 2473978  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0595   |\n",
            "|    n_updates        | 605994   |\n",
            "----------------------------------\n",
            "Num timesteps: 2474000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 238.98\n",
            "Num timesteps: 2475000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 238.72\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 374      |\n",
            "|    ep_rew_mean      | 238      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4548     |\n",
            "|    fps              | 571      |\n",
            "|    time_elapsed     | 4333     |\n",
            "|    total_timesteps  | 2475441  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0721   |\n",
            "|    n_updates        | 606360   |\n",
            "----------------------------------\n",
            "Num timesteps: 2476000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 239.96\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 368      |\n",
            "|    ep_rew_mean      | 240      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4552     |\n",
            "|    fps              | 571      |\n",
            "|    time_elapsed     | 4335     |\n",
            "|    total_timesteps  | 2476829  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.488    |\n",
            "|    n_updates        | 606707   |\n",
            "----------------------------------\n",
            "Num timesteps: 2477000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 240.10\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 367      |\n",
            "|    ep_rew_mean      | 240      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4556     |\n",
            "|    fps              | 571      |\n",
            "|    time_elapsed     | 4336     |\n",
            "|    total_timesteps  | 2477992  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0641   |\n",
            "|    n_updates        | 606997   |\n",
            "----------------------------------\n",
            "Num timesteps: 2478000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 240.22\n",
            "Num timesteps: 2479000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 241.09\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 369      |\n",
            "|    ep_rew_mean      | 241      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4560     |\n",
            "|    fps              | 571      |\n",
            "|    time_elapsed     | 4338     |\n",
            "|    total_timesteps  | 2479445  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.14     |\n",
            "|    n_updates        | 607361   |\n",
            "----------------------------------\n",
            "Num timesteps: 2480000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 241.11\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 370      |\n",
            "|    ep_rew_mean      | 242      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4564     |\n",
            "|    fps              | 571      |\n",
            "|    time_elapsed     | 4340     |\n",
            "|    total_timesteps  | 2480931  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0516   |\n",
            "|    n_updates        | 607732   |\n",
            "----------------------------------\n",
            "Num timesteps: 2481000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 241.54\n",
            "Num timesteps: 2482000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 241.39\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 367      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4568     |\n",
            "|    fps              | 571      |\n",
            "|    time_elapsed     | 4342     |\n",
            "|    total_timesteps  | 2482446  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.695    |\n",
            "|    n_updates        | 608111   |\n",
            "----------------------------------\n",
            "Num timesteps: 2483000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 242.95\n",
            "Num timesteps: 2484000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 240.35\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 375      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4572     |\n",
            "|    fps              | 571      |\n",
            "|    time_elapsed     | 4344     |\n",
            "|    total_timesteps  | 2484280  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.71     |\n",
            "|    n_updates        | 608569   |\n",
            "----------------------------------\n",
            "Num timesteps: 2485000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 242.54\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 375      |\n",
            "|    ep_rew_mean      | 242      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4576     |\n",
            "|    fps              | 571      |\n",
            "|    time_elapsed     | 4346     |\n",
            "|    total_timesteps  | 2485807  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0738   |\n",
            "|    n_updates        | 608951   |\n",
            "----------------------------------\n",
            "Num timesteps: 2486000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 242.36\n",
            "Num timesteps: 2487000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 243.34\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 371      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4580     |\n",
            "|    fps              | 571      |\n",
            "|    time_elapsed     | 4348     |\n",
            "|    total_timesteps  | 2487275  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0308   |\n",
            "|    n_updates        | 609318   |\n",
            "----------------------------------\n",
            "Num timesteps: 2488000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 242.71\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 371      |\n",
            "|    ep_rew_mean      | 242      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4584     |\n",
            "|    fps              | 572      |\n",
            "|    time_elapsed     | 4350     |\n",
            "|    total_timesteps  | 2488703  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.74     |\n",
            "|    n_updates        | 609675   |\n",
            "----------------------------------\n",
            "Num timesteps: 2489000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 243.20\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 363      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4588     |\n",
            "|    fps              | 572      |\n",
            "|    time_elapsed     | 4351     |\n",
            "|    total_timesteps  | 2489991  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.33     |\n",
            "|    n_updates        | 609997   |\n",
            "----------------------------------\n",
            "Num timesteps: 2490000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 243.26\n",
            "Num timesteps: 2491000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 241.61\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 366      |\n",
            "|    ep_rew_mean      | 240      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4592     |\n",
            "|    fps              | 572      |\n",
            "|    time_elapsed     | 4354     |\n",
            "|    total_timesteps  | 2491740  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0698   |\n",
            "|    n_updates        | 610434   |\n",
            "----------------------------------\n",
            "Num timesteps: 2492000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 240.00\n",
            "Num timesteps: 2493000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 240.98\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 365      |\n",
            "|    ep_rew_mean      | 241      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4596     |\n",
            "|    fps              | 572      |\n",
            "|    time_elapsed     | 4355     |\n",
            "|    total_timesteps  | 2493013  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.135    |\n",
            "|    n_updates        | 610753   |\n",
            "----------------------------------\n",
            "Num timesteps: 2494000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 240.08\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 367      |\n",
            "|    ep_rew_mean      | 242      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4600     |\n",
            "|    fps              | 572      |\n",
            "|    time_elapsed     | 4357     |\n",
            "|    total_timesteps  | 2494561  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0568   |\n",
            "|    n_updates        | 611140   |\n",
            "----------------------------------\n",
            "Num timesteps: 2495000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 242.57\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 367      |\n",
            "|    ep_rew_mean      | 242      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4604     |\n",
            "|    fps              | 572      |\n",
            "|    time_elapsed     | 4359     |\n",
            "|    total_timesteps  | 2495910  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.158    |\n",
            "|    n_updates        | 611477   |\n",
            "----------------------------------\n",
            "Num timesteps: 2496000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 241.58\n",
            "Num timesteps: 2497000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 241.52\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 366      |\n",
            "|    ep_rew_mean      | 241      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4608     |\n",
            "|    fps              | 572      |\n",
            "|    time_elapsed     | 4360     |\n",
            "|    total_timesteps  | 2497142  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.569    |\n",
            "|    n_updates        | 611785   |\n",
            "----------------------------------\n",
            "Num timesteps: 2498000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 241.46\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 368      |\n",
            "|    ep_rew_mean      | 242      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4612     |\n",
            "|    fps              | 572      |\n",
            "|    time_elapsed     | 4362     |\n",
            "|    total_timesteps  | 2498636  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.76     |\n",
            "|    n_updates        | 612158   |\n",
            "----------------------------------\n",
            "Num timesteps: 2499000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 242.24\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 367      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4616     |\n",
            "|    fps              | 572      |\n",
            "|    time_elapsed     | 4364     |\n",
            "|    total_timesteps  | 2499864  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.211    |\n",
            "|    n_updates        | 612465   |\n",
            "----------------------------------\n",
            "Num timesteps: 2500000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 242.65\n",
            "Num timesteps: 2501000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 242.95\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 366      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4620     |\n",
            "|    fps              | 572      |\n",
            "|    time_elapsed     | 4365     |\n",
            "|    total_timesteps  | 2501170  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0778   |\n",
            "|    n_updates        | 612792   |\n",
            "----------------------------------\n",
            "Num timesteps: 2502000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 243.02\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 360      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4624     |\n",
            "|    fps              | 573      |\n",
            "|    time_elapsed     | 4367     |\n",
            "|    total_timesteps  | 2502587  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0727   |\n",
            "|    n_updates        | 613146   |\n",
            "----------------------------------\n",
            "Num timesteps: 2503000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 243.29\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 362      |\n",
            "|    ep_rew_mean      | 246      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4628     |\n",
            "|    fps              | 573      |\n",
            "|    time_elapsed     | 4369     |\n",
            "|    total_timesteps  | 2503860  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.268    |\n",
            "|    n_updates        | 613464   |\n",
            "----------------------------------\n",
            "Num timesteps: 2504000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 246.07\n",
            "Num timesteps: 2505000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 244.67\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 365      |\n",
            "|    ep_rew_mean      | 245      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4632     |\n",
            "|    fps              | 573      |\n",
            "|    time_elapsed     | 4371     |\n",
            "|    total_timesteps  | 2505545  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0716   |\n",
            "|    n_updates        | 613886   |\n",
            "----------------------------------\n",
            "Num timesteps: 2506000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 244.04\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 363      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4636     |\n",
            "|    fps              | 573      |\n",
            "|    time_elapsed     | 4372     |\n",
            "|    total_timesteps  | 2506884  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0806   |\n",
            "|    n_updates        | 614220   |\n",
            "----------------------------------\n",
            "Num timesteps: 2507000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 243.32\n",
            "Num timesteps: 2508000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 245.66\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 361      |\n",
            "|    ep_rew_mean      | 246      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4640     |\n",
            "|    fps              | 573      |\n",
            "|    time_elapsed     | 4374     |\n",
            "|    total_timesteps  | 2508266  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.74     |\n",
            "|    n_updates        | 614566   |\n",
            "----------------------------------\n",
            "Num timesteps: 2509000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 245.86\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 357      |\n",
            "|    ep_rew_mean      | 247      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4644     |\n",
            "|    fps              | 573      |\n",
            "|    time_elapsed     | 4376     |\n",
            "|    total_timesteps  | 2509713  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.148    |\n",
            "|    n_updates        | 614928   |\n",
            "----------------------------------\n",
            "Num timesteps: 2510000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 246.57\n",
            "Num timesteps: 2511000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 247.05\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 358      |\n",
            "|    ep_rew_mean      | 248      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4648     |\n",
            "|    fps              | 573      |\n",
            "|    time_elapsed     | 4378     |\n",
            "|    total_timesteps  | 2511264  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.132    |\n",
            "|    n_updates        | 615315   |\n",
            "----------------------------------\n",
            "Num timesteps: 2512000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 247.65\n",
            "Num timesteps: 2513000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 247.32\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 369      |\n",
            "|    ep_rew_mean      | 246      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4652     |\n",
            "|    fps              | 573      |\n",
            "|    time_elapsed     | 4382     |\n",
            "|    total_timesteps  | 2513756  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0663   |\n",
            "|    n_updates        | 615938   |\n",
            "----------------------------------\n",
            "Num timesteps: 2514000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 246.49\n",
            "Num timesteps: 2515000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 246.91\n",
            "Num timesteps: 2516000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 244.94\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 383      |\n",
            "|    ep_rew_mean      | 245      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4656     |\n",
            "|    fps              | 573      |\n",
            "|    time_elapsed     | 4385     |\n",
            "|    total_timesteps  | 2516250  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.182    |\n",
            "|    n_updates        | 616562   |\n",
            "----------------------------------\n",
            "Num timesteps: 2517000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 244.06\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 383      |\n",
            "|    ep_rew_mean      | 244      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4660     |\n",
            "|    fps              | 573      |\n",
            "|    time_elapsed     | 4387     |\n",
            "|    total_timesteps  | 2517729  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0961   |\n",
            "|    n_updates        | 616932   |\n",
            "----------------------------------\n",
            "Num timesteps: 2518000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 243.57\n",
            "Num timesteps: 2519000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 243.13\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 385      |\n",
            "|    ep_rew_mean      | 244      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4664     |\n",
            "|    fps              | 573      |\n",
            "|    time_elapsed     | 4390     |\n",
            "|    total_timesteps  | 2519458  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0782   |\n",
            "|    n_updates        | 617364   |\n",
            "----------------------------------\n",
            "Num timesteps: 2520000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 243.78\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 381      |\n",
            "|    ep_rew_mean      | 244      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4668     |\n",
            "|    fps              | 573      |\n",
            "|    time_elapsed     | 4391     |\n",
            "|    total_timesteps  | 2520585  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0836   |\n",
            "|    n_updates        | 617646   |\n",
            "----------------------------------\n",
            "Num timesteps: 2521000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 244.62\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 376      |\n",
            "|    ep_rew_mean      | 247      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4672     |\n",
            "|    fps              | 574      |\n",
            "|    time_elapsed     | 4393     |\n",
            "|    total_timesteps  | 2521907  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.59     |\n",
            "|    n_updates        | 617976   |\n",
            "----------------------------------\n",
            "Num timesteps: 2522000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 246.78\n",
            "Num timesteps: 2523000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 246.57\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 375      |\n",
            "|    ep_rew_mean      | 247      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4676     |\n",
            "|    fps              | 574      |\n",
            "|    time_elapsed     | 4394     |\n",
            "|    total_timesteps  | 2523307  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0774   |\n",
            "|    n_updates        | 618326   |\n",
            "----------------------------------\n",
            "Num timesteps: 2524000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 247.47\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 376      |\n",
            "|    ep_rew_mean      | 248      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4680     |\n",
            "|    fps              | 574      |\n",
            "|    time_elapsed     | 4396     |\n",
            "|    total_timesteps  | 2524873  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0652   |\n",
            "|    n_updates        | 618718   |\n",
            "----------------------------------\n",
            "Num timesteps: 2525000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 247.79\n",
            "Num timesteps: 2526000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 248.47\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 376      |\n",
            "|    ep_rew_mean      | 248      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4684     |\n",
            "|    fps              | 574      |\n",
            "|    time_elapsed     | 4398     |\n",
            "|    total_timesteps  | 2526297  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.731    |\n",
            "|    n_updates        | 619074   |\n",
            "----------------------------------\n",
            "Num timesteps: 2527000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 247.14\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 375      |\n",
            "|    ep_rew_mean      | 248      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4688     |\n",
            "|    fps              | 574      |\n",
            "|    time_elapsed     | 4399     |\n",
            "|    total_timesteps  | 2527516  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0666   |\n",
            "|    n_updates        | 619378   |\n",
            "----------------------------------\n",
            "Num timesteps: 2528000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 247.53\n",
            "Num timesteps: 2529000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 251.00\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 374      |\n",
            "|    ep_rew_mean      | 251      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4692     |\n",
            "|    fps              | 574      |\n",
            "|    time_elapsed     | 4402     |\n",
            "|    total_timesteps  | 2529108  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.574    |\n",
            "|    n_updates        | 619776   |\n",
            "----------------------------------\n",
            "Num timesteps: 2530000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 249.59\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 372      |\n",
            "|    ep_rew_mean      | 250      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4696     |\n",
            "|    fps              | 574      |\n",
            "|    time_elapsed     | 4403     |\n",
            "|    total_timesteps  | 2530232  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0844   |\n",
            "|    n_updates        | 620057   |\n",
            "----------------------------------\n",
            "Num timesteps: 2531000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 250.61\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 370      |\n",
            "|    ep_rew_mean      | 250      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4700     |\n",
            "|    fps              | 574      |\n",
            "|    time_elapsed     | 4404     |\n",
            "|    total_timesteps  | 2531594  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.59     |\n",
            "|    n_updates        | 620398   |\n",
            "----------------------------------\n",
            "Num timesteps: 2532000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 250.24\n",
            "Num timesteps: 2533000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 250.35\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 374      |\n",
            "|    ep_rew_mean      | 251      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4704     |\n",
            "|    fps              | 574      |\n",
            "|    time_elapsed     | 4407     |\n",
            "|    total_timesteps  | 2533301  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.108    |\n",
            "|    n_updates        | 620825   |\n",
            "----------------------------------\n",
            "Num timesteps: 2534000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 251.83\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 376      |\n",
            "|    ep_rew_mean      | 251      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4708     |\n",
            "|    fps              | 574      |\n",
            "|    time_elapsed     | 4409     |\n",
            "|    total_timesteps  | 2534721  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0642   |\n",
            "|    n_updates        | 621180   |\n",
            "----------------------------------\n",
            "Num timesteps: 2535000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 251.28\n",
            "Num timesteps: 2536000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 251.06\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 375      |\n",
            "|    ep_rew_mean      | 251      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4712     |\n",
            "|    fps              | 574      |\n",
            "|    time_elapsed     | 4410     |\n",
            "|    total_timesteps  | 2536135  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.955    |\n",
            "|    n_updates        | 621533   |\n",
            "----------------------------------\n",
            "Num timesteps: 2537000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 250.65\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 378      |\n",
            "|    ep_rew_mean      | 249      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4716     |\n",
            "|    fps              | 575      |\n",
            "|    time_elapsed     | 4412     |\n",
            "|    total_timesteps  | 2537685  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0571   |\n",
            "|    n_updates        | 621921   |\n",
            "----------------------------------\n",
            "Num timesteps: 2538000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 248.96\n",
            "Num timesteps: 2539000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 248.20\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 384      |\n",
            "|    ep_rew_mean      | 245      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4720     |\n",
            "|    fps              | 575      |\n",
            "|    time_elapsed     | 4415     |\n",
            "|    total_timesteps  | 2539520  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.232    |\n",
            "|    n_updates        | 622379   |\n",
            "----------------------------------\n",
            "Num timesteps: 2540000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 245.83\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 381      |\n",
            "|    ep_rew_mean      | 246      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4724     |\n",
            "|    fps              | 575      |\n",
            "|    time_elapsed     | 4416     |\n",
            "|    total_timesteps  | 2540721  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.423    |\n",
            "|    n_updates        | 622680   |\n",
            "----------------------------------\n",
            "Num timesteps: 2541000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 246.13\n",
            "Num timesteps: 2542000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 245.87\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 383      |\n",
            "|    ep_rew_mean      | 245      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4728     |\n",
            "|    fps              | 575      |\n",
            "|    time_elapsed     | 4418     |\n",
            "|    total_timesteps  | 2542147  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0639   |\n",
            "|    n_updates        | 623036   |\n",
            "----------------------------------\n",
            "Num timesteps: 2543000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 246.30\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 380      |\n",
            "|    ep_rew_mean      | 247      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4732     |\n",
            "|    fps              | 575      |\n",
            "|    time_elapsed     | 4420     |\n",
            "|    total_timesteps  | 2543498  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0805   |\n",
            "|    n_updates        | 623374   |\n",
            "----------------------------------\n",
            "Num timesteps: 2544000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 246.60\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 380      |\n",
            "|    ep_rew_mean      | 247      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4736     |\n",
            "|    fps              | 575      |\n",
            "|    time_elapsed     | 4421     |\n",
            "|    total_timesteps  | 2544878  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0669   |\n",
            "|    n_updates        | 623719   |\n",
            "----------------------------------\n",
            "Num timesteps: 2545000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 247.20\n",
            "Num timesteps: 2546000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 247.52\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 381      |\n",
            "|    ep_rew_mean      | 248      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4740     |\n",
            "|    fps              | 575      |\n",
            "|    time_elapsed     | 4423     |\n",
            "|    total_timesteps  | 2546347  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.041    |\n",
            "|    n_updates        | 624086   |\n",
            "----------------------------------\n",
            "Num timesteps: 2547000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 248.08\n",
            "Num timesteps: 2548000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 247.17\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 385      |\n",
            "|    ep_rew_mean      | 247      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4744     |\n",
            "|    fps              | 575      |\n",
            "|    time_elapsed     | 4426     |\n",
            "|    total_timesteps  | 2548189  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0788   |\n",
            "|    n_updates        | 624547   |\n",
            "----------------------------------\n",
            "Num timesteps: 2549000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 247.14\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 380      |\n",
            "|    ep_rew_mean      | 247      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4748     |\n",
            "|    fps              | 575      |\n",
            "|    time_elapsed     | 4427     |\n",
            "|    total_timesteps  | 2549252  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0744   |\n",
            "|    n_updates        | 624812   |\n",
            "----------------------------------\n",
            "Num timesteps: 2550000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 246.66\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 368      |\n",
            "|    ep_rew_mean      | 248      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4752     |\n",
            "|    fps              | 575      |\n",
            "|    time_elapsed     | 4429     |\n",
            "|    total_timesteps  | 2550548  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.369    |\n",
            "|    n_updates        | 625136   |\n",
            "----------------------------------\n",
            "Num timesteps: 2551000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 248.02\n",
            "Num timesteps: 2552000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 248.87\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 359      |\n",
            "|    ep_rew_mean      | 250      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4756     |\n",
            "|    fps              | 575      |\n",
            "|    time_elapsed     | 4431     |\n",
            "|    total_timesteps  | 2552102  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.13     |\n",
            "|    n_updates        | 625525   |\n",
            "----------------------------------\n",
            "Num timesteps: 2553000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 249.31\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 360      |\n",
            "|    ep_rew_mean      | 248      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4760     |\n",
            "|    fps              | 576      |\n",
            "|    time_elapsed     | 4433     |\n",
            "|    total_timesteps  | 2553718  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0589   |\n",
            "|    n_updates        | 625929   |\n",
            "----------------------------------\n",
            "Num timesteps: 2554000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 248.43\n",
            "Num timesteps: 2555000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 245.45\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 360      |\n",
            "|    ep_rew_mean      | 245      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4764     |\n",
            "|    fps              | 576      |\n",
            "|    time_elapsed     | 4435     |\n",
            "|    total_timesteps  | 2555432  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.128    |\n",
            "|    n_updates        | 626357   |\n",
            "----------------------------------\n",
            "Num timesteps: 2556000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 245.03\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 362      |\n",
            "|    ep_rew_mean      | 246      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4768     |\n",
            "|    fps              | 576      |\n",
            "|    time_elapsed     | 4437     |\n",
            "|    total_timesteps  | 2556740  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.13     |\n",
            "|    n_updates        | 626684   |\n",
            "----------------------------------\n",
            "Num timesteps: 2557000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 245.61\n",
            "Num timesteps: 2558000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 245.09\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 365      |\n",
            "|    ep_rew_mean      | 245      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4772     |\n",
            "|    fps              | 576      |\n",
            "|    time_elapsed     | 4439     |\n",
            "|    total_timesteps  | 2558381  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0387   |\n",
            "|    n_updates        | 627095   |\n",
            "----------------------------------\n",
            "Num timesteps: 2559000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 244.87\n",
            "Num timesteps: 2560000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 244.74\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 368      |\n",
            "|    ep_rew_mean      | 245      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4776     |\n",
            "|    fps              | 576      |\n",
            "|    time_elapsed     | 4441     |\n",
            "|    total_timesteps  | 2560131  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0751   |\n",
            "|    n_updates        | 627532   |\n",
            "----------------------------------\n",
            "Num timesteps: 2561000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 245.02\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 367      |\n",
            "|    ep_rew_mean      | 244      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4780     |\n",
            "|    fps              | 576      |\n",
            "|    time_elapsed     | 4443     |\n",
            "|    total_timesteps  | 2561556  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0478   |\n",
            "|    n_updates        | 627888   |\n",
            "----------------------------------\n",
            "Num timesteps: 2562000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 244.00\n",
            "Num timesteps: 2563000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 243.45\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 371      |\n",
            "|    ep_rew_mean      | 244      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4784     |\n",
            "|    fps              | 576      |\n",
            "|    time_elapsed     | 4446     |\n",
            "|    total_timesteps  | 2563359  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.981    |\n",
            "|    n_updates        | 628339   |\n",
            "----------------------------------\n",
            "Num timesteps: 2564000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 244.29\n",
            "Num timesteps: 2565000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 242.84\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 377      |\n",
            "|    ep_rew_mean      | 242      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4788     |\n",
            "|    fps              | 576      |\n",
            "|    time_elapsed     | 4448     |\n",
            "|    total_timesteps  | 2565256  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.33     |\n",
            "|    n_updates        | 628813   |\n",
            "----------------------------------\n",
            "Num timesteps: 2566000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 242.05\n",
            "Num timesteps: 2567000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 240.65\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 379      |\n",
            "|    ep_rew_mean      | 241      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4792     |\n",
            "|    fps              | 576      |\n",
            "|    time_elapsed     | 4451     |\n",
            "|    total_timesteps  | 2567023  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.643    |\n",
            "|    n_updates        | 629255   |\n",
            "----------------------------------\n",
            "Num timesteps: 2568000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 242.26\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 387      |\n",
            "|    ep_rew_mean      | 242      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4796     |\n",
            "|    fps              | 576      |\n",
            "|    time_elapsed     | 4453     |\n",
            "|    total_timesteps  | 2568898  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0872   |\n",
            "|    n_updates        | 629724   |\n",
            "----------------------------------\n",
            "Num timesteps: 2569000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 242.10\n",
            "Num timesteps: 2570000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 242.64\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 386      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4800     |\n",
            "|    fps              | 576      |\n",
            "|    time_elapsed     | 4455     |\n",
            "|    total_timesteps  | 2570171  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0781   |\n",
            "|    n_updates        | 630042   |\n",
            "----------------------------------\n",
            "Num timesteps: 2571000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 242.13\n",
            "Num timesteps: 2572000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 241.90\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 387      |\n",
            "|    ep_rew_mean      | 242      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4804     |\n",
            "|    fps              | 576      |\n",
            "|    time_elapsed     | 4457     |\n",
            "|    total_timesteps  | 2572002  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.342    |\n",
            "|    n_updates        | 630500   |\n",
            "----------------------------------\n",
            "Num timesteps: 2573000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 240.96\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 391      |\n",
            "|    ep_rew_mean      | 241      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4808     |\n",
            "|    fps              | 577      |\n",
            "|    time_elapsed     | 4460     |\n",
            "|    total_timesteps  | 2573867  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0701   |\n",
            "|    n_updates        | 630966   |\n",
            "----------------------------------\n",
            "Num timesteps: 2574000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 240.76\n",
            "Num timesteps: 2575000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 240.96\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 397      |\n",
            "|    ep_rew_mean      | 240      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4812     |\n",
            "|    fps              | 577      |\n",
            "|    time_elapsed     | 4463     |\n",
            "|    total_timesteps  | 2575799  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.14     |\n",
            "|    n_updates        | 631449   |\n",
            "----------------------------------\n",
            "Num timesteps: 2576000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 239.81\n",
            "Num timesteps: 2577000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 240.60\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 399      |\n",
            "|    ep_rew_mean      | 240      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4816     |\n",
            "|    fps              | 577      |\n",
            "|    time_elapsed     | 4465     |\n",
            "|    total_timesteps  | 2577539  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.196    |\n",
            "|    n_updates        | 631884   |\n",
            "----------------------------------\n",
            "Num timesteps: 2578000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 240.80\n",
            "Num timesteps: 2579000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 240.63\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 398      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4820     |\n",
            "|    fps              | 577      |\n",
            "|    time_elapsed     | 4468     |\n",
            "|    total_timesteps  | 2579319  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.79     |\n",
            "|    n_updates        | 632329   |\n",
            "----------------------------------\n",
            "Num timesteps: 2580000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 243.89\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 399      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4824     |\n",
            "|    fps              | 577      |\n",
            "|    time_elapsed     | 4469     |\n",
            "|    total_timesteps  | 2580605  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0446   |\n",
            "|    n_updates        | 632651   |\n",
            "----------------------------------\n",
            "Num timesteps: 2581000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 243.04\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 397      |\n",
            "|    ep_rew_mean      | 244      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4828     |\n",
            "|    fps              | 577      |\n",
            "|    time_elapsed     | 4471     |\n",
            "|    total_timesteps  | 2581896  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.02     |\n",
            "|    n_updates        | 632973   |\n",
            "----------------------------------\n",
            "Num timesteps: 2582000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 243.89\n",
            "Num timesteps: 2583000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 244.23\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 395      |\n",
            "|    ep_rew_mean      | 244      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4832     |\n",
            "|    fps              | 577      |\n",
            "|    time_elapsed     | 4472     |\n",
            "|    total_timesteps  | 2583039  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.57     |\n",
            "|    n_updates        | 633259   |\n",
            "----------------------------------\n",
            "Num timesteps: 2584000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 245.09\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 397      |\n",
            "|    ep_rew_mean      | 245      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4836     |\n",
            "|    fps              | 577      |\n",
            "|    time_elapsed     | 4474     |\n",
            "|    total_timesteps  | 2584590  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0676   |\n",
            "|    n_updates        | 633647   |\n",
            "----------------------------------\n",
            "Num timesteps: 2585000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 243.00\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 395      |\n",
            "|    ep_rew_mean      | 242      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4840     |\n",
            "|    fps              | 577      |\n",
            "|    time_elapsed     | 4476     |\n",
            "|    total_timesteps  | 2585866  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.01     |\n",
            "|    n_updates        | 633966   |\n",
            "----------------------------------\n",
            "Num timesteps: 2586000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 242.21\n",
            "Num timesteps: 2587000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 241.91\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 392      |\n",
            "|    ep_rew_mean      | 241      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4844     |\n",
            "|    fps              | 577      |\n",
            "|    time_elapsed     | 4478     |\n",
            "|    total_timesteps  | 2587375  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0891   |\n",
            "|    n_updates        | 634343   |\n",
            "----------------------------------\n",
            "Num timesteps: 2588000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 240.65\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 392      |\n",
            "|    ep_rew_mean      | 241      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4848     |\n",
            "|    fps              | 577      |\n",
            "|    time_elapsed     | 4479     |\n",
            "|    total_timesteps  | 2588450  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.24     |\n",
            "|    n_updates        | 634612   |\n",
            "----------------------------------\n",
            "Num timesteps: 2589000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 240.81\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 394      |\n",
            "|    ep_rew_mean      | 240      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4852     |\n",
            "|    fps              | 577      |\n",
            "|    time_elapsed     | 4481     |\n",
            "|    total_timesteps  | 2589997  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.17     |\n",
            "|    n_updates        | 634999   |\n",
            "----------------------------------\n",
            "Num timesteps: 2590000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 240.25\n",
            "Num timesteps: 2591000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 240.04\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 397      |\n",
            "|    ep_rew_mean      | 240      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4856     |\n",
            "|    fps              | 578      |\n",
            "|    time_elapsed     | 4483     |\n",
            "|    total_timesteps  | 2591758  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.067    |\n",
            "|    n_updates        | 635439   |\n",
            "----------------------------------\n",
            "Num timesteps: 2592000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 240.14\n",
            "Num timesteps: 2593000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 240.41\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 396      |\n",
            "|    ep_rew_mean      | 241      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4860     |\n",
            "|    fps              | 578      |\n",
            "|    time_elapsed     | 4486     |\n",
            "|    total_timesteps  | 2593346  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.135    |\n",
            "|    n_updates        | 635836   |\n",
            "----------------------------------\n",
            "Num timesteps: 2594000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 241.79\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 395      |\n",
            "|    ep_rew_mean      | 244      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4864     |\n",
            "|    fps              | 578      |\n",
            "|    time_elapsed     | 4487     |\n",
            "|    total_timesteps  | 2594902  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.088    |\n",
            "|    n_updates        | 636225   |\n",
            "----------------------------------\n",
            "Num timesteps: 2595000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 244.25\n",
            "Num timesteps: 2596000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 243.67\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 396      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4868     |\n",
            "|    fps              | 578      |\n",
            "|    time_elapsed     | 4489     |\n",
            "|    total_timesteps  | 2596344  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.616    |\n",
            "|    n_updates        | 636585   |\n",
            "----------------------------------\n",
            "Num timesteps: 2597000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 243.61\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 395      |\n",
            "|    ep_rew_mean      | 245      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4872     |\n",
            "|    fps              | 578      |\n",
            "|    time_elapsed     | 4491     |\n",
            "|    total_timesteps  | 2597919  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0664   |\n",
            "|    n_updates        | 636979   |\n",
            "----------------------------------\n",
            "Num timesteps: 2598000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 244.68\n",
            "Num timesteps: 2599000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 244.53\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 394      |\n",
            "|    ep_rew_mean      | 245      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4876     |\n",
            "|    fps              | 578      |\n",
            "|    time_elapsed     | 4493     |\n",
            "|    total_timesteps  | 2599542  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.089    |\n",
            "|    n_updates        | 637385   |\n",
            "----------------------------------\n",
            "Num timesteps: 2600000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 244.69\n",
            "Num timesteps: 2601000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 244.24\n",
            "Num timesteps: 2602000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 244.55\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 406      |\n",
            "|    ep_rew_mean      | 245      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4880     |\n",
            "|    fps              | 578      |\n",
            "|    time_elapsed     | 4498     |\n",
            "|    total_timesteps  | 2602165  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0792   |\n",
            "|    n_updates        | 638041   |\n",
            "----------------------------------\n",
            "Num timesteps: 2603000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 245.28\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 400      |\n",
            "|    ep_rew_mean      | 246      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4884     |\n",
            "|    fps              | 578      |\n",
            "|    time_elapsed     | 4499     |\n",
            "|    total_timesteps  | 2603345  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0809   |\n",
            "|    n_updates        | 638336   |\n",
            "----------------------------------\n",
            "Num timesteps: 2604000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 246.05\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 395      |\n",
            "|    ep_rew_mean      | 246      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4888     |\n",
            "|    fps              | 578      |\n",
            "|    time_elapsed     | 4501     |\n",
            "|    total_timesteps  | 2604800  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.389    |\n",
            "|    n_updates        | 638699   |\n",
            "----------------------------------\n",
            "Num timesteps: 2605000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 246.22\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 389      |\n",
            "|    ep_rew_mean      | 248      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4892     |\n",
            "|    fps              | 578      |\n",
            "|    time_elapsed     | 4502     |\n",
            "|    total_timesteps  | 2605956  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0718   |\n",
            "|    n_updates        | 638988   |\n",
            "----------------------------------\n",
            "Num timesteps: 2606000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 247.96\n",
            "Num timesteps: 2607000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 248.27\n",
            "Num timesteps: 2608000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 247.88\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 398      |\n",
            "|    ep_rew_mean      | 247      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4896     |\n",
            "|    fps              | 578      |\n",
            "|    time_elapsed     | 4507     |\n",
            "|    total_timesteps  | 2608740  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0764   |\n",
            "|    n_updates        | 639684   |\n",
            "----------------------------------\n",
            "Num timesteps: 2609000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 246.89\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 397      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4900     |\n",
            "|    fps              | 578      |\n",
            "|    time_elapsed     | 4508     |\n",
            "|    total_timesteps  | 2609841  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.214    |\n",
            "|    n_updates        | 639960   |\n",
            "----------------------------------\n",
            "Num timesteps: 2610000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 242.95\n",
            "Num timesteps: 2611000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 243.95\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 394      |\n",
            "|    ep_rew_mean      | 243      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4904     |\n",
            "|    fps              | 578      |\n",
            "|    time_elapsed     | 4510     |\n",
            "|    total_timesteps  | 2611417  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0583   |\n",
            "|    n_updates        | 640354   |\n",
            "----------------------------------\n",
            "Num timesteps: 2612000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 243.43\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 389      |\n",
            "|    ep_rew_mean      | 244      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4908     |\n",
            "|    fps              | 579      |\n",
            "|    time_elapsed     | 4512     |\n",
            "|    total_timesteps  | 2612799  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0799   |\n",
            "|    n_updates        | 640699   |\n",
            "----------------------------------\n",
            "Num timesteps: 2613000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 244.11\n",
            "Num timesteps: 2614000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 245.22\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 385      |\n",
            "|    ep_rew_mean      | 245      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4912     |\n",
            "|    fps              | 579      |\n",
            "|    time_elapsed     | 4514     |\n",
            "|    total_timesteps  | 2614285  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.23     |\n",
            "|    n_updates        | 641071   |\n",
            "----------------------------------\n",
            "Num timesteps: 2615000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 245.46\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 383      |\n",
            "|    ep_rew_mean      | 246      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4916     |\n",
            "|    fps              | 579      |\n",
            "|    time_elapsed     | 4516     |\n",
            "|    total_timesteps  | 2615833  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.22     |\n",
            "|    n_updates        | 641458   |\n",
            "----------------------------------\n",
            "Num timesteps: 2616000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 245.98\n",
            "Num timesteps: 2617000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 246.19\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 377      |\n",
            "|    ep_rew_mean      | 247      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4920     |\n",
            "|    fps              | 579      |\n",
            "|    time_elapsed     | 4517     |\n",
            "|    total_timesteps  | 2617031  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0634   |\n",
            "|    n_updates        | 641757   |\n",
            "----------------------------------\n",
            "Num timesteps: 2618000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 246.01\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 380      |\n",
            "|    ep_rew_mean      | 246      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4924     |\n",
            "|    fps              | 579      |\n",
            "|    time_elapsed     | 4519     |\n",
            "|    total_timesteps  | 2618559  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.943    |\n",
            "|    n_updates        | 642139   |\n",
            "----------------------------------\n",
            "Num timesteps: 2619000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 246.05\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 380      |\n",
            "|    ep_rew_mean      | 245      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4928     |\n",
            "|    fps              | 579      |\n",
            "|    time_elapsed     | 4521     |\n",
            "|    total_timesteps  | 2619902  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0606   |\n",
            "|    n_updates        | 642475   |\n",
            "----------------------------------\n",
            "Num timesteps: 2620000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 245.26\n",
            "Num timesteps: 2621000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 244.70\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 382      |\n",
            "|    ep_rew_mean      | 245      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4932     |\n",
            "|    fps              | 579      |\n",
            "|    time_elapsed     | 4522     |\n",
            "|    total_timesteps  | 2621190  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0552   |\n",
            "|    n_updates        | 642797   |\n",
            "----------------------------------\n",
            "Num timesteps: 2622000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 244.20\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 379      |\n",
            "|    ep_rew_mean      | 244      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4936     |\n",
            "|    fps              | 579      |\n",
            "|    time_elapsed     | 4524     |\n",
            "|    total_timesteps  | 2622459  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0892   |\n",
            "|    n_updates        | 643114   |\n",
            "----------------------------------\n",
            "Num timesteps: 2623000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 243.42\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 381      |\n",
            "|    ep_rew_mean      | 246      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4940     |\n",
            "|    fps              | 579      |\n",
            "|    time_elapsed     | 4526     |\n",
            "|    total_timesteps  | 2623938  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.149    |\n",
            "|    n_updates        | 643484   |\n",
            "----------------------------------\n",
            "Num timesteps: 2624000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 245.65\n",
            "Num timesteps: 2625000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 245.44\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 382      |\n",
            "|    ep_rew_mean      | 245      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4944     |\n",
            "|    fps              | 579      |\n",
            "|    time_elapsed     | 4528     |\n",
            "|    total_timesteps  | 2625538  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0451   |\n",
            "|    n_updates        | 643884   |\n",
            "----------------------------------\n",
            "Num timesteps: 2626000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 244.98\n",
            "Num timesteps: 2627000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 245.28\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 388      |\n",
            "|    ep_rew_mean      | 245      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4948     |\n",
            "|    fps              | 579      |\n",
            "|    time_elapsed     | 4530     |\n",
            "|    total_timesteps  | 2627291  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.1      |\n",
            "|    n_updates        | 644322   |\n",
            "----------------------------------\n",
            "Num timesteps: 2628000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 244.95\n",
            "Num timesteps: 2629000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 244.99\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 390      |\n",
            "|    ep_rew_mean      | 245      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4952     |\n",
            "|    fps              | 579      |\n",
            "|    time_elapsed     | 4533     |\n",
            "|    total_timesteps  | 2629047  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.85     |\n",
            "|    n_updates        | 644761   |\n",
            "----------------------------------\n",
            "Num timesteps: 2630000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 245.90\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 386      |\n",
            "|    ep_rew_mean      | 246      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4956     |\n",
            "|    fps              | 580      |\n",
            "|    time_elapsed     | 4534     |\n",
            "|    total_timesteps  | 2630391  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0783   |\n",
            "|    n_updates        | 645097   |\n",
            "----------------------------------\n",
            "Num timesteps: 2631000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 245.07\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 384      |\n",
            "|    ep_rew_mean      | 246      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4960     |\n",
            "|    fps              | 580      |\n",
            "|    time_elapsed     | 4536     |\n",
            "|    total_timesteps  | 2631723  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.131    |\n",
            "|    n_updates        | 645430   |\n",
            "----------------------------------\n",
            "Num timesteps: 2632000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 245.83\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 379      |\n",
            "|    ep_rew_mean      | 246      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4964     |\n",
            "|    fps              | 580      |\n",
            "|    time_elapsed     | 4537     |\n",
            "|    total_timesteps  | 2632816  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.191    |\n",
            "|    n_updates        | 645703   |\n",
            "----------------------------------\n",
            "Num timesteps: 2633000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 246.35\n",
            "Num timesteps: 2634000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 246.17\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 380      |\n",
            "|    ep_rew_mean      | 246      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4968     |\n",
            "|    fps              | 580      |\n",
            "|    time_elapsed     | 4540     |\n",
            "|    total_timesteps  | 2634391  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0555   |\n",
            "|    n_updates        | 646097   |\n",
            "----------------------------------\n",
            "Num timesteps: 2635000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 245.54\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 380      |\n",
            "|    ep_rew_mean      | 246      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4972     |\n",
            "|    fps              | 580      |\n",
            "|    time_elapsed     | 4542     |\n",
            "|    total_timesteps  | 2635968  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.16     |\n",
            "|    n_updates        | 646491   |\n",
            "----------------------------------\n",
            "Num timesteps: 2636000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 245.66\n",
            "Num timesteps: 2637000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 246.83\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 377      |\n",
            "|    ep_rew_mean      | 247      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4976     |\n",
            "|    fps              | 580      |\n",
            "|    time_elapsed     | 4543     |\n",
            "|    total_timesteps  | 2637281  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0762   |\n",
            "|    n_updates        | 646820   |\n",
            "----------------------------------\n",
            "Num timesteps: 2638000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 247.87\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 365      |\n",
            "|    ep_rew_mean      | 248      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4980     |\n",
            "|    fps              | 580      |\n",
            "|    time_elapsed     | 4545     |\n",
            "|    total_timesteps  | 2638704  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.285    |\n",
            "|    n_updates        | 647175   |\n",
            "----------------------------------\n",
            "Num timesteps: 2639000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 247.87\n",
            "Num timesteps: 2640000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 247.17\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 370      |\n",
            "|    ep_rew_mean      | 246      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4984     |\n",
            "|    fps              | 580      |\n",
            "|    time_elapsed     | 4547     |\n",
            "|    total_timesteps  | 2640353  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.13     |\n",
            "|    n_updates        | 647588   |\n",
            "----------------------------------\n",
            "Num timesteps: 2641000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 246.51\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 371      |\n",
            "|    ep_rew_mean      | 247      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4988     |\n",
            "|    fps              | 580      |\n",
            "|    time_elapsed     | 4549     |\n",
            "|    total_timesteps  | 2641903  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.117    |\n",
            "|    n_updates        | 647975   |\n",
            "----------------------------------\n",
            "Num timesteps: 2642000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 247.00\n",
            "Num timesteps: 2643000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 247.37\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 373      |\n",
            "|    ep_rew_mean      | 247      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4992     |\n",
            "|    fps              | 580      |\n",
            "|    time_elapsed     | 4551     |\n",
            "|    total_timesteps  | 2643292  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.094    |\n",
            "|    n_updates        | 648322   |\n",
            "----------------------------------\n",
            "Num timesteps: 2644000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 247.20\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 360      |\n",
            "|    ep_rew_mean      | 248      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4996     |\n",
            "|    fps              | 580      |\n",
            "|    time_elapsed     | 4553     |\n",
            "|    total_timesteps  | 2644765  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0698   |\n",
            "|    n_updates        | 648691   |\n",
            "----------------------------------\n",
            "Num timesteps: 2645000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 247.84\n",
            "Num timesteps: 2646000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 250.69\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 369      |\n",
            "|    ep_rew_mean      | 250      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5000     |\n",
            "|    fps              | 580      |\n",
            "|    time_elapsed     | 4556     |\n",
            "|    total_timesteps  | 2646705  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0849   |\n",
            "|    n_updates        | 649176   |\n",
            "----------------------------------\n",
            "Num timesteps: 2647000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 249.79\n",
            "Num timesteps: 2648000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 249.27\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 367      |\n",
            "|    ep_rew_mean      | 249      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5004     |\n",
            "|    fps              | 580      |\n",
            "|    time_elapsed     | 4558     |\n",
            "|    total_timesteps  | 2648077  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.106    |\n",
            "|    n_updates        | 649519   |\n",
            "----------------------------------\n",
            "Num timesteps: 2649000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 250.03\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 366      |\n",
            "|    ep_rew_mean      | 250      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5008     |\n",
            "|    fps              | 581      |\n",
            "|    time_elapsed     | 4559     |\n",
            "|    total_timesteps  | 2649428  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0689   |\n",
            "|    n_updates        | 649856   |\n",
            "----------------------------------\n",
            "Num timesteps: 2650000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 249.90\n",
            "Num timesteps: 2651000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 248.12\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 374      |\n",
            "|    ep_rew_mean      | 249      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5012     |\n",
            "|    fps              | 581      |\n",
            "|    time_elapsed     | 4562     |\n",
            "|    total_timesteps  | 2651635  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0535   |\n",
            "|    n_updates        | 650408   |\n",
            "----------------------------------\n",
            "Num timesteps: 2652000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 249.36\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 369      |\n",
            "|    ep_rew_mean      | 250      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5016     |\n",
            "|    fps              | 581      |\n",
            "|    time_elapsed     | 4564     |\n",
            "|    total_timesteps  | 2652762  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.142    |\n",
            "|    n_updates        | 650690   |\n",
            "----------------------------------\n",
            "Num timesteps: 2653000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 249.94\n",
            "Num timesteps: 2654000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 249.89\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 371      |\n",
            "|    ep_rew_mean      | 250      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5020     |\n",
            "|    fps              | 581      |\n",
            "|    time_elapsed     | 4566     |\n",
            "|    total_timesteps  | 2654180  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.82     |\n",
            "|    n_updates        | 651044   |\n",
            "----------------------------------\n",
            "Num timesteps: 2655000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 250.05\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 370      |\n",
            "|    ep_rew_mean      | 250      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5024     |\n",
            "|    fps              | 581      |\n",
            "|    time_elapsed     | 4567     |\n",
            "|    total_timesteps  | 2655573  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0951   |\n",
            "|    n_updates        | 651393   |\n",
            "----------------------------------\n",
            "Num timesteps: 2656000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 250.73\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 371      |\n",
            "|    ep_rew_mean      | 251      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5028     |\n",
            "|    fps              | 581      |\n",
            "|    time_elapsed     | 4569     |\n",
            "|    total_timesteps  | 2656980  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.66     |\n",
            "|    n_updates        | 651744   |\n",
            "----------------------------------\n",
            "Num timesteps: 2657000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 251.42\n",
            "Num timesteps: 2658000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 251.52\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 371      |\n",
            "|    ep_rew_mean      | 252      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5032     |\n",
            "|    fps              | 581      |\n",
            "|    time_elapsed     | 4571     |\n",
            "|    total_timesteps  | 2658326  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.43     |\n",
            "|    n_updates        | 652081   |\n",
            "----------------------------------\n",
            "Num timesteps: 2659000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 251.22\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 373      |\n",
            "|    ep_rew_mean      | 251      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5036     |\n",
            "|    fps              | 581      |\n",
            "|    time_elapsed     | 4572     |\n",
            "|    total_timesteps  | 2659722  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0878   |\n",
            "|    n_updates        | 652430   |\n",
            "----------------------------------\n",
            "Num timesteps: 2660000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 251.17\n",
            "Num timesteps: 2661000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 252.12\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 373      |\n",
            "|    ep_rew_mean      | 252      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5040     |\n",
            "|    fps              | 581      |\n",
            "|    time_elapsed     | 4574     |\n",
            "|    total_timesteps  | 2661279  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0848   |\n",
            "|    n_updates        | 652819   |\n",
            "----------------------------------\n",
            "Num timesteps: 2662000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 252.02\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 374      |\n",
            "|    ep_rew_mean      | 253      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5044     |\n",
            "|    fps              | 581      |\n",
            "|    time_elapsed     | 4577     |\n",
            "|    total_timesteps  | 2662939  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.173    |\n",
            "|    n_updates        | 653234   |\n",
            "----------------------------------\n",
            "Num timesteps: 2663000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 252.52\n",
            "Num timesteps: 2664000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 252.53\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 372      |\n",
            "|    ep_rew_mean      | 252      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5048     |\n",
            "|    fps              | 581      |\n",
            "|    time_elapsed     | 4579     |\n",
            "|    total_timesteps  | 2664541  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0861   |\n",
            "|    n_updates        | 653635   |\n",
            "----------------------------------\n",
            "Num timesteps: 2665000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 251.99\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 367      |\n",
            "|    ep_rew_mean      | 252      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5052     |\n",
            "|    fps              | 581      |\n",
            "|    time_elapsed     | 4580     |\n",
            "|    total_timesteps  | 2665704  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.245    |\n",
            "|    n_updates        | 653925   |\n",
            "----------------------------------\n",
            "Num timesteps: 2666000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 252.89\n",
            "Num timesteps: 2667000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 253.21\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 368      |\n",
            "|    ep_rew_mean      | 253      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5056     |\n",
            "|    fps              | 582      |\n",
            "|    time_elapsed     | 4582     |\n",
            "|    total_timesteps  | 2667170  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0818   |\n",
            "|    n_updates        | 654292   |\n",
            "----------------------------------\n",
            "Num timesteps: 2668000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 252.83\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 372      |\n",
            "|    ep_rew_mean      | 252      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5060     |\n",
            "|    fps              | 582      |\n",
            "|    time_elapsed     | 4584     |\n",
            "|    total_timesteps  | 2668894  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0952   |\n",
            "|    n_updates        | 654723   |\n",
            "----------------------------------\n",
            "Num timesteps: 2669000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 251.92\n",
            "Num timesteps: 2670000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 252.01\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 379      |\n",
            "|    ep_rew_mean      | 251      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5064     |\n",
            "|    fps              | 582      |\n",
            "|    time_elapsed     | 4587     |\n",
            "|    total_timesteps  | 2670688  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0372   |\n",
            "|    n_updates        | 655171   |\n",
            "----------------------------------\n",
            "Num timesteps: 2671000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 251.40\n",
            "Num timesteps: 2672000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 252.03\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 378      |\n",
            "|    ep_rew_mean      | 252      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5068     |\n",
            "|    fps              | 582      |\n",
            "|    time_elapsed     | 4589     |\n",
            "|    total_timesteps  | 2672164  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.01     |\n",
            "|    n_updates        | 655540   |\n",
            "----------------------------------\n",
            "Num timesteps: 2673000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 252.49\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 374      |\n",
            "|    ep_rew_mean      | 253      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5072     |\n",
            "|    fps              | 582      |\n",
            "|    time_elapsed     | 4590     |\n",
            "|    total_timesteps  | 2673379  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.54     |\n",
            "|    n_updates        | 655844   |\n",
            "----------------------------------\n",
            "Num timesteps: 2674000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 252.74\n",
            "Num timesteps: 2675000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 251.54\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 379      |\n",
            "|    ep_rew_mean      | 251      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5076     |\n",
            "|    fps              | 582      |\n",
            "|    time_elapsed     | 4593     |\n",
            "|    total_timesteps  | 2675217  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0713   |\n",
            "|    n_updates        | 656304   |\n",
            "----------------------------------\n",
            "Num timesteps: 2676000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 251.24\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 376      |\n",
            "|    ep_rew_mean      | 252      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5080     |\n",
            "|    fps              | 582      |\n",
            "|    time_elapsed     | 4594     |\n",
            "|    total_timesteps  | 2676286  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.379    |\n",
            "|    n_updates        | 656571   |\n",
            "----------------------------------\n",
            "Num timesteps: 2677000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 252.26\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 371      |\n",
            "|    ep_rew_mean      | 250      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5084     |\n",
            "|    fps              | 582      |\n",
            "|    time_elapsed     | 4595     |\n",
            "|    total_timesteps  | 2677460  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.14     |\n",
            "|    n_updates        | 656864   |\n",
            "----------------------------------\n",
            "Num timesteps: 2678000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 250.22\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 371      |\n",
            "|    ep_rew_mean      | 251      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5088     |\n",
            "|    fps              | 582      |\n",
            "|    time_elapsed     | 4597     |\n",
            "|    total_timesteps  | 2678996  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0819   |\n",
            "|    n_updates        | 657248   |\n",
            "----------------------------------\n",
            "Num timesteps: 2679000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 250.73\n",
            "Num timesteps: 2680000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 251.14\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 371      |\n",
            "|    ep_rew_mean      | 251      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5092     |\n",
            "|    fps              | 582      |\n",
            "|    time_elapsed     | 4599     |\n",
            "|    total_timesteps  | 2680361  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0507   |\n",
            "|    n_updates        | 657590   |\n",
            "----------------------------------\n",
            "Num timesteps: 2681000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 251.50\n",
            "Num timesteps: 2682000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 251.74\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 375      |\n",
            "|    ep_rew_mean      | 252      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5096     |\n",
            "|    fps              | 582      |\n",
            "|    time_elapsed     | 4602     |\n",
            "|    total_timesteps  | 2682218  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.147    |\n",
            "|    n_updates        | 658054   |\n",
            "----------------------------------\n",
            "Num timesteps: 2683000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 252.01\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 369      |\n",
            "|    ep_rew_mean      | 252      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5100     |\n",
            "|    fps              | 582      |\n",
            "|    time_elapsed     | 4603     |\n",
            "|    total_timesteps  | 2683561  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.72     |\n",
            "|    n_updates        | 658390   |\n",
            "----------------------------------\n",
            "Num timesteps: 2684000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 252.44\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 368      |\n",
            "|    ep_rew_mean      | 254      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5104     |\n",
            "|    fps              | 582      |\n",
            "|    time_elapsed     | 4605     |\n",
            "|    total_timesteps  | 2684918  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.695    |\n",
            "|    n_updates        | 658729   |\n",
            "----------------------------------\n",
            "Num timesteps: 2685000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 253.55\n",
            "Num timesteps: 2686000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 251.64\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 367      |\n",
            "|    ep_rew_mean      | 252      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5108     |\n",
            "|    fps              | 583      |\n",
            "|    time_elapsed     | 4607     |\n",
            "|    total_timesteps  | 2686168  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.322    |\n",
            "|    n_updates        | 659041   |\n",
            "----------------------------------\n",
            "Num timesteps: 2687000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 252.78\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 359      |\n",
            "|    ep_rew_mean      | 252      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5112     |\n",
            "|    fps              | 583      |\n",
            "|    time_elapsed     | 4608     |\n",
            "|    total_timesteps  | 2687492  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.107    |\n",
            "|    n_updates        | 659372   |\n",
            "----------------------------------\n",
            "Num timesteps: 2688000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 251.69\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 361      |\n",
            "|    ep_rew_mean      | 252      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5116     |\n",
            "|    fps              | 583      |\n",
            "|    time_elapsed     | 4610     |\n",
            "|    total_timesteps  | 2688891  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.293    |\n",
            "|    n_updates        | 659722   |\n",
            "----------------------------------\n",
            "Num timesteps: 2689000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 252.16\n",
            "Num timesteps: 2690000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 252.72\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 361      |\n",
            "|    ep_rew_mean      | 253      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5120     |\n",
            "|    fps              | 583      |\n",
            "|    time_elapsed     | 4612     |\n",
            "|    total_timesteps  | 2690249  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.106    |\n",
            "|    n_updates        | 660062   |\n",
            "----------------------------------\n",
            "Num timesteps: 2691000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 252.41\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 359      |\n",
            "|    ep_rew_mean      | 253      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5124     |\n",
            "|    fps              | 583      |\n",
            "|    time_elapsed     | 4613     |\n",
            "|    total_timesteps  | 2691504  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.387    |\n",
            "|    n_updates        | 660375   |\n",
            "----------------------------------\n",
            "Num timesteps: 2692000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 251.91\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 358      |\n",
            "|    ep_rew_mean      | 253      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5128     |\n",
            "|    fps              | 583      |\n",
            "|    time_elapsed     | 4615     |\n",
            "|    total_timesteps  | 2692824  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0833   |\n",
            "|    n_updates        | 660705   |\n",
            "----------------------------------\n",
            "Num timesteps: 2693000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 252.64\n",
            "Num timesteps: 2694000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 252.07\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 359      |\n",
            "|    ep_rew_mean      | 252      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5132     |\n",
            "|    fps              | 583      |\n",
            "|    time_elapsed     | 4616     |\n",
            "|    total_timesteps  | 2694240  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0831   |\n",
            "|    n_updates        | 661059   |\n",
            "----------------------------------\n",
            "Num timesteps: 2695000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 251.96\n",
            "Num timesteps: 2696000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 252.43\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 365      |\n",
            "|    ep_rew_mean      | 252      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5136     |\n",
            "|    fps              | 583      |\n",
            "|    time_elapsed     | 4619     |\n",
            "|    total_timesteps  | 2696223  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.107    |\n",
            "|    n_updates        | 661555   |\n",
            "----------------------------------\n",
            "Num timesteps: 2697000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 252.01\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 364      |\n",
            "|    ep_rew_mean      | 252      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5140     |\n",
            "|    fps              | 583      |\n",
            "|    time_elapsed     | 4620     |\n",
            "|    total_timesteps  | 2697641  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.454    |\n",
            "|    n_updates        | 661910   |\n",
            "----------------------------------\n",
            "Num timesteps: 2698000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 252.23\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 359      |\n",
            "|    ep_rew_mean      | 254      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5144     |\n",
            "|    fps              | 583      |\n",
            "|    time_elapsed     | 4622     |\n",
            "|    total_timesteps  | 2698873  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.546    |\n",
            "|    n_updates        | 662218   |\n",
            "----------------------------------\n",
            "Num timesteps: 2699000\n",
            "Best mean reward: 253.82 - Last mean reward per episode: 254.24\n",
            "Saving new best model to log_dir_DQN/best_model.zip\n",
            "Num timesteps: 2700000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 251.77\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 357      |\n",
            "|    ep_rew_mean      | 252      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5148     |\n",
            "|    fps              | 583      |\n",
            "|    time_elapsed     | 4624     |\n",
            "|    total_timesteps  | 2700217  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.423    |\n",
            "|    n_updates        | 662554   |\n",
            "----------------------------------\n",
            "Num timesteps: 2701000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 251.79\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 359      |\n",
            "|    ep_rew_mean      | 252      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5152     |\n",
            "|    fps              | 584      |\n",
            "|    time_elapsed     | 4625     |\n",
            "|    total_timesteps  | 2701565  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0808   |\n",
            "|    n_updates        | 662891   |\n",
            "----------------------------------\n",
            "Num timesteps: 2702000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 252.17\n",
            "Num timesteps: 2703000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 251.59\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 359      |\n",
            "|    ep_rew_mean      | 249      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5156     |\n",
            "|    fps              | 584      |\n",
            "|    time_elapsed     | 4627     |\n",
            "|    total_timesteps  | 2703057  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.8      |\n",
            "|    n_updates        | 663264   |\n",
            "----------------------------------\n",
            "Num timesteps: 2704000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 249.79\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 356      |\n",
            "|    ep_rew_mean      | 250      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5160     |\n",
            "|    fps              | 584      |\n",
            "|    time_elapsed     | 4629     |\n",
            "|    total_timesteps  | 2704513  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0482   |\n",
            "|    n_updates        | 663628   |\n",
            "----------------------------------\n",
            "Num timesteps: 2705000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 250.01\n",
            "Num timesteps: 2706000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 250.68\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 355      |\n",
            "|    ep_rew_mean      | 251      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5164     |\n",
            "|    fps              | 584      |\n",
            "|    time_elapsed     | 4631     |\n",
            "|    total_timesteps  | 2706169  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.383    |\n",
            "|    n_updates        | 664042   |\n",
            "----------------------------------\n",
            "Num timesteps: 2707000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 250.51\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 356      |\n",
            "|    ep_rew_mean      | 250      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5168     |\n",
            "|    fps              | 584      |\n",
            "|    time_elapsed     | 4633     |\n",
            "|    total_timesteps  | 2707733  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.128    |\n",
            "|    n_updates        | 664433   |\n",
            "----------------------------------\n",
            "Num timesteps: 2708000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 249.65\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 355      |\n",
            "|    ep_rew_mean      | 249      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5172     |\n",
            "|    fps              | 584      |\n",
            "|    time_elapsed     | 4635     |\n",
            "|    total_timesteps  | 2708856  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0646   |\n",
            "|    n_updates        | 664713   |\n",
            "----------------------------------\n",
            "Num timesteps: 2709000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 248.65\n",
            "Num timesteps: 2710000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 248.91\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 356      |\n",
            "|    ep_rew_mean      | 248      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5176     |\n",
            "|    fps              | 584      |\n",
            "|    time_elapsed     | 4637     |\n",
            "|    total_timesteps  | 2710863  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.231    |\n",
            "|    n_updates        | 665215   |\n",
            "----------------------------------\n",
            "Num timesteps: 2711000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 247.90\n",
            "Num timesteps: 2712000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 247.39\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 357      |\n",
            "|    ep_rew_mean      | 247      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5180     |\n",
            "|    fps              | 584      |\n",
            "|    time_elapsed     | 4639     |\n",
            "|    total_timesteps  | 2712028  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0923   |\n",
            "|    n_updates        | 665506   |\n",
            "----------------------------------\n",
            "Num timesteps: 2713000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 247.95\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 358      |\n",
            "|    ep_rew_mean      | 250      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5184     |\n",
            "|    fps              | 584      |\n",
            "|    time_elapsed     | 4640     |\n",
            "|    total_timesteps  | 2713296  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.128    |\n",
            "|    n_updates        | 665823   |\n",
            "----------------------------------\n",
            "Num timesteps: 2714000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 250.45\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 355      |\n",
            "|    ep_rew_mean      | 251      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5188     |\n",
            "|    fps              | 584      |\n",
            "|    time_elapsed     | 4642     |\n",
            "|    total_timesteps  | 2714469  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0865   |\n",
            "|    n_updates        | 666117   |\n",
            "----------------------------------\n",
            "Num timesteps: 2715000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 250.41\n",
            "Num timesteps: 2716000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 250.26\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 358      |\n",
            "|    ep_rew_mean      | 250      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5192     |\n",
            "|    fps              | 584      |\n",
            "|    time_elapsed     | 4644     |\n",
            "|    total_timesteps  | 2716166  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0588   |\n",
            "|    n_updates        | 666541   |\n",
            "----------------------------------\n",
            "Num timesteps: 2717000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 249.56\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 353      |\n",
            "|    ep_rew_mean      | 250      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5196     |\n",
            "|    fps              | 584      |\n",
            "|    time_elapsed     | 4646     |\n",
            "|    total_timesteps  | 2717552  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.123    |\n",
            "|    n_updates        | 666887   |\n",
            "----------------------------------\n",
            "Num timesteps: 2718000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 250.18\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 351      |\n",
            "|    ep_rew_mean      | 251      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5200     |\n",
            "|    fps              | 584      |\n",
            "|    time_elapsed     | 4647     |\n",
            "|    total_timesteps  | 2718709  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.096    |\n",
            "|    n_updates        | 667177   |\n",
            "----------------------------------\n",
            "Num timesteps: 2719000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 251.02\n",
            "Num timesteps: 2720000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 250.81\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 353      |\n",
            "|    ep_rew_mean      | 250      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5204     |\n",
            "|    fps              | 585      |\n",
            "|    time_elapsed     | 4649     |\n",
            "|    total_timesteps  | 2720192  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.785    |\n",
            "|    n_updates        | 667547   |\n",
            "----------------------------------\n",
            "Num timesteps: 2721000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 249.97\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 354      |\n",
            "|    ep_rew_mean      | 250      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5208     |\n",
            "|    fps              | 585      |\n",
            "|    time_elapsed     | 4651     |\n",
            "|    total_timesteps  | 2721546  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0741   |\n",
            "|    n_updates        | 667886   |\n",
            "----------------------------------\n",
            "Num timesteps: 2722000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 249.71\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 354      |\n",
            "|    ep_rew_mean      | 250      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5212     |\n",
            "|    fps              | 585      |\n",
            "|    time_elapsed     | 4653     |\n",
            "|    total_timesteps  | 2722872  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.252    |\n",
            "|    n_updates        | 668217   |\n",
            "----------------------------------\n",
            "Num timesteps: 2723000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 250.13\n",
            "Num timesteps: 2724000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 249.66\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 357      |\n",
            "|    ep_rew_mean      | 249      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5216     |\n",
            "|    fps              | 585      |\n",
            "|    time_elapsed     | 4655     |\n",
            "|    total_timesteps  | 2724559  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.78     |\n",
            "|    n_updates        | 668639   |\n",
            "----------------------------------\n",
            "Num timesteps: 2725000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 249.11\n",
            "Num timesteps: 2726000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 248.71\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 358      |\n",
            "|    ep_rew_mean      | 248      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5220     |\n",
            "|    fps              | 585      |\n",
            "|    time_elapsed     | 4657     |\n",
            "|    total_timesteps  | 2726069  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.365    |\n",
            "|    n_updates        | 669017   |\n",
            "----------------------------------\n",
            "Num timesteps: 2727000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 248.64\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 358      |\n",
            "|    ep_rew_mean      | 248      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5224     |\n",
            "|    fps              | 585      |\n",
            "|    time_elapsed     | 4658     |\n",
            "|    total_timesteps  | 2727290  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.111    |\n",
            "|    n_updates        | 669322   |\n",
            "----------------------------------\n",
            "Num timesteps: 2728000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 248.15\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 357      |\n",
            "|    ep_rew_mean      | 249      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5228     |\n",
            "|    fps              | 585      |\n",
            "|    time_elapsed     | 4660     |\n",
            "|    total_timesteps  | 2728561  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.48     |\n",
            "|    n_updates        | 669640   |\n",
            "----------------------------------\n",
            "Num timesteps: 2729000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 248.77\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 357      |\n",
            "|    ep_rew_mean      | 249      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5232     |\n",
            "|    fps              | 585      |\n",
            "|    time_elapsed     | 4661     |\n",
            "|    total_timesteps  | 2729983  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.12     |\n",
            "|    n_updates        | 669995   |\n",
            "----------------------------------\n",
            "Num timesteps: 2730000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 248.91\n",
            "Num timesteps: 2731000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 248.75\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 352      |\n",
            "|    ep_rew_mean      | 248      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5236     |\n",
            "|    fps              | 585      |\n",
            "|    time_elapsed     | 4663     |\n",
            "|    total_timesteps  | 2731386  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.27     |\n",
            "|    n_updates        | 670346   |\n",
            "----------------------------------\n",
            "Num timesteps: 2732000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 248.53\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 351      |\n",
            "|    ep_rew_mean      | 249      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5240     |\n",
            "|    fps              | 585      |\n",
            "|    time_elapsed     | 4665     |\n",
            "|    total_timesteps  | 2732721  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.483    |\n",
            "|    n_updates        | 670680   |\n",
            "----------------------------------\n",
            "Num timesteps: 2733000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 248.85\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 349      |\n",
            "|    ep_rew_mean      | 246      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5244     |\n",
            "|    fps              | 585      |\n",
            "|    time_elapsed     | 4666     |\n",
            "|    total_timesteps  | 2733731  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0596   |\n",
            "|    n_updates        | 670932   |\n",
            "----------------------------------\n",
            "Num timesteps: 2734000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 245.51\n",
            "Num timesteps: 2735000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 248.77\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 348      |\n",
            "|    ep_rew_mean      | 249      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5248     |\n",
            "|    fps              | 585      |\n",
            "|    time_elapsed     | 4668     |\n",
            "|    total_timesteps  | 2735003  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.955    |\n",
            "|    n_updates        | 671250   |\n",
            "----------------------------------\n",
            "Num timesteps: 2736000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 248.81\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 348      |\n",
            "|    ep_rew_mean      | 248      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5252     |\n",
            "|    fps              | 585      |\n",
            "|    time_elapsed     | 4669     |\n",
            "|    total_timesteps  | 2736404  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0968   |\n",
            "|    n_updates        | 671600   |\n",
            "----------------------------------\n",
            "Num timesteps: 2737000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 247.83\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 344      |\n",
            "|    ep_rew_mean      | 251      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5256     |\n",
            "|    fps              | 586      |\n",
            "|    time_elapsed     | 4671     |\n",
            "|    total_timesteps  | 2737507  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0662   |\n",
            "|    n_updates        | 671876   |\n",
            "----------------------------------\n",
            "Num timesteps: 2738000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 251.32\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 345      |\n",
            "|    ep_rew_mean      | 251      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5260     |\n",
            "|    fps              | 586      |\n",
            "|    time_elapsed     | 4672     |\n",
            "|    total_timesteps  | 2738978  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.74     |\n",
            "|    n_updates        | 672244   |\n",
            "----------------------------------\n",
            "Num timesteps: 2739000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 251.28\n",
            "Num timesteps: 2740000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 251.43\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 340      |\n",
            "|    ep_rew_mean      | 252      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5264     |\n",
            "|    fps              | 586      |\n",
            "|    time_elapsed     | 4674     |\n",
            "|    total_timesteps  | 2740216  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0437   |\n",
            "|    n_updates        | 672553   |\n",
            "----------------------------------\n",
            "Num timesteps: 2741000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 249.22\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 336      |\n",
            "|    ep_rew_mean      | 250      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5268     |\n",
            "|    fps              | 586      |\n",
            "|    time_elapsed     | 4675     |\n",
            "|    total_timesteps  | 2741297  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0731   |\n",
            "|    n_updates        | 672824   |\n",
            "----------------------------------\n",
            "Num timesteps: 2742000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 247.12\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 338      |\n",
            "|    ep_rew_mean      | 247      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5272     |\n",
            "|    fps              | 586      |\n",
            "|    time_elapsed     | 4677     |\n",
            "|    total_timesteps  | 2742634  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.743    |\n",
            "|    n_updates        | 673158   |\n",
            "----------------------------------\n",
            "Num timesteps: 2743000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 247.45\n",
            "Num timesteps: 2744000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 247.93\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 332      |\n",
            "|    ep_rew_mean      | 249      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5276     |\n",
            "|    fps              | 586      |\n",
            "|    time_elapsed     | 4678     |\n",
            "|    total_timesteps  | 2744020  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.125    |\n",
            "|    n_updates        | 673504   |\n",
            "----------------------------------\n",
            "Num timesteps: 2745000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 249.03\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 332      |\n",
            "|    ep_rew_mean      | 249      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5280     |\n",
            "|    fps              | 586      |\n",
            "|    time_elapsed     | 4680     |\n",
            "|    total_timesteps  | 2745192  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0505   |\n",
            "|    n_updates        | 673797   |\n",
            "----------------------------------\n",
            "Num timesteps: 2746000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 248.53\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 336      |\n",
            "|    ep_rew_mean      | 248      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5284     |\n",
            "|    fps              | 586      |\n",
            "|    time_elapsed     | 4682     |\n",
            "|    total_timesteps  | 2746925  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0506   |\n",
            "|    n_updates        | 674231   |\n",
            "----------------------------------\n",
            "Num timesteps: 2747000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 247.50\n",
            "Num timesteps: 2748000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 247.10\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 338      |\n",
            "|    ep_rew_mean      | 247      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5288     |\n",
            "|    fps              | 586      |\n",
            "|    time_elapsed     | 4684     |\n",
            "|    total_timesteps  | 2748285  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.52     |\n",
            "|    n_updates        | 674571   |\n",
            "----------------------------------\n",
            "Num timesteps: 2749000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 247.08\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 336      |\n",
            "|    ep_rew_mean      | 244      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5292     |\n",
            "|    fps              | 586      |\n",
            "|    time_elapsed     | 4686     |\n",
            "|    total_timesteps  | 2749768  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0645   |\n",
            "|    n_updates        | 674941   |\n",
            "----------------------------------\n",
            "Num timesteps: 2750000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 244.14\n",
            "Num timesteps: 2751000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 244.19\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 336      |\n",
            "|    ep_rew_mean      | 242      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5296     |\n",
            "|    fps              | 586      |\n",
            "|    time_elapsed     | 4687     |\n",
            "|    total_timesteps  | 2751121  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.181    |\n",
            "|    n_updates        | 675280   |\n",
            "----------------------------------\n",
            "Num timesteps: 2752000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 238.22\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 335      |\n",
            "|    ep_rew_mean      | 238      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5300     |\n",
            "|    fps              | 586      |\n",
            "|    time_elapsed     | 4689     |\n",
            "|    total_timesteps  | 2752207  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.505    |\n",
            "|    n_updates        | 675551   |\n",
            "----------------------------------\n",
            "Num timesteps: 2753000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 237.95\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 334      |\n",
            "|    ep_rew_mean      | 239      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5304     |\n",
            "|    fps              | 587      |\n",
            "|    time_elapsed     | 4690     |\n",
            "|    total_timesteps  | 2753593  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.21     |\n",
            "|    n_updates        | 675898   |\n",
            "----------------------------------\n",
            "Num timesteps: 2754000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 241.08\n",
            "Num timesteps: 2755000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 240.61\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 336      |\n",
            "|    ep_rew_mean      | 241      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5308     |\n",
            "|    fps              | 587      |\n",
            "|    time_elapsed     | 4692     |\n",
            "|    total_timesteps  | 2755150  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.119    |\n",
            "|    n_updates        | 676287   |\n",
            "----------------------------------\n",
            "Num timesteps: 2756000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 240.91\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 336      |\n",
            "|    ep_rew_mean      | 240      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5312     |\n",
            "|    fps              | 587      |\n",
            "|    time_elapsed     | 4694     |\n",
            "|    total_timesteps  | 2756434  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0741   |\n",
            "|    n_updates        | 676608   |\n",
            "----------------------------------\n",
            "Num timesteps: 2757000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 240.29\n",
            "Num timesteps: 2758000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 239.43\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 338      |\n",
            "|    ep_rew_mean      | 239      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5316     |\n",
            "|    fps              | 587      |\n",
            "|    time_elapsed     | 4696     |\n",
            "|    total_timesteps  | 2758393  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.16     |\n",
            "|    n_updates        | 677098   |\n",
            "----------------------------------\n",
            "Num timesteps: 2759000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 239.63\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 338      |\n",
            "|    ep_rew_mean      | 240      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5320     |\n",
            "|    fps              | 587      |\n",
            "|    time_elapsed     | 4699     |\n",
            "|    total_timesteps  | 2759860  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.118    |\n",
            "|    n_updates        | 677464   |\n",
            "----------------------------------\n",
            "Num timesteps: 2760000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 239.82\n",
            "Num timesteps: 2761000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 239.94\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 341      |\n",
            "|    ep_rew_mean      | 240      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5324     |\n",
            "|    fps              | 587      |\n",
            "|    time_elapsed     | 4701     |\n",
            "|    total_timesteps  | 2761360  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0811   |\n",
            "|    n_updates        | 677839   |\n",
            "----------------------------------\n",
            "Num timesteps: 2762000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 239.61\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 341      |\n",
            "|    ep_rew_mean      | 238      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5328     |\n",
            "|    fps              | 587      |\n",
            "|    time_elapsed     | 4702     |\n",
            "|    total_timesteps  | 2762619  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.085    |\n",
            "|    n_updates        | 678154   |\n",
            "----------------------------------\n",
            "Num timesteps: 2763000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 238.41\n",
            "Num timesteps: 2764000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 239.13\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 341      |\n",
            "|    ep_rew_mean      | 239      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5332     |\n",
            "|    fps              | 587      |\n",
            "|    time_elapsed     | 4704     |\n",
            "|    total_timesteps  | 2764090  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.262    |\n",
            "|    n_updates        | 678522   |\n",
            "----------------------------------\n",
            "Num timesteps: 2765000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 236.24\n",
            "Num timesteps: 2766000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 233.45\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 348      |\n",
            "|    ep_rew_mean      | 233      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5336     |\n",
            "|    fps              | 587      |\n",
            "|    time_elapsed     | 4707     |\n",
            "|    total_timesteps  | 2766161  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.154    |\n",
            "|    n_updates        | 679040   |\n",
            "----------------------------------\n",
            "Num timesteps: 2767000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 233.00\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 348      |\n",
            "|    ep_rew_mean      | 233      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5340     |\n",
            "|    fps              | 587      |\n",
            "|    time_elapsed     | 4708     |\n",
            "|    total_timesteps  | 2767521  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0902   |\n",
            "|    n_updates        | 679380   |\n",
            "----------------------------------\n",
            "Num timesteps: 2768000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 232.56\n",
            "Num timesteps: 2769000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 231.08\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 360      |\n",
            "|    ep_rew_mean      | 233      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5344     |\n",
            "|    fps              | 587      |\n",
            "|    time_elapsed     | 4711     |\n",
            "|    total_timesteps  | 2769724  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.02     |\n",
            "|    n_updates        | 679930   |\n",
            "----------------------------------\n",
            "Num timesteps: 2770000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 232.92\n",
            "Num timesteps: 2771000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 229.52\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 360      |\n",
            "|    ep_rew_mean      | 229      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5348     |\n",
            "|    fps              | 587      |\n",
            "|    time_elapsed     | 4713     |\n",
            "|    total_timesteps  | 2771026  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.104    |\n",
            "|    n_updates        | 680256   |\n",
            "----------------------------------\n",
            "Num timesteps: 2772000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 229.39\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 365      |\n",
            "|    ep_rew_mean      | 229      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5352     |\n",
            "|    fps              | 587      |\n",
            "|    time_elapsed     | 4716     |\n",
            "|    total_timesteps  | 2772903  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0606   |\n",
            "|    n_updates        | 680725   |\n",
            "----------------------------------\n",
            "Num timesteps: 2773000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 229.00\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 363      |\n",
            "|    ep_rew_mean      | 221      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5356     |\n",
            "|    fps              | 588      |\n",
            "|    time_elapsed     | 4717     |\n",
            "|    total_timesteps  | 2773797  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0483   |\n",
            "|    n_updates        | 680949   |\n",
            "----------------------------------\n",
            "Num timesteps: 2774000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 221.25\n",
            "Num timesteps: 2775000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 218.58\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 361      |\n",
            "|    ep_rew_mean      | 219      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5360     |\n",
            "|    fps              | 588      |\n",
            "|    time_elapsed     | 4718     |\n",
            "|    total_timesteps  | 2775047  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.532    |\n",
            "|    n_updates        | 681261   |\n",
            "----------------------------------\n",
            "Num timesteps: 2776000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 215.87\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 362      |\n",
            "|    ep_rew_mean      | 216      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5364     |\n",
            "|    fps              | 588      |\n",
            "|    time_elapsed     | 4720     |\n",
            "|    total_timesteps  | 2776378  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.11     |\n",
            "|    n_updates        | 681594   |\n",
            "----------------------------------\n",
            "Num timesteps: 2777000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 218.13\n",
            "Num timesteps: 2778000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 216.93\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 370      |\n",
            "|    ep_rew_mean      | 217      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5368     |\n",
            "|    fps              | 588      |\n",
            "|    time_elapsed     | 4722     |\n",
            "|    total_timesteps  | 2778299  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.16     |\n",
            "|    n_updates        | 682074   |\n",
            "----------------------------------\n",
            "Num timesteps: 2779000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 216.65\n",
            "Num timesteps: 2780000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 219.10\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 378      |\n",
            "|    ep_rew_mean      | 218      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5372     |\n",
            "|    fps              | 588      |\n",
            "|    time_elapsed     | 4725     |\n",
            "|    total_timesteps  | 2780396  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.138    |\n",
            "|    n_updates        | 682598   |\n",
            "----------------------------------\n",
            "Num timesteps: 2781000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 218.40\n",
            "Num timesteps: 2782000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 214.84\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 388      |\n",
            "|    ep_rew_mean      | 214      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5376     |\n",
            "|    fps              | 588      |\n",
            "|    time_elapsed     | 4729     |\n",
            "|    total_timesteps  | 2782859  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.526    |\n",
            "|    n_updates        | 683214   |\n",
            "----------------------------------\n",
            "Num timesteps: 2783000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 211.64\n",
            "Num timesteps: 2784000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 208.46\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 388      |\n",
            "|    ep_rew_mean      | 208      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5380     |\n",
            "|    fps              | 588      |\n",
            "|    time_elapsed     | 4730     |\n",
            "|    total_timesteps  | 2784000  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.62     |\n",
            "|    n_updates        | 683499   |\n",
            "----------------------------------\n",
            "Num timesteps: 2785000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 209.18\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 386      |\n",
            "|    ep_rew_mean      | 209      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5384     |\n",
            "|    fps              | 588      |\n",
            "|    time_elapsed     | 4732     |\n",
            "|    total_timesteps  | 2785569  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.136    |\n",
            "|    n_updates        | 683892   |\n",
            "----------------------------------\n",
            "Num timesteps: 2786000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 208.62\n",
            "Num timesteps: 2787000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 207.89\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 393      |\n",
            "|    ep_rew_mean      | 207      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5388     |\n",
            "|    fps              | 588      |\n",
            "|    time_elapsed     | 4735     |\n",
            "|    total_timesteps  | 2787604  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0773   |\n",
            "|    n_updates        | 684400   |\n",
            "----------------------------------\n",
            "Num timesteps: 2788000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 206.94\n",
            "Num timesteps: 2789000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 204.50\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 398      |\n",
            "|    ep_rew_mean      | 207      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5392     |\n",
            "|    fps              | 588      |\n",
            "|    time_elapsed     | 4738     |\n",
            "|    total_timesteps  | 2789603  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.429    |\n",
            "|    n_updates        | 684900   |\n",
            "----------------------------------\n",
            "Num timesteps: 2790000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 207.72\n",
            "Num timesteps: 2791000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 206.42\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 401      |\n",
            "|    ep_rew_mean      | 209      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5396     |\n",
            "|    fps              | 588      |\n",
            "|    time_elapsed     | 4740     |\n",
            "|    total_timesteps  | 2791198  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.103    |\n",
            "|    n_updates        | 685299   |\n",
            "----------------------------------\n",
            "Num timesteps: 2792000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 208.77\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 404      |\n",
            "|    ep_rew_mean      | 208      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5400     |\n",
            "|    fps              | 588      |\n",
            "|    time_elapsed     | 4742     |\n",
            "|    total_timesteps  | 2792578  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0859   |\n",
            "|    n_updates        | 685644   |\n",
            "----------------------------------\n",
            "Num timesteps: 2793000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 206.39\n",
            "Num timesteps: 2794000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 206.26\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 406      |\n",
            "|    ep_rew_mean      | 206      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5404     |\n",
            "|    fps              | 588      |\n",
            "|    time_elapsed     | 4744     |\n",
            "|    total_timesteps  | 2794145  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.193    |\n",
            "|    n_updates        | 686036   |\n",
            "----------------------------------\n",
            "Num timesteps: 2795000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 205.45\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 403      |\n",
            "|    ep_rew_mean      | 203      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5408     |\n",
            "|    fps              | 588      |\n",
            "|    time_elapsed     | 4746     |\n",
            "|    total_timesteps  | 2795437  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0955   |\n",
            "|    n_updates        | 686359   |\n",
            "----------------------------------\n",
            "Num timesteps: 2796000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 202.72\n",
            "Num timesteps: 2797000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 202.49\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 407      |\n",
            "|    ep_rew_mean      | 202      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5412     |\n",
            "|    fps              | 589      |\n",
            "|    time_elapsed     | 4748     |\n",
            "|    total_timesteps  | 2797178  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0591   |\n",
            "|    n_updates        | 686794   |\n",
            "----------------------------------\n",
            "Num timesteps: 2798000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 202.57\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 405      |\n",
            "|    ep_rew_mean      | 203      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5416     |\n",
            "|    fps              | 589      |\n",
            "|    time_elapsed     | 4750     |\n",
            "|    total_timesteps  | 2798927  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.85     |\n",
            "|    n_updates        | 687231   |\n",
            "----------------------------------\n",
            "Num timesteps: 2799000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 203.02\n",
            "Num timesteps: 2800000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 202.72\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 405      |\n",
            "|    ep_rew_mean      | 202      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5420     |\n",
            "|    fps              | 589      |\n",
            "|    time_elapsed     | 4752     |\n",
            "|    total_timesteps  | 2800378  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0847   |\n",
            "|    n_updates        | 687594   |\n",
            "----------------------------------\n",
            "Num timesteps: 2801000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 201.98\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 405      |\n",
            "|    ep_rew_mean      | 203      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5424     |\n",
            "|    fps              | 589      |\n",
            "|    time_elapsed     | 4754     |\n",
            "|    total_timesteps  | 2801822  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.105    |\n",
            "|    n_updates        | 687955   |\n",
            "----------------------------------\n",
            "Num timesteps: 2802000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 202.77\n",
            "Num timesteps: 2803000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 199.66\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 406      |\n",
            "|    ep_rew_mean      | 200      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5428     |\n",
            "|    fps              | 589      |\n",
            "|    time_elapsed     | 4756     |\n",
            "|    total_timesteps  | 2803268  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.063    |\n",
            "|    n_updates        | 688316   |\n",
            "----------------------------------\n",
            "Num timesteps: 2804000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 199.94\n",
            "Num timesteps: 2805000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 197.23\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 415      |\n",
            "|    ep_rew_mean      | 197      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5432     |\n",
            "|    fps              | 589      |\n",
            "|    time_elapsed     | 4759     |\n",
            "|    total_timesteps  | 2805571  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.453    |\n",
            "|    n_updates        | 688892   |\n",
            "----------------------------------\n",
            "Num timesteps: 2806000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 199.66\n",
            "Num timesteps: 2807000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 202.12\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 410      |\n",
            "|    ep_rew_mean      | 202      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5436     |\n",
            "|    fps              | 589      |\n",
            "|    time_elapsed     | 4761     |\n",
            "|    total_timesteps  | 2807168  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.146    |\n",
            "|    n_updates        | 689291   |\n",
            "----------------------------------\n",
            "Num timesteps: 2808000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 202.48\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 409      |\n",
            "|    ep_rew_mean      | 203      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5440     |\n",
            "|    fps              | 589      |\n",
            "|    time_elapsed     | 4763     |\n",
            "|    total_timesteps  | 2808423  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.521    |\n",
            "|    n_updates        | 689605   |\n",
            "----------------------------------\n",
            "Num timesteps: 2809000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 204.10\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 400      |\n",
            "|    ep_rew_mean      | 204      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5444     |\n",
            "|    fps              | 589      |\n",
            "|    time_elapsed     | 4764     |\n",
            "|    total_timesteps  | 2809730  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.18     |\n",
            "|    n_updates        | 689932   |\n",
            "----------------------------------\n",
            "Num timesteps: 2810000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 207.53\n",
            "Num timesteps: 2811000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 208.19\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 401      |\n",
            "|    ep_rew_mean      | 209      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5448     |\n",
            "|    fps              | 589      |\n",
            "|    time_elapsed     | 4766     |\n",
            "|    total_timesteps  | 2811117  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0767   |\n",
            "|    n_updates        | 690279   |\n",
            "----------------------------------\n",
            "Num timesteps: 2812000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 208.56\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 399      |\n",
            "|    ep_rew_mean      | 209      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5452     |\n",
            "|    fps              | 589      |\n",
            "|    time_elapsed     | 4769     |\n",
            "|    total_timesteps  | 2812814  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.154    |\n",
            "|    n_updates        | 690703   |\n",
            "----------------------------------\n",
            "Num timesteps: 2813000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 209.02\n",
            "Num timesteps: 2814000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 213.46\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 403      |\n",
            "|    ep_rew_mean      | 216      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5456     |\n",
            "|    fps              | 589      |\n",
            "|    time_elapsed     | 4770     |\n",
            "|    total_timesteps  | 2814089  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.105    |\n",
            "|    n_updates        | 691022   |\n",
            "----------------------------------\n",
            "Num timesteps: 2815000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 218.99\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 403      |\n",
            "|    ep_rew_mean      | 219      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5460     |\n",
            "|    fps              | 589      |\n",
            "|    time_elapsed     | 4772     |\n",
            "|    total_timesteps  | 2815353  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.114    |\n",
            "|    n_updates        | 691338   |\n",
            "----------------------------------\n",
            "Num timesteps: 2816000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 221.42\n",
            "Num timesteps: 2817000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 221.57\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 409      |\n",
            "|    ep_rew_mean      | 221      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5464     |\n",
            "|    fps              | 590      |\n",
            "|    time_elapsed     | 4774     |\n",
            "|    total_timesteps  | 2817318  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.113    |\n",
            "|    n_updates        | 691829   |\n",
            "----------------------------------\n",
            "Num timesteps: 2818000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 222.94\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 403      |\n",
            "|    ep_rew_mean      | 223      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5468     |\n",
            "|    fps              | 590      |\n",
            "|    time_elapsed     | 4776     |\n",
            "|    total_timesteps  | 2818623  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.509    |\n",
            "|    n_updates        | 692155   |\n",
            "----------------------------------\n",
            "Num timesteps: 2819000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 223.12\n",
            "Num timesteps: 2820000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 220.67\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 399      |\n",
            "|    ep_rew_mean      | 221      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5472     |\n",
            "|    fps              | 590      |\n",
            "|    time_elapsed     | 4778     |\n",
            "|    total_timesteps  | 2820253  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0933   |\n",
            "|    n_updates        | 692563   |\n",
            "----------------------------------\n",
            "Num timesteps: 2821000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 224.59\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 386      |\n",
            "|    ep_rew_mean      | 223      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5476     |\n",
            "|    fps              | 590      |\n",
            "|    time_elapsed     | 4780     |\n",
            "|    total_timesteps  | 2821417  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.479    |\n",
            "|    n_updates        | 692854   |\n",
            "----------------------------------\n",
            "Num timesteps: 2822000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 225.02\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 389      |\n",
            "|    ep_rew_mean      | 227      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5480     |\n",
            "|    fps              | 590      |\n",
            "|    time_elapsed     | 4781     |\n",
            "|    total_timesteps  | 2822896  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0478   |\n",
            "|    n_updates        | 693223   |\n",
            "----------------------------------\n",
            "Num timesteps: 2823000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 227.01\n",
            "Num timesteps: 2824000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 227.19\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 388      |\n",
            "|    ep_rew_mean      | 225      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5484     |\n",
            "|    fps              | 590      |\n",
            "|    time_elapsed     | 4783     |\n",
            "|    total_timesteps  | 2824368  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0959   |\n",
            "|    n_updates        | 693591   |\n",
            "----------------------------------\n",
            "Num timesteps: 2825000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 226.20\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 381      |\n",
            "|    ep_rew_mean      | 227      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5488     |\n",
            "|    fps              | 590      |\n",
            "|    time_elapsed     | 4785     |\n",
            "|    total_timesteps  | 2825704  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.187    |\n",
            "|    n_updates        | 693925   |\n",
            "----------------------------------\n",
            "Num timesteps: 2826000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 226.78\n",
            "Num timesteps: 2827000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 224.12\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 378      |\n",
            "|    ep_rew_mean      | 224      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5492     |\n",
            "|    fps              | 590      |\n",
            "|    time_elapsed     | 4787     |\n",
            "|    total_timesteps  | 2827375  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.135    |\n",
            "|    n_updates        | 694343   |\n",
            "----------------------------------\n",
            "Num timesteps: 2828000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 223.69\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 376      |\n",
            "|    ep_rew_mean      | 224      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5496     |\n",
            "|    fps              | 590      |\n",
            "|    time_elapsed     | 4789     |\n",
            "|    total_timesteps  | 2828828  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.84     |\n",
            "|    n_updates        | 694706   |\n",
            "----------------------------------\n",
            "Num timesteps: 2829000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 224.29\n",
            "Num timesteps: 2830000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 223.13\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 381      |\n",
            "|    ep_rew_mean      | 223      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5500     |\n",
            "|    fps              | 590      |\n",
            "|    time_elapsed     | 4791     |\n",
            "|    total_timesteps  | 2830657  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0363   |\n",
            "|    n_updates        | 695164   |\n",
            "----------------------------------\n",
            "Num timesteps: 2831000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 222.84\n",
            "Num timesteps: 2832000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 225.10\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 385      |\n",
            "|    ep_rew_mean      | 220      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5504     |\n",
            "|    fps              | 590      |\n",
            "|    time_elapsed     | 4794     |\n",
            "|    total_timesteps  | 2832643  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.421    |\n",
            "|    n_updates        | 695660   |\n",
            "----------------------------------\n",
            "Num timesteps: 2833000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 219.64\n",
            "Num timesteps: 2834000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 219.26\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 394      |\n",
            "|    ep_rew_mean      | 222      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5508     |\n",
            "|    fps              | 590      |\n",
            "|    time_elapsed     | 4797     |\n",
            "|    total_timesteps  | 2834819  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.123    |\n",
            "|    n_updates        | 696204   |\n",
            "----------------------------------\n",
            "Num timesteps: 2835000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 221.60\n",
            "Num timesteps: 2836000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 222.59\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 389      |\n",
            "|    ep_rew_mean      | 223      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5512     |\n",
            "|    fps              | 590      |\n",
            "|    time_elapsed     | 4799     |\n",
            "|    total_timesteps  | 2836127  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0961   |\n",
            "|    n_updates        | 696531   |\n",
            "----------------------------------\n",
            "Num timesteps: 2837000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 223.54\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 386      |\n",
            "|    ep_rew_mean      | 224      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5516     |\n",
            "|    fps              | 591      |\n",
            "|    time_elapsed     | 4800     |\n",
            "|    total_timesteps  | 2837510  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0754   |\n",
            "|    n_updates        | 696877   |\n",
            "----------------------------------\n",
            "Num timesteps: 2838000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 223.98\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 386      |\n",
            "|    ep_rew_mean      | 221      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5520     |\n",
            "|    fps              | 591      |\n",
            "|    time_elapsed     | 4802     |\n",
            "|    total_timesteps  | 2838999  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.79     |\n",
            "|    n_updates        | 697249   |\n",
            "----------------------------------\n",
            "Num timesteps: 2839000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 221.48\n",
            "Num timesteps: 2840000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 221.43\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 387      |\n",
            "|    ep_rew_mean      | 221      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5524     |\n",
            "|    fps              | 591      |\n",
            "|    time_elapsed     | 4804     |\n",
            "|    total_timesteps  | 2840500  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0746   |\n",
            "|    n_updates        | 697624   |\n",
            "----------------------------------\n",
            "Num timesteps: 2841000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 221.26\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 385      |\n",
            "|    ep_rew_mean      | 224      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5528     |\n",
            "|    fps              | 591      |\n",
            "|    time_elapsed     | 4806     |\n",
            "|    total_timesteps  | 2841798  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0259   |\n",
            "|    n_updates        | 697949   |\n",
            "----------------------------------\n",
            "Num timesteps: 2842000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 224.09\n",
            "Num timesteps: 2843000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 228.31\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 376      |\n",
            "|    ep_rew_mean      | 228      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5532     |\n",
            "|    fps              | 591      |\n",
            "|    time_elapsed     | 4807     |\n",
            "|    total_timesteps  | 2843182  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0765   |\n",
            "|    n_updates        | 698295   |\n",
            "----------------------------------\n",
            "Num timesteps: 2844000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 228.26\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 375      |\n",
            "|    ep_rew_mean      | 229      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5536     |\n",
            "|    fps              | 591      |\n",
            "|    time_elapsed     | 4809     |\n",
            "|    total_timesteps  | 2844663  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.275    |\n",
            "|    n_updates        | 698665   |\n",
            "----------------------------------\n",
            "Num timesteps: 2845000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 228.74\n",
            "Num timesteps: 2846000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 224.90\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 377      |\n",
            "|    ep_rew_mean      | 225      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5540     |\n",
            "|    fps              | 591      |\n",
            "|    time_elapsed     | 4811     |\n",
            "|    total_timesteps  | 2846112  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.06     |\n",
            "|    n_updates        | 699027   |\n",
            "----------------------------------\n",
            "Num timesteps: 2847000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 224.92\n",
            "Num timesteps: 2848000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 222.11\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 384      |\n",
            "|    ep_rew_mean      | 222      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5544     |\n",
            "|    fps              | 591      |\n",
            "|    time_elapsed     | 4814     |\n",
            "|    total_timesteps  | 2848156  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.113    |\n",
            "|    n_updates        | 699538   |\n",
            "----------------------------------\n",
            "Num timesteps: 2849000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 222.33\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 384      |\n",
            "|    ep_rew_mean      | 222      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5548     |\n",
            "|    fps              | 591      |\n",
            "|    time_elapsed     | 4816     |\n",
            "|    total_timesteps  | 2849554  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.943    |\n",
            "|    n_updates        | 699888   |\n",
            "----------------------------------\n",
            "Num timesteps: 2850000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 222.34\n",
            "Num timesteps: 2851000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 215.92\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 389      |\n",
            "|    ep_rew_mean      | 213      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5552     |\n",
            "|    fps              | 591      |\n",
            "|    time_elapsed     | 4819     |\n",
            "|    total_timesteps  | 2851671  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.188    |\n",
            "|    n_updates        | 700417   |\n",
            "----------------------------------\n",
            "Num timesteps: 2852000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 213.52\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 388      |\n",
            "|    ep_rew_mean      | 213      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5556     |\n",
            "|    fps              | 591      |\n",
            "|    time_elapsed     | 4820     |\n",
            "|    total_timesteps  | 2852920  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0793   |\n",
            "|    n_updates        | 700729   |\n",
            "----------------------------------\n",
            "Num timesteps: 2853000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 213.42\n",
            "Num timesteps: 2854000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 212.78\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 390      |\n",
            "|    ep_rew_mean      | 212      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5560     |\n",
            "|    fps              | 591      |\n",
            "|    time_elapsed     | 4822     |\n",
            "|    total_timesteps  | 2854393  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0644   |\n",
            "|    n_updates        | 701098   |\n",
            "----------------------------------\n",
            "Num timesteps: 2855000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 212.55\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 385      |\n",
            "|    ep_rew_mean      | 210      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5564     |\n",
            "|    fps              | 591      |\n",
            "|    time_elapsed     | 4824     |\n",
            "|    total_timesteps  | 2855802  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0601   |\n",
            "|    n_updates        | 701450   |\n",
            "----------------------------------\n",
            "Num timesteps: 2856000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 209.74\n",
            "Num timesteps: 2857000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 208.88\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 387      |\n",
            "|    ep_rew_mean      | 209      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5568     |\n",
            "|    fps              | 592      |\n",
            "|    time_elapsed     | 4826     |\n",
            "|    total_timesteps  | 2857296  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.687    |\n",
            "|    n_updates        | 701823   |\n",
            "----------------------------------\n",
            "Num timesteps: 2858000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 209.52\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 385      |\n",
            "|    ep_rew_mean      | 212      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5572     |\n",
            "|    fps              | 592      |\n",
            "|    time_elapsed     | 4828     |\n",
            "|    total_timesteps  | 2858757  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.652    |\n",
            "|    n_updates        | 702189   |\n",
            "----------------------------------\n",
            "Num timesteps: 2859000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 211.78\n",
            "Num timesteps: 2860000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 211.56\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 388      |\n",
            "|    ep_rew_mean      | 212      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5576     |\n",
            "|    fps              | 592      |\n",
            "|    time_elapsed     | 4830     |\n",
            "|    total_timesteps  | 2860243  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.323    |\n",
            "|    n_updates        | 702560   |\n",
            "----------------------------------\n",
            "Num timesteps: 2861000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 209.13\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 386      |\n",
            "|    ep_rew_mean      | 209      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5580     |\n",
            "|    fps              | 592      |\n",
            "|    time_elapsed     | 4831     |\n",
            "|    total_timesteps  | 2861516  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.88     |\n",
            "|    n_updates        | 702878   |\n",
            "----------------------------------\n",
            "Num timesteps: 2862000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 208.95\n",
            "Num timesteps: 2863000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 212.53\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 387      |\n",
            "|    ep_rew_mean      | 212      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5584     |\n",
            "|    fps              | 592      |\n",
            "|    time_elapsed     | 4833     |\n",
            "|    total_timesteps  | 2863060  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.135    |\n",
            "|    n_updates        | 703264   |\n",
            "----------------------------------\n",
            "Num timesteps: 2864000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 211.70\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 391      |\n",
            "|    ep_rew_mean      | 208      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5588     |\n",
            "|    fps              | 592      |\n",
            "|    time_elapsed     | 4836     |\n",
            "|    total_timesteps  | 2864824  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0682   |\n",
            "|    n_updates        | 703705   |\n",
            "----------------------------------\n",
            "Num timesteps: 2865000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 208.23\n",
            "Num timesteps: 2866000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 213.55\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 388      |\n",
            "|    ep_rew_mean      | 214      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5592     |\n",
            "|    fps              | 592      |\n",
            "|    time_elapsed     | 4837     |\n",
            "|    total_timesteps  | 2866163  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.434    |\n",
            "|    n_updates        | 704040   |\n",
            "----------------------------------\n",
            "Num timesteps: 2867000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 213.75\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 391      |\n",
            "|    ep_rew_mean      | 211      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5596     |\n",
            "|    fps              | 592      |\n",
            "|    time_elapsed     | 4840     |\n",
            "|    total_timesteps  | 2867923  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.962    |\n",
            "|    n_updates        | 704480   |\n",
            "----------------------------------\n",
            "Num timesteps: 2868000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 211.49\n",
            "Num timesteps: 2869000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 214.42\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 388      |\n",
            "|    ep_rew_mean      | 215      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5600     |\n",
            "|    fps              | 592      |\n",
            "|    time_elapsed     | 4842     |\n",
            "|    total_timesteps  | 2869506  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0609   |\n",
            "|    n_updates        | 704876   |\n",
            "----------------------------------\n",
            "Num timesteps: 2870000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 214.82\n",
            "Num timesteps: 2871000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 214.19\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 389      |\n",
            "|    ep_rew_mean      | 218      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5604     |\n",
            "|    fps              | 592      |\n",
            "|    time_elapsed     | 4845     |\n",
            "|    total_timesteps  | 2871504  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.962    |\n",
            "|    n_updates        | 705375   |\n",
            "----------------------------------\n",
            "Num timesteps: 2872000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 217.78\n",
            "Num timesteps: 2873000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 218.03\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 382      |\n",
            "|    ep_rew_mean      | 218      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5608     |\n",
            "|    fps              | 592      |\n",
            "|    time_elapsed     | 4847     |\n",
            "|    total_timesteps  | 2873013  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0727   |\n",
            "|    n_updates        | 705753   |\n",
            "----------------------------------\n",
            "Num timesteps: 2874000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 217.76\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 381      |\n",
            "|    ep_rew_mean      | 215      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5612     |\n",
            "|    fps              | 592      |\n",
            "|    time_elapsed     | 4848     |\n",
            "|    total_timesteps  | 2874253  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0939   |\n",
            "|    n_updates        | 706063   |\n",
            "----------------------------------\n",
            "Num timesteps: 2875000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 215.15\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 383      |\n",
            "|    ep_rew_mean      | 214      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5616     |\n",
            "|    fps              | 592      |\n",
            "|    time_elapsed     | 4850     |\n",
            "|    total_timesteps  | 2875845  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0797   |\n",
            "|    n_updates        | 706461   |\n",
            "----------------------------------\n",
            "Num timesteps: 2876000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 214.17\n",
            "Num timesteps: 2877000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 214.25\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 388      |\n",
            "|    ep_rew_mean      | 214      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5620     |\n",
            "|    fps              | 592      |\n",
            "|    time_elapsed     | 4853     |\n",
            "|    total_timesteps  | 2877827  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.618    |\n",
            "|    n_updates        | 706956   |\n",
            "----------------------------------\n",
            "Num timesteps: 2878000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 213.76\n",
            "Num timesteps: 2879000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 213.18\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 390      |\n",
            "|    ep_rew_mean      | 213      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5624     |\n",
            "|    fps              | 592      |\n",
            "|    time_elapsed     | 4856     |\n",
            "|    total_timesteps  | 2879517  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0361   |\n",
            "|    n_updates        | 707379   |\n",
            "----------------------------------\n",
            "Num timesteps: 2880000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 212.39\n",
            "Num timesteps: 2881000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 211.64\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 399      |\n",
            "|    ep_rew_mean      | 209      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5628     |\n",
            "|    fps              | 593      |\n",
            "|    time_elapsed     | 4859     |\n",
            "|    total_timesteps  | 2881701  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.103    |\n",
            "|    n_updates        | 707925   |\n",
            "----------------------------------\n",
            "Num timesteps: 2882000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 209.09\n",
            "Num timesteps: 2883000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 207.17\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 400      |\n",
            "|    ep_rew_mean      | 207      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5632     |\n",
            "|    fps              | 593      |\n",
            "|    time_elapsed     | 4861     |\n",
            "|    total_timesteps  | 2883228  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.12     |\n",
            "|    n_updates        | 708306   |\n",
            "----------------------------------\n",
            "Num timesteps: 2884000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 207.76\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 401      |\n",
            "|    ep_rew_mean      | 208      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5636     |\n",
            "|    fps              | 593      |\n",
            "|    time_elapsed     | 4863     |\n",
            "|    total_timesteps  | 2884787  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.159    |\n",
            "|    n_updates        | 708696   |\n",
            "----------------------------------\n",
            "Num timesteps: 2885000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 207.75\n",
            "Num timesteps: 2886000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 210.92\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 403      |\n",
            "|    ep_rew_mean      | 211      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5640     |\n",
            "|    fps              | 593      |\n",
            "|    time_elapsed     | 4865     |\n",
            "|    total_timesteps  | 2886412  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.156    |\n",
            "|    n_updates        | 709102   |\n",
            "----------------------------------\n",
            "Num timesteps: 2887000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 210.64\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 397      |\n",
            "|    ep_rew_mean      | 210      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5644     |\n",
            "|    fps              | 593      |\n",
            "|    time_elapsed     | 4867     |\n",
            "|    total_timesteps  | 2887814  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0872   |\n",
            "|    n_updates        | 709453   |\n",
            "----------------------------------\n",
            "Num timesteps: 2888000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 210.25\n",
            "Num timesteps: 2889000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 208.17\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 400      |\n",
            "|    ep_rew_mean      | 208      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5648     |\n",
            "|    fps              | 593      |\n",
            "|    time_elapsed     | 4869     |\n",
            "|    total_timesteps  | 2889542  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0602   |\n",
            "|    n_updates        | 709885   |\n",
            "----------------------------------\n",
            "Num timesteps: 2890000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 211.51\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 393      |\n",
            "|    ep_rew_mean      | 217      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5652     |\n",
            "|    fps              | 593      |\n",
            "|    time_elapsed     | 4871     |\n",
            "|    total_timesteps  | 2890951  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0892   |\n",
            "|    n_updates        | 710237   |\n",
            "----------------------------------\n",
            "Num timesteps: 2891000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 217.36\n",
            "Num timesteps: 2892000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 217.49\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 394      |\n",
            "|    ep_rew_mean      | 217      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5656     |\n",
            "|    fps              | 593      |\n",
            "|    time_elapsed     | 4873     |\n",
            "|    total_timesteps  | 2892316  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0798   |\n",
            "|    n_updates        | 710578   |\n",
            "----------------------------------\n",
            "Num timesteps: 2893000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 216.70\n",
            "Num timesteps: 2894000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 217.28\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 397      |\n",
            "|    ep_rew_mean      | 218      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5660     |\n",
            "|    fps              | 593      |\n",
            "|    time_elapsed     | 4875     |\n",
            "|    total_timesteps  | 2894105  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.77     |\n",
            "|    n_updates        | 711026   |\n",
            "----------------------------------\n",
            "Num timesteps: 2895000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 220.78\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 397      |\n",
            "|    ep_rew_mean      | 221      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5664     |\n",
            "|    fps              | 593      |\n",
            "|    time_elapsed     | 4877     |\n",
            "|    total_timesteps  | 2895463  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.1      |\n",
            "|    n_updates        | 711365   |\n",
            "----------------------------------\n",
            "Num timesteps: 2896000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 221.69\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 396      |\n",
            "|    ep_rew_mean      | 221      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5668     |\n",
            "|    fps              | 593      |\n",
            "|    time_elapsed     | 4879     |\n",
            "|    total_timesteps  | 2896865  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.951    |\n",
            "|    n_updates        | 711716   |\n",
            "----------------------------------\n",
            "Num timesteps: 2897000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 220.51\n",
            "Num timesteps: 2898000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 220.18\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 401      |\n",
            "|    ep_rew_mean      | 216      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5672     |\n",
            "|    fps              | 593      |\n",
            "|    time_elapsed     | 4881     |\n",
            "|    total_timesteps  | 2898901  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.46     |\n",
            "|    n_updates        | 712225   |\n",
            "----------------------------------\n",
            "Num timesteps: 2899000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 215.86\n",
            "Num timesteps: 2900000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 212.20\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 405      |\n",
            "|    ep_rew_mean      | 210      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5676     |\n",
            "|    fps              | 593      |\n",
            "|    time_elapsed     | 4884     |\n",
            "|    total_timesteps  | 2900758  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0389   |\n",
            "|    n_updates        | 712689   |\n",
            "----------------------------------\n",
            "Num timesteps: 2901000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 210.26\n",
            "Num timesteps: 2902000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 210.02\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 406      |\n",
            "|    ep_rew_mean      | 211      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5680     |\n",
            "|    fps              | 593      |\n",
            "|    time_elapsed     | 4885     |\n",
            "|    total_timesteps  | 2902096  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.148    |\n",
            "|    n_updates        | 713023   |\n",
            "----------------------------------\n",
            "Num timesteps: 2903000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 208.20\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 405      |\n",
            "|    ep_rew_mean      | 209      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5684     |\n",
            "|    fps              | 594      |\n",
            "|    time_elapsed     | 4887     |\n",
            "|    total_timesteps  | 2903534  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.541    |\n",
            "|    n_updates        | 713383   |\n",
            "----------------------------------\n",
            "Num timesteps: 2904000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 208.48\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 401      |\n",
            "|    ep_rew_mean      | 212      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5688     |\n",
            "|    fps              | 594      |\n",
            "|    time_elapsed     | 4889     |\n",
            "|    total_timesteps  | 2904877  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.907    |\n",
            "|    n_updates        | 713719   |\n",
            "----------------------------------\n",
            "Num timesteps: 2905000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 211.63\n",
            "Num timesteps: 2906000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 211.07\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 402      |\n",
            "|    ep_rew_mean      | 212      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5692     |\n",
            "|    fps              | 594      |\n",
            "|    time_elapsed     | 4891     |\n",
            "|    total_timesteps  | 2906320  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.105    |\n",
            "|    n_updates        | 714079   |\n",
            "----------------------------------\n",
            "Num timesteps: 2907000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 211.47\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 401      |\n",
            "|    ep_rew_mean      | 212      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5696     |\n",
            "|    fps              | 594      |\n",
            "|    time_elapsed     | 4893     |\n",
            "|    total_timesteps  | 2907999  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0763   |\n",
            "|    n_updates        | 714499   |\n",
            "----------------------------------\n",
            "Num timesteps: 2908000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 211.52\n",
            "Num timesteps: 2909000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 212.19\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 398      |\n",
            "|    ep_rew_mean      | 210      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5700     |\n",
            "|    fps              | 594      |\n",
            "|    time_elapsed     | 4895     |\n",
            "|    total_timesteps  | 2909289  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.256    |\n",
            "|    n_updates        | 714822   |\n",
            "----------------------------------\n",
            "Num timesteps: 2910000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 213.72\n",
            "Num timesteps: 2911000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 213.49\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 398      |\n",
            "|    ep_rew_mean      | 211      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5704     |\n",
            "|    fps              | 594      |\n",
            "|    time_elapsed     | 4898     |\n",
            "|    total_timesteps  | 2911260  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0615   |\n",
            "|    n_updates        | 715314   |\n",
            "----------------------------------\n",
            "Num timesteps: 2912000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 211.32\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 394      |\n",
            "|    ep_rew_mean      | 209      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5708     |\n",
            "|    fps              | 594      |\n",
            "|    time_elapsed     | 4899     |\n",
            "|    total_timesteps  | 2912421  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.24     |\n",
            "|    n_updates        | 715605   |\n",
            "----------------------------------\n",
            "Num timesteps: 2913000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 209.33\n",
            "Num timesteps: 2914000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 206.17\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 399      |\n",
            "|    ep_rew_mean      | 208      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5712     |\n",
            "|    fps              | 594      |\n",
            "|    time_elapsed     | 4902     |\n",
            "|    total_timesteps  | 2914201  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0746   |\n",
            "|    n_updates        | 716050   |\n",
            "----------------------------------\n",
            "Num timesteps: 2915000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 208.37\n",
            "Num timesteps: 2916000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 208.43\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 402      |\n",
            "|    ep_rew_mean      | 208      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5716     |\n",
            "|    fps              | 594      |\n",
            "|    time_elapsed     | 4905     |\n",
            "|    total_timesteps  | 2916018  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.167    |\n",
            "|    n_updates        | 716504   |\n",
            "----------------------------------\n",
            "Num timesteps: 2917000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 208.13\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 395      |\n",
            "|    ep_rew_mean      | 212      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5720     |\n",
            "|    fps              | 594      |\n",
            "|    time_elapsed     | 4906     |\n",
            "|    total_timesteps  | 2917339  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0566   |\n",
            "|    n_updates        | 716834   |\n",
            "----------------------------------\n",
            "Num timesteps: 2918000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 211.82\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 394      |\n",
            "|    ep_rew_mean      | 213      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5724     |\n",
            "|    fps              | 594      |\n",
            "|    time_elapsed     | 4908     |\n",
            "|    total_timesteps  | 2918963  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.251    |\n",
            "|    n_updates        | 717240   |\n",
            "----------------------------------\n",
            "Num timesteps: 2919000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 212.79\n",
            "Num timesteps: 2920000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 211.87\n",
            "Num timesteps: 2921000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 211.59\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 393      |\n",
            "|    ep_rew_mean      | 214      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5728     |\n",
            "|    fps              | 594      |\n",
            "|    time_elapsed     | 4911     |\n",
            "|    total_timesteps  | 2921026  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0691   |\n",
            "|    n_updates        | 717756   |\n",
            "----------------------------------\n",
            "Num timesteps: 2922000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 210.76\n",
            "Num timesteps: 2923000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 211.42\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 398      |\n",
            "|    ep_rew_mean      | 211      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5732     |\n",
            "|    fps              | 594      |\n",
            "|    time_elapsed     | 4914     |\n",
            "|    total_timesteps  | 2923034  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.99     |\n",
            "|    n_updates        | 718258   |\n",
            "----------------------------------\n",
            "Num timesteps: 2924000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 211.13\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 396      |\n",
            "|    ep_rew_mean      | 211      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5736     |\n",
            "|    fps              | 594      |\n",
            "|    time_elapsed     | 4916     |\n",
            "|    total_timesteps  | 2924429  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0818   |\n",
            "|    n_updates        | 718607   |\n",
            "----------------------------------\n",
            "Num timesteps: 2925000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 211.27\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 395      |\n",
            "|    ep_rew_mean      | 211      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5740     |\n",
            "|    fps              | 594      |\n",
            "|    time_elapsed     | 4918     |\n",
            "|    total_timesteps  | 2925903  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.25     |\n",
            "|    n_updates        | 718975   |\n",
            "----------------------------------\n",
            "Num timesteps: 2926000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 211.43\n",
            "Num timesteps: 2927000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 211.19\n",
            "Num timesteps: 2928000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 212.06\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 405      |\n",
            "|    ep_rew_mean      | 213      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5744     |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 4921     |\n",
            "|    total_timesteps  | 2928286  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0877   |\n",
            "|    n_updates        | 719571   |\n",
            "----------------------------------\n",
            "Num timesteps: 2929000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 212.80\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 403      |\n",
            "|    ep_rew_mean      | 214      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5748     |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 4923     |\n",
            "|    total_timesteps  | 2929820  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.07     |\n",
            "|    n_updates        | 719954   |\n",
            "----------------------------------\n",
            "Num timesteps: 2930000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 213.88\n",
            "Num timesteps: 2931000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 213.37\n",
            "Num timesteps: 2932000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 210.23\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 414      |\n",
            "|    ep_rew_mean      | 210      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5752     |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 4927     |\n",
            "|    total_timesteps  | 2932337  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.118    |\n",
            "|    n_updates        | 720584   |\n",
            "----------------------------------\n",
            "Num timesteps: 2933000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 208.89\n",
            "Num timesteps: 2934000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 208.80\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 417      |\n",
            "|    ep_rew_mean      | 209      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5756     |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 4929     |\n",
            "|    total_timesteps  | 2934000  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.52     |\n",
            "|    n_updates        | 720999   |\n",
            "----------------------------------\n",
            "Num timesteps: 2935000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 206.15\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 412      |\n",
            "|    ep_rew_mean      | 206      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5760     |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 4931     |\n",
            "|    total_timesteps  | 2935330  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0876   |\n",
            "|    n_updates        | 721332   |\n",
            "----------------------------------\n",
            "Num timesteps: 2936000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 205.67\n",
            "Num timesteps: 2937000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 205.16\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 416      |\n",
            "|    ep_rew_mean      | 203      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5764     |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 4933     |\n",
            "|    total_timesteps  | 2937029  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.122    |\n",
            "|    n_updates        | 721757   |\n",
            "----------------------------------\n",
            "Num timesteps: 2938000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 201.97\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 416      |\n",
            "|    ep_rew_mean      | 200      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5768     |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 4935     |\n",
            "|    total_timesteps  | 2938512  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.108    |\n",
            "|    n_updates        | 722127   |\n",
            "----------------------------------\n",
            "Num timesteps: 2939000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 200.39\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 410      |\n",
            "|    ep_rew_mean      | 202      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5772     |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 4937     |\n",
            "|    total_timesteps  | 2939868  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.112    |\n",
            "|    n_updates        | 722466   |\n",
            "----------------------------------\n",
            "Num timesteps: 2940000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 202.34\n",
            "Num timesteps: 2941000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 205.43\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 410      |\n",
            "|    ep_rew_mean      | 210      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5776     |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 4939     |\n",
            "|    total_timesteps  | 2941792  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.054    |\n",
            "|    n_updates        | 722947   |\n",
            "----------------------------------\n",
            "Num timesteps: 2942000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 209.79\n",
            "Num timesteps: 2943000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 212.30\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 411      |\n",
            "|    ep_rew_mean      | 212      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5780     |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 4941     |\n",
            "|    total_timesteps  | 2943229  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.119    |\n",
            "|    n_updates        | 723307   |\n",
            "----------------------------------\n",
            "Num timesteps: 2944000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 214.98\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 410      |\n",
            "|    ep_rew_mean      | 212      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5784     |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 4943     |\n",
            "|    total_timesteps  | 2944494  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.099    |\n",
            "|    n_updates        | 723623   |\n",
            "----------------------------------\n",
            "Num timesteps: 2945000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 212.09\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 411      |\n",
            "|    ep_rew_mean      | 209      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5788     |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 4945     |\n",
            "|    total_timesteps  | 2945967  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.526    |\n",
            "|    n_updates        | 723991   |\n",
            "----------------------------------\n",
            "Num timesteps: 2946000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 208.66\n",
            "Num timesteps: 2947000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 205.41\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 410      |\n",
            "|    ep_rew_mean      | 205      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5792     |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 4946     |\n",
            "|    total_timesteps  | 2947314  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.85     |\n",
            "|    n_updates        | 724328   |\n",
            "----------------------------------\n",
            "Num timesteps: 2948000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 206.74\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 405      |\n",
            "|    ep_rew_mean      | 209      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5796     |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 4948     |\n",
            "|    total_timesteps  | 2948502  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.108    |\n",
            "|    n_updates        | 724625   |\n",
            "----------------------------------\n",
            "Num timesteps: 2949000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 209.14\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 407      |\n",
            "|    ep_rew_mean      | 211      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5800     |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 4950     |\n",
            "|    total_timesteps  | 2949967  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0848   |\n",
            "|    n_updates        | 724991   |\n",
            "----------------------------------\n",
            "Num timesteps: 2950000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 211.45\n",
            "Num timesteps: 2951000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 208.26\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 402      |\n",
            "|    ep_rew_mean      | 211      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5804     |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 4952     |\n",
            "|    total_timesteps  | 2951454  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0742   |\n",
            "|    n_updates        | 725363   |\n",
            "----------------------------------\n",
            "Num timesteps: 2952000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 210.40\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 401      |\n",
            "|    ep_rew_mean      | 207      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5808     |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 4953     |\n",
            "|    total_timesteps  | 2952473  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0925   |\n",
            "|    n_updates        | 725618   |\n",
            "----------------------------------\n",
            "Num timesteps: 2953000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 207.01\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 396      |\n",
            "|    ep_rew_mean      | 209      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5812     |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 4955     |\n",
            "|    total_timesteps  | 2953819  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.096    |\n",
            "|    n_updates        | 725954   |\n",
            "----------------------------------\n",
            "Num timesteps: 2954000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 208.57\n",
            "Num timesteps: 2955000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 207.74\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 394      |\n",
            "|    ep_rew_mean      | 208      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5816     |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 4957     |\n",
            "|    total_timesteps  | 2955401  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.08     |\n",
            "|    n_updates        | 726350   |\n",
            "----------------------------------\n",
            "Num timesteps: 2956000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 208.28\n",
            "Num timesteps: 2957000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 207.89\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 397      |\n",
            "|    ep_rew_mean      | 207      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5820     |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 4959     |\n",
            "|    total_timesteps  | 2957014  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.996    |\n",
            "|    n_updates        | 726753   |\n",
            "----------------------------------\n",
            "Num timesteps: 2958000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 208.03\n",
            "Num timesteps: 2959000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 203.60\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 403      |\n",
            "|    ep_rew_mean      | 203      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5824     |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 4962     |\n",
            "|    total_timesteps  | 2959258  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0826   |\n",
            "|    n_updates        | 727314   |\n",
            "----------------------------------\n",
            "Num timesteps: 2960000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 205.28\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 396      |\n",
            "|    ep_rew_mean      | 205      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5828     |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 4964     |\n",
            "|    total_timesteps  | 2960650  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.19     |\n",
            "|    n_updates        | 727662   |\n",
            "----------------------------------\n",
            "Num timesteps: 2961000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 208.37\n",
            "Num timesteps: 2962000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 208.75\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 391      |\n",
            "|    ep_rew_mean      | 209      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5832     |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 4966     |\n",
            "|    total_timesteps  | 2962107  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.88     |\n",
            "|    n_updates        | 728026   |\n",
            "----------------------------------\n",
            "Num timesteps: 2963000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 208.07\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 392      |\n",
            "|    ep_rew_mean      | 207      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5836     |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 4968     |\n",
            "|    total_timesteps  | 2963643  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.18     |\n",
            "|    n_updates        | 728410   |\n",
            "----------------------------------\n",
            "Num timesteps: 2964000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 205.05\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 391      |\n",
            "|    ep_rew_mean      | 205      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5840     |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 4969     |\n",
            "|    total_timesteps  | 2964987  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0405   |\n",
            "|    n_updates        | 728746   |\n",
            "----------------------------------\n",
            "Num timesteps: 2965000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 204.94\n",
            "Num timesteps: 2966000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 203.29\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 382      |\n",
            "|    ep_rew_mean      | 204      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5844     |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 4971     |\n",
            "|    total_timesteps  | 2966535  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.29     |\n",
            "|    n_updates        | 729133   |\n",
            "----------------------------------\n",
            "Num timesteps: 2967000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 203.83\n",
            "Num timesteps: 2968000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 203.61\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 387      |\n",
            "|    ep_rew_mean      | 199      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5848     |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 4974     |\n",
            "|    total_timesteps  | 2968537  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.551    |\n",
            "|    n_updates        | 729634   |\n",
            "----------------------------------\n",
            "Num timesteps: 2969000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 199.40\n",
            "Num timesteps: 2970000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 201.95\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 383      |\n",
            "|    ep_rew_mean      | 199      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5852     |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 4978     |\n",
            "|    total_timesteps  | 2970646  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.078    |\n",
            "|    n_updates        | 730161   |\n",
            "----------------------------------\n",
            "Num timesteps: 2971000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 199.68\n",
            "Num timesteps: 2972000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 199.62\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 383      |\n",
            "|    ep_rew_mean      | 200      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5856     |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 4980     |\n",
            "|    total_timesteps  | 2972311  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.747    |\n",
            "|    n_updates        | 730577   |\n",
            "----------------------------------\n",
            "Num timesteps: 2973000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 202.15\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 383      |\n",
            "|    ep_rew_mean      | 201      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5860     |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 4981     |\n",
            "|    total_timesteps  | 2973655  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.363    |\n",
            "|    n_updates        | 730913   |\n",
            "----------------------------------\n",
            "Num timesteps: 2974000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 201.46\n",
            "Num timesteps: 2975000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 202.30\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 383      |\n",
            "|    ep_rew_mean      | 205      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5864     |\n",
            "|    fps              | 596      |\n",
            "|    time_elapsed     | 4984     |\n",
            "|    total_timesteps  | 2975339  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0736   |\n",
            "|    n_updates        | 731334   |\n",
            "----------------------------------\n",
            "Num timesteps: 2976000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 203.18\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 381      |\n",
            "|    ep_rew_mean      | 206      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5868     |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 4985     |\n",
            "|    total_timesteps  | 2976604  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.34     |\n",
            "|    n_updates        | 731650   |\n",
            "----------------------------------\n",
            "Num timesteps: 2977000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 205.81\n",
            "Num timesteps: 2978000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 208.77\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 385      |\n",
            "|    ep_rew_mean      | 208      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5872     |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 4987     |\n",
            "|    total_timesteps  | 2978332  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.496    |\n",
            "|    n_updates        | 732082   |\n",
            "----------------------------------\n",
            "Num timesteps: 2979000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 208.38\n",
            "Num timesteps: 2980000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 207.84\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 383      |\n",
            "|    ep_rew_mean      | 207      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5876     |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 4990     |\n",
            "|    total_timesteps  | 2980048  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0865   |\n",
            "|    n_updates        | 732511   |\n",
            "----------------------------------\n",
            "Num timesteps: 2981000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 207.96\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 383      |\n",
            "|    ep_rew_mean      | 208      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5880     |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 4992     |\n",
            "|    total_timesteps  | 2981501  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0661   |\n",
            "|    n_updates        | 732875   |\n",
            "----------------------------------\n",
            "Num timesteps: 2982000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 207.92\n",
            "Num timesteps: 2983000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 209.96\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 387      |\n",
            "|    ep_rew_mean      | 210      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5884     |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 4994     |\n",
            "|    total_timesteps  | 2983211  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.269    |\n",
            "|    n_updates        | 733302   |\n",
            "----------------------------------\n",
            "Num timesteps: 2984000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 210.10\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 386      |\n",
            "|    ep_rew_mean      | 211      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5888     |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 4996     |\n",
            "|    total_timesteps  | 2984613  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.185    |\n",
            "|    n_updates        | 733653   |\n",
            "----------------------------------\n",
            "Num timesteps: 2985000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 211.11\n",
            "Num timesteps: 2986000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 214.32\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 388      |\n",
            "|    ep_rew_mean      | 214      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5892     |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 4998     |\n",
            "|    total_timesteps  | 2986110  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.7      |\n",
            "|    n_updates        | 734027   |\n",
            "----------------------------------\n",
            "Num timesteps: 2987000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 213.96\n",
            "Num timesteps: 2988000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 207.08\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 397      |\n",
            "|    ep_rew_mean      | 207      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5896     |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 5000     |\n",
            "|    total_timesteps  | 2988172  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.46     |\n",
            "|    n_updates        | 734542   |\n",
            "----------------------------------\n",
            "Num timesteps: 2989000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 207.38\n",
            "Num timesteps: 2990000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 206.30\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 401      |\n",
            "|    ep_rew_mean      | 207      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5900     |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 5003     |\n",
            "|    total_timesteps  | 2990037  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.107    |\n",
            "|    n_updates        | 735009   |\n",
            "----------------------------------\n",
            "Num timesteps: 2991000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 207.22\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 401      |\n",
            "|    ep_rew_mean      | 208      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5904     |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 5005     |\n",
            "|    total_timesteps  | 2991578  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.396    |\n",
            "|    n_updates        | 735394   |\n",
            "----------------------------------\n",
            "Num timesteps: 2992000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 207.81\n",
            "Num timesteps: 2993000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 210.12\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 406      |\n",
            "|    ep_rew_mean      | 212      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5908     |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 5007     |\n",
            "|    total_timesteps  | 2993119  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.83     |\n",
            "|    n_updates        | 735779   |\n",
            "----------------------------------\n",
            "Num timesteps: 2994000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 208.56\n",
            "Num timesteps: 2995000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 206.94\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 412      |\n",
            "|    ep_rew_mean      | 207      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5912     |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 5010     |\n",
            "|    total_timesteps  | 2995063  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.368    |\n",
            "|    n_updates        | 736265   |\n",
            "----------------------------------\n",
            "Num timesteps: 2996000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 207.38\n",
            "Num timesteps: 2997000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 204.79\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 417      |\n",
            "|    ep_rew_mean      | 205      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5916     |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 5013     |\n",
            "|    total_timesteps  | 2997106  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.105    |\n",
            "|    n_updates        | 736776   |\n",
            "----------------------------------\n",
            "Num timesteps: 2998000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 205.76\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 416      |\n",
            "|    ep_rew_mean      | 206      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 5920     |\n",
            "|    fps              | 597      |\n",
            "|    time_elapsed     | 5015     |\n",
            "|    total_timesteps  | 2998564  |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.194    |\n",
            "|    n_updates        | 737140   |\n",
            "----------------------------------\n",
            "Num timesteps: 2999000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 205.81\n",
            "Num timesteps: 3000000\n",
            "Best mean reward: 254.24 - Last mean reward per episode: 210.02\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<stable_baselines3.dqn.dqn.DQN at 0x7fb2e00807f0>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_from_file = False\n",
        "# Hyperparameters are from RL_Zoo\n",
        "# https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/dqn.yml\n",
        "\n",
        "\n",
        "n_timesteps = 3e6\n",
        "policy =  'MlpPolicy'\n",
        "learning_rate = 6.3e-4\n",
        "batch_size = 128\n",
        "buffer_size = 50000\n",
        "# learning_starts = 0\n",
        "gamma = 0.99\n",
        "target_update_interval = 250\n",
        "train_freq = 4\n",
        "gradient_steps = -1\n",
        "exploration_fraction = 0.12\n",
        "exploration_final_eps = 0.1\n",
        "policy_kwargs = \"dict(net_arch=[256, 256])\"\n",
        "\n",
        "n_envs = 1\n",
        "\n",
        "\n",
        "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=\"log_dir_DQN/\")\n",
        "\n",
        "# env\n",
        "env = make_vec_env(\"LunarLander-v2\", n_envs=n_envs, monitor_dir=\"log_dir_DQN/\")\n",
        "\n",
        "# instantiate the agent\n",
        "if train_from_file:\n",
        "  model = DQN.load(path=\"log_dir_DQN/best_model.zip\", env=env)\n",
        "else:\n",
        "  model = DQN(\n",
        "      policy,\n",
        "      env,\n",
        "      # learning_rate = learning_rate,\n",
        "      # batch_size = batch_size,\n",
        "      # buffer_size = buffer_size,\n",
        "      # learning_starts = learning_starts,\n",
        "      # gamma = gamma,\n",
        "      # target_update_interval = target_update_interval,\n",
        "      # train_freq = train_freq,\n",
        "      # gradient_steps = gradient_steps,\n",
        "      # exploration_fraction = exploration_fraction,\n",
        "      # exploration_final_eps = exploration_final_eps,\n",
        "      # policy_kwargs = dict(net_arch=[256, 256]),\n",
        "      tensorboard_log=\"./TensorBoardLog/\", verbose=1)\n",
        "\n",
        "# train the agent\n",
        "model.learn(total_timesteps=n_timesteps, callback=callback)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b1dc8fa",
      "metadata": {
        "id": "5b1dc8fa"
      },
      "source": [
        "# Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "366b80a8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "366b80a8",
        "outputId": "e3a334d7-5726-4a4a-82b5-d3d3401ea4e2"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def moving_average(values, window):\n",
        "    \"\"\"\n",
        "    Smooth values by doing a moving average\n",
        "    :param values: (numpy array)\n",
        "    :param window: (int)\n",
        "    :return: (numpy array)\n",
        "    \"\"\"\n",
        "    weights = np.repeat(1.0, window) / window\n",
        "    return np.convolve(values, weights, 'valid')\n",
        "\n",
        "def plot_results(log_folder, title='Learning Curve'):\n",
        "    \"\"\"\n",
        "    plot the results\n",
        "\n",
        "    :param log_folder: (str) the save location of the results to plot\n",
        "    :param title: (str) the title of the task to plot\n",
        "    \"\"\"\n",
        "\n",
        "    x, y = ts2xy(load_results(log_folder), 'timesteps')\n",
        "    y = moving_average(y, window=100)\n",
        "    # Truncate x\n",
        "    x = x[len(x) - len(y):]\n",
        "    fig = plt.figure(title, figsize=(12,5))\n",
        "    plt.plot(x, y)\n",
        "    plt.xlabel('Number of Timesteps')\n",
        "    plt.ylabel('Rewards')\n",
        "    plt.title(title + \" Smoothed DQN\")\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "plot_results(\"log_dir_DQN\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b00f2a81",
      "metadata": {
        "id": "b00f2a81"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "815393a0",
      "metadata": {
        "id": "815393a0"
      },
      "outputs": [],
      "source": [
        "env = make_vec_env(\"LunarLander-v2\", n_envs=1,monitor_dir=\"evalaute_log_dir_DQN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "63611e6e",
      "metadata": {
        "id": "63611e6e"
      },
      "outputs": [],
      "source": [
        "model = DQN.load(path=\"log_dir_DQN/best_model.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b06e1a3",
      "metadata": {
        "id": "3b06e1a3"
      },
      "source": [
        "#### Stable Baseline 3 Evaluation Function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "9d4fd326",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d4fd326",
        "outputId": "b2dc4cc4-6e34-4cf1-8b97-127a57015b9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean & Std Reward after 10 max run is 232.9583598 & 45.38257482693998\n"
          ]
        }
      ],
      "source": [
        "mean_reward, std_reward = evaluate_policy(model, env,n_eval_episodes=10, render=True, deterministic=True)\n",
        "print(\"Mean & Std Reward after {} max run is {} & {}\".format(10,mean_reward, std_reward))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e49c5168",
      "metadata": {
        "id": "e49c5168"
      },
      "source": [
        "# GIF of a Train Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "60cc63dc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60cc63dc",
        "outputId": "def8954a-9a49-4664-d769-e14b4c57d174"
      },
      "outputs": [],
      "source": [
        "env = make_vec_env(\"LunarLander-v2\", n_envs=1)\n",
        "model = DQN.load(path=\"log_dir_DQN/best_model.zip\")\n",
        "\n",
        "images = []\n",
        "obs = env.reset()\n",
        "img = env.render(mode=\"rgb_array\")\n",
        "for i in range(1000):\n",
        "    images.append(img)\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, _, _ ,_ = env.step(action)\n",
        "    img = env.render(mode=\"rgb_array\")\n",
        "\n",
        "imageio.mimsave(\"lunar lander_DQN.gif\", [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3afc060b",
      "metadata": {
        "id": "3afc060b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "857970f990130bbcaee778cf1846f7875676d945310dca1379fe4b5ef3d258a5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
