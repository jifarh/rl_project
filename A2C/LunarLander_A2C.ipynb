{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "57601915",
      "metadata": {
        "id": "57601915"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# !pip install shap\n",
        "# !pip install opencv-python\n",
        "# !pip install swig\n",
        "# !pip install Box2D\n",
        "\n",
        "\n",
        "# # !pip install box2d pygame\n",
        "\n",
        "\n",
        "# !pip install gym\n",
        "# !pip install pyglet==1.5.27\n",
        "# !pip install stable-baseline3\n",
        "# !pip install \"gymnasium[all]\"\n",
        "\n",
        "# !pip install stable_baselines3\n",
        "\n",
        "## FOR LOCAL Jupyter notebook\n",
        "# !pip install tensorflow\n",
        "# !pip install torch\n",
        "# !pip install pygame\n",
        "# !pip install tensorboard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "b00a128f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b00a128f",
        "outputId": "f0a196be-2f8b-4fb1-a62f-bfdd10254bae"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import imageio\n",
        "import os\n",
        "from stable_baselines3 import PPO, A2C\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack, SubprocVecEnv, VecNormalize\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common import results_plotter\n",
        "import gymnasium  as gym\n",
        "import matplotlib.pyplot as plt\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "import scipy.stats as stats\n",
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "import tensorflow as tf\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "YtZN-eC7NwuS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtZN-eC7NwuS",
        "outputId": "2fc6167f-8579-442f-f41c-6cae6b8abde7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n",
            "GPU not found. Please ensure that GPU is enabled in Colab.\n",
            "Num GPUs Available:  0\n"
          ]
        }
      ],
      "source": [
        "# seeds\n",
        "# Set seed for numpy\n",
        "np.random.seed(100)\n",
        "\n",
        "# Set seed for Python random module\n",
        "import random\n",
        "random.seed(100)\n",
        "\n",
        "# Set seed for TensorFlow\n",
        "tf.random.set_seed(100)\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "if tf.test.gpu_device_name():\n",
        "    print('Default GPU Device:', tf.test.gpu_device_name())\n",
        "else:\n",
        "    print(\"GPU not found. Please ensure that GPU is enabled in Colab.\")\n",
        "    \n",
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "780afb92",
      "metadata": {
        "id": "780afb92"
      },
      "source": [
        "<h1> Important Libraries To Install </h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2826cd85",
      "metadata": {
        "id": "2826cd85"
      },
      "source": [
        "<h1> Parameter & Environment Information </h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87ef75ca",
      "metadata": {
        "id": "87ef75ca"
      },
      "source": [
        "<p>\n",
        "    This environment is part of the Box2D environments.\n",
        "</p>\n",
        "\n",
        "<ul>\n",
        "    <li> Action Space Discrete(4) </li>\n",
        "    <li> Observation Shape (8,) </li>\n",
        "    <li> Observation High [1.5 1.5 5. 5. 3.14 5. 1. 1. ] </li>\n",
        "    <li> Observation Low [-1.5 -1.5 -5. -5. -3.14 -5. -0. -0. ] </li>\n",
        "    <li> Import gymnasium.make(\"LunarLander-v2\") </li>\n",
        "</ul>\n",
        "\n",
        "<h3> Description </h3>\n",
        "<p>This environment is a classic rocket trajectory optimization problem. According to Pontryagin’s maximum principle, it is optimal to fire the engine at full throttle or turn it off. This is the reason why this environment has discrete actions: engine on or off.\n",
        "\n",
        "There are two environment versions: discrete or continuous. The landing pad is always at coordinates (0,0). The coordinates are the first two numbers in the state vector. Landing outside of the landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt.</p>\n",
        "\n",
        "<h3> Action Space </h3>\n",
        "<p>\n",
        "There are four discrete actions available:\n",
        "\n",
        "* 0: do nothing\n",
        "* 1: fire left orientation engine\n",
        "* 2: fire main engine\n",
        "* 3: fire right orientation engine\n",
        "\n",
        "</p>\n",
        "\n",
        "<h3> Observation Space </h3>\n",
        "<p>\n",
        "The state is an 8-dimensional vector: the coordinates of the lander in x & y, its linear velocities in x & y, its angle, its angular velocity, and two booleans that represent whether each leg is in contact with the ground or not.\n",
        "</p>\n",
        "\n",
        "<h3> Reward </h3>\n",
        "<p>\n",
        "After every step a reward is granted. The total reward of an episode is the sum of the rewards for all the steps within that episode.\n",
        "\n",
        "For each step, the reward:\n",
        "\n",
        "* is increased/decreased the closer/further the lander is to the landing pad.\n",
        "* is increased/decreased the slower/faster the lander is moving.\n",
        "* is decreased the more the lander is tilted (angle not horizontal).\n",
        "* is increased by 10 points for each leg that is in contact with the ground.\n",
        "* is decreased by 0.03 points each frame a side engine is firing.\n",
        "* is decreased by 0.3 points each frame the main engine is firing.\n",
        "\n",
        "The episode receive an additional reward of -100 or +100 points for crashing or landing safely respectively.\n",
        "\n",
        "An episode is considered a solution if it scores at least 200 points.\n",
        "</p>\n",
        "\n",
        "<h3> Starting State </h3>\n",
        "\n",
        "<p>The lander starts at the top center of the viewport with a random initial force applied to its center of mass.</p>\n",
        "\n",
        "<h3> Episode Termination </h3>\n",
        "<p> The episode finishes if:<br>\n",
        "    \n",
        "1. the lander crashes (the lander body gets in contact with the moon);<br>\n",
        "2. the lander gets outside of the viewport (x coordinate is greater than 1);<br>\n",
        "3. the lander is not awake. From the Box2D docs, a body which is not awake is a body which doesn’t move and doesn’t collide with any other body:<br>\n",
        "\n",
        "When Box2D determines that a body (or group of bodies) has come to rest, the body enters a sleep state which has very little CPU overhead. If a body is awake and collides with a sleeping body, then the sleeping body wakes up. Bodies will also wake up if a joint or contact attached to them is destroyed.\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "3d0543a5",
      "metadata": {
        "id": "3d0543a5",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "b3fb45b2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3fb45b2",
        "outputId": "2416218d-5bf4-48aa-fd24-8899b2af453e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Action inter is descrete 4\n",
            "Shape of Observation is (8,)\n"
          ]
        }
      ],
      "source": [
        "print(\"The Action inter is descrete {}\".format(env.action_space.n))\n",
        "print(\"Shape of Observation is {}\".format(env.observation_space.sample().shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b9ff9c9",
      "metadata": {
        "id": "8b9ff9c9"
      },
      "source": [
        "<h1> Baseline Model. </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "d35101af",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d35101af",
        "outputId": "4873fc86-1916-4745-d40f-732c7dca2d00",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Reward after 10 max run is -0.034079400484295454\n"
          ]
        }
      ],
      "source": [
        "rewards = []\n",
        "obs = env.reset()\n",
        "done = False\n",
        "MAX_RUN = 10\n",
        "\n",
        "for i in range(MAX_RUN):\n",
        "    while not done:\n",
        "        env.render()\n",
        "        action_sample = env.action_space.sample()\n",
        "        # let's take a step in the environment\n",
        "        obs, rwd, done, info ,_  = env.step(action_sample)\n",
        "        rewards.append(rwd)\n",
        "env.close()\n",
        "print(\"Mean Reward after {} max run is {}\".format(MAX_RUN, np.mean(np.array(rewards))))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac737551",
      "metadata": {
        "id": "ac737551"
      },
      "source": [
        "<h1> Reinforcement Learning For Training The Model </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "bdaa0e55",
      "metadata": {
        "id": "bdaa0e55"
      },
      "outputs": [],
      "source": [
        "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
        "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
        "\n",
        "    :param check_freq: (int)\n",
        "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
        "      It must contains the file created by the ``Monitor`` wrapper.\n",
        "    :param verbose: (int)\n",
        "    \"\"\"\n",
        "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
        "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
        "        self.check_freq = check_freq\n",
        "        self.log_dir = log_dir\n",
        "        self.save_path = os.path.join(log_dir, 'best_model')\n",
        "        self.best_mean_reward = -np.inf\n",
        "\n",
        "    def _init_callback(self) -> None:\n",
        "        # Create folder if needed\n",
        "        if self.save_path is not None:\n",
        "            os.makedirs(self.save_path, exist_ok=True)\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.n_calls % self.check_freq == 0:\n",
        "\n",
        "          # Retrieve training reward\n",
        "          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
        "          if len(x) > 0:\n",
        "              # Mean training reward over the last 100 episodes\n",
        "              mean_reward = np.mean(y[-100:])\n",
        "              if self.verbose > 0:\n",
        "                print(f\"Num timesteps: {self.num_timesteps}\")\n",
        "                print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n",
        "\n",
        "              # New best model, you could save the agent here\n",
        "              if mean_reward > self.best_mean_reward:\n",
        "                  self.best_mean_reward = mean_reward\n",
        "                  # Example for saving best model\n",
        "                  if self.verbose > 0:\n",
        "                    print(f\"Saving new best model to {self.save_path}.zip\")\n",
        "                  self.model.save(self.save_path)\n",
        "\n",
        "        return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "2d311d6a",
      "metadata": {
        "id": "2d311d6a",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "Logging to ./TensorBoardLog/A2C_5\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 110      |\n",
            "|    ep_rew_mean        | -158     |\n",
            "| time/                 |          |\n",
            "|    fps                | 2848     |\n",
            "|    iterations         | 100      |\n",
            "|    time_elapsed       | 1        |\n",
            "|    total_timesteps    | 4000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.12    |\n",
            "|    explained_variance | 0.0863   |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 99       |\n",
            "|    policy_loss        | 4.36     |\n",
            "|    value_loss         | 678      |\n",
            "------------------------------------\n",
            "Num timesteps: 8000\n",
            "Best mean reward: -inf - Last mean reward per episode: -121.72\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 139      |\n",
            "|    ep_rew_mean        | -122     |\n",
            "| time/                 |          |\n",
            "|    fps                | 2403     |\n",
            "|    iterations         | 200      |\n",
            "|    time_elapsed       | 3        |\n",
            "|    total_timesteps    | 8000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.901   |\n",
            "|    explained_variance | 0.0939   |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 199      |\n",
            "|    policy_loss        | -0.842   |\n",
            "|    value_loss         | 40.4     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 163      |\n",
            "|    ep_rew_mean        | -118     |\n",
            "| time/                 |          |\n",
            "|    fps                | 2048     |\n",
            "|    iterations         | 300      |\n",
            "|    time_elapsed       | 5        |\n",
            "|    total_timesteps    | 12000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.866   |\n",
            "|    explained_variance | 0.243    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 299      |\n",
            "|    policy_loss        | -0.179   |\n",
            "|    value_loss         | 38.1     |\n",
            "------------------------------------\n",
            "Num timesteps: 16000\n",
            "Best mean reward: -121.72 - Last mean reward per episode: -120.99\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 193      |\n",
            "|    ep_rew_mean        | -121     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1912     |\n",
            "|    iterations         | 400      |\n",
            "|    time_elapsed       | 8        |\n",
            "|    total_timesteps    | 16000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.723   |\n",
            "|    explained_variance | 0.483    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 399      |\n",
            "|    policy_loss        | -1.46    |\n",
            "|    value_loss         | 12.7     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 205      |\n",
            "|    ep_rew_mean        | -118     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1842     |\n",
            "|    iterations         | 500      |\n",
            "|    time_elapsed       | 10       |\n",
            "|    total_timesteps    | 20000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.652   |\n",
            "|    explained_variance | 0.801    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 499      |\n",
            "|    policy_loss        | -1.69    |\n",
            "|    value_loss         | 15.1     |\n",
            "------------------------------------\n",
            "Num timesteps: 24000\n",
            "Best mean reward: -120.99 - Last mean reward per episode: -98.51\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 229      |\n",
            "|    ep_rew_mean        | -98.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1728     |\n",
            "|    iterations         | 600      |\n",
            "|    time_elapsed       | 13       |\n",
            "|    total_timesteps    | 24000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.694   |\n",
            "|    explained_variance | 0.724    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 599      |\n",
            "|    policy_loss        | -1.99    |\n",
            "|    value_loss         | 32.1     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 251      |\n",
            "|    ep_rew_mean        | -87.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1692     |\n",
            "|    iterations         | 700      |\n",
            "|    time_elapsed       | 16       |\n",
            "|    total_timesteps    | 28000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.622   |\n",
            "|    explained_variance | 0.607    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 699      |\n",
            "|    policy_loss        | 1.66     |\n",
            "|    value_loss         | 62.9     |\n",
            "------------------------------------\n",
            "Num timesteps: 32000\n",
            "Best mean reward: -98.51 - Last mean reward per episode: -76.93\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 266      |\n",
            "|    ep_rew_mean        | -76.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1634     |\n",
            "|    iterations         | 800      |\n",
            "|    time_elapsed       | 19       |\n",
            "|    total_timesteps    | 32000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.77    |\n",
            "|    explained_variance | 0.832    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 799      |\n",
            "|    policy_loss        | -0.175   |\n",
            "|    value_loss         | 35.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 296      |\n",
            "|    ep_rew_mean        | -65      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1641     |\n",
            "|    iterations         | 900      |\n",
            "|    time_elapsed       | 21       |\n",
            "|    total_timesteps    | 36000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.631   |\n",
            "|    explained_variance | 0.465    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 899      |\n",
            "|    policy_loss        | 0.386    |\n",
            "|    value_loss         | 63.8     |\n",
            "------------------------------------\n",
            "Num timesteps: 40000\n",
            "Best mean reward: -76.93 - Last mean reward per episode: -66.07\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 299      |\n",
            "|    ep_rew_mean        | -66.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1629     |\n",
            "|    iterations         | 1000     |\n",
            "|    time_elapsed       | 24       |\n",
            "|    total_timesteps    | 40000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.795   |\n",
            "|    explained_variance | 0.859    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 999      |\n",
            "|    policy_loss        | -1.04    |\n",
            "|    value_loss         | 18.8     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 314      |\n",
            "|    ep_rew_mean        | -62.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1591     |\n",
            "|    iterations         | 1100     |\n",
            "|    time_elapsed       | 27       |\n",
            "|    total_timesteps    | 44000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.732   |\n",
            "|    explained_variance | 0.919    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 1099     |\n",
            "|    policy_loss        | 0.347    |\n",
            "|    value_loss         | 18.3     |\n",
            "------------------------------------\n",
            "Num timesteps: 48000\n",
            "Best mean reward: -66.07 - Last mean reward per episode: -57.35\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 321      |\n",
            "|    ep_rew_mean        | -57.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1557     |\n",
            "|    iterations         | 1200     |\n",
            "|    time_elapsed       | 30       |\n",
            "|    total_timesteps    | 48000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.648   |\n",
            "|    explained_variance | 0.849    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 1199     |\n",
            "|    policy_loss        | 2.15     |\n",
            "|    value_loss         | 48.7     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 341      |\n",
            "|    ep_rew_mean        | -49      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1568     |\n",
            "|    iterations         | 1300     |\n",
            "|    time_elapsed       | 33       |\n",
            "|    total_timesteps    | 52000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.629   |\n",
            "|    explained_variance | 0.907    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 1299     |\n",
            "|    policy_loss        | 0.46     |\n",
            "|    value_loss         | 23.7     |\n",
            "------------------------------------\n",
            "Num timesteps: 56000\n",
            "Best mean reward: -57.35 - Last mean reward per episode: -51.99\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 315      |\n",
            "|    ep_rew_mean        | -52      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1586     |\n",
            "|    iterations         | 1400     |\n",
            "|    time_elapsed       | 35       |\n",
            "|    total_timesteps    | 56000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.658   |\n",
            "|    explained_variance | 0.833    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 1399     |\n",
            "|    policy_loss        | -0.443   |\n",
            "|    value_loss         | 54.3     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 316      |\n",
            "|    ep_rew_mean        | -42.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1594     |\n",
            "|    iterations         | 1500     |\n",
            "|    time_elapsed       | 37       |\n",
            "|    total_timesteps    | 60000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.329   |\n",
            "|    explained_variance | -0.00977 |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 1499     |\n",
            "|    policy_loss        | -2.3     |\n",
            "|    value_loss         | 624      |\n",
            "------------------------------------\n",
            "Num timesteps: 64000\n",
            "Best mean reward: -51.99 - Last mean reward per episode: -44.46\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 287      |\n",
            "|    ep_rew_mean        | -44.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1613     |\n",
            "|    iterations         | 1600     |\n",
            "|    time_elapsed       | 39       |\n",
            "|    total_timesteps    | 64000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.74    |\n",
            "|    explained_variance | 0.929    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 1599     |\n",
            "|    policy_loss        | -0.692   |\n",
            "|    value_loss         | 25.9     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 290      |\n",
            "|    ep_rew_mean        | -25.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1622     |\n",
            "|    iterations         | 1700     |\n",
            "|    time_elapsed       | 41       |\n",
            "|    total_timesteps    | 68000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.717   |\n",
            "|    explained_variance | 0.823    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 1699     |\n",
            "|    policy_loss        | 0.0539   |\n",
            "|    value_loss         | 42.6     |\n",
            "------------------------------------\n",
            "Num timesteps: 72000\n",
            "Best mean reward: -44.46 - Last mean reward per episode: -27.06\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 231      |\n",
            "|    ep_rew_mean        | -27.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1629     |\n",
            "|    iterations         | 1800     |\n",
            "|    time_elapsed       | 44       |\n",
            "|    total_timesteps    | 72000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.631   |\n",
            "|    explained_variance | 0.891    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 1799     |\n",
            "|    policy_loss        | 0.867    |\n",
            "|    value_loss         | 36.2     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 217      |\n",
            "|    ep_rew_mean        | -15.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1640     |\n",
            "|    iterations         | 1900     |\n",
            "|    time_elapsed       | 46       |\n",
            "|    total_timesteps    | 76000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.537   |\n",
            "|    explained_variance | 0.18     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 1899     |\n",
            "|    policy_loss        | -12      |\n",
            "|    value_loss         | 978      |\n",
            "------------------------------------\n",
            "Num timesteps: 80000\n",
            "Best mean reward: -27.06 - Last mean reward per episode: -3.99\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 225      |\n",
            "|    ep_rew_mean        | -3.99    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1652     |\n",
            "|    iterations         | 2000     |\n",
            "|    time_elapsed       | 48       |\n",
            "|    total_timesteps    | 80000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.449   |\n",
            "|    explained_variance | 0.925    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 1999     |\n",
            "|    policy_loss        | 4.19     |\n",
            "|    value_loss         | 49.1     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 214      |\n",
            "|    ep_rew_mean        | 7.77     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1668     |\n",
            "|    iterations         | 2100     |\n",
            "|    time_elapsed       | 50       |\n",
            "|    total_timesteps    | 84000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.323   |\n",
            "|    explained_variance | 0.96     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 2099     |\n",
            "|    policy_loss        | -0.0625  |\n",
            "|    value_loss         | 12.7     |\n",
            "------------------------------------\n",
            "Num timesteps: 88000\n",
            "Best mean reward: -3.99 - Last mean reward per episode: 15.28\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 215      |\n",
            "|    ep_rew_mean        | 15.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1681     |\n",
            "|    iterations         | 2200     |\n",
            "|    time_elapsed       | 52       |\n",
            "|    total_timesteps    | 88000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.664   |\n",
            "|    explained_variance | 0.881    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 2199     |\n",
            "|    policy_loss        | 0.153    |\n",
            "|    value_loss         | 58.9     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 216      |\n",
            "|    ep_rew_mean        | 23.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1698     |\n",
            "|    iterations         | 2300     |\n",
            "|    time_elapsed       | 54       |\n",
            "|    total_timesteps    | 92000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.745   |\n",
            "|    explained_variance | 0.536    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 2299     |\n",
            "|    policy_loss        | 0.291    |\n",
            "|    value_loss         | 585      |\n",
            "------------------------------------\n",
            "Num timesteps: 96000\n",
            "Best mean reward: 15.28 - Last mean reward per episode: 25.42\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 220      |\n",
            "|    ep_rew_mean        | 25.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1695     |\n",
            "|    iterations         | 2400     |\n",
            "|    time_elapsed       | 56       |\n",
            "|    total_timesteps    | 96000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.668   |\n",
            "|    explained_variance | 0.737    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 2399     |\n",
            "|    policy_loss        | -0.909   |\n",
            "|    value_loss         | 20.2     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 244      |\n",
            "|    ep_rew_mean        | 37.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1688     |\n",
            "|    iterations         | 2500     |\n",
            "|    time_elapsed       | 59       |\n",
            "|    total_timesteps    | 100000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.537   |\n",
            "|    explained_variance | 0.953    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 2499     |\n",
            "|    policy_loss        | 1.14     |\n",
            "|    value_loss         | 20.2     |\n",
            "------------------------------------\n",
            "Num timesteps: 104000\n",
            "Best mean reward: 25.42 - Last mean reward per episode: 39.42\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 237      |\n",
            "|    ep_rew_mean        | 39.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1688     |\n",
            "|    iterations         | 2600     |\n",
            "|    time_elapsed       | 61       |\n",
            "|    total_timesteps    | 104000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.609   |\n",
            "|    explained_variance | 0.93     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 2599     |\n",
            "|    policy_loss        | -0.0865  |\n",
            "|    value_loss         | 17.6     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 257      |\n",
            "|    ep_rew_mean        | 52       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1675     |\n",
            "|    iterations         | 2700     |\n",
            "|    time_elapsed       | 64       |\n",
            "|    total_timesteps    | 108000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.469   |\n",
            "|    explained_variance | 0.941    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 2699     |\n",
            "|    policy_loss        | 1.99     |\n",
            "|    value_loss         | 41.7     |\n",
            "------------------------------------\n",
            "Num timesteps: 112000\n",
            "Best mean reward: 39.42 - Last mean reward per episode: 52.60\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 250      |\n",
            "|    ep_rew_mean        | 52.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1684     |\n",
            "|    iterations         | 2800     |\n",
            "|    time_elapsed       | 66       |\n",
            "|    total_timesteps    | 112000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.497   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 2799     |\n",
            "|    policy_loss        | -0.843   |\n",
            "|    value_loss         | 6.02     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 255      |\n",
            "|    ep_rew_mean        | 54.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1689     |\n",
            "|    iterations         | 2900     |\n",
            "|    time_elapsed       | 68       |\n",
            "|    total_timesteps    | 116000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.727   |\n",
            "|    explained_variance | 0.961    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 2899     |\n",
            "|    policy_loss        | -0.91    |\n",
            "|    value_loss         | 12.3     |\n",
            "------------------------------------\n",
            "Num timesteps: 120000\n",
            "Best mean reward: 52.60 - Last mean reward per episode: 69.64\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 280      |\n",
            "|    ep_rew_mean        | 69.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1689     |\n",
            "|    iterations         | 3000     |\n",
            "|    time_elapsed       | 71       |\n",
            "|    total_timesteps    | 120000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.766   |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 2999     |\n",
            "|    policy_loss        | 0.271    |\n",
            "|    value_loss         | 6.01     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 277      |\n",
            "|    ep_rew_mean        | 88.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1698     |\n",
            "|    iterations         | 3100     |\n",
            "|    time_elapsed       | 73       |\n",
            "|    total_timesteps    | 124000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.667   |\n",
            "|    explained_variance | 0.977    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 3099     |\n",
            "|    policy_loss        | -0.505   |\n",
            "|    value_loss         | 6.37     |\n",
            "------------------------------------\n",
            "Num timesteps: 128000\n",
            "Best mean reward: 69.64 - Last mean reward per episode: 92.49\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 263      |\n",
            "|    ep_rew_mean        | 92.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1703     |\n",
            "|    iterations         | 3200     |\n",
            "|    time_elapsed       | 75       |\n",
            "|    total_timesteps    | 128000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.84    |\n",
            "|    explained_variance | 0.972    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 3199     |\n",
            "|    policy_loss        | -0.618   |\n",
            "|    value_loss         | 5.88     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 274      |\n",
            "|    ep_rew_mean        | 96.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1711     |\n",
            "|    iterations         | 3300     |\n",
            "|    time_elapsed       | 77       |\n",
            "|    total_timesteps    | 132000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.789   |\n",
            "|    explained_variance | 0.913    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 3299     |\n",
            "|    policy_loss        | 1.07     |\n",
            "|    value_loss         | 14.8     |\n",
            "------------------------------------\n",
            "Num timesteps: 136000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: 80.40\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 250      |\n",
            "|    ep_rew_mean        | 80.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1720     |\n",
            "|    iterations         | 3400     |\n",
            "|    time_elapsed       | 79       |\n",
            "|    total_timesteps    | 136000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1       |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 3399     |\n",
            "|    policy_loss        | 0.661    |\n",
            "|    value_loss         | 2.5      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 245      |\n",
            "|    ep_rew_mean        | 71.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1731     |\n",
            "|    iterations         | 3500     |\n",
            "|    time_elapsed       | 80       |\n",
            "|    total_timesteps    | 140000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.05    |\n",
            "|    explained_variance | 0.794    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 3499     |\n",
            "|    policy_loss        | -0.335   |\n",
            "|    value_loss         | 4.15     |\n",
            "------------------------------------\n",
            "Num timesteps: 144000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: 51.15\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 228      |\n",
            "|    ep_rew_mean        | 51.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1737     |\n",
            "|    iterations         | 3600     |\n",
            "|    time_elapsed       | 82       |\n",
            "|    total_timesteps    | 144000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.984   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 3599     |\n",
            "|    policy_loss        | 0.272    |\n",
            "|    value_loss         | 5.82     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 213      |\n",
            "|    ep_rew_mean        | 26.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1751     |\n",
            "|    iterations         | 3700     |\n",
            "|    time_elapsed       | 84       |\n",
            "|    total_timesteps    | 148000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.931   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 3699     |\n",
            "|    policy_loss        | -0.181   |\n",
            "|    value_loss         | 5.79     |\n",
            "------------------------------------\n",
            "Num timesteps: 152000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: 0.58\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 199      |\n",
            "|    ep_rew_mean        | 0.579    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1762     |\n",
            "|    iterations         | 3800     |\n",
            "|    time_elapsed       | 86       |\n",
            "|    total_timesteps    | 152000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.948   |\n",
            "|    explained_variance | 0.952    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 3799     |\n",
            "|    policy_loss        | 0.379    |\n",
            "|    value_loss         | 12.2     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 214      |\n",
            "|    ep_rew_mean        | -17.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1767     |\n",
            "|    iterations         | 3900     |\n",
            "|    time_elapsed       | 88       |\n",
            "|    total_timesteps    | 156000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.961   |\n",
            "|    explained_variance | 0.92     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 3899     |\n",
            "|    policy_loss        | 0.117    |\n",
            "|    value_loss         | 6.29     |\n",
            "------------------------------------\n",
            "Num timesteps: 160000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -27.65\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 228      |\n",
            "|    ep_rew_mean        | -27.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1758     |\n",
            "|    iterations         | 4000     |\n",
            "|    time_elapsed       | 90       |\n",
            "|    total_timesteps    | 160000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.891   |\n",
            "|    explained_variance | 0.965    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 3999     |\n",
            "|    policy_loss        | 0.189    |\n",
            "|    value_loss         | 10.4     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 248      |\n",
            "|    ep_rew_mean        | -29.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1736     |\n",
            "|    iterations         | 4100     |\n",
            "|    time_elapsed       | 94       |\n",
            "|    total_timesteps    | 164000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.825   |\n",
            "|    explained_variance | 0.88     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 4099     |\n",
            "|    policy_loss        | -0.757   |\n",
            "|    value_loss         | 6.94     |\n",
            "------------------------------------\n",
            "Num timesteps: 168000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -39.94\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 278      |\n",
            "|    ep_rew_mean        | -39.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1695     |\n",
            "|    iterations         | 4200     |\n",
            "|    time_elapsed       | 99       |\n",
            "|    total_timesteps    | 168000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.967   |\n",
            "|    explained_variance | 0.965    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 4199     |\n",
            "|    policy_loss        | -0.364   |\n",
            "|    value_loss         | 2.54     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 293      |\n",
            "|    ep_rew_mean        | -45.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1663     |\n",
            "|    iterations         | 4300     |\n",
            "|    time_elapsed       | 103      |\n",
            "|    total_timesteps    | 172000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.736   |\n",
            "|    explained_variance | 0.922    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 4299     |\n",
            "|    policy_loss        | 0.432    |\n",
            "|    value_loss         | 2.52     |\n",
            "------------------------------------\n",
            "Num timesteps: 176000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -52.98\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 323      |\n",
            "|    ep_rew_mean        | -53      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1627     |\n",
            "|    iterations         | 4400     |\n",
            "|    time_elapsed       | 108      |\n",
            "|    total_timesteps    | 176000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.876   |\n",
            "|    explained_variance | 0.945    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 4399     |\n",
            "|    policy_loss        | -0.514   |\n",
            "|    value_loss         | 6.25     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 348      |\n",
            "|    ep_rew_mean        | -62.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1600     |\n",
            "|    iterations         | 4500     |\n",
            "|    time_elapsed       | 112      |\n",
            "|    total_timesteps    | 180000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.983   |\n",
            "|    explained_variance | 0.906    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 4499     |\n",
            "|    policy_loss        | -0.486   |\n",
            "|    value_loss         | 2.84     |\n",
            "------------------------------------\n",
            "Num timesteps: 184000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -74.63\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 377      |\n",
            "|    ep_rew_mean        | -74.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1588     |\n",
            "|    iterations         | 4600     |\n",
            "|    time_elapsed       | 115      |\n",
            "|    total_timesteps    | 184000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.856   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 4599     |\n",
            "|    policy_loss        | 0.0173   |\n",
            "|    value_loss         | 2.42     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 393      |\n",
            "|    ep_rew_mean        | -79      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1561     |\n",
            "|    iterations         | 4700     |\n",
            "|    time_elapsed       | 120      |\n",
            "|    total_timesteps    | 188000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.934   |\n",
            "|    explained_variance | 0.894    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 4699     |\n",
            "|    policy_loss        | -0.185   |\n",
            "|    value_loss         | 5.52     |\n",
            "------------------------------------\n",
            "Num timesteps: 192000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -94.08\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 441      |\n",
            "|    ep_rew_mean        | -94.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1549     |\n",
            "|    iterations         | 4800     |\n",
            "|    time_elapsed       | 123      |\n",
            "|    total_timesteps    | 192000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.748   |\n",
            "|    explained_variance | 0.879    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 4799     |\n",
            "|    policy_loss        | 0.184    |\n",
            "|    value_loss         | 5.21     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 459      |\n",
            "|    ep_rew_mean        | -114     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1546     |\n",
            "|    iterations         | 4900     |\n",
            "|    time_elapsed       | 126      |\n",
            "|    total_timesteps    | 196000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.753   |\n",
            "|    explained_variance | 0.952    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 4899     |\n",
            "|    policy_loss        | -0.585   |\n",
            "|    value_loss         | 6.01     |\n",
            "------------------------------------\n",
            "Num timesteps: 200000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -123.42\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 469      |\n",
            "|    ep_rew_mean        | -123     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1528     |\n",
            "|    iterations         | 5000     |\n",
            "|    time_elapsed       | 130      |\n",
            "|    total_timesteps    | 200000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.928   |\n",
            "|    explained_variance | 0.931    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 4999     |\n",
            "|    policy_loss        | -0.308   |\n",
            "|    value_loss         | 6.52     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 494      |\n",
            "|    ep_rew_mean        | -131     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1527     |\n",
            "|    iterations         | 5100     |\n",
            "|    time_elapsed       | 133      |\n",
            "|    total_timesteps    | 204000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.856   |\n",
            "|    explained_variance | 0.946    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 5099     |\n",
            "|    policy_loss        | -1.19    |\n",
            "|    value_loss         | 3.02     |\n",
            "------------------------------------\n",
            "Num timesteps: 208000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -131.01\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 497      |\n",
            "|    ep_rew_mean        | -131     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1515     |\n",
            "|    iterations         | 5200     |\n",
            "|    time_elapsed       | 137      |\n",
            "|    total_timesteps    | 208000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.01    |\n",
            "|    explained_variance | 0.975    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 5199     |\n",
            "|    policy_loss        | -0.181   |\n",
            "|    value_loss         | 1.92     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 523      |\n",
            "|    ep_rew_mean        | -141     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1507     |\n",
            "|    iterations         | 5300     |\n",
            "|    time_elapsed       | 140      |\n",
            "|    total_timesteps    | 212000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.778   |\n",
            "|    explained_variance | 0.895    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 5299     |\n",
            "|    policy_loss        | 0.607    |\n",
            "|    value_loss         | 3.36     |\n",
            "------------------------------------\n",
            "Num timesteps: 216000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -148.32\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 504      |\n",
            "|    ep_rew_mean        | -148     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1497     |\n",
            "|    iterations         | 5400     |\n",
            "|    time_elapsed       | 144      |\n",
            "|    total_timesteps    | 216000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.834   |\n",
            "|    explained_variance | 0.903    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 5399     |\n",
            "|    policy_loss        | -0.367   |\n",
            "|    value_loss         | 3.5      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 458      |\n",
            "|    ep_rew_mean        | -152     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1488     |\n",
            "|    iterations         | 5500     |\n",
            "|    time_elapsed       | 147      |\n",
            "|    total_timesteps    | 220000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.555   |\n",
            "|    explained_variance | 0.969    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 5499     |\n",
            "|    policy_loss        | 0.339    |\n",
            "|    value_loss         | 4.1      |\n",
            "------------------------------------\n",
            "Num timesteps: 224000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -154.66\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 421      |\n",
            "|    ep_rew_mean        | -155     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1484     |\n",
            "|    iterations         | 5600     |\n",
            "|    time_elapsed       | 150      |\n",
            "|    total_timesteps    | 224000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.904   |\n",
            "|    explained_variance | 0.958    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 5599     |\n",
            "|    policy_loss        | -1.18    |\n",
            "|    value_loss         | 2.86     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 425      |\n",
            "|    ep_rew_mean        | -157     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1464     |\n",
            "|    iterations         | 5700     |\n",
            "|    time_elapsed       | 155      |\n",
            "|    total_timesteps    | 228000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.854   |\n",
            "|    explained_variance | 0.974    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 5699     |\n",
            "|    policy_loss        | 0.159    |\n",
            "|    value_loss         | 2.96     |\n",
            "------------------------------------\n",
            "Num timesteps: 232000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -159.20\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 398      |\n",
            "|    ep_rew_mean        | -159     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1456     |\n",
            "|    iterations         | 5800     |\n",
            "|    time_elapsed       | 159      |\n",
            "|    total_timesteps    | 232000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.74    |\n",
            "|    explained_variance | 0.969    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 5799     |\n",
            "|    policy_loss        | -1.07    |\n",
            "|    value_loss         | 3.56     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 396      |\n",
            "|    ep_rew_mean        | -156     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1442     |\n",
            "|    iterations         | 5900     |\n",
            "|    time_elapsed       | 163      |\n",
            "|    total_timesteps    | 236000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.867   |\n",
            "|    explained_variance | 0.874    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 5899     |\n",
            "|    policy_loss        | -0.377   |\n",
            "|    value_loss         | 4.51     |\n",
            "------------------------------------\n",
            "Num timesteps: 240000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -151.14\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 399      |\n",
            "|    ep_rew_mean        | -151     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1435     |\n",
            "|    iterations         | 6000     |\n",
            "|    time_elapsed       | 167      |\n",
            "|    total_timesteps    | 240000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.685   |\n",
            "|    explained_variance | 0.943    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 5999     |\n",
            "|    policy_loss        | -0.188   |\n",
            "|    value_loss         | 2        |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 397      |\n",
            "|    ep_rew_mean        | -152     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1421     |\n",
            "|    iterations         | 6100     |\n",
            "|    time_elapsed       | 171      |\n",
            "|    total_timesteps    | 244000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.874   |\n",
            "|    explained_variance | 0.958    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 6099     |\n",
            "|    policy_loss        | 0.249    |\n",
            "|    value_loss         | 2.3      |\n",
            "------------------------------------\n",
            "Num timesteps: 248000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -152.41\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 419      |\n",
            "|    ep_rew_mean        | -152     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1413     |\n",
            "|    iterations         | 6200     |\n",
            "|    time_elapsed       | 175      |\n",
            "|    total_timesteps    | 248000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.738   |\n",
            "|    explained_variance | 0.948    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 6199     |\n",
            "|    policy_loss        | -0.648   |\n",
            "|    value_loss         | 2.94     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 389      |\n",
            "|    ep_rew_mean        | -150     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1410     |\n",
            "|    iterations         | 6300     |\n",
            "|    time_elapsed       | 178      |\n",
            "|    total_timesteps    | 252000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.763   |\n",
            "|    explained_variance | 0.805    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 6299     |\n",
            "|    policy_loss        | -0.0862  |\n",
            "|    value_loss         | 5.1      |\n",
            "------------------------------------\n",
            "Num timesteps: 256000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -149.86\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 397      |\n",
            "|    ep_rew_mean        | -150     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1400     |\n",
            "|    iterations         | 6400     |\n",
            "|    time_elapsed       | 182      |\n",
            "|    total_timesteps    | 256000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.621   |\n",
            "|    explained_variance | 0.881    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 6399     |\n",
            "|    policy_loss        | 0.397    |\n",
            "|    value_loss         | 6.21     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 409      |\n",
            "|    ep_rew_mean        | -145     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1398     |\n",
            "|    iterations         | 6500     |\n",
            "|    time_elapsed       | 185      |\n",
            "|    total_timesteps    | 260000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.82    |\n",
            "|    explained_variance | 0.952    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 6499     |\n",
            "|    policy_loss        | 0.225    |\n",
            "|    value_loss         | 3.46     |\n",
            "------------------------------------\n",
            "Num timesteps: 264000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -144.14\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 406      |\n",
            "|    ep_rew_mean        | -144     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1399     |\n",
            "|    iterations         | 6600     |\n",
            "|    time_elapsed       | 188      |\n",
            "|    total_timesteps    | 264000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.755   |\n",
            "|    explained_variance | 0.911    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 6599     |\n",
            "|    policy_loss        | -0.145   |\n",
            "|    value_loss         | 2.28     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 401      |\n",
            "|    ep_rew_mean        | -143     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1389     |\n",
            "|    iterations         | 6700     |\n",
            "|    time_elapsed       | 192      |\n",
            "|    total_timesteps    | 268000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.549   |\n",
            "|    explained_variance | 0.287    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 6699     |\n",
            "|    policy_loss        | -3.31    |\n",
            "|    value_loss         | 403      |\n",
            "------------------------------------\n",
            "Num timesteps: 272000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -140.61\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 403      |\n",
            "|    ep_rew_mean        | -141     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1386     |\n",
            "|    iterations         | 6800     |\n",
            "|    time_elapsed       | 196      |\n",
            "|    total_timesteps    | 272000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.753   |\n",
            "|    explained_variance | 0.941    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 6799     |\n",
            "|    policy_loss        | 0.0763   |\n",
            "|    value_loss         | 1.99     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 407      |\n",
            "|    ep_rew_mean        | -144     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1375     |\n",
            "|    iterations         | 6900     |\n",
            "|    time_elapsed       | 200      |\n",
            "|    total_timesteps    | 276000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.748   |\n",
            "|    explained_variance | 0.364    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 6899     |\n",
            "|    policy_loss        | -13.2    |\n",
            "|    value_loss         | 407      |\n",
            "------------------------------------\n",
            "Num timesteps: 280000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -146.48\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 424      |\n",
            "|    ep_rew_mean        | -146     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1367     |\n",
            "|    iterations         | 7000     |\n",
            "|    time_elapsed       | 204      |\n",
            "|    total_timesteps    | 280000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.732   |\n",
            "|    explained_variance | 0.945    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 6999     |\n",
            "|    policy_loss        | -0.362   |\n",
            "|    value_loss         | 4.12     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 413      |\n",
            "|    ep_rew_mean        | -149     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1352     |\n",
            "|    iterations         | 7100     |\n",
            "|    time_elapsed       | 209      |\n",
            "|    total_timesteps    | 284000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.76    |\n",
            "|    explained_variance | 0.958    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 7099     |\n",
            "|    policy_loss        | -0.0577  |\n",
            "|    value_loss         | 3.52     |\n",
            "------------------------------------\n",
            "Num timesteps: 288000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -148.52\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 428      |\n",
            "|    ep_rew_mean        | -149     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1343     |\n",
            "|    iterations         | 7200     |\n",
            "|    time_elapsed       | 214      |\n",
            "|    total_timesteps    | 288000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.731   |\n",
            "|    explained_variance | 0.961    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 7199     |\n",
            "|    policy_loss        | 0.0892   |\n",
            "|    value_loss         | 1.74     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 420      |\n",
            "|    ep_rew_mean        | -149     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1338     |\n",
            "|    iterations         | 7300     |\n",
            "|    time_elapsed       | 218      |\n",
            "|    total_timesteps    | 292000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.631   |\n",
            "|    explained_variance | 0.927    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 7299     |\n",
            "|    policy_loss        | 0.642    |\n",
            "|    value_loss         | 3.42     |\n",
            "------------------------------------\n",
            "Num timesteps: 296000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -149.64\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 446      |\n",
            "|    ep_rew_mean        | -150     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1327     |\n",
            "|    iterations         | 7400     |\n",
            "|    time_elapsed       | 222      |\n",
            "|    total_timesteps    | 296000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.732   |\n",
            "|    explained_variance | 0.931    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 7399     |\n",
            "|    policy_loss        | -1.47    |\n",
            "|    value_loss         | 4.38     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 462      |\n",
            "|    ep_rew_mean        | -151     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1319     |\n",
            "|    iterations         | 7500     |\n",
            "|    time_elapsed       | 227      |\n",
            "|    total_timesteps    | 300000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.763   |\n",
            "|    explained_variance | 0.863    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 7499     |\n",
            "|    policy_loss        | 0.422    |\n",
            "|    value_loss         | 4.06     |\n",
            "------------------------------------\n",
            "Num timesteps: 304000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -152.36\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 481      |\n",
            "|    ep_rew_mean        | -152     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1315     |\n",
            "|    iterations         | 7600     |\n",
            "|    time_elapsed       | 231      |\n",
            "|    total_timesteps    | 304000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.743   |\n",
            "|    explained_variance | 0.899    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 7599     |\n",
            "|    policy_loss        | 0.116    |\n",
            "|    value_loss         | 5.45     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 475      |\n",
            "|    ep_rew_mean        | -155     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1312     |\n",
            "|    iterations         | 7700     |\n",
            "|    time_elapsed       | 234      |\n",
            "|    total_timesteps    | 308000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.637   |\n",
            "|    explained_variance | 0.977    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 7699     |\n",
            "|    policy_loss        | -0.408   |\n",
            "|    value_loss         | 3.3      |\n",
            "------------------------------------\n",
            "Num timesteps: 312000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -156.88\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 501      |\n",
            "|    ep_rew_mean        | -157     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1304     |\n",
            "|    iterations         | 7800     |\n",
            "|    time_elapsed       | 239      |\n",
            "|    total_timesteps    | 312000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.715   |\n",
            "|    explained_variance | 0.978    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 7799     |\n",
            "|    policy_loss        | -0.273   |\n",
            "|    value_loss         | 2.28     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 501      |\n",
            "|    ep_rew_mean        | -157     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1305     |\n",
            "|    iterations         | 7900     |\n",
            "|    time_elapsed       | 242      |\n",
            "|    total_timesteps    | 316000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.741   |\n",
            "|    explained_variance | 0.971    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 7899     |\n",
            "|    policy_loss        | -0.333   |\n",
            "|    value_loss         | 2.59     |\n",
            "------------------------------------\n",
            "Num timesteps: 320000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -158.31\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 513      |\n",
            "|    ep_rew_mean        | -158     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1298     |\n",
            "|    iterations         | 8000     |\n",
            "|    time_elapsed       | 246      |\n",
            "|    total_timesteps    | 320000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.492   |\n",
            "|    explained_variance | 0.689    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 7999     |\n",
            "|    policy_loss        | -3.52    |\n",
            "|    value_loss         | 115      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 514      |\n",
            "|    ep_rew_mean        | -159     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1298     |\n",
            "|    iterations         | 8100     |\n",
            "|    time_elapsed       | 249      |\n",
            "|    total_timesteps    | 324000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.503   |\n",
            "|    explained_variance | 0.934    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 8099     |\n",
            "|    policy_loss        | 0.26     |\n",
            "|    value_loss         | 2.71     |\n",
            "------------------------------------\n",
            "Num timesteps: 328000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -158.80\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 515      |\n",
            "|    ep_rew_mean        | -159     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1290     |\n",
            "|    iterations         | 8200     |\n",
            "|    time_elapsed       | 254      |\n",
            "|    total_timesteps    | 328000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.707   |\n",
            "|    explained_variance | 0.926    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 8199     |\n",
            "|    policy_loss        | 0.0303   |\n",
            "|    value_loss         | 3.51     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 537      |\n",
            "|    ep_rew_mean        | -157     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1284     |\n",
            "|    iterations         | 8300     |\n",
            "|    time_elapsed       | 258      |\n",
            "|    total_timesteps    | 332000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.724   |\n",
            "|    explained_variance | 0.955    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 8299     |\n",
            "|    policy_loss        | 0.37     |\n",
            "|    value_loss         | 3.87     |\n",
            "------------------------------------\n",
            "Num timesteps: 336000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -153.81\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 521      |\n",
            "|    ep_rew_mean        | -154     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1287     |\n",
            "|    iterations         | 8400     |\n",
            "|    time_elapsed       | 261      |\n",
            "|    total_timesteps    | 336000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.697   |\n",
            "|    explained_variance | 0.952    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 8399     |\n",
            "|    policy_loss        | -0.203   |\n",
            "|    value_loss         | 1.91     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 512      |\n",
            "|    ep_rew_mean        | -156     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1281     |\n",
            "|    iterations         | 8500     |\n",
            "|    time_elapsed       | 265      |\n",
            "|    total_timesteps    | 340000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.614   |\n",
            "|    explained_variance | 0.933    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 8499     |\n",
            "|    policy_loss        | -0.0899  |\n",
            "|    value_loss         | 5.61     |\n",
            "------------------------------------\n",
            "Num timesteps: 344000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -155.24\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 522      |\n",
            "|    ep_rew_mean        | -155     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1278     |\n",
            "|    iterations         | 8600     |\n",
            "|    time_elapsed       | 269      |\n",
            "|    total_timesteps    | 344000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.666   |\n",
            "|    explained_variance | 0.973    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 8599     |\n",
            "|    policy_loss        | 0.337    |\n",
            "|    value_loss         | 2.52     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 510      |\n",
            "|    ep_rew_mean        | -152     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1274     |\n",
            "|    iterations         | 8700     |\n",
            "|    time_elapsed       | 272      |\n",
            "|    total_timesteps    | 348000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.559   |\n",
            "|    explained_variance | 0.853    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 8699     |\n",
            "|    policy_loss        | 0.248    |\n",
            "|    value_loss         | 3.3      |\n",
            "------------------------------------\n",
            "Num timesteps: 352000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -149.36\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 504      |\n",
            "|    ep_rew_mean        | -149     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1272     |\n",
            "|    iterations         | 8800     |\n",
            "|    time_elapsed       | 276      |\n",
            "|    total_timesteps    | 352000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.601   |\n",
            "|    explained_variance | 0.403    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 8799     |\n",
            "|    policy_loss        | -2.14    |\n",
            "|    value_loss         | 389      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 493      |\n",
            "|    ep_rew_mean        | -148     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1271     |\n",
            "|    iterations         | 8900     |\n",
            "|    time_elapsed       | 279      |\n",
            "|    total_timesteps    | 356000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.613   |\n",
            "|    explained_variance | 0.97     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 8899     |\n",
            "|    policy_loss        | -0.487   |\n",
            "|    value_loss         | 3.26     |\n",
            "------------------------------------\n",
            "Num timesteps: 360000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -146.18\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 465      |\n",
            "|    ep_rew_mean        | -146     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1271     |\n",
            "|    iterations         | 9000     |\n",
            "|    time_elapsed       | 283      |\n",
            "|    total_timesteps    | 360000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.694   |\n",
            "|    explained_variance | 0.852    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 8999     |\n",
            "|    policy_loss        | -0.544   |\n",
            "|    value_loss         | 6.14     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 479      |\n",
            "|    ep_rew_mean        | -149     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1271     |\n",
            "|    iterations         | 9100     |\n",
            "|    time_elapsed       | 286      |\n",
            "|    total_timesteps    | 364000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.64    |\n",
            "|    explained_variance | 0.955    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 9099     |\n",
            "|    policy_loss        | 0.0145   |\n",
            "|    value_loss         | 2.91     |\n",
            "------------------------------------\n",
            "Num timesteps: 368000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -143.26\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 462      |\n",
            "|    ep_rew_mean        | -143     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1271     |\n",
            "|    iterations         | 9200     |\n",
            "|    time_elapsed       | 289      |\n",
            "|    total_timesteps    | 368000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.672   |\n",
            "|    explained_variance | 0.96     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 9199     |\n",
            "|    policy_loss        | 0.0102   |\n",
            "|    value_loss         | 2.67     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 461      |\n",
            "|    ep_rew_mean        | -143     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1267     |\n",
            "|    iterations         | 9300     |\n",
            "|    time_elapsed       | 293      |\n",
            "|    total_timesteps    | 372000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.672   |\n",
            "|    explained_variance | 0.933    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 9299     |\n",
            "|    policy_loss        | -0.56    |\n",
            "|    value_loss         | 4.07     |\n",
            "------------------------------------\n",
            "Num timesteps: 376000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -145.26\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 460      |\n",
            "|    ep_rew_mean        | -145     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1262     |\n",
            "|    iterations         | 9400     |\n",
            "|    time_elapsed       | 297      |\n",
            "|    total_timesteps    | 376000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.654   |\n",
            "|    explained_variance | 0.972    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 9399     |\n",
            "|    policy_loss        | -0.286   |\n",
            "|    value_loss         | 1.71     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 456      |\n",
            "|    ep_rew_mean        | -146     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1260     |\n",
            "|    iterations         | 9500     |\n",
            "|    time_elapsed       | 301      |\n",
            "|    total_timesteps    | 380000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.608   |\n",
            "|    explained_variance | 0.946    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 9499     |\n",
            "|    policy_loss        | -0.176   |\n",
            "|    value_loss         | 4.84     |\n",
            "------------------------------------\n",
            "Num timesteps: 384000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -145.77\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 438      |\n",
            "|    ep_rew_mean        | -146     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1261     |\n",
            "|    iterations         | 9600     |\n",
            "|    time_elapsed       | 304      |\n",
            "|    total_timesteps    | 384000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.551   |\n",
            "|    explained_variance | 0.962    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 9599     |\n",
            "|    policy_loss        | 0.404    |\n",
            "|    value_loss         | 5.48     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 431      |\n",
            "|    ep_rew_mean        | -147     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1262     |\n",
            "|    iterations         | 9700     |\n",
            "|    time_elapsed       | 307      |\n",
            "|    total_timesteps    | 388000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.604   |\n",
            "|    explained_variance | 0.971    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 9699     |\n",
            "|    policy_loss        | -0.365   |\n",
            "|    value_loss         | 2.11     |\n",
            "------------------------------------\n",
            "Num timesteps: 392000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -149.21\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 430      |\n",
            "|    ep_rew_mean        | -149     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1261     |\n",
            "|    iterations         | 9800     |\n",
            "|    time_elapsed       | 310      |\n",
            "|    total_timesteps    | 392000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.689   |\n",
            "|    explained_variance | 0.941    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 9799     |\n",
            "|    policy_loss        | 0.12     |\n",
            "|    value_loss         | 4.58     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 418      |\n",
            "|    ep_rew_mean        | -151     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1263     |\n",
            "|    iterations         | 9900     |\n",
            "|    time_elapsed       | 313      |\n",
            "|    total_timesteps    | 396000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.725   |\n",
            "|    explained_variance | 0.935    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 9899     |\n",
            "|    policy_loss        | 0.562    |\n",
            "|    value_loss         | 4.91     |\n",
            "------------------------------------\n",
            "Num timesteps: 400000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -153.18\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 428      |\n",
            "|    ep_rew_mean        | -153     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1255     |\n",
            "|    iterations         | 10000    |\n",
            "|    time_elapsed       | 318      |\n",
            "|    total_timesteps    | 400000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.583   |\n",
            "|    explained_variance | 0.898    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 9999     |\n",
            "|    policy_loss        | 0.569    |\n",
            "|    value_loss         | 4.67     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 439      |\n",
            "|    ep_rew_mean        | -155     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1250     |\n",
            "|    iterations         | 10100    |\n",
            "|    time_elapsed       | 323      |\n",
            "|    total_timesteps    | 404000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.674   |\n",
            "|    explained_variance | 0.916    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 10099    |\n",
            "|    policy_loss        | 0.163    |\n",
            "|    value_loss         | 4.46     |\n",
            "------------------------------------\n",
            "Num timesteps: 408000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -150.40\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 430      |\n",
            "|    ep_rew_mean        | -150     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1249     |\n",
            "|    iterations         | 10200    |\n",
            "|    time_elapsed       | 326      |\n",
            "|    total_timesteps    | 408000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.44    |\n",
            "|    explained_variance | 0.769    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 10199    |\n",
            "|    policy_loss        | 0.0423   |\n",
            "|    value_loss         | 7.04     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 444      |\n",
            "|    ep_rew_mean        | -150     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1245     |\n",
            "|    iterations         | 10300    |\n",
            "|    time_elapsed       | 330      |\n",
            "|    total_timesteps    | 412000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.685   |\n",
            "|    explained_variance | 0.962    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 10299    |\n",
            "|    policy_loss        | 0.541    |\n",
            "|    value_loss         | 3.53     |\n",
            "------------------------------------\n",
            "Num timesteps: 416000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -152.74\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 467      |\n",
            "|    ep_rew_mean        | -153     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1240     |\n",
            "|    iterations         | 10400    |\n",
            "|    time_elapsed       | 335      |\n",
            "|    total_timesteps    | 416000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.514   |\n",
            "|    explained_variance | 0.937    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 10399    |\n",
            "|    policy_loss        | -0.439   |\n",
            "|    value_loss         | 3.49     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 450      |\n",
            "|    ep_rew_mean        | -151     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1236     |\n",
            "|    iterations         | 10500    |\n",
            "|    time_elapsed       | 339      |\n",
            "|    total_timesteps    | 420000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.706   |\n",
            "|    explained_variance | 0.97     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 10499    |\n",
            "|    policy_loss        | -0.197   |\n",
            "|    value_loss         | 1.72     |\n",
            "------------------------------------\n",
            "Num timesteps: 424000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -151.73\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 463      |\n",
            "|    ep_rew_mean        | -152     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1237     |\n",
            "|    iterations         | 10600    |\n",
            "|    time_elapsed       | 342      |\n",
            "|    total_timesteps    | 424000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.713   |\n",
            "|    explained_variance | 0.916    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 10599    |\n",
            "|    policy_loss        | -0.178   |\n",
            "|    value_loss         | 3.31     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 462      |\n",
            "|    ep_rew_mean        | -151     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1234     |\n",
            "|    iterations         | 10700    |\n",
            "|    time_elapsed       | 346      |\n",
            "|    total_timesteps    | 428000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.603   |\n",
            "|    explained_variance | 0.801    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 10699    |\n",
            "|    policy_loss        | 0.168    |\n",
            "|    value_loss         | 3.35     |\n",
            "------------------------------------\n",
            "Num timesteps: 432000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -149.78\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 499      |\n",
            "|    ep_rew_mean        | -150     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1228     |\n",
            "|    iterations         | 10800    |\n",
            "|    time_elapsed       | 351      |\n",
            "|    total_timesteps    | 432000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.609   |\n",
            "|    explained_variance | 0.952    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 10799    |\n",
            "|    policy_loss        | -0.145   |\n",
            "|    value_loss         | 4.72     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 513      |\n",
            "|    ep_rew_mean        | -148     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1226     |\n",
            "|    iterations         | 10900    |\n",
            "|    time_elapsed       | 355      |\n",
            "|    total_timesteps    | 436000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.658   |\n",
            "|    explained_variance | 0.943    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 10899    |\n",
            "|    policy_loss        | -0.0197  |\n",
            "|    value_loss         | 2.37     |\n",
            "------------------------------------\n",
            "Num timesteps: 440000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -144.64\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 534      |\n",
            "|    ep_rew_mean        | -145     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1223     |\n",
            "|    iterations         | 11000    |\n",
            "|    time_elapsed       | 359      |\n",
            "|    total_timesteps    | 440000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.561   |\n",
            "|    explained_variance | 0.949    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 10999    |\n",
            "|    policy_loss        | -0.0926  |\n",
            "|    value_loss         | 4.39     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 528      |\n",
            "|    ep_rew_mean        | -144     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1222     |\n",
            "|    iterations         | 11100    |\n",
            "|    time_elapsed       | 363      |\n",
            "|    total_timesteps    | 444000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.659   |\n",
            "|    explained_variance | 0.942    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 11099    |\n",
            "|    policy_loss        | 0.353    |\n",
            "|    value_loss         | 4.14     |\n",
            "------------------------------------\n",
            "Num timesteps: 448000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -142.21\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 541      |\n",
            "|    ep_rew_mean        | -142     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1220     |\n",
            "|    iterations         | 11200    |\n",
            "|    time_elapsed       | 367      |\n",
            "|    total_timesteps    | 448000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.656   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 11199    |\n",
            "|    policy_loss        | -0.937   |\n",
            "|    value_loss         | 4.74     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 555      |\n",
            "|    ep_rew_mean        | -143     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1220     |\n",
            "|    iterations         | 11300    |\n",
            "|    time_elapsed       | 370      |\n",
            "|    total_timesteps    | 452000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.623   |\n",
            "|    explained_variance | 0.811    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 11299    |\n",
            "|    policy_loss        | -0.106   |\n",
            "|    value_loss         | 3.36     |\n",
            "------------------------------------\n",
            "Num timesteps: 456000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -141.99\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 553      |\n",
            "|    ep_rew_mean        | -142     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1215     |\n",
            "|    iterations         | 11400    |\n",
            "|    time_elapsed       | 375      |\n",
            "|    total_timesteps    | 456000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.541   |\n",
            "|    explained_variance | 0.671    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 11399    |\n",
            "|    policy_loss        | 0.225    |\n",
            "|    value_loss         | 6.24     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 560      |\n",
            "|    ep_rew_mean        | -134     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1214     |\n",
            "|    iterations         | 11500    |\n",
            "|    time_elapsed       | 378      |\n",
            "|    total_timesteps    | 460000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.639   |\n",
            "|    explained_variance | 0.978    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 11499    |\n",
            "|    policy_loss        | -0.38    |\n",
            "|    value_loss         | 2.28     |\n",
            "------------------------------------\n",
            "Num timesteps: 464000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -134.49\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 565      |\n",
            "|    ep_rew_mean        | -134     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1210     |\n",
            "|    iterations         | 11600    |\n",
            "|    time_elapsed       | 383      |\n",
            "|    total_timesteps    | 464000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.667   |\n",
            "|    explained_variance | 0.954    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 11599    |\n",
            "|    policy_loss        | -0.696   |\n",
            "|    value_loss         | 2.6      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 596      |\n",
            "|    ep_rew_mean        | -135     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1207     |\n",
            "|    iterations         | 11700    |\n",
            "|    time_elapsed       | 387      |\n",
            "|    total_timesteps    | 468000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.632   |\n",
            "|    explained_variance | 0.959    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 11699    |\n",
            "|    policy_loss        | -0.273   |\n",
            "|    value_loss         | 3.14     |\n",
            "------------------------------------\n",
            "Num timesteps: 472000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -134.95\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 591      |\n",
            "|    ep_rew_mean        | -135     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1203     |\n",
            "|    iterations         | 11800    |\n",
            "|    time_elapsed       | 392      |\n",
            "|    total_timesteps    | 472000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.707   |\n",
            "|    explained_variance | 0.937    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 11799    |\n",
            "|    policy_loss        | 0.593    |\n",
            "|    value_loss         | 6.34     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 597      |\n",
            "|    ep_rew_mean        | -134     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1201     |\n",
            "|    iterations         | 11900    |\n",
            "|    time_elapsed       | 396      |\n",
            "|    total_timesteps    | 476000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.639   |\n",
            "|    explained_variance | 0.915    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 11899    |\n",
            "|    policy_loss        | -0.105   |\n",
            "|    value_loss         | 1.67     |\n",
            "------------------------------------\n",
            "Num timesteps: 480000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -133.47\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 611      |\n",
            "|    ep_rew_mean        | -133     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 12000    |\n",
            "|    time_elapsed       | 401      |\n",
            "|    total_timesteps    | 480000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.544   |\n",
            "|    explained_variance | 0.897    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 11999    |\n",
            "|    policy_loss        | 0.584    |\n",
            "|    value_loss         | 8.13     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 615      |\n",
            "|    ep_rew_mean        | -131     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 12100    |\n",
            "|    time_elapsed       | 405      |\n",
            "|    total_timesteps    | 484000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.583   |\n",
            "|    explained_variance | 0.978    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 12099    |\n",
            "|    policy_loss        | 0.218    |\n",
            "|    value_loss         | 1.52     |\n",
            "------------------------------------\n",
            "Num timesteps: 488000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -126.64\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 640      |\n",
            "|    ep_rew_mean        | -127     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1189     |\n",
            "|    iterations         | 12200    |\n",
            "|    time_elapsed       | 410      |\n",
            "|    total_timesteps    | 488000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.515   |\n",
            "|    explained_variance | 0.977    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 12199    |\n",
            "|    policy_loss        | -0.186   |\n",
            "|    value_loss         | 2.25     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 660      |\n",
            "|    ep_rew_mean        | -125     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1186     |\n",
            "|    iterations         | 12300    |\n",
            "|    time_elapsed       | 414      |\n",
            "|    total_timesteps    | 492000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.613   |\n",
            "|    explained_variance | 0.975    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 12299    |\n",
            "|    policy_loss        | -0.194   |\n",
            "|    value_loss         | 2.99     |\n",
            "------------------------------------\n",
            "Num timesteps: 496000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -124.85\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 666      |\n",
            "|    ep_rew_mean        | -125     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1182     |\n",
            "|    iterations         | 12400    |\n",
            "|    time_elapsed       | 419      |\n",
            "|    total_timesteps    | 496000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.672   |\n",
            "|    explained_variance | 0.975    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 12399    |\n",
            "|    policy_loss        | 0.0583   |\n",
            "|    value_loss         | 2.99     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 664      |\n",
            "|    ep_rew_mean        | -122     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1179     |\n",
            "|    iterations         | 12500    |\n",
            "|    time_elapsed       | 423      |\n",
            "|    total_timesteps    | 500000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.681   |\n",
            "|    explained_variance | 0.965    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 12499    |\n",
            "|    policy_loss        | 0.283    |\n",
            "|    value_loss         | 5.09     |\n",
            "------------------------------------\n",
            "Num timesteps: 504000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -121.96\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 659      |\n",
            "|    ep_rew_mean        | -122     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1178     |\n",
            "|    iterations         | 12600    |\n",
            "|    time_elapsed       | 427      |\n",
            "|    total_timesteps    | 504000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.592   |\n",
            "|    explained_variance | 0.963    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 12599    |\n",
            "|    policy_loss        | 0.00572  |\n",
            "|    value_loss         | 3.81     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 647      |\n",
            "|    ep_rew_mean        | -119     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1178     |\n",
            "|    iterations         | 12700    |\n",
            "|    time_elapsed       | 431      |\n",
            "|    total_timesteps    | 508000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.553   |\n",
            "|    explained_variance | 0.972    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 12699    |\n",
            "|    policy_loss        | 0.196    |\n",
            "|    value_loss         | 4.45     |\n",
            "------------------------------------\n",
            "Num timesteps: 512000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -114.67\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 648      |\n",
            "|    ep_rew_mean        | -115     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1178     |\n",
            "|    iterations         | 12800    |\n",
            "|    time_elapsed       | 434      |\n",
            "|    total_timesteps    | 512000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.535   |\n",
            "|    explained_variance | 0.974    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 12799    |\n",
            "|    policy_loss        | 0.437    |\n",
            "|    value_loss         | 5.53     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 645      |\n",
            "|    ep_rew_mean        | -111     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1176     |\n",
            "|    iterations         | 12900    |\n",
            "|    time_elapsed       | 438      |\n",
            "|    total_timesteps    | 516000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.466   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 12899    |\n",
            "|    policy_loss        | -0.37    |\n",
            "|    value_loss         | 2.75     |\n",
            "------------------------------------\n",
            "Num timesteps: 520000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -107.08\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 646      |\n",
            "|    ep_rew_mean        | -107     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1175     |\n",
            "|    iterations         | 13000    |\n",
            "|    time_elapsed       | 442      |\n",
            "|    total_timesteps    | 520000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.59    |\n",
            "|    explained_variance | 0.942    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 12999    |\n",
            "|    policy_loss        | -0.624   |\n",
            "|    value_loss         | 6.5      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 635      |\n",
            "|    ep_rew_mean        | -107     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1173     |\n",
            "|    iterations         | 13100    |\n",
            "|    time_elapsed       | 446      |\n",
            "|    total_timesteps    | 524000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.623   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 13099    |\n",
            "|    policy_loss        | 0.14     |\n",
            "|    value_loss         | 2.96     |\n",
            "------------------------------------\n",
            "Num timesteps: 528000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -107.44\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 646      |\n",
            "|    ep_rew_mean        | -107     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1168     |\n",
            "|    iterations         | 13200    |\n",
            "|    time_elapsed       | 451      |\n",
            "|    total_timesteps    | 528000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.54    |\n",
            "|    explained_variance | 0.944    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 13199    |\n",
            "|    policy_loss        | -0.289   |\n",
            "|    value_loss         | 4.54     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 643      |\n",
            "|    ep_rew_mean        | -104     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1167     |\n",
            "|    iterations         | 13300    |\n",
            "|    time_elapsed       | 455      |\n",
            "|    total_timesteps    | 532000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.483   |\n",
            "|    explained_variance | 0.885    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 13299    |\n",
            "|    policy_loss        | -0.568   |\n",
            "|    value_loss         | 2.72     |\n",
            "------------------------------------\n",
            "Num timesteps: 536000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -104.54\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 636      |\n",
            "|    ep_rew_mean        | -105     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1163     |\n",
            "|    iterations         | 13400    |\n",
            "|    time_elapsed       | 460      |\n",
            "|    total_timesteps    | 536000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.517   |\n",
            "|    explained_variance | 0.972    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 13399    |\n",
            "|    policy_loss        | -0.28    |\n",
            "|    value_loss         | 2.34     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 637      |\n",
            "|    ep_rew_mean        | -103     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1162     |\n",
            "|    iterations         | 13500    |\n",
            "|    time_elapsed       | 464      |\n",
            "|    total_timesteps    | 540000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.49    |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 13499    |\n",
            "|    policy_loss        | 0.0304   |\n",
            "|    value_loss         | 1.36     |\n",
            "------------------------------------\n",
            "Num timesteps: 544000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -103.41\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 636      |\n",
            "|    ep_rew_mean        | -103     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1159     |\n",
            "|    iterations         | 13600    |\n",
            "|    time_elapsed       | 469      |\n",
            "|    total_timesteps    | 544000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.586   |\n",
            "|    explained_variance | 0.765    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 13599    |\n",
            "|    policy_loss        | 0.332    |\n",
            "|    value_loss         | 2.69     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 640      |\n",
            "|    ep_rew_mean        | -105     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1155     |\n",
            "|    iterations         | 13700    |\n",
            "|    time_elapsed       | 474      |\n",
            "|    total_timesteps    | 548000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.555   |\n",
            "|    explained_variance | 0.914    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 13699    |\n",
            "|    policy_loss        | -0.557   |\n",
            "|    value_loss         | 4.38     |\n",
            "------------------------------------\n",
            "Num timesteps: 552000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -107.88\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 626      |\n",
            "|    ep_rew_mean        | -108     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1153     |\n",
            "|    iterations         | 13800    |\n",
            "|    time_elapsed       | 478      |\n",
            "|    total_timesteps    | 552000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.651   |\n",
            "|    explained_variance | 0.826    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 13799    |\n",
            "|    policy_loss        | 0.0197   |\n",
            "|    value_loss         | 3.16     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 624      |\n",
            "|    ep_rew_mean        | -109     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1148     |\n",
            "|    iterations         | 13900    |\n",
            "|    time_elapsed       | 484      |\n",
            "|    total_timesteps    | 556000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.588   |\n",
            "|    explained_variance | 0.673    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 13899    |\n",
            "|    policy_loss        | -0.223   |\n",
            "|    value_loss         | 3.81     |\n",
            "------------------------------------\n",
            "Num timesteps: 560000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -109.43\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 622      |\n",
            "|    ep_rew_mean        | -109     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1147     |\n",
            "|    iterations         | 14000    |\n",
            "|    time_elapsed       | 487      |\n",
            "|    total_timesteps    | 560000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.568   |\n",
            "|    explained_variance | 0.887    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 13999    |\n",
            "|    policy_loss        | -0.297   |\n",
            "|    value_loss         | 1.71     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 635      |\n",
            "|    ep_rew_mean        | -109     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1146     |\n",
            "|    iterations         | 14100    |\n",
            "|    time_elapsed       | 491      |\n",
            "|    total_timesteps    | 564000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.5     |\n",
            "|    explained_variance | 0.926    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 14099    |\n",
            "|    policy_loss        | 0.64     |\n",
            "|    value_loss         | 2.13     |\n",
            "------------------------------------\n",
            "Num timesteps: 568000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -106.58\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 647      |\n",
            "|    ep_rew_mean        | -107     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1146     |\n",
            "|    iterations         | 14200    |\n",
            "|    time_elapsed       | 495      |\n",
            "|    total_timesteps    | 568000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.495   |\n",
            "|    explained_variance | 0.93     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 14199    |\n",
            "|    policy_loss        | -0.313   |\n",
            "|    value_loss         | 2.83     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 653      |\n",
            "|    ep_rew_mean        | -107     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1144     |\n",
            "|    iterations         | 14300    |\n",
            "|    time_elapsed       | 499      |\n",
            "|    total_timesteps    | 572000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.626   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 14299    |\n",
            "|    policy_loss        | 0.148    |\n",
            "|    value_loss         | 1.21     |\n",
            "------------------------------------\n",
            "Num timesteps: 576000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -107.13\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 671      |\n",
            "|    ep_rew_mean        | -107     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1142     |\n",
            "|    iterations         | 14400    |\n",
            "|    time_elapsed       | 504      |\n",
            "|    total_timesteps    | 576000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.535   |\n",
            "|    explained_variance | 0.891    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 14399    |\n",
            "|    policy_loss        | -0.231   |\n",
            "|    value_loss         | 2.26     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 686      |\n",
            "|    ep_rew_mean        | -108     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1141     |\n",
            "|    iterations         | 14500    |\n",
            "|    time_elapsed       | 508      |\n",
            "|    total_timesteps    | 580000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.527   |\n",
            "|    explained_variance | 0.972    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 14499    |\n",
            "|    policy_loss        | 0.129    |\n",
            "|    value_loss         | 2.01     |\n",
            "------------------------------------\n",
            "Num timesteps: 584000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -108.04\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 703      |\n",
            "|    ep_rew_mean        | -108     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1138     |\n",
            "|    iterations         | 14600    |\n",
            "|    time_elapsed       | 512      |\n",
            "|    total_timesteps    | 584000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.662   |\n",
            "|    explained_variance | 0.631    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 14599    |\n",
            "|    policy_loss        | -0.824   |\n",
            "|    value_loss         | 3.54     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 711      |\n",
            "|    ep_rew_mean        | -112     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1136     |\n",
            "|    iterations         | 14700    |\n",
            "|    time_elapsed       | 517      |\n",
            "|    total_timesteps    | 588000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.619   |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 14699    |\n",
            "|    policy_loss        | 0.0428   |\n",
            "|    value_loss         | 2.37     |\n",
            "------------------------------------\n",
            "Num timesteps: 592000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -112.05\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 715      |\n",
            "|    ep_rew_mean        | -112     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1137     |\n",
            "|    iterations         | 14800    |\n",
            "|    time_elapsed       | 520      |\n",
            "|    total_timesteps    | 592000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.486   |\n",
            "|    explained_variance | 0.971    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 14799    |\n",
            "|    policy_loss        | 0.0918   |\n",
            "|    value_loss         | 2.23     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 713      |\n",
            "|    ep_rew_mean        | -113     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1137     |\n",
            "|    iterations         | 14900    |\n",
            "|    time_elapsed       | 524      |\n",
            "|    total_timesteps    | 596000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.512   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 14899    |\n",
            "|    policy_loss        | 0.0705   |\n",
            "|    value_loss         | 3.31     |\n",
            "------------------------------------\n",
            "Num timesteps: 600000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -114.91\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 696      |\n",
            "|    ep_rew_mean        | -115     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1137     |\n",
            "|    iterations         | 15000    |\n",
            "|    time_elapsed       | 527      |\n",
            "|    total_timesteps    | 600000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.482   |\n",
            "|    explained_variance | 0.978    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 14999    |\n",
            "|    policy_loss        | -0.314   |\n",
            "|    value_loss         | 3.07     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 700      |\n",
            "|    ep_rew_mean        | -114     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1135     |\n",
            "|    iterations         | 15100    |\n",
            "|    time_elapsed       | 531      |\n",
            "|    total_timesteps    | 604000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.662   |\n",
            "|    explained_variance | 0.972    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 15099    |\n",
            "|    policy_loss        | -0.247   |\n",
            "|    value_loss         | 3.23     |\n",
            "------------------------------------\n",
            "Num timesteps: 608000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -113.50\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 698      |\n",
            "|    ep_rew_mean        | -113     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1132     |\n",
            "|    iterations         | 15200    |\n",
            "|    time_elapsed       | 536      |\n",
            "|    total_timesteps    | 608000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.498   |\n",
            "|    explained_variance | 0.971    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 15199    |\n",
            "|    policy_loss        | 0.357    |\n",
            "|    value_loss         | 2.9      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 705      |\n",
            "|    ep_rew_mean        | -113     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1131     |\n",
            "|    iterations         | 15300    |\n",
            "|    time_elapsed       | 540      |\n",
            "|    total_timesteps    | 612000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.601   |\n",
            "|    explained_variance | 0.965    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 15299    |\n",
            "|    policy_loss        | 0.294    |\n",
            "|    value_loss         | 2.62     |\n",
            "------------------------------------\n",
            "Num timesteps: 616000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -112.58\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 711      |\n",
            "|    ep_rew_mean        | -113     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1127     |\n",
            "|    iterations         | 15400    |\n",
            "|    time_elapsed       | 546      |\n",
            "|    total_timesteps    | 616000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.512   |\n",
            "|    explained_variance | 0.936    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 15399    |\n",
            "|    policy_loss        | 0.376    |\n",
            "|    value_loss         | 4.17     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 703      |\n",
            "|    ep_rew_mean        | -110     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1129     |\n",
            "|    iterations         | 15500    |\n",
            "|    time_elapsed       | 549      |\n",
            "|    total_timesteps    | 620000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.631   |\n",
            "|    explained_variance | 0.98     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 15499    |\n",
            "|    policy_loss        | 0.322    |\n",
            "|    value_loss         | 2.18     |\n",
            "------------------------------------\n",
            "Num timesteps: 624000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -110.70\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 698      |\n",
            "|    ep_rew_mean        | -111     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1128     |\n",
            "|    iterations         | 15600    |\n",
            "|    time_elapsed       | 552      |\n",
            "|    total_timesteps    | 624000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.547   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 15599    |\n",
            "|    policy_loss        | -0.476   |\n",
            "|    value_loss         | 3.58     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 678      |\n",
            "|    ep_rew_mean        | -110     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1128     |\n",
            "|    iterations         | 15700    |\n",
            "|    time_elapsed       | 556      |\n",
            "|    total_timesteps    | 628000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.608   |\n",
            "|    explained_variance | 0.947    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 15699    |\n",
            "|    policy_loss        | -0.0836  |\n",
            "|    value_loss         | 4.31     |\n",
            "------------------------------------\n",
            "Num timesteps: 632000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -110.85\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 666      |\n",
            "|    ep_rew_mean        | -111     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1127     |\n",
            "|    iterations         | 15800    |\n",
            "|    time_elapsed       | 560      |\n",
            "|    total_timesteps    | 632000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.543   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 15799    |\n",
            "|    policy_loss        | -0.36    |\n",
            "|    value_loss         | 3.93     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 640      |\n",
            "|    ep_rew_mean        | -109     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1129     |\n",
            "|    iterations         | 15900    |\n",
            "|    time_elapsed       | 563      |\n",
            "|    total_timesteps    | 636000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.615   |\n",
            "|    explained_variance | 0.958    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 15899    |\n",
            "|    policy_loss        | -0.224   |\n",
            "|    value_loss         | 1.93     |\n",
            "------------------------------------\n",
            "Num timesteps: 640000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -108.57\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 640      |\n",
            "|    ep_rew_mean        | -109     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1123     |\n",
            "|    iterations         | 16000    |\n",
            "|    time_elapsed       | 569      |\n",
            "|    total_timesteps    | 640000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.615   |\n",
            "|    explained_variance | 0.958    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 15999    |\n",
            "|    policy_loss        | 0.0611   |\n",
            "|    value_loss         | 4.23     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 646      |\n",
            "|    ep_rew_mean        | -107     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1122     |\n",
            "|    iterations         | 16100    |\n",
            "|    time_elapsed       | 573      |\n",
            "|    total_timesteps    | 644000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.565   |\n",
            "|    explained_variance | 0.392    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 16099    |\n",
            "|    policy_loss        | 0.112    |\n",
            "|    value_loss         | 531      |\n",
            "------------------------------------\n",
            "Num timesteps: 648000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -107.77\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 622      |\n",
            "|    ep_rew_mean        | -108     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1122     |\n",
            "|    iterations         | 16200    |\n",
            "|    time_elapsed       | 577      |\n",
            "|    total_timesteps    | 648000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.584   |\n",
            "|    explained_variance | 0.848    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 16199    |\n",
            "|    policy_loss        | -0.0931  |\n",
            "|    value_loss         | 2.17     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 615      |\n",
            "|    ep_rew_mean        | -106     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1121     |\n",
            "|    iterations         | 16300    |\n",
            "|    time_elapsed       | 581      |\n",
            "|    total_timesteps    | 652000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.602   |\n",
            "|    explained_variance | 0.98     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 16299    |\n",
            "|    policy_loss        | -0.135   |\n",
            "|    value_loss         | 1.78     |\n",
            "------------------------------------\n",
            "Num timesteps: 656000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -103.99\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 620      |\n",
            "|    ep_rew_mean        | -104     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1120     |\n",
            "|    iterations         | 16400    |\n",
            "|    time_elapsed       | 585      |\n",
            "|    total_timesteps    | 656000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.587   |\n",
            "|    explained_variance | 0.96     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 16399    |\n",
            "|    policy_loss        | -0.0748  |\n",
            "|    value_loss         | 3.45     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 635      |\n",
            "|    ep_rew_mean        | -102     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 16500    |\n",
            "|    time_elapsed       | 590      |\n",
            "|    total_timesteps    | 660000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.614   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 16499    |\n",
            "|    policy_loss        | -0.588   |\n",
            "|    value_loss         | 2.86     |\n",
            "------------------------------------\n",
            "Num timesteps: 664000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -100.38\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 631      |\n",
            "|    ep_rew_mean        | -100     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1115     |\n",
            "|    iterations         | 16600    |\n",
            "|    time_elapsed       | 595      |\n",
            "|    total_timesteps    | 664000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.457   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 16599    |\n",
            "|    policy_loss        | -0.492   |\n",
            "|    value_loss         | 1.87     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 634      |\n",
            "|    ep_rew_mean        | -92.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1115     |\n",
            "|    iterations         | 16700    |\n",
            "|    time_elapsed       | 598      |\n",
            "|    total_timesteps    | 668000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.439   |\n",
            "|    explained_variance | 0.93     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 16699    |\n",
            "|    policy_loss        | -0.209   |\n",
            "|    value_loss         | 3.62     |\n",
            "------------------------------------\n",
            "Num timesteps: 672000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -92.48\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 592      |\n",
            "|    ep_rew_mean        | -92.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1115     |\n",
            "|    iterations         | 16800    |\n",
            "|    time_elapsed       | 602      |\n",
            "|    total_timesteps    | 672000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.518   |\n",
            "|    explained_variance | 0.973    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 16799    |\n",
            "|    policy_loss        | 0.0226   |\n",
            "|    value_loss         | 3.29     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 575      |\n",
            "|    ep_rew_mean        | -91.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1115     |\n",
            "|    iterations         | 16900    |\n",
            "|    time_elapsed       | 606      |\n",
            "|    total_timesteps    | 676000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.453   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 16899    |\n",
            "|    policy_loss        | -0.518   |\n",
            "|    value_loss         | 2.52     |\n",
            "------------------------------------\n",
            "Num timesteps: 680000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -90.78\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 580      |\n",
            "|    ep_rew_mean        | -90.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1114     |\n",
            "|    iterations         | 17000    |\n",
            "|    time_elapsed       | 610      |\n",
            "|    total_timesteps    | 680000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.49    |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 16999    |\n",
            "|    policy_loss        | -0.131   |\n",
            "|    value_loss         | 5.2      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 580      |\n",
            "|    ep_rew_mean        | -89.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1113     |\n",
            "|    iterations         | 17100    |\n",
            "|    time_elapsed       | 614      |\n",
            "|    total_timesteps    | 684000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.637   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 17099    |\n",
            "|    policy_loss        | 0.502    |\n",
            "|    value_loss         | 2.87     |\n",
            "------------------------------------\n",
            "Num timesteps: 688000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -88.21\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 594      |\n",
            "|    ep_rew_mean        | -88.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1110     |\n",
            "|    iterations         | 17200    |\n",
            "|    time_elapsed       | 619      |\n",
            "|    total_timesteps    | 688000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.606   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 17199    |\n",
            "|    policy_loss        | -0.538   |\n",
            "|    value_loss         | 3.32     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 580      |\n",
            "|    ep_rew_mean        | -88.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1110     |\n",
            "|    iterations         | 17300    |\n",
            "|    time_elapsed       | 623      |\n",
            "|    total_timesteps    | 692000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.444   |\n",
            "|    explained_variance | 0.943    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 17299    |\n",
            "|    policy_loss        | -0.398   |\n",
            "|    value_loss         | 5.98     |\n",
            "------------------------------------\n",
            "Num timesteps: 696000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -92.56\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 526      |\n",
            "|    ep_rew_mean        | -92.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1111     |\n",
            "|    iterations         | 17400    |\n",
            "|    time_elapsed       | 626      |\n",
            "|    total_timesteps    | 696000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.53    |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 17399    |\n",
            "|    policy_loss        | -0.11    |\n",
            "|    value_loss         | 2.42     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 526      |\n",
            "|    ep_rew_mean        | -91.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1112     |\n",
            "|    iterations         | 17500    |\n",
            "|    time_elapsed       | 629      |\n",
            "|    total_timesteps    | 700000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.553   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 17499    |\n",
            "|    policy_loss        | 0.0672   |\n",
            "|    value_loss         | 1.72     |\n",
            "------------------------------------\n",
            "Num timesteps: 704000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -95.51\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 506      |\n",
            "|    ep_rew_mean        | -95.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1111     |\n",
            "|    iterations         | 17600    |\n",
            "|    time_elapsed       | 633      |\n",
            "|    total_timesteps    | 704000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.542   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 17599    |\n",
            "|    policy_loss        | -0.369   |\n",
            "|    value_loss         | 2.88     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 482      |\n",
            "|    ep_rew_mean        | -97.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1111     |\n",
            "|    iterations         | 17700    |\n",
            "|    time_elapsed       | 636      |\n",
            "|    total_timesteps    | 708000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.395   |\n",
            "|    explained_variance | 0.979    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 17699    |\n",
            "|    policy_loss        | -0.091   |\n",
            "|    value_loss         | 3.53     |\n",
            "------------------------------------\n",
            "Num timesteps: 712000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -104.99\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 463      |\n",
            "|    ep_rew_mean        | -105     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1111     |\n",
            "|    iterations         | 17800    |\n",
            "|    time_elapsed       | 640      |\n",
            "|    total_timesteps    | 712000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.499   |\n",
            "|    explained_variance | 0.95     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 17799    |\n",
            "|    policy_loss        | 0.83     |\n",
            "|    value_loss         | 6.05     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 470      |\n",
            "|    ep_rew_mean        | -109     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1111     |\n",
            "|    iterations         | 17900    |\n",
            "|    time_elapsed       | 644      |\n",
            "|    total_timesteps    | 716000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.391   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 17899    |\n",
            "|    policy_loss        | 0.299    |\n",
            "|    value_loss         | 4.44     |\n",
            "------------------------------------\n",
            "Num timesteps: 720000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -109.30\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 466      |\n",
            "|    ep_rew_mean        | -109     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1111     |\n",
            "|    iterations         | 18000    |\n",
            "|    time_elapsed       | 647      |\n",
            "|    total_timesteps    | 720000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.586   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 17999    |\n",
            "|    policy_loss        | 0.287    |\n",
            "|    value_loss         | 3.37     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 456      |\n",
            "|    ep_rew_mean        | -110     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1111     |\n",
            "|    iterations         | 18100    |\n",
            "|    time_elapsed       | 651      |\n",
            "|    total_timesteps    | 724000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.509   |\n",
            "|    explained_variance | 0.927    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 18099    |\n",
            "|    policy_loss        | 0.0507   |\n",
            "|    value_loss         | 2.44     |\n",
            "------------------------------------\n",
            "Num timesteps: 728000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -111.46\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 454      |\n",
            "|    ep_rew_mean        | -111     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1110     |\n",
            "|    iterations         | 18200    |\n",
            "|    time_elapsed       | 655      |\n",
            "|    total_timesteps    | 728000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.579   |\n",
            "|    explained_variance | 0.925    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 18199    |\n",
            "|    policy_loss        | -1.22    |\n",
            "|    value_loss         | 5.94     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 460      |\n",
            "|    ep_rew_mean        | -111     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1107     |\n",
            "|    iterations         | 18300    |\n",
            "|    time_elapsed       | 660      |\n",
            "|    total_timesteps    | 732000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.588   |\n",
            "|    explained_variance | 0.959    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 18299    |\n",
            "|    policy_loss        | -0.0083  |\n",
            "|    value_loss         | 2.82     |\n",
            "------------------------------------\n",
            "Num timesteps: 736000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -115.66\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 471      |\n",
            "|    ep_rew_mean        | -116     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1106     |\n",
            "|    iterations         | 18400    |\n",
            "|    time_elapsed       | 665      |\n",
            "|    total_timesteps    | 736000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.493   |\n",
            "|    explained_variance | 0.947    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 18399    |\n",
            "|    policy_loss        | -0.19    |\n",
            "|    value_loss         | 7.08     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 471      |\n",
            "|    ep_rew_mean        | -120     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1106     |\n",
            "|    iterations         | 18500    |\n",
            "|    time_elapsed       | 668      |\n",
            "|    total_timesteps    | 740000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.51    |\n",
            "|    explained_variance | 0.924    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 18499    |\n",
            "|    policy_loss        | -0.127   |\n",
            "|    value_loss         | 3.44     |\n",
            "------------------------------------\n",
            "Num timesteps: 744000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -122.30\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 503      |\n",
            "|    ep_rew_mean        | -122     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1105     |\n",
            "|    iterations         | 18600    |\n",
            "|    time_elapsed       | 672      |\n",
            "|    total_timesteps    | 744000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.453   |\n",
            "|    explained_variance | 0.92     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 18599    |\n",
            "|    policy_loss        | 0.0954   |\n",
            "|    value_loss         | 3.42     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 502      |\n",
            "|    ep_rew_mean        | -126     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1105     |\n",
            "|    iterations         | 18700    |\n",
            "|    time_elapsed       | 676      |\n",
            "|    total_timesteps    | 748000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.631   |\n",
            "|    explained_variance | 0.952    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 18699    |\n",
            "|    policy_loss        | -0.217   |\n",
            "|    value_loss         | 3.97     |\n",
            "------------------------------------\n",
            "Num timesteps: 752000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -128.13\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 511      |\n",
            "|    ep_rew_mean        | -128     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1104     |\n",
            "|    iterations         | 18800    |\n",
            "|    time_elapsed       | 680      |\n",
            "|    total_timesteps    | 752000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.535   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 18799    |\n",
            "|    policy_loss        | 0.498    |\n",
            "|    value_loss         | 3.14     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 515      |\n",
            "|    ep_rew_mean        | -129     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1104     |\n",
            "|    iterations         | 18900    |\n",
            "|    time_elapsed       | 684      |\n",
            "|    total_timesteps    | 756000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.364   |\n",
            "|    explained_variance | 0.973    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 18899    |\n",
            "|    policy_loss        | -0.0596  |\n",
            "|    value_loss         | 3.95     |\n",
            "------------------------------------\n",
            "Num timesteps: 760000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -128.44\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 513      |\n",
            "|    ep_rew_mean        | -128     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1104     |\n",
            "|    iterations         | 19000    |\n",
            "|    time_elapsed       | 687      |\n",
            "|    total_timesteps    | 760000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.498   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 18999    |\n",
            "|    policy_loss        | -0.0408  |\n",
            "|    value_loss         | 2.04     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 524      |\n",
            "|    ep_rew_mean        | -127     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1104     |\n",
            "|    iterations         | 19100    |\n",
            "|    time_elapsed       | 691      |\n",
            "|    total_timesteps    | 764000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.437   |\n",
            "|    explained_variance | 0.968    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 19099    |\n",
            "|    policy_loss        | 0.444    |\n",
            "|    value_loss         | 6.23     |\n",
            "------------------------------------\n",
            "Num timesteps: 768000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -127.27\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 512      |\n",
            "|    ep_rew_mean        | -127     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1106     |\n",
            "|    iterations         | 19200    |\n",
            "|    time_elapsed       | 693      |\n",
            "|    total_timesteps    | 768000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.435   |\n",
            "|    explained_variance | 0.963    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 19199    |\n",
            "|    policy_loss        | -0.0115  |\n",
            "|    value_loss         | 2.48     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 510      |\n",
            "|    ep_rew_mean        | -130     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1106     |\n",
            "|    iterations         | 19300    |\n",
            "|    time_elapsed       | 697      |\n",
            "|    total_timesteps    | 772000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.472   |\n",
            "|    explained_variance | 0.945    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 19299    |\n",
            "|    policy_loss        | -0.327   |\n",
            "|    value_loss         | 4.36     |\n",
            "------------------------------------\n",
            "Num timesteps: 776000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -129.85\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 521      |\n",
            "|    ep_rew_mean        | -130     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1105     |\n",
            "|    iterations         | 19400    |\n",
            "|    time_elapsed       | 701      |\n",
            "|    total_timesteps    | 776000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.438   |\n",
            "|    explained_variance | 0.8      |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 19399    |\n",
            "|    policy_loss        | -0.352   |\n",
            "|    value_loss         | 3.68     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 474      |\n",
            "|    ep_rew_mean        | -128     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1106     |\n",
            "|    iterations         | 19500    |\n",
            "|    time_elapsed       | 705      |\n",
            "|    total_timesteps    | 780000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.359   |\n",
            "|    explained_variance | 0.606    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 19499    |\n",
            "|    policy_loss        | -3.62    |\n",
            "|    value_loss         | 372      |\n",
            "------------------------------------\n",
            "Num timesteps: 784000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -126.11\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 452      |\n",
            "|    ep_rew_mean        | -126     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1106     |\n",
            "|    iterations         | 19600    |\n",
            "|    time_elapsed       | 708      |\n",
            "|    total_timesteps    | 784000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.38    |\n",
            "|    explained_variance | 0.951    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 19599    |\n",
            "|    policy_loss        | -0.327   |\n",
            "|    value_loss         | 2.69     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 458      |\n",
            "|    ep_rew_mean        | -124     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1106     |\n",
            "|    iterations         | 19700    |\n",
            "|    time_elapsed       | 712      |\n",
            "|    total_timesteps    | 788000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.448   |\n",
            "|    explained_variance | 0.951    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 19699    |\n",
            "|    policy_loss        | -0.296   |\n",
            "|    value_loss         | 5.45     |\n",
            "------------------------------------\n",
            "Num timesteps: 792000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -118.70\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 460      |\n",
            "|    ep_rew_mean        | -119     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1105     |\n",
            "|    iterations         | 19800    |\n",
            "|    time_elapsed       | 716      |\n",
            "|    total_timesteps    | 792000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.474   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 19799    |\n",
            "|    policy_loss        | -0.907   |\n",
            "|    value_loss         | 5.03     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 457      |\n",
            "|    ep_rew_mean        | -114     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1105     |\n",
            "|    iterations         | 19900    |\n",
            "|    time_elapsed       | 720      |\n",
            "|    total_timesteps    | 796000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.465   |\n",
            "|    explained_variance | 0.972    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 19899    |\n",
            "|    policy_loss        | 0.0301   |\n",
            "|    value_loss         | 5.39     |\n",
            "------------------------------------\n",
            "Num timesteps: 800000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -109.34\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 455      |\n",
            "|    ep_rew_mean        | -109     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1106     |\n",
            "|    iterations         | 20000    |\n",
            "|    time_elapsed       | 723      |\n",
            "|    total_timesteps    | 800000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.484   |\n",
            "|    explained_variance | 0.977    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 19999    |\n",
            "|    policy_loss        | 0.185    |\n",
            "|    value_loss         | 5.09     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 465      |\n",
            "|    ep_rew_mean        | -100     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1107     |\n",
            "|    iterations         | 20100    |\n",
            "|    time_elapsed       | 726      |\n",
            "|    total_timesteps    | 804000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.417   |\n",
            "|    explained_variance | 0.97     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 20099    |\n",
            "|    policy_loss        | 0.229    |\n",
            "|    value_loss         | 2.69     |\n",
            "------------------------------------\n",
            "Num timesteps: 808000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -96.63\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 461      |\n",
            "|    ep_rew_mean        | -96.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1107     |\n",
            "|    iterations         | 20200    |\n",
            "|    time_elapsed       | 729      |\n",
            "|    total_timesteps    | 808000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.493   |\n",
            "|    explained_variance | 0.948    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 20199    |\n",
            "|    policy_loss        | -0.115   |\n",
            "|    value_loss         | 9.17     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 460      |\n",
            "|    ep_rew_mean        | -95.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1107     |\n",
            "|    iterations         | 20300    |\n",
            "|    time_elapsed       | 732      |\n",
            "|    total_timesteps    | 812000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.472   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 20299    |\n",
            "|    policy_loss        | 0.0598   |\n",
            "|    value_loss         | 3.39     |\n",
            "------------------------------------\n",
            "Num timesteps: 816000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -90.71\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 474      |\n",
            "|    ep_rew_mean        | -90.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1108     |\n",
            "|    iterations         | 20400    |\n",
            "|    time_elapsed       | 736      |\n",
            "|    total_timesteps    | 816000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.455   |\n",
            "|    explained_variance | 0.96     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 20399    |\n",
            "|    policy_loss        | -0.443   |\n",
            "|    value_loss         | 4.15     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 460      |\n",
            "|    ep_rew_mean        | -88.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 20500    |\n",
            "|    time_elapsed       | 739      |\n",
            "|    total_timesteps    | 820000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.483   |\n",
            "|    explained_variance | 0.977    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 20499    |\n",
            "|    policy_loss        | -0.589   |\n",
            "|    value_loss         | 2.87     |\n",
            "------------------------------------\n",
            "Num timesteps: 824000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -87.09\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 486      |\n",
            "|    ep_rew_mean        | -87.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 20600    |\n",
            "|    time_elapsed       | 742      |\n",
            "|    total_timesteps    | 824000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.346   |\n",
            "|    explained_variance | 0.878    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 20599    |\n",
            "|    policy_loss        | 0.558    |\n",
            "|    value_loss         | 2.45     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 485      |\n",
            "|    ep_rew_mean        | -88.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1107     |\n",
            "|    iterations         | 20700    |\n",
            "|    time_elapsed       | 747      |\n",
            "|    total_timesteps    | 828000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.439   |\n",
            "|    explained_variance | 0.949    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 20699    |\n",
            "|    policy_loss        | 0.236    |\n",
            "|    value_loss         | 3.99     |\n",
            "------------------------------------\n",
            "Num timesteps: 832000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -85.11\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 510      |\n",
            "|    ep_rew_mean        | -85.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1107     |\n",
            "|    iterations         | 20800    |\n",
            "|    time_elapsed       | 751      |\n",
            "|    total_timesteps    | 832000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.382   |\n",
            "|    explained_variance | 0.98     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 20799    |\n",
            "|    policy_loss        | 0.206    |\n",
            "|    value_loss         | 3.72     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 511      |\n",
            "|    ep_rew_mean        | -86.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1108     |\n",
            "|    iterations         | 20900    |\n",
            "|    time_elapsed       | 754      |\n",
            "|    total_timesteps    | 836000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.407   |\n",
            "|    explained_variance | 0.963    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 20899    |\n",
            "|    policy_loss        | -0.191   |\n",
            "|    value_loss         | 5.14     |\n",
            "------------------------------------\n",
            "Num timesteps: 840000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -87.16\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 478      |\n",
            "|    ep_rew_mean        | -87.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 21000    |\n",
            "|    time_elapsed       | 756      |\n",
            "|    total_timesteps    | 840000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.379   |\n",
            "|    explained_variance | 0.939    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 20999    |\n",
            "|    policy_loss        | -0.233   |\n",
            "|    value_loss         | 4.69     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 464      |\n",
            "|    ep_rew_mean        | -89.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1110     |\n",
            "|    iterations         | 21100    |\n",
            "|    time_elapsed       | 760      |\n",
            "|    total_timesteps    | 844000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.489   |\n",
            "|    explained_variance | 0.883    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 21099    |\n",
            "|    policy_loss        | -0.249   |\n",
            "|    value_loss         | 2.73     |\n",
            "------------------------------------\n",
            "Num timesteps: 848000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -100.31\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 469      |\n",
            "|    ep_rew_mean        | -100     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1110     |\n",
            "|    iterations         | 21200    |\n",
            "|    time_elapsed       | 763      |\n",
            "|    total_timesteps    | 848000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.398   |\n",
            "|    explained_variance | 0.979    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 21199    |\n",
            "|    policy_loss        | 0.421    |\n",
            "|    value_loss         | 3.84     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 473      |\n",
            "|    ep_rew_mean        | -106     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1110     |\n",
            "|    iterations         | 21300    |\n",
            "|    time_elapsed       | 767      |\n",
            "|    total_timesteps    | 852000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.437   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 21299    |\n",
            "|    policy_loss        | 0.192    |\n",
            "|    value_loss         | 2.25     |\n",
            "------------------------------------\n",
            "Num timesteps: 856000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -108.72\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 466      |\n",
            "|    ep_rew_mean        | -109     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 21400    |\n",
            "|    time_elapsed       | 771      |\n",
            "|    total_timesteps    | 856000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.435   |\n",
            "|    explained_variance | 0.978    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 21399    |\n",
            "|    policy_loss        | -0.475   |\n",
            "|    value_loss         | 1.65     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 494      |\n",
            "|    ep_rew_mean        | -113     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1107     |\n",
            "|    iterations         | 21500    |\n",
            "|    time_elapsed       | 776      |\n",
            "|    total_timesteps    | 860000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.41    |\n",
            "|    explained_variance | 0.941    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 21499    |\n",
            "|    policy_loss        | -0.137   |\n",
            "|    value_loss         | 3.55     |\n",
            "------------------------------------\n",
            "Num timesteps: 864000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -116.86\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 482      |\n",
            "|    ep_rew_mean        | -117     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1108     |\n",
            "|    iterations         | 21600    |\n",
            "|    time_elapsed       | 779      |\n",
            "|    total_timesteps    | 864000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.451   |\n",
            "|    explained_variance | 0.96     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 21599    |\n",
            "|    policy_loss        | 0.141    |\n",
            "|    value_loss         | 2.82     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 489      |\n",
            "|    ep_rew_mean        | -122     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1107     |\n",
            "|    iterations         | 21700    |\n",
            "|    time_elapsed       | 783      |\n",
            "|    total_timesteps    | 868000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.399   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 21699    |\n",
            "|    policy_loss        | -0.468   |\n",
            "|    value_loss         | 1.85     |\n",
            "------------------------------------\n",
            "Num timesteps: 872000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -123.42\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 496      |\n",
            "|    ep_rew_mean        | -123     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1105     |\n",
            "|    iterations         | 21800    |\n",
            "|    time_elapsed       | 788      |\n",
            "|    total_timesteps    | 872000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.38    |\n",
            "|    explained_variance | 0.706    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 21799    |\n",
            "|    policy_loss        | -0.0996  |\n",
            "|    value_loss         | 15       |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 493      |\n",
            "|    ep_rew_mean        | -124     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1104     |\n",
            "|    iterations         | 21900    |\n",
            "|    time_elapsed       | 793      |\n",
            "|    total_timesteps    | 876000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.371   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 21899    |\n",
            "|    policy_loss        | 0.175    |\n",
            "|    value_loss         | 1.54     |\n",
            "------------------------------------\n",
            "Num timesteps: 880000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -128.09\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 485      |\n",
            "|    ep_rew_mean        | -128     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1104     |\n",
            "|    iterations         | 22000    |\n",
            "|    time_elapsed       | 796      |\n",
            "|    total_timesteps    | 880000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.425   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 21999    |\n",
            "|    policy_loss        | -0.105   |\n",
            "|    value_loss         | 2.25     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 490      |\n",
            "|    ep_rew_mean        | -129     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1104     |\n",
            "|    iterations         | 22100    |\n",
            "|    time_elapsed       | 800      |\n",
            "|    total_timesteps    | 884000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.501   |\n",
            "|    explained_variance | 0.965    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 22099    |\n",
            "|    policy_loss        | -0.159   |\n",
            "|    value_loss         | 1.81     |\n",
            "------------------------------------\n",
            "Num timesteps: 888000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -131.83\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 504      |\n",
            "|    ep_rew_mean        | -132     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1103     |\n",
            "|    iterations         | 22200    |\n",
            "|    time_elapsed       | 804      |\n",
            "|    total_timesteps    | 888000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.505   |\n",
            "|    explained_variance | 0.742    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 22199    |\n",
            "|    policy_loss        | -0.889   |\n",
            "|    value_loss         | 134      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 516      |\n",
            "|    ep_rew_mean        | -133     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1103     |\n",
            "|    iterations         | 22300    |\n",
            "|    time_elapsed       | 808      |\n",
            "|    total_timesteps    | 892000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.438   |\n",
            "|    explained_variance | 0.944    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 22299    |\n",
            "|    policy_loss        | -0.0151  |\n",
            "|    value_loss         | 5.34     |\n",
            "------------------------------------\n",
            "Num timesteps: 896000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -128.65\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 539      |\n",
            "|    ep_rew_mean        | -129     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1103     |\n",
            "|    iterations         | 22400    |\n",
            "|    time_elapsed       | 812      |\n",
            "|    total_timesteps    | 896000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.405   |\n",
            "|    explained_variance | 0.978    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 22399    |\n",
            "|    policy_loss        | 0.258    |\n",
            "|    value_loss         | 3.54     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 539      |\n",
            "|    ep_rew_mean        | -127     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 22500    |\n",
            "|    time_elapsed       | 816      |\n",
            "|    total_timesteps    | 900000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.341   |\n",
            "|    explained_variance | 0.882    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 22499    |\n",
            "|    policy_loss        | -0.0863  |\n",
            "|    value_loss         | 6.85     |\n",
            "------------------------------------\n",
            "Num timesteps: 904000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -122.74\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 534      |\n",
            "|    ep_rew_mean        | -123     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1103     |\n",
            "|    iterations         | 22600    |\n",
            "|    time_elapsed       | 819      |\n",
            "|    total_timesteps    | 904000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.406   |\n",
            "|    explained_variance | 0.965    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 22599    |\n",
            "|    policy_loss        | -0.127   |\n",
            "|    value_loss         | 2.8      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 546      |\n",
            "|    ep_rew_mean        | -120     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 22700    |\n",
            "|    time_elapsed       | 823      |\n",
            "|    total_timesteps    | 908000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.349   |\n",
            "|    explained_variance | 0.934    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 22699    |\n",
            "|    policy_loss        | 0.0214   |\n",
            "|    value_loss         | 4.28     |\n",
            "------------------------------------\n",
            "Num timesteps: 912000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -117.75\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 566      |\n",
            "|    ep_rew_mean        | -118     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 22800    |\n",
            "|    time_elapsed       | 827      |\n",
            "|    total_timesteps    | 912000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.38    |\n",
            "|    explained_variance | 0.942    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 22799    |\n",
            "|    policy_loss        | -0.0774  |\n",
            "|    value_loss         | 3.09     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 553      |\n",
            "|    ep_rew_mean        | -118     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 22900    |\n",
            "|    time_elapsed       | 830      |\n",
            "|    total_timesteps    | 916000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.389   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 22899    |\n",
            "|    policy_loss        | -0.0931  |\n",
            "|    value_loss         | 2.16     |\n",
            "------------------------------------\n",
            "Num timesteps: 920000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -116.01\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 546      |\n",
            "|    ep_rew_mean        | -116     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 23000    |\n",
            "|    time_elapsed       | 834      |\n",
            "|    total_timesteps    | 920000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.556   |\n",
            "|    explained_variance | 0.958    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 22999    |\n",
            "|    policy_loss        | -0.152   |\n",
            "|    value_loss         | 4.18     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 549      |\n",
            "|    ep_rew_mean        | -114     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 23100    |\n",
            "|    time_elapsed       | 838      |\n",
            "|    total_timesteps    | 924000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.401   |\n",
            "|    explained_variance | 0.928    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 23099    |\n",
            "|    policy_loss        | 0.0586   |\n",
            "|    value_loss         | 3.25     |\n",
            "------------------------------------\n",
            "Num timesteps: 928000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -113.18\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 549      |\n",
            "|    ep_rew_mean        | -113     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 23200    |\n",
            "|    time_elapsed       | 843      |\n",
            "|    total_timesteps    | 928000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.41    |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 23199    |\n",
            "|    policy_loss        | -0.423   |\n",
            "|    value_loss         | 1.5      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 546      |\n",
            "|    ep_rew_mean        | -110     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 23300    |\n",
            "|    time_elapsed       | 846      |\n",
            "|    total_timesteps    | 932000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.381   |\n",
            "|    explained_variance | 0.971    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 23299    |\n",
            "|    policy_loss        | 0.124    |\n",
            "|    value_loss         | 3.42     |\n",
            "------------------------------------\n",
            "Num timesteps: 936000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -109.65\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 559      |\n",
            "|    ep_rew_mean        | -110     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 23400    |\n",
            "|    time_elapsed       | 851      |\n",
            "|    total_timesteps    | 936000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.419   |\n",
            "|    explained_variance | 0.941    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 23399    |\n",
            "|    policy_loss        | -0.117   |\n",
            "|    value_loss         | 5.69     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 567      |\n",
            "|    ep_rew_mean        | -107     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 23500    |\n",
            "|    time_elapsed       | 854      |\n",
            "|    total_timesteps    | 940000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.418   |\n",
            "|    explained_variance | 0.896    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 23499    |\n",
            "|    policy_loss        | -0.732   |\n",
            "|    value_loss         | 7.11     |\n",
            "------------------------------------\n",
            "Num timesteps: 944000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -103.89\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 556      |\n",
            "|    ep_rew_mean        | -104     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 23600    |\n",
            "|    time_elapsed       | 858      |\n",
            "|    total_timesteps    | 944000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.44    |\n",
            "|    explained_variance | 0.901    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 23599    |\n",
            "|    policy_loss        | -0.00904 |\n",
            "|    value_loss         | 5.81     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 576      |\n",
            "|    ep_rew_mean        | -99.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 23700    |\n",
            "|    time_elapsed       | 863      |\n",
            "|    total_timesteps    | 948000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.453   |\n",
            "|    explained_variance | 0.973    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 23699    |\n",
            "|    policy_loss        | -0.0163  |\n",
            "|    value_loss         | 1.58     |\n",
            "------------------------------------\n",
            "Num timesteps: 952000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -99.11\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 581      |\n",
            "|    ep_rew_mean        | -99.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 23800    |\n",
            "|    time_elapsed       | 868      |\n",
            "|    total_timesteps    | 952000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.501   |\n",
            "|    explained_variance | 0.93     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 23799    |\n",
            "|    policy_loss        | 0.287    |\n",
            "|    value_loss         | 3.22     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 596      |\n",
            "|    ep_rew_mean        | -98.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 23900    |\n",
            "|    time_elapsed       | 872      |\n",
            "|    total_timesteps    | 956000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.518   |\n",
            "|    explained_variance | 0.958    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 23899    |\n",
            "|    policy_loss        | -0.223   |\n",
            "|    value_loss         | 2.4      |\n",
            "------------------------------------\n",
            "Num timesteps: 960000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -97.52\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 595      |\n",
            "|    ep_rew_mean        | -97.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 24000    |\n",
            "|    time_elapsed       | 876      |\n",
            "|    total_timesteps    | 960000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.382   |\n",
            "|    explained_variance | 0.92     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 23999    |\n",
            "|    policy_loss        | -0.209   |\n",
            "|    value_loss         | 2.9      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 605      |\n",
            "|    ep_rew_mean        | -96.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 24100    |\n",
            "|    time_elapsed       | 880      |\n",
            "|    total_timesteps    | 964000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.39    |\n",
            "|    explained_variance | 0.972    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 24099    |\n",
            "|    policy_loss        | -0.123   |\n",
            "|    value_loss         | 1.85     |\n",
            "------------------------------------\n",
            "Num timesteps: 968000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -98.08\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 617      |\n",
            "|    ep_rew_mean        | -98.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1093     |\n",
            "|    iterations         | 24200    |\n",
            "|    time_elapsed       | 885      |\n",
            "|    total_timesteps    | 968000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.409   |\n",
            "|    explained_variance | 0.566    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 24199    |\n",
            "|    policy_loss        | -0.478   |\n",
            "|    value_loss         | 246      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 599      |\n",
            "|    ep_rew_mean        | -98.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1093     |\n",
            "|    iterations         | 24300    |\n",
            "|    time_elapsed       | 888      |\n",
            "|    total_timesteps    | 972000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.407   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 24299    |\n",
            "|    policy_loss        | -0.465   |\n",
            "|    value_loss         | 4.21     |\n",
            "------------------------------------\n",
            "Num timesteps: 976000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -96.53\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 595      |\n",
            "|    ep_rew_mean        | -96.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1093     |\n",
            "|    iterations         | 24400    |\n",
            "|    time_elapsed       | 892      |\n",
            "|    total_timesteps    | 976000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.404   |\n",
            "|    explained_variance | 0.964    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 24399    |\n",
            "|    policy_loss        | -0.272   |\n",
            "|    value_loss         | 2.45     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 611      |\n",
            "|    ep_rew_mean        | -94.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1093     |\n",
            "|    iterations         | 24500    |\n",
            "|    time_elapsed       | 896      |\n",
            "|    total_timesteps    | 980000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.419   |\n",
            "|    explained_variance | 0.959    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 24499    |\n",
            "|    policy_loss        | -0.0356  |\n",
            "|    value_loss         | 4.07     |\n",
            "------------------------------------\n",
            "Num timesteps: 984000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -95.74\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 611      |\n",
            "|    ep_rew_mean        | -95.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1093     |\n",
            "|    iterations         | 24600    |\n",
            "|    time_elapsed       | 899      |\n",
            "|    total_timesteps    | 984000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.385   |\n",
            "|    explained_variance | 0.931    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 24599    |\n",
            "|    policy_loss        | 0.131    |\n",
            "|    value_loss         | 4.07     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 592      |\n",
            "|    ep_rew_mean        | -95.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 24700    |\n",
            "|    time_elapsed       | 902      |\n",
            "|    total_timesteps    | 988000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.418   |\n",
            "|    explained_variance | 0.971    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 24699    |\n",
            "|    policy_loss        | 0.131    |\n",
            "|    value_loss         | 4.5      |\n",
            "------------------------------------\n",
            "Num timesteps: 992000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -94.69\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 600      |\n",
            "|    ep_rew_mean        | -94.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 24800    |\n",
            "|    time_elapsed       | 906      |\n",
            "|    total_timesteps    | 992000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.363   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 24799    |\n",
            "|    policy_loss        | -0.056   |\n",
            "|    value_loss         | 3.28     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 590      |\n",
            "|    ep_rew_mean        | -91.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 24900    |\n",
            "|    time_elapsed       | 909      |\n",
            "|    total_timesteps    | 996000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.381   |\n",
            "|    explained_variance | 0.965    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 24899    |\n",
            "|    policy_loss        | 0.128    |\n",
            "|    value_loss         | 5.26     |\n",
            "------------------------------------\n",
            "Num timesteps: 1000000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -91.36\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 566      |\n",
            "|    ep_rew_mean        | -91.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 25000    |\n",
            "|    time_elapsed       | 913      |\n",
            "|    total_timesteps    | 1000000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.433   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 24999    |\n",
            "|    policy_loss        | 0.0536   |\n",
            "|    value_loss         | 2.29     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 543      |\n",
            "|    ep_rew_mean        | -95.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 25100    |\n",
            "|    time_elapsed       | 916      |\n",
            "|    total_timesteps    | 1004000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.374   |\n",
            "|    explained_variance | 0.889    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 25099    |\n",
            "|    policy_loss        | 0.848    |\n",
            "|    value_loss         | 51.4     |\n",
            "------------------------------------\n",
            "Num timesteps: 1008000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -96.16\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 538      |\n",
            "|    ep_rew_mean        | -96.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 25200    |\n",
            "|    time_elapsed       | 920      |\n",
            "|    total_timesteps    | 1008000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.481   |\n",
            "|    explained_variance | 0.901    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 25199    |\n",
            "|    policy_loss        | 1.06     |\n",
            "|    value_loss         | 44.1     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 527      |\n",
            "|    ep_rew_mean        | -99.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 25300    |\n",
            "|    time_elapsed       | 924      |\n",
            "|    total_timesteps    | 1012000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.51    |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 25299    |\n",
            "|    policy_loss        | 0.131    |\n",
            "|    value_loss         | 2.96     |\n",
            "------------------------------------\n",
            "Num timesteps: 1016000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -95.66\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 523      |\n",
            "|    ep_rew_mean        | -95.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 25400    |\n",
            "|    time_elapsed       | 928      |\n",
            "|    total_timesteps    | 1016000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.48    |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 25399    |\n",
            "|    policy_loss        | -0.00371 |\n",
            "|    value_loss         | 3.21     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 519      |\n",
            "|    ep_rew_mean        | -95.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 25500    |\n",
            "|    time_elapsed       | 931      |\n",
            "|    total_timesteps    | 1020000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.444   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 25499    |\n",
            "|    policy_loss        | 0.935    |\n",
            "|    value_loss         | 4.93     |\n",
            "------------------------------------\n",
            "Num timesteps: 1024000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -94.31\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 510      |\n",
            "|    ep_rew_mean        | -94.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 25600    |\n",
            "|    time_elapsed       | 935      |\n",
            "|    total_timesteps    | 1024000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.437   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 25599    |\n",
            "|    policy_loss        | 0.383    |\n",
            "|    value_loss         | 2.96     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 495      |\n",
            "|    ep_rew_mean        | -94.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 25700    |\n",
            "|    time_elapsed       | 938      |\n",
            "|    total_timesteps    | 1028000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.497   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 25699    |\n",
            "|    policy_loss        | 0.197    |\n",
            "|    value_loss         | 3.22     |\n",
            "------------------------------------\n",
            "Num timesteps: 1032000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -93.68\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 516      |\n",
            "|    ep_rew_mean        | -93.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 25800    |\n",
            "|    time_elapsed       | 942      |\n",
            "|    total_timesteps    | 1032000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.443   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 25799    |\n",
            "|    policy_loss        | -0.466   |\n",
            "|    value_loss         | 4.81     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 484      |\n",
            "|    ep_rew_mean        | -89      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 25900    |\n",
            "|    time_elapsed       | 945      |\n",
            "|    total_timesteps    | 1036000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.488   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 25899    |\n",
            "|    policy_loss        | -0.157   |\n",
            "|    value_loss         | 3.55     |\n",
            "------------------------------------\n",
            "Num timesteps: 1040000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -83.04\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 488      |\n",
            "|    ep_rew_mean        | -83      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 26000    |\n",
            "|    time_elapsed       | 949      |\n",
            "|    total_timesteps    | 1040000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.357   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 25999    |\n",
            "|    policy_loss        | -0.207   |\n",
            "|    value_loss         | 3.38     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 485      |\n",
            "|    ep_rew_mean        | -84.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 26100    |\n",
            "|    time_elapsed       | 952      |\n",
            "|    total_timesteps    | 1044000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.377   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 26099    |\n",
            "|    policy_loss        | 0.229    |\n",
            "|    value_loss         | 2.36     |\n",
            "------------------------------------\n",
            "Num timesteps: 1048000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -80.16\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 480      |\n",
            "|    ep_rew_mean        | -80.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 26200    |\n",
            "|    time_elapsed       | 955      |\n",
            "|    total_timesteps    | 1048000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.279   |\n",
            "|    explained_variance | 0.775    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 26199    |\n",
            "|    policy_loss        | -0.357   |\n",
            "|    value_loss         | 23       |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 459      |\n",
            "|    ep_rew_mean        | -68.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 26300    |\n",
            "|    time_elapsed       | 959      |\n",
            "|    total_timesteps    | 1052000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.453   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 26299    |\n",
            "|    policy_loss        | 0.142    |\n",
            "|    value_loss         | 5.45     |\n",
            "------------------------------------\n",
            "Num timesteps: 1056000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -56.55\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 445      |\n",
            "|    ep_rew_mean        | -56.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 26400    |\n",
            "|    time_elapsed       | 962      |\n",
            "|    total_timesteps    | 1056000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.48    |\n",
            "|    explained_variance | 0.944    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 26399    |\n",
            "|    policy_loss        | 0.354    |\n",
            "|    value_loss         | 18.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 409      |\n",
            "|    ep_rew_mean        | -38.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 26500    |\n",
            "|    time_elapsed       | 965      |\n",
            "|    total_timesteps    | 1060000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.313   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 26499    |\n",
            "|    policy_loss        | -0.228   |\n",
            "|    value_loss         | 2.49     |\n",
            "------------------------------------\n",
            "Num timesteps: 1064000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -30.52\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 395      |\n",
            "|    ep_rew_mean        | -30.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 26600    |\n",
            "|    time_elapsed       | 967      |\n",
            "|    total_timesteps    | 1064000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.367   |\n",
            "|    explained_variance | 0.976    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 26599    |\n",
            "|    policy_loss        | 0.0342   |\n",
            "|    value_loss         | 15.7     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 402      |\n",
            "|    ep_rew_mean        | -32.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 26700    |\n",
            "|    time_elapsed       | 970      |\n",
            "|    total_timesteps    | 1068000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.401   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 26699    |\n",
            "|    policy_loss        | 0.206    |\n",
            "|    value_loss         | 3.12     |\n",
            "------------------------------------\n",
            "Num timesteps: 1072000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -30.95\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 385      |\n",
            "|    ep_rew_mean        | -31      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 26800    |\n",
            "|    time_elapsed       | 975      |\n",
            "|    total_timesteps    | 1072000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.396   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 26799    |\n",
            "|    policy_loss        | -0.0532  |\n",
            "|    value_loss         | 4.64     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 384      |\n",
            "|    ep_rew_mean        | -30.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 26900    |\n",
            "|    time_elapsed       | 977      |\n",
            "|    total_timesteps    | 1076000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.385   |\n",
            "|    explained_variance | 0.976    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 26899    |\n",
            "|    policy_loss        | -0.246   |\n",
            "|    value_loss         | 2.16     |\n",
            "------------------------------------\n",
            "Num timesteps: 1080000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -37.36\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 388      |\n",
            "|    ep_rew_mean        | -37.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 27000    |\n",
            "|    time_elapsed       | 981      |\n",
            "|    total_timesteps    | 1080000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.397   |\n",
            "|    explained_variance | 0.867    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 26999    |\n",
            "|    policy_loss        | 0.871    |\n",
            "|    value_loss         | 40.6     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 372      |\n",
            "|    ep_rew_mean        | -32.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 27100    |\n",
            "|    time_elapsed       | 984      |\n",
            "|    total_timesteps    | 1084000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.432   |\n",
            "|    explained_variance | 0.895    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 27099    |\n",
            "|    policy_loss        | -0.668   |\n",
            "|    value_loss         | 12.3     |\n",
            "------------------------------------\n",
            "Num timesteps: 1088000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -38.38\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 366      |\n",
            "|    ep_rew_mean        | -38.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 27200    |\n",
            "|    time_elapsed       | 989      |\n",
            "|    total_timesteps    | 1088000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.412   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 27199    |\n",
            "|    policy_loss        | -0.213   |\n",
            "|    value_loss         | 1.76     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 369      |\n",
            "|    ep_rew_mean        | -53.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 27300    |\n",
            "|    time_elapsed       | 992      |\n",
            "|    total_timesteps    | 1092000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.417   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 27299    |\n",
            "|    policy_loss        | -0.203   |\n",
            "|    value_loss         | 2.49     |\n",
            "------------------------------------\n",
            "Num timesteps: 1096000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -64.51\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 373      |\n",
            "|    ep_rew_mean        | -64.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 27400    |\n",
            "|    time_elapsed       | 996      |\n",
            "|    total_timesteps    | 1096000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.52    |\n",
            "|    explained_variance | 0.961    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 27399    |\n",
            "|    policy_loss        | 0.534    |\n",
            "|    value_loss         | 8.12     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 392      |\n",
            "|    ep_rew_mean        | -78.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 27500    |\n",
            "|    time_elapsed       | 999      |\n",
            "|    total_timesteps    | 1100000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.457   |\n",
            "|    explained_variance | 0.975    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 27499    |\n",
            "|    policy_loss        | 0.495    |\n",
            "|    value_loss         | 5.63     |\n",
            "------------------------------------\n",
            "Num timesteps: 1104000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -86.18\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 392      |\n",
            "|    ep_rew_mean        | -86.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 27600    |\n",
            "|    time_elapsed       | 1002     |\n",
            "|    total_timesteps    | 1104000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.398   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 27599    |\n",
            "|    policy_loss        | 0.0159   |\n",
            "|    value_loss         | 1.84     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 404      |\n",
            "|    ep_rew_mean        | -85.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 27700    |\n",
            "|    time_elapsed       | 1007     |\n",
            "|    total_timesteps    | 1108000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.433   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 27699    |\n",
            "|    policy_loss        | 0.0827   |\n",
            "|    value_loss         | 1.66     |\n",
            "------------------------------------\n",
            "Num timesteps: 1112000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -86.22\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 416      |\n",
            "|    ep_rew_mean        | -86.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 27800    |\n",
            "|    time_elapsed       | 1010     |\n",
            "|    total_timesteps    | 1112000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.455   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 27799    |\n",
            "|    policy_loss        | 0.236    |\n",
            "|    value_loss         | 1.55     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 424      |\n",
            "|    ep_rew_mean        | -82.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 27900    |\n",
            "|    time_elapsed       | 1014     |\n",
            "|    total_timesteps    | 1116000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.481   |\n",
            "|    explained_variance | 0.498    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 27899    |\n",
            "|    policy_loss        | -3.34    |\n",
            "|    value_loss         | 569      |\n",
            "------------------------------------\n",
            "Num timesteps: 1120000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -83.95\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 435      |\n",
            "|    ep_rew_mean        | -84      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 28000    |\n",
            "|    time_elapsed       | 1018     |\n",
            "|    total_timesteps    | 1120000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.439   |\n",
            "|    explained_variance | 0.976    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 27999    |\n",
            "|    policy_loss        | -0.179   |\n",
            "|    value_loss         | 2.16     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 465      |\n",
            "|    ep_rew_mean        | -84.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 28100    |\n",
            "|    time_elapsed       | 1022     |\n",
            "|    total_timesteps    | 1124000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.533   |\n",
            "|    explained_variance | 0.96     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 28099    |\n",
            "|    policy_loss        | 0.238    |\n",
            "|    value_loss         | 3.64     |\n",
            "------------------------------------\n",
            "Num timesteps: 1128000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -85.36\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 479      |\n",
            "|    ep_rew_mean        | -85.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 28200    |\n",
            "|    time_elapsed       | 1027     |\n",
            "|    total_timesteps    | 1128000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.534   |\n",
            "|    explained_variance | 0.947    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 28199    |\n",
            "|    policy_loss        | -0.446   |\n",
            "|    value_loss         | 6.57     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 501      |\n",
            "|    ep_rew_mean        | -90.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 28300    |\n",
            "|    time_elapsed       | 1032     |\n",
            "|    total_timesteps    | 1132000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.394   |\n",
            "|    explained_variance | 0.789    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 28299    |\n",
            "|    policy_loss        | -2.42    |\n",
            "|    value_loss         | 180      |\n",
            "------------------------------------\n",
            "Num timesteps: 1136000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -94.47\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 524      |\n",
            "|    ep_rew_mean        | -94.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 28400    |\n",
            "|    time_elapsed       | 1036     |\n",
            "|    total_timesteps    | 1136000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.363   |\n",
            "|    explained_variance | 0.935    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 28399    |\n",
            "|    policy_loss        | -0.129   |\n",
            "|    value_loss         | 8.98     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 534      |\n",
            "|    ep_rew_mean        | -98.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 28500    |\n",
            "|    time_elapsed       | 1039     |\n",
            "|    total_timesteps    | 1140000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.36    |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 28499    |\n",
            "|    policy_loss        | -0.303   |\n",
            "|    value_loss         | 2.33     |\n",
            "------------------------------------\n",
            "Num timesteps: 1144000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -97.83\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 531      |\n",
            "|    ep_rew_mean        | -97.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 28600    |\n",
            "|    time_elapsed       | 1042     |\n",
            "|    total_timesteps    | 1144000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.415   |\n",
            "|    explained_variance | 0.976    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 28599    |\n",
            "|    policy_loss        | 0.0343   |\n",
            "|    value_loss         | 3.52     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 546      |\n",
            "|    ep_rew_mean        | -98.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 28700    |\n",
            "|    time_elapsed       | 1045     |\n",
            "|    total_timesteps    | 1148000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.384   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 28699    |\n",
            "|    policy_loss        | -0.0968  |\n",
            "|    value_loss         | 2.64     |\n",
            "------------------------------------\n",
            "Num timesteps: 1152000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -98.70\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 561      |\n",
            "|    ep_rew_mean        | -98.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 28800    |\n",
            "|    time_elapsed       | 1049     |\n",
            "|    total_timesteps    | 1152000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.432   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 28799    |\n",
            "|    policy_loss        | -0.256   |\n",
            "|    value_loss         | 2.57     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 558      |\n",
            "|    ep_rew_mean        | -97.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 28900    |\n",
            "|    time_elapsed       | 1053     |\n",
            "|    total_timesteps    | 1156000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.447   |\n",
            "|    explained_variance | 0.952    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 28899    |\n",
            "|    policy_loss        | -0.425   |\n",
            "|    value_loss         | 6.49     |\n",
            "------------------------------------\n",
            "Num timesteps: 1160000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -96.66\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 584      |\n",
            "|    ep_rew_mean        | -96.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 29000    |\n",
            "|    time_elapsed       | 1059     |\n",
            "|    total_timesteps    | 1160000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.502   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 28999    |\n",
            "|    policy_loss        | 0.298    |\n",
            "|    value_loss         | 2.96     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 584      |\n",
            "|    ep_rew_mean        | -97      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 29100    |\n",
            "|    time_elapsed       | 1062     |\n",
            "|    total_timesteps    | 1164000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.445   |\n",
            "|    explained_variance | 0.977    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 29099    |\n",
            "|    policy_loss        | -0.112   |\n",
            "|    value_loss         | 2.84     |\n",
            "------------------------------------\n",
            "Num timesteps: 1168000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -96.29\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 590      |\n",
            "|    ep_rew_mean        | -96.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 29200    |\n",
            "|    time_elapsed       | 1066     |\n",
            "|    total_timesteps    | 1168000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.426   |\n",
            "|    explained_variance | 0.978    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 29199    |\n",
            "|    policy_loss        | -0.51    |\n",
            "|    value_loss         | 2.68     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 593      |\n",
            "|    ep_rew_mean        | -99.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 29300    |\n",
            "|    time_elapsed       | 1070     |\n",
            "|    total_timesteps    | 1172000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.443   |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 29299    |\n",
            "|    policy_loss        | -0.127   |\n",
            "|    value_loss         | 2.66     |\n",
            "------------------------------------\n",
            "Num timesteps: 1176000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -100.46\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 593      |\n",
            "|    ep_rew_mean        | -100     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 29400    |\n",
            "|    time_elapsed       | 1074     |\n",
            "|    total_timesteps    | 1176000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.383   |\n",
            "|    explained_variance | 0.977    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 29399    |\n",
            "|    policy_loss        | -0.726   |\n",
            "|    value_loss         | 5.58     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 597      |\n",
            "|    ep_rew_mean        | -103     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1093     |\n",
            "|    iterations         | 29500    |\n",
            "|    time_elapsed       | 1078     |\n",
            "|    total_timesteps    | 1180000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.44    |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 29499    |\n",
            "|    policy_loss        | -0.145   |\n",
            "|    value_loss         | 4.25     |\n",
            "------------------------------------\n",
            "Num timesteps: 1184000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -101.51\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 565      |\n",
            "|    ep_rew_mean        | -102     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 29600    |\n",
            "|    time_elapsed       | 1082     |\n",
            "|    total_timesteps    | 1184000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.413   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 29599    |\n",
            "|    policy_loss        | -0.0116  |\n",
            "|    value_loss         | 2.89     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 546      |\n",
            "|    ep_rew_mean        | -98.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 29700    |\n",
            "|    time_elapsed       | 1085     |\n",
            "|    total_timesteps    | 1188000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.367   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 29699    |\n",
            "|    policy_loss        | -0.00102 |\n",
            "|    value_loss         | 1.44     |\n",
            "------------------------------------\n",
            "Num timesteps: 1192000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -94.43\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 534      |\n",
            "|    ep_rew_mean        | -94.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 29800    |\n",
            "|    time_elapsed       | 1089     |\n",
            "|    total_timesteps    | 1192000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.47    |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 29799    |\n",
            "|    policy_loss        | -0.0672  |\n",
            "|    value_loss         | 2.42     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 531      |\n",
            "|    ep_rew_mean        | -91.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 29900    |\n",
            "|    time_elapsed       | 1092     |\n",
            "|    total_timesteps    | 1196000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.582   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 29899    |\n",
            "|    policy_loss        | -0.682   |\n",
            "|    value_loss         | 3.51     |\n",
            "------------------------------------\n",
            "Num timesteps: 1200000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -92.13\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 514      |\n",
            "|    ep_rew_mean        | -92.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 30000    |\n",
            "|    time_elapsed       | 1095     |\n",
            "|    total_timesteps    | 1200000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.481   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 29999    |\n",
            "|    policy_loss        | -0.354   |\n",
            "|    value_loss         | 2.5      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 515      |\n",
            "|    ep_rew_mean        | -93.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 30100    |\n",
            "|    time_elapsed       | 1099     |\n",
            "|    total_timesteps    | 1204000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.486   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 30099    |\n",
            "|    policy_loss        | 0.206    |\n",
            "|    value_loss         | 2.1      |\n",
            "------------------------------------\n",
            "Num timesteps: 1208000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -94.24\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 522      |\n",
            "|    ep_rew_mean        | -94.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 30200    |\n",
            "|    time_elapsed       | 1102     |\n",
            "|    total_timesteps    | 1208000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.523   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 30199    |\n",
            "|    policy_loss        | -0.0502  |\n",
            "|    value_loss         | 1.36     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 508      |\n",
            "|    ep_rew_mean        | -92.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 30300    |\n",
            "|    time_elapsed       | 1106     |\n",
            "|    total_timesteps    | 1212000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.458   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 30299    |\n",
            "|    policy_loss        | -0.386   |\n",
            "|    value_loss         | 2.78     |\n",
            "------------------------------------\n",
            "Num timesteps: 1216000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -93.84\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 516      |\n",
            "|    ep_rew_mean        | -93.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 30400    |\n",
            "|    time_elapsed       | 1111     |\n",
            "|    total_timesteps    | 1216000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.416   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 30399    |\n",
            "|    policy_loss        | -0.00474 |\n",
            "|    value_loss         | 2.57     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 501      |\n",
            "|    ep_rew_mean        | -94      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 30500    |\n",
            "|    time_elapsed       | 1114     |\n",
            "|    total_timesteps    | 1220000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.438   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 30499    |\n",
            "|    policy_loss        | -0.00304 |\n",
            "|    value_loss         | 2.35     |\n",
            "------------------------------------\n",
            "Num timesteps: 1224000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -91.42\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 504      |\n",
            "|    ep_rew_mean        | -91.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1093     |\n",
            "|    iterations         | 30600    |\n",
            "|    time_elapsed       | 1119     |\n",
            "|    total_timesteps    | 1224000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.483   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 30599    |\n",
            "|    policy_loss        | -0.101   |\n",
            "|    value_loss         | 2.01     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 495      |\n",
            "|    ep_rew_mean        | -87.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1093     |\n",
            "|    iterations         | 30700    |\n",
            "|    time_elapsed       | 1122     |\n",
            "|    total_timesteps    | 1228000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.507   |\n",
            "|    explained_variance | 0.877    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 30699    |\n",
            "|    policy_loss        | -0.143   |\n",
            "|    value_loss         | 1.24     |\n",
            "------------------------------------\n",
            "Num timesteps: 1232000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -86.79\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 493      |\n",
            "|    ep_rew_mean        | -86.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1092     |\n",
            "|    iterations         | 30800    |\n",
            "|    time_elapsed       | 1128     |\n",
            "|    total_timesteps    | 1232000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.331   |\n",
            "|    explained_variance | 0.775    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 30799    |\n",
            "|    policy_loss        | -2.59    |\n",
            "|    value_loss         | 176      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 532      |\n",
            "|    ep_rew_mean        | -88.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1092     |\n",
            "|    iterations         | 30900    |\n",
            "|    time_elapsed       | 1131     |\n",
            "|    total_timesteps    | 1236000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.455   |\n",
            "|    explained_variance | 0.952    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 30899    |\n",
            "|    policy_loss        | 0.247    |\n",
            "|    value_loss         | 3.07     |\n",
            "------------------------------------\n",
            "Num timesteps: 1240000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -87.51\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 530      |\n",
            "|    ep_rew_mean        | -87.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1090     |\n",
            "|    iterations         | 31000    |\n",
            "|    time_elapsed       | 1136     |\n",
            "|    total_timesteps    | 1240000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.416   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 30999    |\n",
            "|    policy_loss        | -0.0941  |\n",
            "|    value_loss         | 2.61     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 533      |\n",
            "|    ep_rew_mean        | -85.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1092     |\n",
            "|    iterations         | 31100    |\n",
            "|    time_elapsed       | 1139     |\n",
            "|    total_timesteps    | 1244000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.509   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 31099    |\n",
            "|    policy_loss        | -0.481   |\n",
            "|    value_loss         | 2.08     |\n",
            "------------------------------------\n",
            "Num timesteps: 1248000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -88.25\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 544      |\n",
            "|    ep_rew_mean        | -88.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1091     |\n",
            "|    iterations         | 31200    |\n",
            "|    time_elapsed       | 1143     |\n",
            "|    total_timesteps    | 1248000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.366   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 31199    |\n",
            "|    policy_loss        | 0.273    |\n",
            "|    value_loss         | 2.12     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 549      |\n",
            "|    ep_rew_mean        | -91.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1091     |\n",
            "|    iterations         | 31300    |\n",
            "|    time_elapsed       | 1146     |\n",
            "|    total_timesteps    | 1252000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.495   |\n",
            "|    explained_variance | 0.951    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 31299    |\n",
            "|    policy_loss        | -0.117   |\n",
            "|    value_loss         | 3.98     |\n",
            "------------------------------------\n",
            "Num timesteps: 1256000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -90.44\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 571      |\n",
            "|    ep_rew_mean        | -90.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1091     |\n",
            "|    iterations         | 31400    |\n",
            "|    time_elapsed       | 1150     |\n",
            "|    total_timesteps    | 1256000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.376   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 31399    |\n",
            "|    policy_loss        | 0.088    |\n",
            "|    value_loss         | 3.2      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 570      |\n",
            "|    ep_rew_mean        | -89.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1091     |\n",
            "|    iterations         | 31500    |\n",
            "|    time_elapsed       | 1153     |\n",
            "|    total_timesteps    | 1260000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.401   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 31499    |\n",
            "|    policy_loss        | 0.102    |\n",
            "|    value_loss         | 2.89     |\n",
            "------------------------------------\n",
            "Num timesteps: 1264000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -86.73\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 550      |\n",
            "|    ep_rew_mean        | -86.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1092     |\n",
            "|    iterations         | 31600    |\n",
            "|    time_elapsed       | 1157     |\n",
            "|    total_timesteps    | 1264000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.447   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 31599    |\n",
            "|    policy_loss        | -0.122   |\n",
            "|    value_loss         | 11.4     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 560      |\n",
            "|    ep_rew_mean        | -82.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1092     |\n",
            "|    iterations         | 31700    |\n",
            "|    time_elapsed       | 1161     |\n",
            "|    total_timesteps    | 1268000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.322   |\n",
            "|    explained_variance | 0.945    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 31699    |\n",
            "|    policy_loss        | -0.519   |\n",
            "|    value_loss         | 27.5     |\n",
            "------------------------------------\n",
            "Num timesteps: 1272000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -81.12\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 517      |\n",
            "|    ep_rew_mean        | -81.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1092     |\n",
            "|    iterations         | 31800    |\n",
            "|    time_elapsed       | 1164     |\n",
            "|    total_timesteps    | 1272000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.408   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 31799    |\n",
            "|    policy_loss        | 0.357    |\n",
            "|    value_loss         | 2.49     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 480      |\n",
            "|    ep_rew_mean        | -82.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1093     |\n",
            "|    iterations         | 31900    |\n",
            "|    time_elapsed       | 1166     |\n",
            "|    total_timesteps    | 1276000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.405   |\n",
            "|    explained_variance | 0.505    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 31899    |\n",
            "|    policy_loss        | -1.57    |\n",
            "|    value_loss         | 482      |\n",
            "------------------------------------\n",
            "Num timesteps: 1280000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -81.47\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 457      |\n",
            "|    ep_rew_mean        | -81.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1093     |\n",
            "|    iterations         | 32000    |\n",
            "|    time_elapsed       | 1170     |\n",
            "|    total_timesteps    | 1280000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.332   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 31999    |\n",
            "|    policy_loss        | 0.0306   |\n",
            "|    value_loss         | 4.17     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 426      |\n",
            "|    ep_rew_mean        | -79.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1093     |\n",
            "|    iterations         | 32100    |\n",
            "|    time_elapsed       | 1174     |\n",
            "|    total_timesteps    | 1284000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.444   |\n",
            "|    explained_variance | 0.967    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 32099    |\n",
            "|    policy_loss        | 0.0288   |\n",
            "|    value_loss         | 6.93     |\n",
            "------------------------------------\n",
            "Num timesteps: 1288000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -78.41\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 435      |\n",
            "|    ep_rew_mean        | -78.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1092     |\n",
            "|    iterations         | 32200    |\n",
            "|    time_elapsed       | 1178     |\n",
            "|    total_timesteps    | 1288000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.482   |\n",
            "|    explained_variance | 0.968    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 32199    |\n",
            "|    policy_loss        | 0.472    |\n",
            "|    value_loss         | 2.99     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 464      |\n",
            "|    ep_rew_mean        | -80.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1091     |\n",
            "|    iterations         | 32300    |\n",
            "|    time_elapsed       | 1183     |\n",
            "|    total_timesteps    | 1292000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.374   |\n",
            "|    explained_variance | 0.977    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 32299    |\n",
            "|    policy_loss        | -0.449   |\n",
            "|    value_loss         | 2.5      |\n",
            "------------------------------------\n",
            "Num timesteps: 1296000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -81.62\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 450      |\n",
            "|    ep_rew_mean        | -81.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1091     |\n",
            "|    iterations         | 32400    |\n",
            "|    time_elapsed       | 1186     |\n",
            "|    total_timesteps    | 1296000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.437   |\n",
            "|    explained_variance | 0.94     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 32399    |\n",
            "|    policy_loss        | -0.198   |\n",
            "|    value_loss         | 4.97     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 456      |\n",
            "|    ep_rew_mean        | -81.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1091     |\n",
            "|    iterations         | 32500    |\n",
            "|    time_elapsed       | 1190     |\n",
            "|    total_timesteps    | 1300000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.365   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 32499    |\n",
            "|    policy_loss        | -0.162   |\n",
            "|    value_loss         | 1.54     |\n",
            "------------------------------------\n",
            "Num timesteps: 1304000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -81.60\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 455      |\n",
            "|    ep_rew_mean        | -81.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1091     |\n",
            "|    iterations         | 32600    |\n",
            "|    time_elapsed       | 1194     |\n",
            "|    total_timesteps    | 1304000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.387   |\n",
            "|    explained_variance | 0.886    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 32599    |\n",
            "|    policy_loss        | -0.45    |\n",
            "|    value_loss         | 3.65     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 470      |\n",
            "|    ep_rew_mean        | -80.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1091     |\n",
            "|    iterations         | 32700    |\n",
            "|    time_elapsed       | 1198     |\n",
            "|    total_timesteps    | 1308000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.405   |\n",
            "|    explained_variance | 0.957    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 32699    |\n",
            "|    policy_loss        | -0.307   |\n",
            "|    value_loss         | 1.27     |\n",
            "------------------------------------\n",
            "Num timesteps: 1312000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -78.73\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 486      |\n",
            "|    ep_rew_mean        | -78.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1090     |\n",
            "|    iterations         | 32800    |\n",
            "|    time_elapsed       | 1202     |\n",
            "|    total_timesteps    | 1312000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.419   |\n",
            "|    explained_variance | 0.98     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 32799    |\n",
            "|    policy_loss        | -0.00554 |\n",
            "|    value_loss         | 1.86     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 501      |\n",
            "|    ep_rew_mean        | -76.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1090     |\n",
            "|    iterations         | 32900    |\n",
            "|    time_elapsed       | 1206     |\n",
            "|    total_timesteps    | 1316000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.434   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 32899    |\n",
            "|    policy_loss        | 0.0778   |\n",
            "|    value_loss         | 1.67     |\n",
            "------------------------------------\n",
            "Num timesteps: 1320000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -81.61\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 502      |\n",
            "|    ep_rew_mean        | -81.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1090     |\n",
            "|    iterations         | 33000    |\n",
            "|    time_elapsed       | 1210     |\n",
            "|    total_timesteps    | 1320000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.438   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 32999    |\n",
            "|    policy_loss        | -0.198   |\n",
            "|    value_loss         | 2.15     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 531      |\n",
            "|    ep_rew_mean        | -82.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1090     |\n",
            "|    iterations         | 33100    |\n",
            "|    time_elapsed       | 1214     |\n",
            "|    total_timesteps    | 1324000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.377   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 33099    |\n",
            "|    policy_loss        | 0.0794   |\n",
            "|    value_loss         | 1.93     |\n",
            "------------------------------------\n",
            "Num timesteps: 1328000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -82.58\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 557      |\n",
            "|    ep_rew_mean        | -82.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1090     |\n",
            "|    iterations         | 33200    |\n",
            "|    time_elapsed       | 1217     |\n",
            "|    total_timesteps    | 1328000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.48    |\n",
            "|    explained_variance | 0.434    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 33199    |\n",
            "|    policy_loss        | -2.07    |\n",
            "|    value_loss         | 565      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 573      |\n",
            "|    ep_rew_mean        | -80.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1090     |\n",
            "|    iterations         | 33300    |\n",
            "|    time_elapsed       | 1221     |\n",
            "|    total_timesteps    | 1332000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.494   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 33299    |\n",
            "|    policy_loss        | -0.408   |\n",
            "|    value_loss         | 1.54     |\n",
            "------------------------------------\n",
            "Num timesteps: 1336000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -78.50\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 608      |\n",
            "|    ep_rew_mean        | -78.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1090     |\n",
            "|    iterations         | 33400    |\n",
            "|    time_elapsed       | 1225     |\n",
            "|    total_timesteps    | 1336000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.42    |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 33399    |\n",
            "|    policy_loss        | 0.264    |\n",
            "|    value_loss         | 3.32     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 616      |\n",
            "|    ep_rew_mean        | -78.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1090     |\n",
            "|    iterations         | 33500    |\n",
            "|    time_elapsed       | 1229     |\n",
            "|    total_timesteps    | 1340000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.393   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 33499    |\n",
            "|    policy_loss        | 0.267    |\n",
            "|    value_loss         | 1.82     |\n",
            "------------------------------------\n",
            "Num timesteps: 1344000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -79.62\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 634      |\n",
            "|    ep_rew_mean        | -79.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1089     |\n",
            "|    iterations         | 33600    |\n",
            "|    time_elapsed       | 1233     |\n",
            "|    total_timesteps    | 1344000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.432   |\n",
            "|    explained_variance | 0.853    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 33599    |\n",
            "|    policy_loss        | -0.0836  |\n",
            "|    value_loss         | 1.81     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 640      |\n",
            "|    ep_rew_mean        | -77      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1088     |\n",
            "|    iterations         | 33700    |\n",
            "|    time_elapsed       | 1238     |\n",
            "|    total_timesteps    | 1348000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.445   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 33699    |\n",
            "|    policy_loss        | 0.0115   |\n",
            "|    value_loss         | 1.33     |\n",
            "------------------------------------\n",
            "Num timesteps: 1352000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -78.28\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 619      |\n",
            "|    ep_rew_mean        | -78.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1088     |\n",
            "|    iterations         | 33800    |\n",
            "|    time_elapsed       | 1241     |\n",
            "|    total_timesteps    | 1352000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.405   |\n",
            "|    explained_variance | 0.978    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 33799    |\n",
            "|    policy_loss        | 0.0113   |\n",
            "|    value_loss         | 4.78     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 603      |\n",
            "|    ep_rew_mean        | -78.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1088     |\n",
            "|    iterations         | 33900    |\n",
            "|    time_elapsed       | 1246     |\n",
            "|    total_timesteps    | 1356000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.406   |\n",
            "|    explained_variance | 0.971    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 33899    |\n",
            "|    policy_loss        | -0.506   |\n",
            "|    value_loss         | 4.55     |\n",
            "------------------------------------\n",
            "Num timesteps: 1360000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -81.08\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 589      |\n",
            "|    ep_rew_mean        | -81.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1088     |\n",
            "|    iterations         | 34000    |\n",
            "|    time_elapsed       | 1249     |\n",
            "|    total_timesteps    | 1360000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.433   |\n",
            "|    explained_variance | 0.979    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 33999    |\n",
            "|    policy_loss        | 0.345    |\n",
            "|    value_loss         | 5.85     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 567      |\n",
            "|    ep_rew_mean        | -83.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1089     |\n",
            "|    iterations         | 34100    |\n",
            "|    time_elapsed       | 1252     |\n",
            "|    total_timesteps    | 1364000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.419   |\n",
            "|    explained_variance | 0.929    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 34099    |\n",
            "|    policy_loss        | -0.806   |\n",
            "|    value_loss         | 4.87     |\n",
            "------------------------------------\n",
            "Num timesteps: 1368000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -86.48\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 536      |\n",
            "|    ep_rew_mean        | -86.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1089     |\n",
            "|    iterations         | 34200    |\n",
            "|    time_elapsed       | 1255     |\n",
            "|    total_timesteps    | 1368000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.441   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 34199    |\n",
            "|    policy_loss        | 0.282    |\n",
            "|    value_loss         | 2.58     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 524      |\n",
            "|    ep_rew_mean        | -84.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1090     |\n",
            "|    iterations         | 34300    |\n",
            "|    time_elapsed       | 1258     |\n",
            "|    total_timesteps    | 1372000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.353   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 34299    |\n",
            "|    policy_loss        | 0.334    |\n",
            "|    value_loss         | 5.09     |\n",
            "------------------------------------\n",
            "Num timesteps: 1376000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -88.01\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 474      |\n",
            "|    ep_rew_mean        | -88      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1090     |\n",
            "|    iterations         | 34400    |\n",
            "|    time_elapsed       | 1261     |\n",
            "|    total_timesteps    | 1376000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.419   |\n",
            "|    explained_variance | 0.676    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 34399    |\n",
            "|    policy_loss        | -1.31    |\n",
            "|    value_loss         | 165      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 436      |\n",
            "|    ep_rew_mean        | -94.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1091     |\n",
            "|    iterations         | 34500    |\n",
            "|    time_elapsed       | 1264     |\n",
            "|    total_timesteps    | 1380000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.463   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 34499    |\n",
            "|    policy_loss        | 0.298    |\n",
            "|    value_loss         | 3.27     |\n",
            "------------------------------------\n",
            "Num timesteps: 1384000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -92.16\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 422      |\n",
            "|    ep_rew_mean        | -92.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1091     |\n",
            "|    iterations         | 34600    |\n",
            "|    time_elapsed       | 1268     |\n",
            "|    total_timesteps    | 1384000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.425   |\n",
            "|    explained_variance | 0.976    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 34599    |\n",
            "|    policy_loss        | 0.193    |\n",
            "|    value_loss         | 2.69     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 426      |\n",
            "|    ep_rew_mean        | -95.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1090     |\n",
            "|    iterations         | 34700    |\n",
            "|    time_elapsed       | 1273     |\n",
            "|    total_timesteps    | 1388000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.418   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 34699    |\n",
            "|    policy_loss        | 0.188    |\n",
            "|    value_loss         | 2.72     |\n",
            "------------------------------------\n",
            "Num timesteps: 1392000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -96.55\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 410      |\n",
            "|    ep_rew_mean        | -96.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1090     |\n",
            "|    iterations         | 34800    |\n",
            "|    time_elapsed       | 1276     |\n",
            "|    total_timesteps    | 1392000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.39    |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 34799    |\n",
            "|    policy_loss        | -0.08    |\n",
            "|    value_loss         | 2.67     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 429      |\n",
            "|    ep_rew_mean        | -97.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1090     |\n",
            "|    iterations         | 34900    |\n",
            "|    time_elapsed       | 1279     |\n",
            "|    total_timesteps    | 1396000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.332   |\n",
            "|    explained_variance | 0.94     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 34899    |\n",
            "|    policy_loss        | 0.01     |\n",
            "|    value_loss         | 4.6      |\n",
            "------------------------------------\n",
            "Num timesteps: 1400000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -94.73\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 424      |\n",
            "|    ep_rew_mean        | -94.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1090     |\n",
            "|    iterations         | 35000    |\n",
            "|    time_elapsed       | 1283     |\n",
            "|    total_timesteps    | 1400000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.393   |\n",
            "|    explained_variance | 0.954    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 34999    |\n",
            "|    policy_loss        | -0.0314  |\n",
            "|    value_loss         | 2.44     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 452      |\n",
            "|    ep_rew_mean        | -92.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1090     |\n",
            "|    iterations         | 35100    |\n",
            "|    time_elapsed       | 1287     |\n",
            "|    total_timesteps    | 1404000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.435   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 35099    |\n",
            "|    policy_loss        | 0.224    |\n",
            "|    value_loss         | 1.39     |\n",
            "------------------------------------\n",
            "Num timesteps: 1408000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -91.98\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 461      |\n",
            "|    ep_rew_mean        | -92      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1090     |\n",
            "|    iterations         | 35200    |\n",
            "|    time_elapsed       | 1291     |\n",
            "|    total_timesteps    | 1408000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.373   |\n",
            "|    explained_variance | 0.95     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 35199    |\n",
            "|    policy_loss        | -0.197   |\n",
            "|    value_loss         | 3.84     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 476      |\n",
            "|    ep_rew_mean        | -91.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1089     |\n",
            "|    iterations         | 35300    |\n",
            "|    time_elapsed       | 1295     |\n",
            "|    total_timesteps    | 1412000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.431   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 35299    |\n",
            "|    policy_loss        | -0.102   |\n",
            "|    value_loss         | 1.54     |\n",
            "------------------------------------\n",
            "Num timesteps: 1416000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -90.50\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 493      |\n",
            "|    ep_rew_mean        | -90.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1088     |\n",
            "|    iterations         | 35400    |\n",
            "|    time_elapsed       | 1300     |\n",
            "|    total_timesteps    | 1416000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.34    |\n",
            "|    explained_variance | 0.943    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 35399    |\n",
            "|    policy_loss        | -0.234   |\n",
            "|    value_loss         | 2.4      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 519      |\n",
            "|    ep_rew_mean        | -88.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1087     |\n",
            "|    iterations         | 35500    |\n",
            "|    time_elapsed       | 1305     |\n",
            "|    total_timesteps    | 1420000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.337   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 35499    |\n",
            "|    policy_loss        | 0.0792   |\n",
            "|    value_loss         | 1.25     |\n",
            "------------------------------------\n",
            "Num timesteps: 1424000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -89.46\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 526      |\n",
            "|    ep_rew_mean        | -89.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1087     |\n",
            "|    iterations         | 35600    |\n",
            "|    time_elapsed       | 1309     |\n",
            "|    total_timesteps    | 1424000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.415   |\n",
            "|    explained_variance | 0.906    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 35599    |\n",
            "|    policy_loss        | -0.129   |\n",
            "|    value_loss         | 2.71     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 543      |\n",
            "|    ep_rew_mean        | -89.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1085     |\n",
            "|    iterations         | 35700    |\n",
            "|    time_elapsed       | 1315     |\n",
            "|    total_timesteps    | 1428000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.334   |\n",
            "|    explained_variance | 0.962    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 35699    |\n",
            "|    policy_loss        | -0.298   |\n",
            "|    value_loss         | 3.32     |\n",
            "------------------------------------\n",
            "Num timesteps: 1432000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -90.76\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 568      |\n",
            "|    ep_rew_mean        | -90.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1085     |\n",
            "|    iterations         | 35800    |\n",
            "|    time_elapsed       | 1318     |\n",
            "|    total_timesteps    | 1432000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.312   |\n",
            "|    explained_variance | 0.932    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 35799    |\n",
            "|    policy_loss        | -0.193   |\n",
            "|    value_loss         | 2.57     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 575      |\n",
            "|    ep_rew_mean        | -89.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1085     |\n",
            "|    iterations         | 35900    |\n",
            "|    time_elapsed       | 1322     |\n",
            "|    total_timesteps    | 1436000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.47    |\n",
            "|    explained_variance | 0.835    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 35899    |\n",
            "|    policy_loss        | 0.00485  |\n",
            "|    value_loss         | 2.78     |\n",
            "------------------------------------\n",
            "Num timesteps: 1440000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -90.20\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 610      |\n",
            "|    ep_rew_mean        | -90.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1085     |\n",
            "|    iterations         | 36000    |\n",
            "|    time_elapsed       | 1326     |\n",
            "|    total_timesteps    | 1440000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.344   |\n",
            "|    explained_variance | 0.934    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 35999    |\n",
            "|    policy_loss        | -0.332   |\n",
            "|    value_loss         | 7.23     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 616      |\n",
            "|    ep_rew_mean        | -92.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1084     |\n",
            "|    iterations         | 36100    |\n",
            "|    time_elapsed       | 1331     |\n",
            "|    total_timesteps    | 1444000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.412   |\n",
            "|    explained_variance | 0.973    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 36099    |\n",
            "|    policy_loss        | -0.16    |\n",
            "|    value_loss         | 2.71     |\n",
            "------------------------------------\n",
            "Num timesteps: 1448000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -99.88\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 611      |\n",
            "|    ep_rew_mean        | -99.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1084     |\n",
            "|    iterations         | 36200    |\n",
            "|    time_elapsed       | 1334     |\n",
            "|    total_timesteps    | 1448000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.351   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 36199    |\n",
            "|    policy_loss        | 0.178    |\n",
            "|    value_loss         | 1.63     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 614      |\n",
            "|    ep_rew_mean        | -103     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1084     |\n",
            "|    iterations         | 36300    |\n",
            "|    time_elapsed       | 1338     |\n",
            "|    total_timesteps    | 1452000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.381   |\n",
            "|    explained_variance | 0.974    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 36299    |\n",
            "|    policy_loss        | -0.48    |\n",
            "|    value_loss         | 1.82     |\n",
            "------------------------------------\n",
            "Num timesteps: 1456000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -103.47\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 645      |\n",
            "|    ep_rew_mean        | -103     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1083     |\n",
            "|    iterations         | 36400    |\n",
            "|    time_elapsed       | 1343     |\n",
            "|    total_timesteps    | 1456000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.336   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 36399    |\n",
            "|    policy_loss        | -0.155   |\n",
            "|    value_loss         | 2.54     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 645      |\n",
            "|    ep_rew_mean        | -105     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1084     |\n",
            "|    iterations         | 36500    |\n",
            "|    time_elapsed       | 1346     |\n",
            "|    total_timesteps    | 1460000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.393   |\n",
            "|    explained_variance | 0.97     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 36499    |\n",
            "|    policy_loss        | 0.0479   |\n",
            "|    value_loss         | 2.37     |\n",
            "------------------------------------\n",
            "Num timesteps: 1464000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -107.41\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 633      |\n",
            "|    ep_rew_mean        | -107     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1084     |\n",
            "|    iterations         | 36600    |\n",
            "|    time_elapsed       | 1350     |\n",
            "|    total_timesteps    | 1464000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.421   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 36599    |\n",
            "|    policy_loss        | 0.526    |\n",
            "|    value_loss         | 4.06     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 613      |\n",
            "|    ep_rew_mean        | -113     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1084     |\n",
            "|    iterations         | 36700    |\n",
            "|    time_elapsed       | 1353     |\n",
            "|    total_timesteps    | 1468000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.31    |\n",
            "|    explained_variance | 0.966    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 36699    |\n",
            "|    policy_loss        | 0.039    |\n",
            "|    value_loss         | 3.31     |\n",
            "------------------------------------\n",
            "Num timesteps: 1472000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -117.42\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 579      |\n",
            "|    ep_rew_mean        | -117     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1085     |\n",
            "|    iterations         | 36800    |\n",
            "|    time_elapsed       | 1356     |\n",
            "|    total_timesteps    | 1472000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.384   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 36799    |\n",
            "|    policy_loss        | -0.646   |\n",
            "|    value_loss         | 5.5      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 550      |\n",
            "|    ep_rew_mean        | -122     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1085     |\n",
            "|    iterations         | 36900    |\n",
            "|    time_elapsed       | 1359     |\n",
            "|    total_timesteps    | 1476000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.4     |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 36899    |\n",
            "|    policy_loss        | 0.25     |\n",
            "|    value_loss         | 4.98     |\n",
            "------------------------------------\n",
            "Num timesteps: 1480000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -120.93\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 528      |\n",
            "|    ep_rew_mean        | -121     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1085     |\n",
            "|    iterations         | 37000    |\n",
            "|    time_elapsed       | 1362     |\n",
            "|    total_timesteps    | 1480000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.429   |\n",
            "|    explained_variance | 0.936    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 36999    |\n",
            "|    policy_loss        | 0.123    |\n",
            "|    value_loss         | 2.71     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 524      |\n",
            "|    ep_rew_mean        | -121     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1085     |\n",
            "|    iterations         | 37100    |\n",
            "|    time_elapsed       | 1366     |\n",
            "|    total_timesteps    | 1484000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.286   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 37099    |\n",
            "|    policy_loss        | -0.00523 |\n",
            "|    value_loss         | 3.72     |\n",
            "------------------------------------\n",
            "Num timesteps: 1488000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -122.30\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 529      |\n",
            "|    ep_rew_mean        | -122     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1085     |\n",
            "|    iterations         | 37200    |\n",
            "|    time_elapsed       | 1370     |\n",
            "|    total_timesteps    | 1488000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.388   |\n",
            "|    explained_variance | 0.885    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 37199    |\n",
            "|    policy_loss        | 0.18     |\n",
            "|    value_loss         | 2.51     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 539      |\n",
            "|    ep_rew_mean        | -119     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1085     |\n",
            "|    iterations         | 37300    |\n",
            "|    time_elapsed       | 1374     |\n",
            "|    total_timesteps    | 1492000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.349   |\n",
            "|    explained_variance | 0.873    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 37299    |\n",
            "|    policy_loss        | 0.0363   |\n",
            "|    value_loss         | 4.58     |\n",
            "------------------------------------\n",
            "Num timesteps: 1496000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -118.96\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 534      |\n",
            "|    ep_rew_mean        | -119     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1085     |\n",
            "|    iterations         | 37400    |\n",
            "|    time_elapsed       | 1378     |\n",
            "|    total_timesteps    | 1496000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.368   |\n",
            "|    explained_variance | 0.914    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 37399    |\n",
            "|    policy_loss        | 0.441    |\n",
            "|    value_loss         | 4.37     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 555      |\n",
            "|    ep_rew_mean        | -115     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1085     |\n",
            "|    iterations         | 37500    |\n",
            "|    time_elapsed       | 1382     |\n",
            "|    total_timesteps    | 1500000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.364   |\n",
            "|    explained_variance | 0.902    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 37499    |\n",
            "|    policy_loss        | 0.0472   |\n",
            "|    value_loss         | 5.73     |\n",
            "------------------------------------\n",
            "Num timesteps: 1504000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -114.45\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 562      |\n",
            "|    ep_rew_mean        | -114     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1085     |\n",
            "|    iterations         | 37600    |\n",
            "|    time_elapsed       | 1385     |\n",
            "|    total_timesteps    | 1504000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.363   |\n",
            "|    explained_variance | 0.941    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 37599    |\n",
            "|    policy_loss        | 0.0458   |\n",
            "|    value_loss         | 2.24     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 561      |\n",
            "|    ep_rew_mean        | -113     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1085     |\n",
            "|    iterations         | 37700    |\n",
            "|    time_elapsed       | 1389     |\n",
            "|    total_timesteps    | 1508000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.32    |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 37699    |\n",
            "|    policy_loss        | 0.364    |\n",
            "|    value_loss         | 3.52     |\n",
            "------------------------------------\n",
            "Num timesteps: 1512000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -112.67\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 558      |\n",
            "|    ep_rew_mean        | -113     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1085     |\n",
            "|    iterations         | 37800    |\n",
            "|    time_elapsed       | 1393     |\n",
            "|    total_timesteps    | 1512000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.349   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 37799    |\n",
            "|    policy_loss        | 0.0729   |\n",
            "|    value_loss         | 1.97     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 558      |\n",
            "|    ep_rew_mean        | -113     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1085     |\n",
            "|    iterations         | 37900    |\n",
            "|    time_elapsed       | 1396     |\n",
            "|    total_timesteps    | 1516000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.394   |\n",
            "|    explained_variance | 0.932    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 37899    |\n",
            "|    policy_loss        | -0.0385  |\n",
            "|    value_loss         | 9.18     |\n",
            "------------------------------------\n",
            "Num timesteps: 1520000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -111.23\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 557      |\n",
            "|    ep_rew_mean        | -111     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1085     |\n",
            "|    iterations         | 38000    |\n",
            "|    time_elapsed       | 1400     |\n",
            "|    total_timesteps    | 1520000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.449   |\n",
            "|    explained_variance | 0.967    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 37999    |\n",
            "|    policy_loss        | -0.675   |\n",
            "|    value_loss         | 6.13     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 557      |\n",
            "|    ep_rew_mean        | -108     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1085     |\n",
            "|    iterations         | 38100    |\n",
            "|    time_elapsed       | 1403     |\n",
            "|    total_timesteps    | 1524000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.462   |\n",
            "|    explained_variance | 0.975    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 38099    |\n",
            "|    policy_loss        | -0.198   |\n",
            "|    value_loss         | 2.63     |\n",
            "------------------------------------\n",
            "Num timesteps: 1528000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -104.47\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 581      |\n",
            "|    ep_rew_mean        | -104     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1085     |\n",
            "|    iterations         | 38200    |\n",
            "|    time_elapsed       | 1407     |\n",
            "|    total_timesteps    | 1528000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.303   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 38199    |\n",
            "|    policy_loss        | -0.281   |\n",
            "|    value_loss         | 3.4      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 597      |\n",
            "|    ep_rew_mean        | -101     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1085     |\n",
            "|    iterations         | 38300    |\n",
            "|    time_elapsed       | 1411     |\n",
            "|    total_timesteps    | 1532000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.46    |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 38299    |\n",
            "|    policy_loss        | -0.182   |\n",
            "|    value_loss         | 4.14     |\n",
            "------------------------------------\n",
            "Num timesteps: 1536000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -97.72\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 601      |\n",
            "|    ep_rew_mean        | -97.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1084     |\n",
            "|    iterations         | 38400    |\n",
            "|    time_elapsed       | 1416     |\n",
            "|    total_timesteps    | 1536000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.44    |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 38399    |\n",
            "|    policy_loss        | 0.426    |\n",
            "|    value_loss         | 3.22     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 617      |\n",
            "|    ep_rew_mean        | -97.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1084     |\n",
            "|    iterations         | 38500    |\n",
            "|    time_elapsed       | 1420     |\n",
            "|    total_timesteps    | 1540000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.359   |\n",
            "|    explained_variance | 0.974    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 38499    |\n",
            "|    policy_loss        | -0.0818  |\n",
            "|    value_loss         | 8.11     |\n",
            "------------------------------------\n",
            "Num timesteps: 1544000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -91.79\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 608      |\n",
            "|    ep_rew_mean        | -91.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1083     |\n",
            "|    iterations         | 38600    |\n",
            "|    time_elapsed       | 1424     |\n",
            "|    total_timesteps    | 1544000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.403   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 38599    |\n",
            "|    policy_loss        | 0.206    |\n",
            "|    value_loss         | 2.29     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 579      |\n",
            "|    ep_rew_mean        | -83.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1083     |\n",
            "|    iterations         | 38700    |\n",
            "|    time_elapsed       | 1429     |\n",
            "|    total_timesteps    | 1548000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.28    |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 38699    |\n",
            "|    policy_loss        | -0.161   |\n",
            "|    value_loss         | 4.22     |\n",
            "------------------------------------\n",
            "Num timesteps: 1552000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -78.63\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 557      |\n",
            "|    ep_rew_mean        | -78.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1083     |\n",
            "|    iterations         | 38800    |\n",
            "|    time_elapsed       | 1432     |\n",
            "|    total_timesteps    | 1552000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.373   |\n",
            "|    explained_variance | 0.965    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 38799    |\n",
            "|    policy_loss        | -0.137   |\n",
            "|    value_loss         | 10.3     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 563      |\n",
            "|    ep_rew_mean        | -80      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1083     |\n",
            "|    iterations         | 38900    |\n",
            "|    time_elapsed       | 1435     |\n",
            "|    total_timesteps    | 1556000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.329   |\n",
            "|    explained_variance | 0.971    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 38899    |\n",
            "|    policy_loss        | -0.0981  |\n",
            "|    value_loss         | 2.21     |\n",
            "------------------------------------\n",
            "Num timesteps: 1560000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -70.24\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 540      |\n",
            "|    ep_rew_mean        | -70.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1083     |\n",
            "|    iterations         | 39000    |\n",
            "|    time_elapsed       | 1439     |\n",
            "|    total_timesteps    | 1560000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.417   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 38999    |\n",
            "|    policy_loss        | -0.229   |\n",
            "|    value_loss         | 2.68     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 524      |\n",
            "|    ep_rew_mean        | -66.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1083     |\n",
            "|    iterations         | 39100    |\n",
            "|    time_elapsed       | 1443     |\n",
            "|    total_timesteps    | 1564000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.328   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 39099    |\n",
            "|    policy_loss        | 0.0904   |\n",
            "|    value_loss         | 1.82     |\n",
            "------------------------------------\n",
            "Num timesteps: 1568000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -64.23\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 535      |\n",
            "|    ep_rew_mean        | -64.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1081     |\n",
            "|    iterations         | 39200    |\n",
            "|    time_elapsed       | 1449     |\n",
            "|    total_timesteps    | 1568000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.425   |\n",
            "|    explained_variance | 0.908    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 39199    |\n",
            "|    policy_loss        | -0.348   |\n",
            "|    value_loss         | 3.08     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 551      |\n",
            "|    ep_rew_mean        | -52.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1081     |\n",
            "|    iterations         | 39300    |\n",
            "|    time_elapsed       | 1453     |\n",
            "|    total_timesteps    | 1572000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.348   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 39299    |\n",
            "|    policy_loss        | -0.161   |\n",
            "|    value_loss         | 2.44     |\n",
            "------------------------------------\n",
            "Num timesteps: 1576000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -49.05\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 549      |\n",
            "|    ep_rew_mean        | -49      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1081     |\n",
            "|    iterations         | 39400    |\n",
            "|    time_elapsed       | 1456     |\n",
            "|    total_timesteps    | 1576000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.388   |\n",
            "|    explained_variance | 0.898    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 39399    |\n",
            "|    policy_loss        | -0.346   |\n",
            "|    value_loss         | 3.1      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 560      |\n",
            "|    ep_rew_mean        | -48.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1081     |\n",
            "|    iterations         | 39500    |\n",
            "|    time_elapsed       | 1460     |\n",
            "|    total_timesteps    | 1580000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.369   |\n",
            "|    explained_variance | 0.964    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 39499    |\n",
            "|    policy_loss        | 0.285    |\n",
            "|    value_loss         | 2.85     |\n",
            "------------------------------------\n",
            "Num timesteps: 1584000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -47.91\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 555      |\n",
            "|    ep_rew_mean        | -47.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1081     |\n",
            "|    iterations         | 39600    |\n",
            "|    time_elapsed       | 1464     |\n",
            "|    total_timesteps    | 1584000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.438   |\n",
            "|    explained_variance | 0.798    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 39599    |\n",
            "|    policy_loss        | -1.52    |\n",
            "|    value_loss         | 193      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 556      |\n",
            "|    ep_rew_mean        | -47.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1081     |\n",
            "|    iterations         | 39700    |\n",
            "|    time_elapsed       | 1468     |\n",
            "|    total_timesteps    | 1588000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.44    |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 39699    |\n",
            "|    policy_loss        | 0.282    |\n",
            "|    value_loss         | 1.9      |\n",
            "------------------------------------\n",
            "Num timesteps: 1592000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -50.49\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 549      |\n",
            "|    ep_rew_mean        | -50.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1080     |\n",
            "|    iterations         | 39800    |\n",
            "|    time_elapsed       | 1473     |\n",
            "|    total_timesteps    | 1592000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.386   |\n",
            "|    explained_variance | 0.975    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 39799    |\n",
            "|    policy_loss        | 0.19     |\n",
            "|    value_loss         | 3.23     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 555      |\n",
            "|    ep_rew_mean        | -47      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1080     |\n",
            "|    iterations         | 39900    |\n",
            "|    time_elapsed       | 1477     |\n",
            "|    total_timesteps    | 1596000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.441   |\n",
            "|    explained_variance | 0.802    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 39899    |\n",
            "|    policy_loss        | -0.0478  |\n",
            "|    value_loss         | 2.67     |\n",
            "------------------------------------\n",
            "Num timesteps: 1600000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -51.28\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 576      |\n",
            "|    ep_rew_mean        | -51.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1080     |\n",
            "|    iterations         | 40000    |\n",
            "|    time_elapsed       | 1480     |\n",
            "|    total_timesteps    | 1600000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.357   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 39999    |\n",
            "|    policy_loss        | 0.0463   |\n",
            "|    value_loss         | 1.37     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 576      |\n",
            "|    ep_rew_mean        | -50.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1080     |\n",
            "|    iterations         | 40100    |\n",
            "|    time_elapsed       | 1484     |\n",
            "|    total_timesteps    | 1604000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.426   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 40099    |\n",
            "|    policy_loss        | 0.0283   |\n",
            "|    value_loss         | 0.969    |\n",
            "------------------------------------\n",
            "Num timesteps: 1608000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -54.05\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 594      |\n",
            "|    ep_rew_mean        | -54      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1080     |\n",
            "|    iterations         | 40200    |\n",
            "|    time_elapsed       | 1488     |\n",
            "|    total_timesteps    | 1608000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.388   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 40199    |\n",
            "|    policy_loss        | -0.451   |\n",
            "|    value_loss         | 2.52     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 603      |\n",
            "|    ep_rew_mean        | -58.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1079     |\n",
            "|    iterations         | 40300    |\n",
            "|    time_elapsed       | 1492     |\n",
            "|    total_timesteps    | 1612000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.368   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 40299    |\n",
            "|    policy_loss        | -0.116   |\n",
            "|    value_loss         | 1.99     |\n",
            "------------------------------------\n",
            "Num timesteps: 1616000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -61.41\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 593      |\n",
            "|    ep_rew_mean        | -61.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1079     |\n",
            "|    iterations         | 40400    |\n",
            "|    time_elapsed       | 1496     |\n",
            "|    total_timesteps    | 1616000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.334   |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 40399    |\n",
            "|    policy_loss        | 0.0199   |\n",
            "|    value_loss         | 6.02     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 593      |\n",
            "|    ep_rew_mean        | -62.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1080     |\n",
            "|    iterations         | 40500    |\n",
            "|    time_elapsed       | 1499     |\n",
            "|    total_timesteps    | 1620000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.397   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 40499    |\n",
            "|    policy_loss        | 0.0691   |\n",
            "|    value_loss         | 1.59     |\n",
            "------------------------------------\n",
            "Num timesteps: 1624000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -70.89\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 563      |\n",
            "|    ep_rew_mean        | -70.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1079     |\n",
            "|    iterations         | 40600    |\n",
            "|    time_elapsed       | 1503     |\n",
            "|    total_timesteps    | 1624000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.276   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 40599    |\n",
            "|    policy_loss        | 0.0212   |\n",
            "|    value_loss         | 8.71     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 549      |\n",
            "|    ep_rew_mean        | -79.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1080     |\n",
            "|    iterations         | 40700    |\n",
            "|    time_elapsed       | 1507     |\n",
            "|    total_timesteps    | 1628000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.343   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 40699    |\n",
            "|    policy_loss        | -0.0553  |\n",
            "|    value_loss         | 1.95     |\n",
            "------------------------------------\n",
            "Num timesteps: 1632000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -82.98\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 527      |\n",
            "|    ep_rew_mean        | -83      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1080     |\n",
            "|    iterations         | 40800    |\n",
            "|    time_elapsed       | 1510     |\n",
            "|    total_timesteps    | 1632000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.388   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 40799    |\n",
            "|    policy_loss        | -0.0139  |\n",
            "|    value_loss         | 1.47     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 515      |\n",
            "|    ep_rew_mean        | -83.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1079     |\n",
            "|    iterations         | 40900    |\n",
            "|    time_elapsed       | 1514     |\n",
            "|    total_timesteps    | 1636000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.424   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 40899    |\n",
            "|    policy_loss        | -0.0533  |\n",
            "|    value_loss         | 2.87     |\n",
            "------------------------------------\n",
            "Num timesteps: 1640000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -82.32\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 521      |\n",
            "|    ep_rew_mean        | -82.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1079     |\n",
            "|    iterations         | 41000    |\n",
            "|    time_elapsed       | 1519     |\n",
            "|    total_timesteps    | 1640000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.384   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 40999    |\n",
            "|    policy_loss        | -0.146   |\n",
            "|    value_loss         | 5.26     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 495      |\n",
            "|    ep_rew_mean        | -76      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1079     |\n",
            "|    iterations         | 41100    |\n",
            "|    time_elapsed       | 1522     |\n",
            "|    total_timesteps    | 1644000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.327   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 41099    |\n",
            "|    policy_loss        | -0.249   |\n",
            "|    value_loss         | 2.98     |\n",
            "------------------------------------\n",
            "Num timesteps: 1648000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -76.31\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 487      |\n",
            "|    ep_rew_mean        | -76.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1079     |\n",
            "|    iterations         | 41200    |\n",
            "|    time_elapsed       | 1526     |\n",
            "|    total_timesteps    | 1648000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.345   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 41199    |\n",
            "|    policy_loss        | 0.231    |\n",
            "|    value_loss         | 3.59     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 483      |\n",
            "|    ep_rew_mean        | -78.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1079     |\n",
            "|    iterations         | 41300    |\n",
            "|    time_elapsed       | 1530     |\n",
            "|    total_timesteps    | 1652000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.424   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 41299    |\n",
            "|    policy_loss        | -0.335   |\n",
            "|    value_loss         | 4.17     |\n",
            "------------------------------------\n",
            "Num timesteps: 1656000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -81.69\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 472      |\n",
            "|    ep_rew_mean        | -81.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1079     |\n",
            "|    iterations         | 41400    |\n",
            "|    time_elapsed       | 1533     |\n",
            "|    total_timesteps    | 1656000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.384   |\n",
            "|    explained_variance | 0.809    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 41399    |\n",
            "|    policy_loss        | -1.45    |\n",
            "|    value_loss         | 258      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 475      |\n",
            "|    ep_rew_mean        | -79.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1079     |\n",
            "|    iterations         | 41500    |\n",
            "|    time_elapsed       | 1537     |\n",
            "|    total_timesteps    | 1660000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.379   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 41499    |\n",
            "|    policy_loss        | -0.06    |\n",
            "|    value_loss         | 1.77     |\n",
            "------------------------------------\n",
            "Num timesteps: 1664000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -77.22\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 484      |\n",
            "|    ep_rew_mean        | -77.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1079     |\n",
            "|    iterations         | 41600    |\n",
            "|    time_elapsed       | 1541     |\n",
            "|    total_timesteps    | 1664000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.443   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 41599    |\n",
            "|    policy_loss        | -0.0905  |\n",
            "|    value_loss         | 1.99     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 496      |\n",
            "|    ep_rew_mean        | -81.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1078     |\n",
            "|    iterations         | 41700    |\n",
            "|    time_elapsed       | 1546     |\n",
            "|    total_timesteps    | 1668000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.412   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 41699    |\n",
            "|    policy_loss        | -0.517   |\n",
            "|    value_loss         | 3.44     |\n",
            "------------------------------------\n",
            "Num timesteps: 1672000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -78.91\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 510      |\n",
            "|    ep_rew_mean        | -78.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1078     |\n",
            "|    iterations         | 41800    |\n",
            "|    time_elapsed       | 1550     |\n",
            "|    total_timesteps    | 1672000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.409   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 41799    |\n",
            "|    policy_loss        | 0.0428   |\n",
            "|    value_loss         | 2.91     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 511      |\n",
            "|    ep_rew_mean        | -78      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1078     |\n",
            "|    iterations         | 41900    |\n",
            "|    time_elapsed       | 1553     |\n",
            "|    total_timesteps    | 1676000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.359   |\n",
            "|    explained_variance | 0.946    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 41899    |\n",
            "|    policy_loss        | -0.192   |\n",
            "|    value_loss         | 4.6      |\n",
            "------------------------------------\n",
            "Num timesteps: 1680000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -75.47\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 518      |\n",
            "|    ep_rew_mean        | -75.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1078     |\n",
            "|    iterations         | 42000    |\n",
            "|    time_elapsed       | 1558     |\n",
            "|    total_timesteps    | 1680000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.326   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 41999    |\n",
            "|    policy_loss        | 0.343    |\n",
            "|    value_loss         | 4.69     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 545      |\n",
            "|    ep_rew_mean        | -71.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1078     |\n",
            "|    iterations         | 42100    |\n",
            "|    time_elapsed       | 1561     |\n",
            "|    total_timesteps    | 1684000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.397   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 42099    |\n",
            "|    policy_loss        | 0.000613 |\n",
            "|    value_loss         | 2.01     |\n",
            "------------------------------------\n",
            "Num timesteps: 1688000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -72.97\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 549      |\n",
            "|    ep_rew_mean        | -73      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1078     |\n",
            "|    iterations         | 42200    |\n",
            "|    time_elapsed       | 1565     |\n",
            "|    total_timesteps    | 1688000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.388   |\n",
            "|    explained_variance | 0.742    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 42199    |\n",
            "|    policy_loss        | 0.0319   |\n",
            "|    value_loss         | 3.26     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 568      |\n",
            "|    ep_rew_mean        | -76.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1077     |\n",
            "|    iterations         | 42300    |\n",
            "|    time_elapsed       | 1569     |\n",
            "|    total_timesteps    | 1692000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.364   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 42299    |\n",
            "|    policy_loss        | -0.308   |\n",
            "|    value_loss         | 2.77     |\n",
            "------------------------------------\n",
            "Num timesteps: 1696000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -73.70\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 585      |\n",
            "|    ep_rew_mean        | -73.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1077     |\n",
            "|    iterations         | 42400    |\n",
            "|    time_elapsed       | 1573     |\n",
            "|    total_timesteps    | 1696000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.359   |\n",
            "|    explained_variance | 0.975    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 42399    |\n",
            "|    policy_loss        | -0.0852  |\n",
            "|    value_loss         | 4.87     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 589      |\n",
            "|    ep_rew_mean        | -75      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1077     |\n",
            "|    iterations         | 42500    |\n",
            "|    time_elapsed       | 1577     |\n",
            "|    total_timesteps    | 1700000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.327   |\n",
            "|    explained_variance | 0.979    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 42499    |\n",
            "|    policy_loss        | -0.184   |\n",
            "|    value_loss         | 3.68     |\n",
            "------------------------------------\n",
            "Num timesteps: 1704000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -80.00\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 587      |\n",
            "|    ep_rew_mean        | -80      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1076     |\n",
            "|    iterations         | 42600    |\n",
            "|    time_elapsed       | 1582     |\n",
            "|    total_timesteps    | 1704000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.438   |\n",
            "|    explained_variance | 0.947    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 42599    |\n",
            "|    policy_loss        | 0.323    |\n",
            "|    value_loss         | 4.35     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 597      |\n",
            "|    ep_rew_mean        | -81.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1075     |\n",
            "|    iterations         | 42700    |\n",
            "|    time_elapsed       | 1587     |\n",
            "|    total_timesteps    | 1708000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.352   |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 42699    |\n",
            "|    policy_loss        | -0.0965  |\n",
            "|    value_loss         | 3.97     |\n",
            "------------------------------------\n",
            "Num timesteps: 1712000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -72.30\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 593      |\n",
            "|    ep_rew_mean        | -72.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1075     |\n",
            "|    iterations         | 42800    |\n",
            "|    time_elapsed       | 1591     |\n",
            "|    total_timesteps    | 1712000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.313   |\n",
            "|    explained_variance | 0.958    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 42799    |\n",
            "|    policy_loss        | -0.0763  |\n",
            "|    value_loss         | 2.47     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 592      |\n",
            "|    ep_rew_mean        | -68.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1075     |\n",
            "|    iterations         | 42900    |\n",
            "|    time_elapsed       | 1595     |\n",
            "|    total_timesteps    | 1716000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.405   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 42899    |\n",
            "|    policy_loss        | 0.0146   |\n",
            "|    value_loss         | 2.61     |\n",
            "------------------------------------\n",
            "Num timesteps: 1720000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -67.58\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 586      |\n",
            "|    ep_rew_mean        | -67.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1075     |\n",
            "|    iterations         | 43000    |\n",
            "|    time_elapsed       | 1598     |\n",
            "|    total_timesteps    | 1720000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.356   |\n",
            "|    explained_variance | 0.686    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 42999    |\n",
            "|    policy_loss        | -0.29    |\n",
            "|    value_loss         | 248      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 605      |\n",
            "|    ep_rew_mean        | -63.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1074     |\n",
            "|    iterations         | 43100    |\n",
            "|    time_elapsed       | 1604     |\n",
            "|    total_timesteps    | 1724000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.365   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 43099    |\n",
            "|    policy_loss        | -0.534   |\n",
            "|    value_loss         | 8.64     |\n",
            "------------------------------------\n",
            "Num timesteps: 1728000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -58.28\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 597      |\n",
            "|    ep_rew_mean        | -58.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1074     |\n",
            "|    iterations         | 43200    |\n",
            "|    time_elapsed       | 1608     |\n",
            "|    total_timesteps    | 1728000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.376   |\n",
            "|    explained_variance | 0.975    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 43199    |\n",
            "|    policy_loss        | -0.202   |\n",
            "|    value_loss         | 1.9      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 608      |\n",
            "|    ep_rew_mean        | -59.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1074     |\n",
            "|    iterations         | 43300    |\n",
            "|    time_elapsed       | 1612     |\n",
            "|    total_timesteps    | 1732000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.325   |\n",
            "|    explained_variance | 0.918    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 43299    |\n",
            "|    policy_loss        | -0.417   |\n",
            "|    value_loss         | 1.64     |\n",
            "------------------------------------\n",
            "Num timesteps: 1736000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -59.61\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 638      |\n",
            "|    ep_rew_mean        | -59.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1073     |\n",
            "|    iterations         | 43400    |\n",
            "|    time_elapsed       | 1617     |\n",
            "|    total_timesteps    | 1736000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.364   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 43399    |\n",
            "|    policy_loss        | 0.0335   |\n",
            "|    value_loss         | 3.09     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 618      |\n",
            "|    ep_rew_mean        | -58.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1072     |\n",
            "|    iterations         | 43500    |\n",
            "|    time_elapsed       | 1621     |\n",
            "|    total_timesteps    | 1740000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.228   |\n",
            "|    explained_variance | 0.966    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 43499    |\n",
            "|    policy_loss        | -0.0797  |\n",
            "|    value_loss         | 11       |\n",
            "------------------------------------\n",
            "Num timesteps: 1744000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -63.58\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 607      |\n",
            "|    ep_rew_mean        | -63.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1073     |\n",
            "|    iterations         | 43600    |\n",
            "|    time_elapsed       | 1625     |\n",
            "|    total_timesteps    | 1744000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.368   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 43599    |\n",
            "|    policy_loss        | -0.43    |\n",
            "|    value_loss         | 2.35     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 614      |\n",
            "|    ep_rew_mean        | -65.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1072     |\n",
            "|    iterations         | 43700    |\n",
            "|    time_elapsed       | 1629     |\n",
            "|    total_timesteps    | 1748000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.314   |\n",
            "|    explained_variance | 0.98     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 43699    |\n",
            "|    policy_loss        | -0.106   |\n",
            "|    value_loss         | 3.72     |\n",
            "------------------------------------\n",
            "Num timesteps: 1752000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -63.54\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 593      |\n",
            "|    ep_rew_mean        | -63.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1072     |\n",
            "|    iterations         | 43800    |\n",
            "|    time_elapsed       | 1632     |\n",
            "|    total_timesteps    | 1752000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.379   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 43799    |\n",
            "|    policy_loss        | 0.092    |\n",
            "|    value_loss         | 2.57     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 578      |\n",
            "|    ep_rew_mean        | -62.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1073     |\n",
            "|    iterations         | 43900    |\n",
            "|    time_elapsed       | 1635     |\n",
            "|    total_timesteps    | 1756000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.37    |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 43899    |\n",
            "|    policy_loss        | -0.193   |\n",
            "|    value_loss         | 3.38     |\n",
            "------------------------------------\n",
            "Num timesteps: 1760000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -61.26\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 578      |\n",
            "|    ep_rew_mean        | -61.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1072     |\n",
            "|    iterations         | 44000    |\n",
            "|    time_elapsed       | 1640     |\n",
            "|    total_timesteps    | 1760000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.307   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 43999    |\n",
            "|    policy_loss        | 0.0844   |\n",
            "|    value_loss         | 0.653    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 600      |\n",
            "|    ep_rew_mean        | -61.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1072     |\n",
            "|    iterations         | 44100    |\n",
            "|    time_elapsed       | 1644     |\n",
            "|    total_timesteps    | 1764000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.324   |\n",
            "|    explained_variance | 0.973    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 44099    |\n",
            "|    policy_loss        | -0.0156  |\n",
            "|    value_loss         | 4.06     |\n",
            "------------------------------------\n",
            "Num timesteps: 1768000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -60.93\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 594      |\n",
            "|    ep_rew_mean        | -60.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1072     |\n",
            "|    iterations         | 44200    |\n",
            "|    time_elapsed       | 1648     |\n",
            "|    total_timesteps    | 1768000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.366   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 44199    |\n",
            "|    policy_loss        | -0.22    |\n",
            "|    value_loss         | 2.71     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 588      |\n",
            "|    ep_rew_mean        | -69.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1072     |\n",
            "|    iterations         | 44300    |\n",
            "|    time_elapsed       | 1652     |\n",
            "|    total_timesteps    | 1772000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.264   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 44299    |\n",
            "|    policy_loss        | -0.159   |\n",
            "|    value_loss         | 0.971    |\n",
            "------------------------------------\n",
            "Num timesteps: 1776000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -73.09\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 597      |\n",
            "|    ep_rew_mean        | -73.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1071     |\n",
            "|    iterations         | 44400    |\n",
            "|    time_elapsed       | 1656     |\n",
            "|    total_timesteps    | 1776000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.28    |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 44399    |\n",
            "|    policy_loss        | 0.101    |\n",
            "|    value_loss         | 3.93     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 601      |\n",
            "|    ep_rew_mean        | -73.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1071     |\n",
            "|    iterations         | 44500    |\n",
            "|    time_elapsed       | 1661     |\n",
            "|    total_timesteps    | 1780000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.389   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 44499    |\n",
            "|    policy_loss        | 0.213    |\n",
            "|    value_loss         | 2.74     |\n",
            "------------------------------------\n",
            "Num timesteps: 1784000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -83.36\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 574      |\n",
            "|    ep_rew_mean        | -83.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1071     |\n",
            "|    iterations         | 44600    |\n",
            "|    time_elapsed       | 1665     |\n",
            "|    total_timesteps    | 1784000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.383   |\n",
            "|    explained_variance | 0.979    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 44599    |\n",
            "|    policy_loss        | -0.166   |\n",
            "|    value_loss         | 5.81     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 552      |\n",
            "|    ep_rew_mean        | -85.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1071     |\n",
            "|    iterations         | 44700    |\n",
            "|    time_elapsed       | 1668     |\n",
            "|    total_timesteps    | 1788000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.237   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 44699    |\n",
            "|    policy_loss        | -0.0036  |\n",
            "|    value_loss         | 1.8      |\n",
            "------------------------------------\n",
            "Num timesteps: 1792000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -85.50\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 541      |\n",
            "|    ep_rew_mean        | -85.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1071     |\n",
            "|    iterations         | 44800    |\n",
            "|    time_elapsed       | 1672     |\n",
            "|    total_timesteps    | 1792000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.305   |\n",
            "|    explained_variance | 0.797    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 44799    |\n",
            "|    policy_loss        | -2.03    |\n",
            "|    value_loss         | 202      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 541      |\n",
            "|    ep_rew_mean        | -84.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1070     |\n",
            "|    iterations         | 44900    |\n",
            "|    time_elapsed       | 1677     |\n",
            "|    total_timesteps    | 1796000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.361   |\n",
            "|    explained_variance | 0.633    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 44899    |\n",
            "|    policy_loss        | 0.0722   |\n",
            "|    value_loss         | 47.6     |\n",
            "------------------------------------\n",
            "Num timesteps: 1800000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -85.54\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 545      |\n",
            "|    ep_rew_mean        | -85.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1070     |\n",
            "|    iterations         | 45000    |\n",
            "|    time_elapsed       | 1680     |\n",
            "|    total_timesteps    | 1800000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.388   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 44999    |\n",
            "|    policy_loss        | -0.0432  |\n",
            "|    value_loss         | 3.2      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 528      |\n",
            "|    ep_rew_mean        | -85.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1070     |\n",
            "|    iterations         | 45100    |\n",
            "|    time_elapsed       | 1685     |\n",
            "|    total_timesteps    | 1804000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.366   |\n",
            "|    explained_variance | 0.951    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 45099    |\n",
            "|    policy_loss        | -0.637   |\n",
            "|    value_loss         | 5.17     |\n",
            "------------------------------------\n",
            "Num timesteps: 1808000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -89.37\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 534      |\n",
            "|    ep_rew_mean        | -89.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1070     |\n",
            "|    iterations         | 45200    |\n",
            "|    time_elapsed       | 1689     |\n",
            "|    total_timesteps    | 1808000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.282   |\n",
            "|    explained_variance | 0.978    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 45199    |\n",
            "|    policy_loss        | -0.0152  |\n",
            "|    value_loss         | 5.35     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 528      |\n",
            "|    ep_rew_mean        | -93      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1069     |\n",
            "|    iterations         | 45300    |\n",
            "|    time_elapsed       | 1693     |\n",
            "|    total_timesteps    | 1812000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.338   |\n",
            "|    explained_variance | 0.961    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 45299    |\n",
            "|    policy_loss        | 0.332    |\n",
            "|    value_loss         | 3.26     |\n",
            "------------------------------------\n",
            "Num timesteps: 1816000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -96.11\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 520      |\n",
            "|    ep_rew_mean        | -96.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1069     |\n",
            "|    iterations         | 45400    |\n",
            "|    time_elapsed       | 1697     |\n",
            "|    total_timesteps    | 1816000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.383   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 45399    |\n",
            "|    policy_loss        | -0.0594  |\n",
            "|    value_loss         | 3.17     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 506      |\n",
            "|    ep_rew_mean        | -97.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1070     |\n",
            "|    iterations         | 45500    |\n",
            "|    time_elapsed       | 1700     |\n",
            "|    total_timesteps    | 1820000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.289   |\n",
            "|    explained_variance | 0.976    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 45499    |\n",
            "|    policy_loss        | 0.143    |\n",
            "|    value_loss         | 4.77     |\n",
            "------------------------------------\n",
            "Num timesteps: 1824000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -96.91\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 504      |\n",
            "|    ep_rew_mean        | -96.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1070     |\n",
            "|    iterations         | 45600    |\n",
            "|    time_elapsed       | 1704     |\n",
            "|    total_timesteps    | 1824000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.24    |\n",
            "|    explained_variance | 0.865    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 45599    |\n",
            "|    policy_loss        | -0.118   |\n",
            "|    value_loss         | 89.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 490      |\n",
            "|    ep_rew_mean        | -96.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1070     |\n",
            "|    iterations         | 45700    |\n",
            "|    time_elapsed       | 1707     |\n",
            "|    total_timesteps    | 1828000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.381   |\n",
            "|    explained_variance | 0.968    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 45699    |\n",
            "|    policy_loss        | -0.0528  |\n",
            "|    value_loss         | 1.13     |\n",
            "------------------------------------\n",
            "Num timesteps: 1832000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -96.28\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 502      |\n",
            "|    ep_rew_mean        | -96.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1070     |\n",
            "|    iterations         | 45800    |\n",
            "|    time_elapsed       | 1712     |\n",
            "|    total_timesteps    | 1832000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.306   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 45799    |\n",
            "|    policy_loss        | -0.175   |\n",
            "|    value_loss         | 3.6      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 509      |\n",
            "|    ep_rew_mean        | -97.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1070     |\n",
            "|    iterations         | 45900    |\n",
            "|    time_elapsed       | 1715     |\n",
            "|    total_timesteps    | 1836000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.307   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 45899    |\n",
            "|    policy_loss        | -0.0289  |\n",
            "|    value_loss         | 2.99     |\n",
            "------------------------------------\n",
            "Num timesteps: 1840000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -100.40\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 508      |\n",
            "|    ep_rew_mean        | -100     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1070     |\n",
            "|    iterations         | 46000    |\n",
            "|    time_elapsed       | 1719     |\n",
            "|    total_timesteps    | 1840000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.249   |\n",
            "|    explained_variance | 0.96     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 45999    |\n",
            "|    policy_loss        | -0.0712  |\n",
            "|    value_loss         | 6.16     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 510      |\n",
            "|    ep_rew_mean        | -97.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1069     |\n",
            "|    iterations         | 46100    |\n",
            "|    time_elapsed       | 1723     |\n",
            "|    total_timesteps    | 1844000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.33    |\n",
            "|    explained_variance | 0.972    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 46099    |\n",
            "|    policy_loss        | 0.0858   |\n",
            "|    value_loss         | 4.72     |\n",
            "------------------------------------\n",
            "Num timesteps: 1848000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -99.40\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 512      |\n",
            "|    ep_rew_mean        | -99.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1069     |\n",
            "|    iterations         | 46200    |\n",
            "|    time_elapsed       | 1727     |\n",
            "|    total_timesteps    | 1848000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.341   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 46199    |\n",
            "|    policy_loss        | -0.284   |\n",
            "|    value_loss         | 2.86     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 520      |\n",
            "|    ep_rew_mean        | -97.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1069     |\n",
            "|    iterations         | 46300    |\n",
            "|    time_elapsed       | 1732     |\n",
            "|    total_timesteps    | 1852000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.341   |\n",
            "|    explained_variance | 0.977    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 46299    |\n",
            "|    policy_loss        | -0.0222  |\n",
            "|    value_loss         | 6.36     |\n",
            "------------------------------------\n",
            "Num timesteps: 1856000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -98.28\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 536      |\n",
            "|    ep_rew_mean        | -98.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1068     |\n",
            "|    iterations         | 46400    |\n",
            "|    time_elapsed       | 1736     |\n",
            "|    total_timesteps    | 1856000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.352   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 46399    |\n",
            "|    policy_loss        | -0.161   |\n",
            "|    value_loss         | 3.46     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 534      |\n",
            "|    ep_rew_mean        | -96      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1068     |\n",
            "|    iterations         | 46500    |\n",
            "|    time_elapsed       | 1740     |\n",
            "|    total_timesteps    | 1860000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.325   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 46499    |\n",
            "|    policy_loss        | 0.611    |\n",
            "|    value_loss         | 9.1      |\n",
            "------------------------------------\n",
            "Num timesteps: 1864000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -95.42\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 525      |\n",
            "|    ep_rew_mean        | -95.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1068     |\n",
            "|    iterations         | 46600    |\n",
            "|    time_elapsed       | 1744     |\n",
            "|    total_timesteps    | 1864000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.286   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 46599    |\n",
            "|    policy_loss        | -0.168   |\n",
            "|    value_loss         | 2.76     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 519      |\n",
            "|    ep_rew_mean        | -90      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1068     |\n",
            "|    iterations         | 46700    |\n",
            "|    time_elapsed       | 1747     |\n",
            "|    total_timesteps    | 1868000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.332   |\n",
            "|    explained_variance | 0.98     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 46699    |\n",
            "|    policy_loss        | 0.293    |\n",
            "|    value_loss         | 2.15     |\n",
            "------------------------------------\n",
            "Num timesteps: 1872000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -87.32\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 534      |\n",
            "|    ep_rew_mean        | -87.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1068     |\n",
            "|    iterations         | 46800    |\n",
            "|    time_elapsed       | 1751     |\n",
            "|    total_timesteps    | 1872000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.32    |\n",
            "|    explained_variance | 0.978    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 46799    |\n",
            "|    policy_loss        | -0.346   |\n",
            "|    value_loss         | 5.09     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 548      |\n",
            "|    ep_rew_mean        | -88.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1068     |\n",
            "|    iterations         | 46900    |\n",
            "|    time_elapsed       | 1756     |\n",
            "|    total_timesteps    | 1876000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.388   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 46899    |\n",
            "|    policy_loss        | -0.395   |\n",
            "|    value_loss         | 3.47     |\n",
            "------------------------------------\n",
            "Num timesteps: 1880000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -88.17\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 561      |\n",
            "|    ep_rew_mean        | -88.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1068     |\n",
            "|    iterations         | 47000    |\n",
            "|    time_elapsed       | 1759     |\n",
            "|    total_timesteps    | 1880000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.375   |\n",
            "|    explained_variance | 0.979    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 46999    |\n",
            "|    policy_loss        | -0.0473  |\n",
            "|    value_loss         | 2.37     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 568      |\n",
            "|    ep_rew_mean        | -89.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1067     |\n",
            "|    iterations         | 47100    |\n",
            "|    time_elapsed       | 1764     |\n",
            "|    total_timesteps    | 1884000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.339   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 47099    |\n",
            "|    policy_loss        | -0.106   |\n",
            "|    value_loss         | 1.88     |\n",
            "------------------------------------\n",
            "Num timesteps: 1888000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -87.50\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 579      |\n",
            "|    ep_rew_mean        | -87.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1067     |\n",
            "|    iterations         | 47200    |\n",
            "|    time_elapsed       | 1768     |\n",
            "|    total_timesteps    | 1888000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.315   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 47199    |\n",
            "|    policy_loss        | 0.0253   |\n",
            "|    value_loss         | 2.48     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 581      |\n",
            "|    ep_rew_mean        | -83.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 47300    |\n",
            "|    time_elapsed       | 1773     |\n",
            "|    total_timesteps    | 1892000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.348   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 47299    |\n",
            "|    policy_loss        | 0.335    |\n",
            "|    value_loss         | 4.24     |\n",
            "------------------------------------\n",
            "Num timesteps: 1896000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -73.92\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 594      |\n",
            "|    ep_rew_mean        | -73.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 47400    |\n",
            "|    time_elapsed       | 1778     |\n",
            "|    total_timesteps    | 1896000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.375   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 47399    |\n",
            "|    policy_loss        | -0.175   |\n",
            "|    value_loss         | 1.82     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 605      |\n",
            "|    ep_rew_mean        | -68.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 47500    |\n",
            "|    time_elapsed       | 1782     |\n",
            "|    total_timesteps    | 1900000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.262   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 47499    |\n",
            "|    policy_loss        | 0.0819   |\n",
            "|    value_loss         | 1.6      |\n",
            "------------------------------------\n",
            "Num timesteps: 1904000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -70.57\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 620      |\n",
            "|    ep_rew_mean        | -70.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 47600    |\n",
            "|    time_elapsed       | 1786     |\n",
            "|    total_timesteps    | 1904000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.331   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 47599    |\n",
            "|    policy_loss        | -0.182   |\n",
            "|    value_loss         | 1.73     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 635      |\n",
            "|    ep_rew_mean        | -69.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 47700    |\n",
            "|    time_elapsed       | 1790     |\n",
            "|    total_timesteps    | 1908000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.322   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 47699    |\n",
            "|    policy_loss        | 0.259    |\n",
            "|    value_loss         | 2.73     |\n",
            "------------------------------------\n",
            "Num timesteps: 1912000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -67.88\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 653      |\n",
            "|    ep_rew_mean        | -67.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 47800    |\n",
            "|    time_elapsed       | 1794     |\n",
            "|    total_timesteps    | 1912000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.33    |\n",
            "|    explained_variance | 0.851    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 47799    |\n",
            "|    policy_loss        | 0.132    |\n",
            "|    value_loss         | 24.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 615      |\n",
            "|    ep_rew_mean        | -63.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 47900    |\n",
            "|    time_elapsed       | 1798     |\n",
            "|    total_timesteps    | 1916000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.256   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 47899    |\n",
            "|    policy_loss        | -0.457   |\n",
            "|    value_loss         | 2.8      |\n",
            "------------------------------------\n",
            "Num timesteps: 1920000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -61.08\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 603      |\n",
            "|    ep_rew_mean        | -61.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 48000    |\n",
            "|    time_elapsed       | 1801     |\n",
            "|    total_timesteps    | 1920000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.341   |\n",
            "|    explained_variance | 0.964    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 47999    |\n",
            "|    policy_loss        | 0.139    |\n",
            "|    value_loss         | 2.66     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 630      |\n",
            "|    ep_rew_mean        | -60.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 48100    |\n",
            "|    time_elapsed       | 1805     |\n",
            "|    total_timesteps    | 1924000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.441   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 48099    |\n",
            "|    policy_loss        | 0.183    |\n",
            "|    value_loss         | 4.26     |\n",
            "------------------------------------\n",
            "Num timesteps: 1928000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -58.41\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 626      |\n",
            "|    ep_rew_mean        | -58.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 48200    |\n",
            "|    time_elapsed       | 1809     |\n",
            "|    total_timesteps    | 1928000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.235   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 48199    |\n",
            "|    policy_loss        | 0.477    |\n",
            "|    value_loss         | 6.67     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 636      |\n",
            "|    ep_rew_mean        | -58.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 48300    |\n",
            "|    time_elapsed       | 1813     |\n",
            "|    total_timesteps    | 1932000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.304   |\n",
            "|    explained_variance | 0.975    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 48299    |\n",
            "|    policy_loss        | -0.22    |\n",
            "|    value_loss         | 3.5      |\n",
            "------------------------------------\n",
            "Num timesteps: 1936000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -57.37\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 638      |\n",
            "|    ep_rew_mean        | -57.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 48400    |\n",
            "|    time_elapsed       | 1817     |\n",
            "|    total_timesteps    | 1936000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.361   |\n",
            "|    explained_variance | 0.98     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 48399    |\n",
            "|    policy_loss        | 0.0851   |\n",
            "|    value_loss         | 1.39     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 622      |\n",
            "|    ep_rew_mean        | -53.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 48500    |\n",
            "|    time_elapsed       | 1821     |\n",
            "|    total_timesteps    | 1940000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.258   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 48499    |\n",
            "|    policy_loss        | -0.217   |\n",
            "|    value_loss         | 4.07     |\n",
            "------------------------------------\n",
            "Num timesteps: 1944000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -53.96\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 615      |\n",
            "|    ep_rew_mean        | -54      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 48600    |\n",
            "|    time_elapsed       | 1826     |\n",
            "|    total_timesteps    | 1944000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.28    |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 48599    |\n",
            "|    policy_loss        | -0.14    |\n",
            "|    value_loss         | 1.59     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 590      |\n",
            "|    ep_rew_mean        | -57.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 48700    |\n",
            "|    time_elapsed       | 1830     |\n",
            "|    total_timesteps    | 1948000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.304   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 48699    |\n",
            "|    policy_loss        | 0.175    |\n",
            "|    value_loss         | 3.25     |\n",
            "------------------------------------\n",
            "Num timesteps: 1952000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -64.90\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 546      |\n",
            "|    ep_rew_mean        | -64.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 48800    |\n",
            "|    time_elapsed       | 1832     |\n",
            "|    total_timesteps    | 1952000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.331   |\n",
            "|    explained_variance | 0.922    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 48799    |\n",
            "|    policy_loss        | 0.146    |\n",
            "|    value_loss         | 3.24     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 524      |\n",
            "|    ep_rew_mean        | -70      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 48900    |\n",
            "|    time_elapsed       | 1837     |\n",
            "|    total_timesteps    | 1956000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.284   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 48899    |\n",
            "|    policy_loss        | -0.149   |\n",
            "|    value_loss         | 1.88     |\n",
            "------------------------------------\n",
            "Num timesteps: 1960000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -68.20\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 499      |\n",
            "|    ep_rew_mean        | -68.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 49000    |\n",
            "|    time_elapsed       | 1841     |\n",
            "|    total_timesteps    | 1960000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.329   |\n",
            "|    explained_variance | 0.946    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 48999    |\n",
            "|    policy_loss        | -0.0936  |\n",
            "|    value_loss         | 3.96     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 503      |\n",
            "|    ep_rew_mean        | -68.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 49100    |\n",
            "|    time_elapsed       | 1845     |\n",
            "|    total_timesteps    | 1964000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.225   |\n",
            "|    explained_variance | -0.0498  |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 49099    |\n",
            "|    policy_loss        | 0.0328   |\n",
            "|    value_loss         | 916      |\n",
            "------------------------------------\n",
            "Num timesteps: 1968000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -69.67\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 516      |\n",
            "|    ep_rew_mean        | -69.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 49200    |\n",
            "|    time_elapsed       | 1849     |\n",
            "|    total_timesteps    | 1968000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.211   |\n",
            "|    explained_variance | 0.955    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 49199    |\n",
            "|    policy_loss        | -0.0716  |\n",
            "|    value_loss         | 2.91     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 539      |\n",
            "|    ep_rew_mean        | -71.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 49300    |\n",
            "|    time_elapsed       | 1853     |\n",
            "|    total_timesteps    | 1972000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.309   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 49299    |\n",
            "|    policy_loss        | -0.171   |\n",
            "|    value_loss         | 6.19     |\n",
            "------------------------------------\n",
            "Num timesteps: 1976000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -71.59\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 518      |\n",
            "|    ep_rew_mean        | -71.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 49400    |\n",
            "|    time_elapsed       | 1856     |\n",
            "|    total_timesteps    | 1976000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.252   |\n",
            "|    explained_variance | 0.905    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 49399    |\n",
            "|    policy_loss        | -0.00494 |\n",
            "|    value_loss         | 4.06     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 530      |\n",
            "|    ep_rew_mean        | -70.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1063     |\n",
            "|    iterations         | 49500    |\n",
            "|    time_elapsed       | 1861     |\n",
            "|    total_timesteps    | 1980000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.304   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 49499    |\n",
            "|    policy_loss        | 0.118    |\n",
            "|    value_loss         | 1.38     |\n",
            "------------------------------------\n",
            "Num timesteps: 1984000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -66.96\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 552      |\n",
            "|    ep_rew_mean        | -67      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1063     |\n",
            "|    iterations         | 49600    |\n",
            "|    time_elapsed       | 1865     |\n",
            "|    total_timesteps    | 1984000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.259   |\n",
            "|    explained_variance | 0.979    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 49599    |\n",
            "|    policy_loss        | -0.557   |\n",
            "|    value_loss         | 4.61     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 544      |\n",
            "|    ep_rew_mean        | -67.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1063     |\n",
            "|    iterations         | 49700    |\n",
            "|    time_elapsed       | 1868     |\n",
            "|    total_timesteps    | 1988000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.336   |\n",
            "|    explained_variance | 0.939    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 49699    |\n",
            "|    policy_loss        | 0.00566  |\n",
            "|    value_loss         | 2.96     |\n",
            "------------------------------------\n",
            "Num timesteps: 1992000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -67.27\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 540      |\n",
            "|    ep_rew_mean        | -67.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1063     |\n",
            "|    iterations         | 49800    |\n",
            "|    time_elapsed       | 1873     |\n",
            "|    total_timesteps    | 1992000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.208   |\n",
            "|    explained_variance | 0.196    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 49799    |\n",
            "|    policy_loss        | -0.218   |\n",
            "|    value_loss         | 1.36e+03 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 548      |\n",
            "|    ep_rew_mean        | -68.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1063     |\n",
            "|    iterations         | 49900    |\n",
            "|    time_elapsed       | 1876     |\n",
            "|    total_timesteps    | 1996000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.264   |\n",
            "|    explained_variance | 0.937    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 49899    |\n",
            "|    policy_loss        | 0.124    |\n",
            "|    value_loss         | 5.25     |\n",
            "------------------------------------\n",
            "Num timesteps: 2000000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -62.19\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 549      |\n",
            "|    ep_rew_mean        | -62.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 50000    |\n",
            "|    time_elapsed       | 1879     |\n",
            "|    total_timesteps    | 2000000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.426   |\n",
            "|    explained_variance | 0.891    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 49999    |\n",
            "|    policy_loss        | 0.0416   |\n",
            "|    value_loss         | 5.03     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 549      |\n",
            "|    ep_rew_mean        | -61.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 50100    |\n",
            "|    time_elapsed       | 1882     |\n",
            "|    total_timesteps    | 2004000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.325   |\n",
            "|    explained_variance | 0.976    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 50099    |\n",
            "|    policy_loss        | -0.101   |\n",
            "|    value_loss         | 2.56     |\n",
            "------------------------------------\n",
            "Num timesteps: 2008000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -58.94\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 562      |\n",
            "|    ep_rew_mean        | -58.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 50200    |\n",
            "|    time_elapsed       | 1886     |\n",
            "|    total_timesteps    | 2008000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.302   |\n",
            "|    explained_variance | 0.937    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 50199    |\n",
            "|    policy_loss        | -0.272   |\n",
            "|    value_loss         | 2.6      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 573      |\n",
            "|    ep_rew_mean        | -57.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1063     |\n",
            "|    iterations         | 50300    |\n",
            "|    time_elapsed       | 1891     |\n",
            "|    total_timesteps    | 2012000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.309   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 50299    |\n",
            "|    policy_loss        | -0.299   |\n",
            "|    value_loss         | 3.84     |\n",
            "------------------------------------\n",
            "Num timesteps: 2016000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -58.76\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 559      |\n",
            "|    ep_rew_mean        | -58.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 50400    |\n",
            "|    time_elapsed       | 1893     |\n",
            "|    total_timesteps    | 2016000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.281   |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 50399    |\n",
            "|    policy_loss        | 0.355    |\n",
            "|    value_loss         | 4.06     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 544      |\n",
            "|    ep_rew_mean        | -55.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 50500    |\n",
            "|    time_elapsed       | 1897     |\n",
            "|    total_timesteps    | 2020000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.278   |\n",
            "|    explained_variance | 0.964    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 50499    |\n",
            "|    policy_loss        | 0.0455   |\n",
            "|    value_loss         | 2.69     |\n",
            "------------------------------------\n",
            "Num timesteps: 2024000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -55.78\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 541      |\n",
            "|    ep_rew_mean        | -55.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 50600    |\n",
            "|    time_elapsed       | 1901     |\n",
            "|    total_timesteps    | 2024000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.263   |\n",
            "|    explained_variance | 0.968    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 50599    |\n",
            "|    policy_loss        | 0.00181  |\n",
            "|    value_loss         | 3.29     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 544      |\n",
            "|    ep_rew_mean        | -51.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 50700    |\n",
            "|    time_elapsed       | 1905     |\n",
            "|    total_timesteps    | 2028000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.179   |\n",
            "|    explained_variance | 0.824    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 50699    |\n",
            "|    policy_loss        | -0.131   |\n",
            "|    value_loss         | 25       |\n",
            "------------------------------------\n",
            "Num timesteps: 2032000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -49.42\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 537      |\n",
            "|    ep_rew_mean        | -49.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 50800    |\n",
            "|    time_elapsed       | 1909     |\n",
            "|    total_timesteps    | 2032000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.258   |\n",
            "|    explained_variance | 0.759    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 50799    |\n",
            "|    policy_loss        | -0.0254  |\n",
            "|    value_loss         | 261      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 526      |\n",
            "|    ep_rew_mean        | -51.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 50900    |\n",
            "|    time_elapsed       | 1912     |\n",
            "|    total_timesteps    | 2036000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.301   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 50899    |\n",
            "|    policy_loss        | -0.125   |\n",
            "|    value_loss         | 1.38     |\n",
            "------------------------------------\n",
            "Num timesteps: 2040000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -56.00\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 527      |\n",
            "|    ep_rew_mean        | -56      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 51000    |\n",
            "|    time_elapsed       | 1916     |\n",
            "|    total_timesteps    | 2040000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.261   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 50999    |\n",
            "|    policy_loss        | 0.179    |\n",
            "|    value_loss         | 1.07     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 505      |\n",
            "|    ep_rew_mean        | -55.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 51100    |\n",
            "|    time_elapsed       | 1920     |\n",
            "|    total_timesteps    | 2044000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.344   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 51099    |\n",
            "|    policy_loss        | 0.446    |\n",
            "|    value_loss         | 2.08     |\n",
            "------------------------------------\n",
            "Num timesteps: 2048000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -56.80\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 507      |\n",
            "|    ep_rew_mean        | -56.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 51200    |\n",
            "|    time_elapsed       | 1923     |\n",
            "|    total_timesteps    | 2048000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.245   |\n",
            "|    explained_variance | 0.974    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 51199    |\n",
            "|    policy_loss        | 0.669    |\n",
            "|    value_loss         | 22.8     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 500      |\n",
            "|    ep_rew_mean        | -57.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 51300    |\n",
            "|    time_elapsed       | 1927     |\n",
            "|    total_timesteps    | 2052000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.206   |\n",
            "|    explained_variance | 0.979    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 51299    |\n",
            "|    policy_loss        | 0.05     |\n",
            "|    value_loss         | 5.21     |\n",
            "------------------------------------\n",
            "Num timesteps: 2056000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -58.26\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 509      |\n",
            "|    ep_rew_mean        | -58.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 51400    |\n",
            "|    time_elapsed       | 1931     |\n",
            "|    total_timesteps    | 2056000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.307   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 51399    |\n",
            "|    policy_loss        | -0.073   |\n",
            "|    value_loss         | 1.66     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 536      |\n",
            "|    ep_rew_mean        | -58.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 51500    |\n",
            "|    time_elapsed       | 1936     |\n",
            "|    total_timesteps    | 2060000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.249   |\n",
            "|    explained_variance | 0.0506   |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 51499    |\n",
            "|    policy_loss        | -0.346   |\n",
            "|    value_loss         | 1.25e+03 |\n",
            "------------------------------------\n",
            "Num timesteps: 2064000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -65.06\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 539      |\n",
            "|    ep_rew_mean        | -65.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1063     |\n",
            "|    iterations         | 51600    |\n",
            "|    time_elapsed       | 1940     |\n",
            "|    total_timesteps    | 2064000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.276   |\n",
            "|    explained_variance | 0.717    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 51599    |\n",
            "|    policy_loss        | -0.138   |\n",
            "|    value_loss         | 238      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 540      |\n",
            "|    ep_rew_mean        | -64.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1063     |\n",
            "|    iterations         | 51700    |\n",
            "|    time_elapsed       | 1944     |\n",
            "|    total_timesteps    | 2068000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.352   |\n",
            "|    explained_variance | 0.931    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 51699    |\n",
            "|    policy_loss        | 0.0741   |\n",
            "|    value_loss         | 3.92     |\n",
            "------------------------------------\n",
            "Num timesteps: 2072000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -66.61\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 561      |\n",
            "|    ep_rew_mean        | -66.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1063     |\n",
            "|    iterations         | 51800    |\n",
            "|    time_elapsed       | 1948     |\n",
            "|    total_timesteps    | 2072000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.294   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 51799    |\n",
            "|    policy_loss        | 0.0587   |\n",
            "|    value_loss         | 1.93     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 573      |\n",
            "|    ep_rew_mean        | -70.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1063     |\n",
            "|    iterations         | 51900    |\n",
            "|    time_elapsed       | 1951     |\n",
            "|    total_timesteps    | 2076000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.317   |\n",
            "|    explained_variance | 0.96     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 51899    |\n",
            "|    policy_loss        | -0.153   |\n",
            "|    value_loss         | 1.49     |\n",
            "------------------------------------\n",
            "Num timesteps: 2080000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -72.19\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 582      |\n",
            "|    ep_rew_mean        | -72.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 52000    |\n",
            "|    time_elapsed       | 1957     |\n",
            "|    total_timesteps    | 2080000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.234   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 51999    |\n",
            "|    policy_loss        | -0.325   |\n",
            "|    value_loss         | 5.3      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 579      |\n",
            "|    ep_rew_mean        | -74.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 52100    |\n",
            "|    time_elapsed       | 1960     |\n",
            "|    total_timesteps    | 2084000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.299   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 52099    |\n",
            "|    policy_loss        | -0.0445  |\n",
            "|    value_loss         | 3.93     |\n",
            "------------------------------------\n",
            "Num timesteps: 2088000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -77.68\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 572      |\n",
            "|    ep_rew_mean        | -77.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1063     |\n",
            "|    iterations         | 52200    |\n",
            "|    time_elapsed       | 1964     |\n",
            "|    total_timesteps    | 2088000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.348   |\n",
            "|    explained_variance | 0.732    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 52199    |\n",
            "|    policy_loss        | -0.201   |\n",
            "|    value_loss         | 78.8     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 579      |\n",
            "|    ep_rew_mean        | -76.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1063     |\n",
            "|    iterations         | 52300    |\n",
            "|    time_elapsed       | 1967     |\n",
            "|    total_timesteps    | 2092000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.272   |\n",
            "|    explained_variance | 0.821    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 52299    |\n",
            "|    policy_loss        | -0.0422  |\n",
            "|    value_loss         | 8.43     |\n",
            "------------------------------------\n",
            "Num timesteps: 2096000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -80.74\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 584      |\n",
            "|    ep_rew_mean        | -80.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1063     |\n",
            "|    iterations         | 52400    |\n",
            "|    time_elapsed       | 1971     |\n",
            "|    total_timesteps    | 2096000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.312   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 52399    |\n",
            "|    policy_loss        | 0.0153   |\n",
            "|    value_loss         | 0.742    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 574      |\n",
            "|    ep_rew_mean        | -78.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 52500    |\n",
            "|    time_elapsed       | 1975     |\n",
            "|    total_timesteps    | 2100000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.364   |\n",
            "|    explained_variance | 0.97     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 52499    |\n",
            "|    policy_loss        | -0.475   |\n",
            "|    value_loss         | 4.62     |\n",
            "------------------------------------\n",
            "Num timesteps: 2104000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -75.69\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 595      |\n",
            "|    ep_rew_mean        | -75.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 52600    |\n",
            "|    time_elapsed       | 1979     |\n",
            "|    total_timesteps    | 2104000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.267   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 52599    |\n",
            "|    policy_loss        | -0.236   |\n",
            "|    value_loss         | 6.1      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 599      |\n",
            "|    ep_rew_mean        | -75.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 52700    |\n",
            "|    time_elapsed       | 1984     |\n",
            "|    total_timesteps    | 2108000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.286   |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 52699    |\n",
            "|    policy_loss        | 0.417    |\n",
            "|    value_loss         | 4.47     |\n",
            "------------------------------------\n",
            "Num timesteps: 2112000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -78.54\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 591      |\n",
            "|    ep_rew_mean        | -78.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 52800    |\n",
            "|    time_elapsed       | 1987     |\n",
            "|    total_timesteps    | 2112000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.231   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 52799    |\n",
            "|    policy_loss        | -0.0472  |\n",
            "|    value_loss         | 2.04     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 572      |\n",
            "|    ep_rew_mean        | -81.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 52900    |\n",
            "|    time_elapsed       | 1991     |\n",
            "|    total_timesteps    | 2116000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.328   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 52899    |\n",
            "|    policy_loss        | -0.0453  |\n",
            "|    value_loss         | 2.57     |\n",
            "------------------------------------\n",
            "Num timesteps: 2120000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -75.38\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 557      |\n",
            "|    ep_rew_mean        | -75.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 53000    |\n",
            "|    time_elapsed       | 1995     |\n",
            "|    total_timesteps    | 2120000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.325   |\n",
            "|    explained_variance | 0.978    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 52999    |\n",
            "|    policy_loss        | -0.235   |\n",
            "|    value_loss         | 5.41     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 559      |\n",
            "|    ep_rew_mean        | -75.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 53100    |\n",
            "|    time_elapsed       | 1998     |\n",
            "|    total_timesteps    | 2124000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.268   |\n",
            "|    explained_variance | 0.945    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 53099    |\n",
            "|    policy_loss        | 0.34     |\n",
            "|    value_loss         | 4.6      |\n",
            "------------------------------------\n",
            "Num timesteps: 2128000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -70.40\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 553      |\n",
            "|    ep_rew_mean        | -70.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 53200    |\n",
            "|    time_elapsed       | 2002     |\n",
            "|    total_timesteps    | 2128000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.375   |\n",
            "|    explained_variance | 0.971    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 53199    |\n",
            "|    policy_loss        | -0.296   |\n",
            "|    value_loss         | 2.36     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 539      |\n",
            "|    ep_rew_mean        | -71.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 53300    |\n",
            "|    time_elapsed       | 2006     |\n",
            "|    total_timesteps    | 2132000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.233   |\n",
            "|    explained_variance | 0.963    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 53299    |\n",
            "|    policy_loss        | -0.29    |\n",
            "|    value_loss         | 2.39     |\n",
            "------------------------------------\n",
            "Num timesteps: 2136000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -71.30\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 530      |\n",
            "|    ep_rew_mean        | -71.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 53400    |\n",
            "|    time_elapsed       | 2010     |\n",
            "|    total_timesteps    | 2136000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.249   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 53399    |\n",
            "|    policy_loss        | 0.161    |\n",
            "|    value_loss         | 2.16     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 538      |\n",
            "|    ep_rew_mean        | -73.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 53500    |\n",
            "|    time_elapsed       | 2014     |\n",
            "|    total_timesteps    | 2140000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.397   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 53499    |\n",
            "|    policy_loss        | -0.0386  |\n",
            "|    value_loss         | 6.01     |\n",
            "------------------------------------\n",
            "Num timesteps: 2144000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -75.23\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 526      |\n",
            "|    ep_rew_mean        | -75.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 53600    |\n",
            "|    time_elapsed       | 2018     |\n",
            "|    total_timesteps    | 2144000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.352   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 53599    |\n",
            "|    policy_loss        | -0.301   |\n",
            "|    value_loss         | 3.66     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 524      |\n",
            "|    ep_rew_mean        | -74.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 53700    |\n",
            "|    time_elapsed       | 2021     |\n",
            "|    total_timesteps    | 2148000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.377   |\n",
            "|    explained_variance | 0.943    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 53699    |\n",
            "|    policy_loss        | 0.111    |\n",
            "|    value_loss         | 3.78     |\n",
            "------------------------------------\n",
            "Num timesteps: 2152000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -75.30\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 528      |\n",
            "|    ep_rew_mean        | -75.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 53800    |\n",
            "|    time_elapsed       | 2026     |\n",
            "|    total_timesteps    | 2152000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.194   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 53799    |\n",
            "|    policy_loss        | 0.13     |\n",
            "|    value_loss         | 4.03     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 496      |\n",
            "|    ep_rew_mean        | -78.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 53900    |\n",
            "|    time_elapsed       | 2029     |\n",
            "|    total_timesteps    | 2156000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.327   |\n",
            "|    explained_variance | 0.784    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 53899    |\n",
            "|    policy_loss        | -0.586   |\n",
            "|    value_loss         | 166      |\n",
            "------------------------------------\n",
            "Num timesteps: 2160000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -75.48\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 504      |\n",
            "|    ep_rew_mean        | -75.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 54000    |\n",
            "|    time_elapsed       | 2032     |\n",
            "|    total_timesteps    | 2160000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.276   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 53999    |\n",
            "|    policy_loss        | 0.00749  |\n",
            "|    value_loss         | 1.9      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 521      |\n",
            "|    ep_rew_mean        | -76.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 54100    |\n",
            "|    time_elapsed       | 2036     |\n",
            "|    total_timesteps    | 2164000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.317   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 54099    |\n",
            "|    policy_loss        | 0.432    |\n",
            "|    value_loss         | 2.19     |\n",
            "------------------------------------\n",
            "Num timesteps: 2168000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -73.59\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 530      |\n",
            "|    ep_rew_mean        | -73.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1061     |\n",
            "|    iterations         | 54200    |\n",
            "|    time_elapsed       | 2041     |\n",
            "|    total_timesteps    | 2168000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.195   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 54199    |\n",
            "|    policy_loss        | 0.139    |\n",
            "|    value_loss         | 1.55     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 529      |\n",
            "|    ep_rew_mean        | -73.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 54300    |\n",
            "|    time_elapsed       | 2045     |\n",
            "|    total_timesteps    | 2172000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.329   |\n",
            "|    explained_variance | 0.915    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 54299    |\n",
            "|    policy_loss        | -0.149   |\n",
            "|    value_loss         | 4.22     |\n",
            "------------------------------------\n",
            "Num timesteps: 2176000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -77.69\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 526      |\n",
            "|    ep_rew_mean        | -77.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 54400    |\n",
            "|    time_elapsed       | 2048     |\n",
            "|    total_timesteps    | 2176000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.348   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 54399    |\n",
            "|    policy_loss        | 0.276    |\n",
            "|    value_loss         | 5.66     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 498      |\n",
            "|    ep_rew_mean        | -79.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 54500    |\n",
            "|    time_elapsed       | 2051     |\n",
            "|    total_timesteps    | 2180000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.386   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 54499    |\n",
            "|    policy_loss        | -0.148   |\n",
            "|    value_loss         | 3.58     |\n",
            "------------------------------------\n",
            "Num timesteps: 2184000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -82.18\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 490      |\n",
            "|    ep_rew_mean        | -82.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 54600    |\n",
            "|    time_elapsed       | 2054     |\n",
            "|    total_timesteps    | 2184000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.328   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 54599    |\n",
            "|    policy_loss        | 0.473    |\n",
            "|    value_loss         | 4.03     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 477      |\n",
            "|    ep_rew_mean        | -79.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1063     |\n",
            "|    iterations         | 54700    |\n",
            "|    time_elapsed       | 2057     |\n",
            "|    total_timesteps    | 2188000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.398   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 54699    |\n",
            "|    policy_loss        | 0.151    |\n",
            "|    value_loss         | 3.65     |\n",
            "------------------------------------\n",
            "Num timesteps: 2192000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -83.13\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 489      |\n",
            "|    ep_rew_mean        | -83.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 54800    |\n",
            "|    time_elapsed       | 2062     |\n",
            "|    total_timesteps    | 2192000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.377   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 54799    |\n",
            "|    policy_loss        | 0.0544   |\n",
            "|    value_loss         | 8.34     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 499      |\n",
            "|    ep_rew_mean        | -80.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1063     |\n",
            "|    iterations         | 54900    |\n",
            "|    time_elapsed       | 2065     |\n",
            "|    total_timesteps    | 2196000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.338   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 54899    |\n",
            "|    policy_loss        | -0.0538  |\n",
            "|    value_loss         | 2.8      |\n",
            "------------------------------------\n",
            "Num timesteps: 2200000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -84.84\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 474      |\n",
            "|    ep_rew_mean        | -84.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1063     |\n",
            "|    iterations         | 55000    |\n",
            "|    time_elapsed       | 2068     |\n",
            "|    total_timesteps    | 2200000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.361   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 54999    |\n",
            "|    policy_loss        | -0.294   |\n",
            "|    value_loss         | 3.41     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 482      |\n",
            "|    ep_rew_mean        | -88.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1063     |\n",
            "|    iterations         | 55100    |\n",
            "|    time_elapsed       | 2071     |\n",
            "|    total_timesteps    | 2204000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.307   |\n",
            "|    explained_variance | 0.978    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 55099    |\n",
            "|    policy_loss        | 0.0728   |\n",
            "|    value_loss         | 3.63     |\n",
            "------------------------------------\n",
            "Num timesteps: 2208000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -90.72\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 479      |\n",
            "|    ep_rew_mean        | -90.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1063     |\n",
            "|    iterations         | 55200    |\n",
            "|    time_elapsed       | 2075     |\n",
            "|    total_timesteps    | 2208000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.319   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 55199    |\n",
            "|    policy_loss        | -0.0288  |\n",
            "|    value_loss         | 1.96     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 469      |\n",
            "|    ep_rew_mean        | -94.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1063     |\n",
            "|    iterations         | 55300    |\n",
            "|    time_elapsed       | 2079     |\n",
            "|    total_timesteps    | 2212000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.289   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 55299    |\n",
            "|    policy_loss        | -0.279   |\n",
            "|    value_loss         | 3.6      |\n",
            "------------------------------------\n",
            "Num timesteps: 2216000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -96.23\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 459      |\n",
            "|    ep_rew_mean        | -96.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1063     |\n",
            "|    iterations         | 55400    |\n",
            "|    time_elapsed       | 2083     |\n",
            "|    total_timesteps    | 2216000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.235   |\n",
            "|    explained_variance | 0.635    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 55399    |\n",
            "|    policy_loss        | -0.1     |\n",
            "|    value_loss         | 538      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 441      |\n",
            "|    ep_rew_mean        | -89.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 55500    |\n",
            "|    time_elapsed       | 2085     |\n",
            "|    total_timesteps    | 2220000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.343   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 55499    |\n",
            "|    policy_loss        | 0.187    |\n",
            "|    value_loss         | 4.27     |\n",
            "------------------------------------\n",
            "Num timesteps: 2224000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -81.47\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 454      |\n",
            "|    ep_rew_mean        | -81.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 55600    |\n",
            "|    time_elapsed       | 2089     |\n",
            "|    total_timesteps    | 2224000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.242   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 55599    |\n",
            "|    policy_loss        | 0.029    |\n",
            "|    value_loss         | 3.3      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 464      |\n",
            "|    ep_rew_mean        | -72.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 55700    |\n",
            "|    time_elapsed       | 2092     |\n",
            "|    total_timesteps    | 2228000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.184   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 55699    |\n",
            "|    policy_loss        | -0.245   |\n",
            "|    value_loss         | 1.53     |\n",
            "------------------------------------\n",
            "Num timesteps: 2232000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -72.92\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 451      |\n",
            "|    ep_rew_mean        | -72.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 55800    |\n",
            "|    time_elapsed       | 2095     |\n",
            "|    total_timesteps    | 2232000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.377   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 55799    |\n",
            "|    policy_loss        | 0.0859   |\n",
            "|    value_loss         | 3.5      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 450      |\n",
            "|    ep_rew_mean        | -67.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 55900    |\n",
            "|    time_elapsed       | 2099     |\n",
            "|    total_timesteps    | 2236000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.333   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 55899    |\n",
            "|    policy_loss        | -0.044   |\n",
            "|    value_loss         | 1.76     |\n",
            "------------------------------------\n",
            "Num timesteps: 2240000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -66.36\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 446      |\n",
            "|    ep_rew_mean        | -66.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 56000    |\n",
            "|    time_elapsed       | 2103     |\n",
            "|    total_timesteps    | 2240000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.252   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 55999    |\n",
            "|    policy_loss        | -0.105   |\n",
            "|    value_loss         | 4.48     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 451      |\n",
            "|    ep_rew_mean        | -61.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 56100    |\n",
            "|    time_elapsed       | 2106     |\n",
            "|    total_timesteps    | 2244000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.335   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 56099    |\n",
            "|    policy_loss        | -0.00973 |\n",
            "|    value_loss         | 4.62     |\n",
            "------------------------------------\n",
            "Num timesteps: 2248000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -56.62\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 435      |\n",
            "|    ep_rew_mean        | -56.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 56200    |\n",
            "|    time_elapsed       | 2109     |\n",
            "|    total_timesteps    | 2248000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.351   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 56199    |\n",
            "|    policy_loss        | 0.407    |\n",
            "|    value_loss         | 6.46     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 417      |\n",
            "|    ep_rew_mean        | -57.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 56300    |\n",
            "|    time_elapsed       | 2112     |\n",
            "|    total_timesteps    | 2252000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.381   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 56299    |\n",
            "|    policy_loss        | -0.13    |\n",
            "|    value_loss         | 1.44     |\n",
            "------------------------------------\n",
            "Num timesteps: 2256000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -58.71\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 414      |\n",
            "|    ep_rew_mean        | -58.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 56400    |\n",
            "|    time_elapsed       | 2116     |\n",
            "|    total_timesteps    | 2256000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.284   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 56399    |\n",
            "|    policy_loss        | -0.0186  |\n",
            "|    value_loss         | 1.09     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 424      |\n",
            "|    ep_rew_mean        | -56.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 56500    |\n",
            "|    time_elapsed       | 2122     |\n",
            "|    total_timesteps    | 2260000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.368   |\n",
            "|    explained_variance | 0.98     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 56499    |\n",
            "|    policy_loss        | 0.859    |\n",
            "|    value_loss         | 6.94     |\n",
            "------------------------------------\n",
            "Num timesteps: 2264000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -55.94\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 448      |\n",
            "|    ep_rew_mean        | -55.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 56600    |\n",
            "|    time_elapsed       | 2125     |\n",
            "|    total_timesteps    | 2264000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.357   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 56599    |\n",
            "|    policy_loss        | -0.0469  |\n",
            "|    value_loss         | 4.19     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 434      |\n",
            "|    ep_rew_mean        | -58.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 56700    |\n",
            "|    time_elapsed       | 2129     |\n",
            "|    total_timesteps    | 2268000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.335   |\n",
            "|    explained_variance | 0.963    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 56699    |\n",
            "|    policy_loss        | -0.105   |\n",
            "|    value_loss         | 2.62     |\n",
            "------------------------------------\n",
            "Num timesteps: 2272000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -65.55\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 452      |\n",
            "|    ep_rew_mean        | -65.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 56800    |\n",
            "|    time_elapsed       | 2133     |\n",
            "|    total_timesteps    | 2272000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.281   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 56799    |\n",
            "|    policy_loss        | 0.11     |\n",
            "|    value_loss         | 5.03     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 451      |\n",
            "|    ep_rew_mean        | -62.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 56900    |\n",
            "|    time_elapsed       | 2136     |\n",
            "|    total_timesteps    | 2276000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.259   |\n",
            "|    explained_variance | 0.965    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 56899    |\n",
            "|    policy_loss        | 0.278    |\n",
            "|    value_loss         | 4.11     |\n",
            "------------------------------------\n",
            "Num timesteps: 2280000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -62.79\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 443      |\n",
            "|    ep_rew_mean        | -62.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 57000    |\n",
            "|    time_elapsed       | 2140     |\n",
            "|    total_timesteps    | 2280000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.277   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 56999    |\n",
            "|    policy_loss        | 0.0111   |\n",
            "|    value_loss         | 2.56     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 438      |\n",
            "|    ep_rew_mean        | -65.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 57100    |\n",
            "|    time_elapsed       | 2142     |\n",
            "|    total_timesteps    | 2284000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.362   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 57099    |\n",
            "|    policy_loss        | 0.0407   |\n",
            "|    value_loss         | 2.31     |\n",
            "------------------------------------\n",
            "Num timesteps: 2288000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -64.70\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 439      |\n",
            "|    ep_rew_mean        | -64.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 57200    |\n",
            "|    time_elapsed       | 2147     |\n",
            "|    total_timesteps    | 2288000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.287   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 57199    |\n",
            "|    policy_loss        | -0.153   |\n",
            "|    value_loss         | 1.95     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 460      |\n",
            "|    ep_rew_mean        | -63.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 57300    |\n",
            "|    time_elapsed       | 2152     |\n",
            "|    total_timesteps    | 2292000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.285   |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 57299    |\n",
            "|    policy_loss        | 0.116    |\n",
            "|    value_loss         | 3.33     |\n",
            "------------------------------------\n",
            "Num timesteps: 2296000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -66.62\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 491      |\n",
            "|    ep_rew_mean        | -66.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 57400    |\n",
            "|    time_elapsed       | 2156     |\n",
            "|    total_timesteps    | 2296000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.347   |\n",
            "|    explained_variance | 0.762    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 57399    |\n",
            "|    policy_loss        | -2.61    |\n",
            "|    value_loss         | 196      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 503      |\n",
            "|    ep_rew_mean        | -69.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 57500    |\n",
            "|    time_elapsed       | 2160     |\n",
            "|    total_timesteps    | 2300000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.32    |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 57499    |\n",
            "|    policy_loss        | 0.334    |\n",
            "|    value_loss         | 1.7      |\n",
            "------------------------------------\n",
            "Num timesteps: 2304000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -71.06\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 492      |\n",
            "|    ep_rew_mean        | -71.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 57600    |\n",
            "|    time_elapsed       | 2163     |\n",
            "|    total_timesteps    | 2304000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.24    |\n",
            "|    explained_variance | 0.921    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 57599    |\n",
            "|    policy_loss        | -0.0647  |\n",
            "|    value_loss         | 36.2     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 491      |\n",
            "|    ep_rew_mean        | -71.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 57700    |\n",
            "|    time_elapsed       | 2166     |\n",
            "|    total_timesteps    | 2308000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.249   |\n",
            "|    explained_variance | 0.968    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 57699    |\n",
            "|    policy_loss        | 0.218    |\n",
            "|    value_loss         | 2.46     |\n",
            "------------------------------------\n",
            "Num timesteps: 2312000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -75.61\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 485      |\n",
            "|    ep_rew_mean        | -75.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 57800    |\n",
            "|    time_elapsed       | 2170     |\n",
            "|    total_timesteps    | 2312000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.327   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 57799    |\n",
            "|    policy_loss        | -0.389   |\n",
            "|    value_loss         | 4.57     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 481      |\n",
            "|    ep_rew_mean        | -78.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 57900    |\n",
            "|    time_elapsed       | 2172     |\n",
            "|    total_timesteps    | 2316000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.324   |\n",
            "|    explained_variance | 0.963    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 57899    |\n",
            "|    policy_loss        | -0.229   |\n",
            "|    value_loss         | 3.38     |\n",
            "------------------------------------\n",
            "Num timesteps: 2320000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -80.56\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 461      |\n",
            "|    ep_rew_mean        | -80.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 58000    |\n",
            "|    time_elapsed       | 2177     |\n",
            "|    total_timesteps    | 2320000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.267   |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 57999    |\n",
            "|    policy_loss        | 0.148    |\n",
            "|    value_loss         | 15.4     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 468      |\n",
            "|    ep_rew_mean        | -83      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 58100    |\n",
            "|    time_elapsed       | 2180     |\n",
            "|    total_timesteps    | 2324000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.358   |\n",
            "|    explained_variance | 0.953    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 58099    |\n",
            "|    policy_loss        | 0.142    |\n",
            "|    value_loss         | 5.44     |\n",
            "------------------------------------\n",
            "Num timesteps: 2328000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -89.10\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 484      |\n",
            "|    ep_rew_mean        | -89.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 58200    |\n",
            "|    time_elapsed       | 2184     |\n",
            "|    total_timesteps    | 2328000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.262   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 58199    |\n",
            "|    policy_loss        | 0.208    |\n",
            "|    value_loss         | 3.68     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 494      |\n",
            "|    ep_rew_mean        | -89.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 58300    |\n",
            "|    time_elapsed       | 2187     |\n",
            "|    total_timesteps    | 2332000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.332   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 58299    |\n",
            "|    policy_loss        | -0.177   |\n",
            "|    value_loss         | 4.48     |\n",
            "------------------------------------\n",
            "Num timesteps: 2336000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -92.28\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 506      |\n",
            "|    ep_rew_mean        | -92.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 58400    |\n",
            "|    time_elapsed       | 2191     |\n",
            "|    total_timesteps    | 2336000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.335   |\n",
            "|    explained_variance | 0.972    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 58399    |\n",
            "|    policy_loss        | -0.122   |\n",
            "|    value_loss         | 3.5      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 514      |\n",
            "|    ep_rew_mean        | -94.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 58500    |\n",
            "|    time_elapsed       | 2196     |\n",
            "|    total_timesteps    | 2340000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.293   |\n",
            "|    explained_variance | 0.977    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 58499    |\n",
            "|    policy_loss        | -0.0792  |\n",
            "|    value_loss         | 1.96     |\n",
            "------------------------------------\n",
            "Num timesteps: 2344000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -89.30\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 519      |\n",
            "|    ep_rew_mean        | -89.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 58600    |\n",
            "|    time_elapsed       | 2200     |\n",
            "|    total_timesteps    | 2344000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.124   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 58599    |\n",
            "|    policy_loss        | 0.134    |\n",
            "|    value_loss         | 1.3      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 506      |\n",
            "|    ep_rew_mean        | -85.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 58700    |\n",
            "|    time_elapsed       | 2204     |\n",
            "|    total_timesteps    | 2348000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.358   |\n",
            "|    explained_variance | 0.948    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 58699    |\n",
            "|    policy_loss        | 0.0497   |\n",
            "|    value_loss         | 3.92     |\n",
            "------------------------------------\n",
            "Num timesteps: 2352000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -80.25\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 506      |\n",
            "|    ep_rew_mean        | -80.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 58800    |\n",
            "|    time_elapsed       | 2207     |\n",
            "|    total_timesteps    | 2352000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.318   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 58799    |\n",
            "|    policy_loss        | -0.0468  |\n",
            "|    value_loss         | 1.66     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 515      |\n",
            "|    ep_rew_mean        | -80.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 58900    |\n",
            "|    time_elapsed       | 2212     |\n",
            "|    total_timesteps    | 2356000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.265   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 58899    |\n",
            "|    policy_loss        | -0.0724  |\n",
            "|    value_loss         | 2.51     |\n",
            "------------------------------------\n",
            "Num timesteps: 2360000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -82.23\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 531      |\n",
            "|    ep_rew_mean        | -82.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 59000    |\n",
            "|    time_elapsed       | 2216     |\n",
            "|    total_timesteps    | 2360000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.276   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 58999    |\n",
            "|    policy_loss        | 0.21     |\n",
            "|    value_loss         | 4.3      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 513      |\n",
            "|    ep_rew_mean        | -81.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 59100    |\n",
            "|    time_elapsed       | 2220     |\n",
            "|    total_timesteps    | 2364000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.163   |\n",
            "|    explained_variance | -0.205   |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 59099    |\n",
            "|    policy_loss        | -0.18    |\n",
            "|    value_loss         | 1.44e+03 |\n",
            "------------------------------------\n",
            "Num timesteps: 2368000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -78.48\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 531      |\n",
            "|    ep_rew_mean        | -78.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 59200    |\n",
            "|    time_elapsed       | 2223     |\n",
            "|    total_timesteps    | 2368000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.24    |\n",
            "|    explained_variance | 0.948    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 59199    |\n",
            "|    policy_loss        | -0.314   |\n",
            "|    value_loss         | 18.6     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 529      |\n",
            "|    ep_rew_mean        | -75.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 59300    |\n",
            "|    time_elapsed       | 2227     |\n",
            "|    total_timesteps    | 2372000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.314   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 59299    |\n",
            "|    policy_loss        | -0.147   |\n",
            "|    value_loss         | 1.04     |\n",
            "------------------------------------\n",
            "Num timesteps: 2376000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -73.81\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 561      |\n",
            "|    ep_rew_mean        | -73.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 59400    |\n",
            "|    time_elapsed       | 2232     |\n",
            "|    total_timesteps    | 2376000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.348   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 59399    |\n",
            "|    policy_loss        | -0.353   |\n",
            "|    value_loss         | 2.72     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 558      |\n",
            "|    ep_rew_mean        | -74.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 59500    |\n",
            "|    time_elapsed       | 2235     |\n",
            "|    total_timesteps    | 2380000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.332   |\n",
            "|    explained_variance | 0.805    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 59499    |\n",
            "|    policy_loss        | -0.423   |\n",
            "|    value_loss         | 214      |\n",
            "------------------------------------\n",
            "Num timesteps: 2384000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -70.22\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 544      |\n",
            "|    ep_rew_mean        | -70.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 59600    |\n",
            "|    time_elapsed       | 2238     |\n",
            "|    total_timesteps    | 2384000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.301   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 59599    |\n",
            "|    policy_loss        | -0.158   |\n",
            "|    value_loss         | 1.21     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 554      |\n",
            "|    ep_rew_mean        | -71.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 59700    |\n",
            "|    time_elapsed       | 2242     |\n",
            "|    total_timesteps    | 2388000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.352   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 59699    |\n",
            "|    policy_loss        | -0.29    |\n",
            "|    value_loss         | 2.87     |\n",
            "------------------------------------\n",
            "Num timesteps: 2392000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -68.83\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 537      |\n",
            "|    ep_rew_mean        | -68.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 59800    |\n",
            "|    time_elapsed       | 2246     |\n",
            "|    total_timesteps    | 2392000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.228   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 59799    |\n",
            "|    policy_loss        | -0.108   |\n",
            "|    value_loss         | 4.66     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 517      |\n",
            "|    ep_rew_mean        | -69.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 59900    |\n",
            "|    time_elapsed       | 2250     |\n",
            "|    total_timesteps    | 2396000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.272   |\n",
            "|    explained_variance | 0.841    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 59899    |\n",
            "|    policy_loss        | 0.0564   |\n",
            "|    value_loss         | 88.1     |\n",
            "------------------------------------\n",
            "Num timesteps: 2400000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -64.87\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 524      |\n",
            "|    ep_rew_mean        | -64.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 60000    |\n",
            "|    time_elapsed       | 2253     |\n",
            "|    total_timesteps    | 2400000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.373   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 59999    |\n",
            "|    policy_loss        | 0.046    |\n",
            "|    value_loss         | 0.959    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 520      |\n",
            "|    ep_rew_mean        | -64      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 60100    |\n",
            "|    time_elapsed       | 2258     |\n",
            "|    total_timesteps    | 2404000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.293   |\n",
            "|    explained_variance | 0.971    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 60099    |\n",
            "|    policy_loss        | -0.102   |\n",
            "|    value_loss         | 2.84     |\n",
            "------------------------------------\n",
            "Num timesteps: 2408000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -55.55\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 497      |\n",
            "|    ep_rew_mean        | -55.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 60200    |\n",
            "|    time_elapsed       | 2260     |\n",
            "|    total_timesteps    | 2408000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.278   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 60199    |\n",
            "|    policy_loss        | -0.273   |\n",
            "|    value_loss         | 5.11     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 484      |\n",
            "|    ep_rew_mean        | -55.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 60300    |\n",
            "|    time_elapsed       | 2263     |\n",
            "|    total_timesteps    | 2412000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.287   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 60299    |\n",
            "|    policy_loss        | 1.5      |\n",
            "|    value_loss         | 4.36     |\n",
            "------------------------------------\n",
            "Num timesteps: 2416000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -54.14\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 487      |\n",
            "|    ep_rew_mean        | -54.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 60400    |\n",
            "|    time_elapsed       | 2267     |\n",
            "|    total_timesteps    | 2416000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.288   |\n",
            "|    explained_variance | 0.859    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 60399    |\n",
            "|    policy_loss        | -0.15    |\n",
            "|    value_loss         | 9.18     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 494      |\n",
            "|    ep_rew_mean        | -53.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 60500    |\n",
            "|    time_elapsed       | 2271     |\n",
            "|    total_timesteps    | 2420000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.23    |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 60499    |\n",
            "|    policy_loss        | 0.279    |\n",
            "|    value_loss         | 2.33     |\n",
            "------------------------------------\n",
            "Num timesteps: 2424000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -45.88\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 462      |\n",
            "|    ep_rew_mean        | -45.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 60600    |\n",
            "|    time_elapsed       | 2274     |\n",
            "|    total_timesteps    | 2424000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.325   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 60599    |\n",
            "|    policy_loss        | 0.022    |\n",
            "|    value_loss         | 2.89     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 454      |\n",
            "|    ep_rew_mean        | -44.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 60700    |\n",
            "|    time_elapsed       | 2278     |\n",
            "|    total_timesteps    | 2428000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.252   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 60699    |\n",
            "|    policy_loss        | -0.133   |\n",
            "|    value_loss         | 2.34     |\n",
            "------------------------------------\n",
            "Num timesteps: 2432000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -41.74\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 440      |\n",
            "|    ep_rew_mean        | -41.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 60800    |\n",
            "|    time_elapsed       | 2282     |\n",
            "|    total_timesteps    | 2432000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.223   |\n",
            "|    explained_variance | 0.968    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 60799    |\n",
            "|    policy_loss        | -0.386   |\n",
            "|    value_loss         | 20.2     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 445      |\n",
            "|    ep_rew_mean        | -36.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 60900    |\n",
            "|    time_elapsed       | 2286     |\n",
            "|    total_timesteps    | 2436000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.305   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 60899    |\n",
            "|    policy_loss        | -0.337   |\n",
            "|    value_loss         | 2.38     |\n",
            "------------------------------------\n",
            "Num timesteps: 2440000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -45.25\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 435      |\n",
            "|    ep_rew_mean        | -45.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 61000    |\n",
            "|    time_elapsed       | 2290     |\n",
            "|    total_timesteps    | 2440000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.349   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 60999    |\n",
            "|    policy_loss        | -0.114   |\n",
            "|    value_loss         | 3.73     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 438      |\n",
            "|    ep_rew_mean        | -48.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 61100    |\n",
            "|    time_elapsed       | 2294     |\n",
            "|    total_timesteps    | 2444000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.347   |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 61099    |\n",
            "|    policy_loss        | 0.103    |\n",
            "|    value_loss         | 3.93     |\n",
            "------------------------------------\n",
            "Num timesteps: 2448000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -55.75\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 425      |\n",
            "|    ep_rew_mean        | -55.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 61200    |\n",
            "|    time_elapsed       | 2298     |\n",
            "|    total_timesteps    | 2448000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.293   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 61199    |\n",
            "|    policy_loss        | -0.121   |\n",
            "|    value_loss         | 2.51     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 437      |\n",
            "|    ep_rew_mean        | -57.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 61300    |\n",
            "|    time_elapsed       | 2301     |\n",
            "|    total_timesteps    | 2452000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.241   |\n",
            "|    explained_variance | 0.977    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 61299    |\n",
            "|    policy_loss        | -0.275   |\n",
            "|    value_loss         | 10.7     |\n",
            "------------------------------------\n",
            "Num timesteps: 2456000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -55.04\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 446      |\n",
            "|    ep_rew_mean        | -55      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 61400    |\n",
            "|    time_elapsed       | 2305     |\n",
            "|    total_timesteps    | 2456000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.331   |\n",
            "|    explained_variance | 0.97     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 61399    |\n",
            "|    policy_loss        | 0.157    |\n",
            "|    value_loss         | 5.02     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 470      |\n",
            "|    ep_rew_mean        | -55.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 61500    |\n",
            "|    time_elapsed       | 2310     |\n",
            "|    total_timesteps    | 2460000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.278   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 61499    |\n",
            "|    policy_loss        | -0.134   |\n",
            "|    value_loss         | 2.63     |\n",
            "------------------------------------\n",
            "Num timesteps: 2464000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -57.67\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 458      |\n",
            "|    ep_rew_mean        | -57.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 61600    |\n",
            "|    time_elapsed       | 2313     |\n",
            "|    total_timesteps    | 2464000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.308   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 61599    |\n",
            "|    policy_loss        | -0.108   |\n",
            "|    value_loss         | 2.72     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 470      |\n",
            "|    ep_rew_mean        | -57.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 61700    |\n",
            "|    time_elapsed       | 2317     |\n",
            "|    total_timesteps    | 2468000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.309   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 61699    |\n",
            "|    policy_loss        | -0.194   |\n",
            "|    value_loss         | 2.58     |\n",
            "------------------------------------\n",
            "Num timesteps: 2472000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -60.10\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 473      |\n",
            "|    ep_rew_mean        | -60.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 61800    |\n",
            "|    time_elapsed       | 2320     |\n",
            "|    total_timesteps    | 2472000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.301   |\n",
            "|    explained_variance | 0.948    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 61799    |\n",
            "|    policy_loss        | -0.185   |\n",
            "|    value_loss         | 5.53     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 490      |\n",
            "|    ep_rew_mean        | -62.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 61900    |\n",
            "|    time_elapsed       | 2323     |\n",
            "|    total_timesteps    | 2476000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.255   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 61899    |\n",
            "|    policy_loss        | 0.182    |\n",
            "|    value_loss         | 3.31     |\n",
            "------------------------------------\n",
            "Num timesteps: 2480000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -63.73\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 475      |\n",
            "|    ep_rew_mean        | -63.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 62000    |\n",
            "|    time_elapsed       | 2327     |\n",
            "|    total_timesteps    | 2480000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.288   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 61999    |\n",
            "|    policy_loss        | -0.0679  |\n",
            "|    value_loss         | 5.23     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 478      |\n",
            "|    ep_rew_mean        | -64.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 62100    |\n",
            "|    time_elapsed       | 2330     |\n",
            "|    total_timesteps    | 2484000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.369   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 62099    |\n",
            "|    policy_loss        | -0.106   |\n",
            "|    value_loss         | 3.09     |\n",
            "------------------------------------\n",
            "Num timesteps: 2488000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -67.60\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 480      |\n",
            "|    ep_rew_mean        | -67.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 62200    |\n",
            "|    time_elapsed       | 2334     |\n",
            "|    total_timesteps    | 2488000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.315   |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 62199    |\n",
            "|    policy_loss        | -0.201   |\n",
            "|    value_loss         | 3.25     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 501      |\n",
            "|    ep_rew_mean        | -66.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 62300    |\n",
            "|    time_elapsed       | 2338     |\n",
            "|    total_timesteps    | 2492000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.422   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 62299    |\n",
            "|    policy_loss        | 0.219    |\n",
            "|    value_loss         | 3.92     |\n",
            "------------------------------------\n",
            "Num timesteps: 2496000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -66.59\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 490      |\n",
            "|    ep_rew_mean        | -66.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 62400    |\n",
            "|    time_elapsed       | 2341     |\n",
            "|    total_timesteps    | 2496000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.425   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 62399    |\n",
            "|    policy_loss        | 0.154    |\n",
            "|    value_loss         | 1.88     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 487      |\n",
            "|    ep_rew_mean        | -61.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 62500    |\n",
            "|    time_elapsed       | 2346     |\n",
            "|    total_timesteps    | 2500000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.292   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 62499    |\n",
            "|    policy_loss        | 0.0504   |\n",
            "|    value_loss         | 1.64     |\n",
            "------------------------------------\n",
            "Num timesteps: 2504000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -61.85\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 505      |\n",
            "|    ep_rew_mean        | -61.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 62600    |\n",
            "|    time_elapsed       | 2349     |\n",
            "|    total_timesteps    | 2504000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.28    |\n",
            "|    explained_variance | 0.475    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 62599    |\n",
            "|    policy_loss        | -0.222   |\n",
            "|    value_loss         | 573      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 501      |\n",
            "|    ep_rew_mean        | -59.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 62700    |\n",
            "|    time_elapsed       | 2354     |\n",
            "|    total_timesteps    | 2508000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.256   |\n",
            "|    explained_variance | 0.933    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 62699    |\n",
            "|    policy_loss        | -0.0665  |\n",
            "|    value_loss         | 5.85     |\n",
            "------------------------------------\n",
            "Num timesteps: 2512000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -60.79\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 509      |\n",
            "|    ep_rew_mean        | -60.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 62800    |\n",
            "|    time_elapsed       | 2358     |\n",
            "|    total_timesteps    | 2512000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.254   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 62799    |\n",
            "|    policy_loss        | 0.291    |\n",
            "|    value_loss         | 5.74     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 495      |\n",
            "|    ep_rew_mean        | -65      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 62900    |\n",
            "|    time_elapsed       | 2360     |\n",
            "|    total_timesteps    | 2516000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.238   |\n",
            "|    explained_variance | 0.955    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 62899    |\n",
            "|    policy_loss        | 0.0675   |\n",
            "|    value_loss         | 3.11     |\n",
            "------------------------------------\n",
            "Num timesteps: 2520000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -64.46\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 494      |\n",
            "|    ep_rew_mean        | -64.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 63000    |\n",
            "|    time_elapsed       | 2363     |\n",
            "|    total_timesteps    | 2520000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.363   |\n",
            "|    explained_variance | 0.977    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 62999    |\n",
            "|    policy_loss        | -0.128   |\n",
            "|    value_loss         | 5.72     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 507      |\n",
            "|    ep_rew_mean        | -61.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 63100    |\n",
            "|    time_elapsed       | 2368     |\n",
            "|    total_timesteps    | 2524000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.194   |\n",
            "|    explained_variance | 0.953    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 63099    |\n",
            "|    policy_loss        | -0.0753  |\n",
            "|    value_loss         | 18       |\n",
            "------------------------------------\n",
            "Num timesteps: 2528000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -55.59\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 508      |\n",
            "|    ep_rew_mean        | -55.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 63200    |\n",
            "|    time_elapsed       | 2371     |\n",
            "|    total_timesteps    | 2528000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.28    |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 63199    |\n",
            "|    policy_loss        | -0.271   |\n",
            "|    value_loss         | 3.02     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 531      |\n",
            "|    ep_rew_mean        | -51.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 63300    |\n",
            "|    time_elapsed       | 2374     |\n",
            "|    total_timesteps    | 2532000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.151   |\n",
            "|    explained_variance | 0.78     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 63299    |\n",
            "|    policy_loss        | -0.232   |\n",
            "|    value_loss         | 86.7     |\n",
            "------------------------------------\n",
            "Num timesteps: 2536000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -49.62\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 518      |\n",
            "|    ep_rew_mean        | -49.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 63400    |\n",
            "|    time_elapsed       | 2377     |\n",
            "|    total_timesteps    | 2536000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.309   |\n",
            "|    explained_variance | 0.867    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 63399    |\n",
            "|    policy_loss        | -0.19    |\n",
            "|    value_loss         | 11.6     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 522      |\n",
            "|    ep_rew_mean        | -50.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 63500    |\n",
            "|    time_elapsed       | 2381     |\n",
            "|    total_timesteps    | 2540000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.246   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 63499    |\n",
            "|    policy_loss        | 0.0731   |\n",
            "|    value_loss         | 2.17     |\n",
            "------------------------------------\n",
            "Num timesteps: 2544000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -43.87\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 524      |\n",
            "|    ep_rew_mean        | -43.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 63600    |\n",
            "|    time_elapsed       | 2385     |\n",
            "|    total_timesteps    | 2544000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.367   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 63599    |\n",
            "|    policy_loss        | -0.145   |\n",
            "|    value_loss         | 1.78     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 533      |\n",
            "|    ep_rew_mean        | -40.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 63700    |\n",
            "|    time_elapsed       | 2389     |\n",
            "|    total_timesteps    | 2548000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.34    |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 63699    |\n",
            "|    policy_loss        | -0.0093  |\n",
            "|    value_loss         | 0.866    |\n",
            "------------------------------------\n",
            "Num timesteps: 2552000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -39.20\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 550      |\n",
            "|    ep_rew_mean        | -39.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 63800    |\n",
            "|    time_elapsed       | 2394     |\n",
            "|    total_timesteps    | 2552000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.269   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 63799    |\n",
            "|    policy_loss        | 0.176    |\n",
            "|    value_loss         | 3.25     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 541      |\n",
            "|    ep_rew_mean        | -41.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 63900    |\n",
            "|    time_elapsed       | 2398     |\n",
            "|    total_timesteps    | 2556000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.216   |\n",
            "|    explained_variance | 0.165    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 63899    |\n",
            "|    policy_loss        | -0.106   |\n",
            "|    value_loss         | 811      |\n",
            "------------------------------------\n",
            "Num timesteps: 2560000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -27.44\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 551      |\n",
            "|    ep_rew_mean        | -27.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 64000    |\n",
            "|    time_elapsed       | 2401     |\n",
            "|    total_timesteps    | 2560000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.4     |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 63999    |\n",
            "|    policy_loss        | -0.0445  |\n",
            "|    value_loss         | 2.42     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 532      |\n",
            "|    ep_rew_mean        | -19.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 64100    |\n",
            "|    time_elapsed       | 2404     |\n",
            "|    total_timesteps    | 2564000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.272   |\n",
            "|    explained_variance | 0.973    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 64099    |\n",
            "|    policy_loss        | -0.0723  |\n",
            "|    value_loss         | 1.69     |\n",
            "------------------------------------\n",
            "Num timesteps: 2568000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -22.56\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 534      |\n",
            "|    ep_rew_mean        | -22.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 64200    |\n",
            "|    time_elapsed       | 2409     |\n",
            "|    total_timesteps    | 2568000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.225   |\n",
            "|    explained_variance | 0.606    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 64199    |\n",
            "|    policy_loss        | -10.4    |\n",
            "|    value_loss         | 522      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 553      |\n",
            "|    ep_rew_mean        | -15.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 64300    |\n",
            "|    time_elapsed       | 2414     |\n",
            "|    total_timesteps    | 2572000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.323   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 64299    |\n",
            "|    policy_loss        | -0.00395 |\n",
            "|    value_loss         | 1.95     |\n",
            "------------------------------------\n",
            "Num timesteps: 2576000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -19.00\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 567      |\n",
            "|    ep_rew_mean        | -19      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 64400    |\n",
            "|    time_elapsed       | 2418     |\n",
            "|    total_timesteps    | 2576000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.329   |\n",
            "|    explained_variance | 0.894    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 64399    |\n",
            "|    policy_loss        | 0.126    |\n",
            "|    value_loss         | 3.86     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        | 558       |\n",
            "|    ep_rew_mean        | -24.8     |\n",
            "| time/                 |           |\n",
            "|    fps                | 1064      |\n",
            "|    iterations         | 64500     |\n",
            "|    time_elapsed       | 2422      |\n",
            "|    total_timesteps    | 2580000   |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.357    |\n",
            "|    explained_variance | 0.99      |\n",
            "|    learning_rate      | 0.00083   |\n",
            "|    n_updates          | 64499     |\n",
            "|    policy_loss        | -5.41e-05 |\n",
            "|    value_loss         | 1.38      |\n",
            "-------------------------------------\n",
            "Num timesteps: 2584000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -31.65\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 534      |\n",
            "|    ep_rew_mean        | -31.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 64600    |\n",
            "|    time_elapsed       | 2425     |\n",
            "|    total_timesteps    | 2584000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.356   |\n",
            "|    explained_variance | 0.882    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 64599    |\n",
            "|    policy_loss        | -3.04    |\n",
            "|    value_loss         | 129      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 542      |\n",
            "|    ep_rew_mean        | -31      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 64700    |\n",
            "|    time_elapsed       | 2430     |\n",
            "|    total_timesteps    | 2588000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.34    |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 64699    |\n",
            "|    policy_loss        | -0.317   |\n",
            "|    value_loss         | 2.93     |\n",
            "------------------------------------\n",
            "Num timesteps: 2592000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -34.09\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 564      |\n",
            "|    ep_rew_mean        | -34.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 64800    |\n",
            "|    time_elapsed       | 2434     |\n",
            "|    total_timesteps    | 2592000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.258   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 64799    |\n",
            "|    policy_loss        | 0.0202   |\n",
            "|    value_loss         | 2.17     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 553      |\n",
            "|    ep_rew_mean        | -35.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 64900    |\n",
            "|    time_elapsed       | 2438     |\n",
            "|    total_timesteps    | 2596000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.312   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 64899    |\n",
            "|    policy_loss        | -0.0956  |\n",
            "|    value_loss         | 1.67     |\n",
            "------------------------------------\n",
            "Num timesteps: 2600000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -39.18\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 558      |\n",
            "|    ep_rew_mean        | -39.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 65000    |\n",
            "|    time_elapsed       | 2442     |\n",
            "|    total_timesteps    | 2600000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.336   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 64999    |\n",
            "|    policy_loss        | -0.147   |\n",
            "|    value_loss         | 2.98     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 540      |\n",
            "|    ep_rew_mean        | -47.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 65100    |\n",
            "|    time_elapsed       | 2446     |\n",
            "|    total_timesteps    | 2604000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.275   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 65099    |\n",
            "|    policy_loss        | -0.0753  |\n",
            "|    value_loss         | 1.84     |\n",
            "------------------------------------\n",
            "Num timesteps: 2608000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -47.19\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 529      |\n",
            "|    ep_rew_mean        | -47.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 65200    |\n",
            "|    time_elapsed       | 2450     |\n",
            "|    total_timesteps    | 2608000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.265   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 65199    |\n",
            "|    policy_loss        | -0.107   |\n",
            "|    value_loss         | 4.35     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 523      |\n",
            "|    ep_rew_mean        | -50.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 65300    |\n",
            "|    time_elapsed       | 2453     |\n",
            "|    total_timesteps    | 2612000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.274   |\n",
            "|    explained_variance | 0.979    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 65299    |\n",
            "|    policy_loss        | -0.0215  |\n",
            "|    value_loss         | 8.13     |\n",
            "------------------------------------\n",
            "Num timesteps: 2616000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -54.91\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 528      |\n",
            "|    ep_rew_mean        | -54.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 65400    |\n",
            "|    time_elapsed       | 2458     |\n",
            "|    total_timesteps    | 2616000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.207   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 65399    |\n",
            "|    policy_loss        | -0.039   |\n",
            "|    value_loss         | 3.13     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 545      |\n",
            "|    ep_rew_mean        | -56.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1063     |\n",
            "|    iterations         | 65500    |\n",
            "|    time_elapsed       | 2462     |\n",
            "|    total_timesteps    | 2620000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.258   |\n",
            "|    explained_variance | 0.916    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 65499    |\n",
            "|    policy_loss        | -0.383   |\n",
            "|    value_loss         | 46.3     |\n",
            "------------------------------------\n",
            "Num timesteps: 2624000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -60.29\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 521      |\n",
            "|    ep_rew_mean        | -60.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 65600    |\n",
            "|    time_elapsed       | 2465     |\n",
            "|    total_timesteps    | 2624000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.24    |\n",
            "|    explained_variance | 0.839    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 65599    |\n",
            "|    policy_loss        | -0.0806  |\n",
            "|    value_loss         | 143      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 511      |\n",
            "|    ep_rew_mean        | -54.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 65700    |\n",
            "|    time_elapsed       | 2469     |\n",
            "|    total_timesteps    | 2628000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.242   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 65699    |\n",
            "|    policy_loss        | 0.0927   |\n",
            "|    value_loss         | 2.71     |\n",
            "------------------------------------\n",
            "Num timesteps: 2632000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -43.13\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 507      |\n",
            "|    ep_rew_mean        | -43.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 65800    |\n",
            "|    time_elapsed       | 2472     |\n",
            "|    total_timesteps    | 2632000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.216   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 65799    |\n",
            "|    policy_loss        | -0.0203  |\n",
            "|    value_loss         | 2.58     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 473      |\n",
            "|    ep_rew_mean        | -34.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 65900    |\n",
            "|    time_elapsed       | 2475     |\n",
            "|    total_timesteps    | 2636000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.271   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 65899    |\n",
            "|    policy_loss        | -0.394   |\n",
            "|    value_loss         | 5.46     |\n",
            "------------------------------------\n",
            "Num timesteps: 2640000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -29.42\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 459      |\n",
            "|    ep_rew_mean        | -29.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 66000    |\n",
            "|    time_elapsed       | 2477     |\n",
            "|    total_timesteps    | 2640000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.195   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 65999    |\n",
            "|    policy_loss        | -0.136   |\n",
            "|    value_loss         | 2.37     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 429      |\n",
            "|    ep_rew_mean        | -24.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 66100    |\n",
            "|    time_elapsed       | 2481     |\n",
            "|    total_timesteps    | 2644000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.278   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 66099    |\n",
            "|    policy_loss        | 0.0761   |\n",
            "|    value_loss         | 1.19     |\n",
            "------------------------------------\n",
            "Num timesteps: 2648000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -13.52\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 445      |\n",
            "|    ep_rew_mean        | -13.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 66200    |\n",
            "|    time_elapsed       | 2485     |\n",
            "|    total_timesteps    | 2648000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.346   |\n",
            "|    explained_variance | 0.795    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 66199    |\n",
            "|    policy_loss        | -0.0588  |\n",
            "|    value_loss         | 178      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 433      |\n",
            "|    ep_rew_mean        | -16.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 66300    |\n",
            "|    time_elapsed       | 2489     |\n",
            "|    total_timesteps    | 2652000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.286   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 66299    |\n",
            "|    policy_loss        | -0.0729  |\n",
            "|    value_loss         | 4.6      |\n",
            "------------------------------------\n",
            "Num timesteps: 2656000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -24.05\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 444      |\n",
            "|    ep_rew_mean        | -24.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 66400    |\n",
            "|    time_elapsed       | 2493     |\n",
            "|    total_timesteps    | 2656000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.15    |\n",
            "|    explained_variance | 0.607    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 66399    |\n",
            "|    policy_loss        | -0.0156  |\n",
            "|    value_loss         | 541      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 425      |\n",
            "|    ep_rew_mean        | -24.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 66500    |\n",
            "|    time_elapsed       | 2495     |\n",
            "|    total_timesteps    | 2660000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.281   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 66499    |\n",
            "|    policy_loss        | -0.382   |\n",
            "|    value_loss         | 4.16     |\n",
            "------------------------------------\n",
            "Num timesteps: 2664000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -19.14\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 434      |\n",
            "|    ep_rew_mean        | -19.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 66600    |\n",
            "|    time_elapsed       | 2499     |\n",
            "|    total_timesteps    | 2664000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.406   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 66599    |\n",
            "|    policy_loss        | 0.21     |\n",
            "|    value_loss         | 3.39     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 442      |\n",
            "|    ep_rew_mean        | -12.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 66700    |\n",
            "|    time_elapsed       | 2502     |\n",
            "|    total_timesteps    | 2668000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.315   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 66699    |\n",
            "|    policy_loss        | -0.0762  |\n",
            "|    value_loss         | 5.69     |\n",
            "------------------------------------\n",
            "Num timesteps: 2672000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -8.11\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 434      |\n",
            "|    ep_rew_mean        | -8.11    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 66800    |\n",
            "|    time_elapsed       | 2506     |\n",
            "|    total_timesteps    | 2672000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.348   |\n",
            "|    explained_variance | 0.979    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 66799    |\n",
            "|    policy_loss        | -0.162   |\n",
            "|    value_loss         | 3.75     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 432      |\n",
            "|    ep_rew_mean        | -5.05    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 66900    |\n",
            "|    time_elapsed       | 2509     |\n",
            "|    total_timesteps    | 2676000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.328   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 66899    |\n",
            "|    policy_loss        | -0.202   |\n",
            "|    value_loss         | 3.06     |\n",
            "------------------------------------\n",
            "Num timesteps: 2680000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -6.87\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 444      |\n",
            "|    ep_rew_mean        | -6.87    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 67000    |\n",
            "|    time_elapsed       | 2514     |\n",
            "|    total_timesteps    | 2680000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.323   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 66999    |\n",
            "|    policy_loss        | -0.266   |\n",
            "|    value_loss         | 4.37     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 461      |\n",
            "|    ep_rew_mean        | -10.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 67100    |\n",
            "|    time_elapsed       | 2517     |\n",
            "|    total_timesteps    | 2684000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.23    |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 67099    |\n",
            "|    policy_loss        | 0.19     |\n",
            "|    value_loss         | 1.7      |\n",
            "------------------------------------\n",
            "Num timesteps: 2688000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -8.99\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 484      |\n",
            "|    ep_rew_mean        | -8.99    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 67200    |\n",
            "|    time_elapsed       | 2522     |\n",
            "|    total_timesteps    | 2688000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.322   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 67199    |\n",
            "|    policy_loss        | 0.31     |\n",
            "|    value_loss         | 2.3      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 507      |\n",
            "|    ep_rew_mean        | -5.82    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 67300    |\n",
            "|    time_elapsed       | 2526     |\n",
            "|    total_timesteps    | 2692000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.261   |\n",
            "|    explained_variance | 0.979    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 67299    |\n",
            "|    policy_loss        | -0.243   |\n",
            "|    value_loss         | 6.06     |\n",
            "------------------------------------\n",
            "Num timesteps: 2696000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -8.67\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 519      |\n",
            "|    ep_rew_mean        | -8.67    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 67400    |\n",
            "|    time_elapsed       | 2530     |\n",
            "|    total_timesteps    | 2696000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.296   |\n",
            "|    explained_variance | 0.321    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 67399    |\n",
            "|    policy_loss        | 0.104    |\n",
            "|    value_loss         | 195      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 522      |\n",
            "|    ep_rew_mean        | -10      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 67500    |\n",
            "|    time_elapsed       | 2534     |\n",
            "|    total_timesteps    | 2700000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.279   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 67499    |\n",
            "|    policy_loss        | -0.0533  |\n",
            "|    value_loss         | 3.43     |\n",
            "------------------------------------\n",
            "Num timesteps: 2704000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -1.23\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 523      |\n",
            "|    ep_rew_mean        | -1.23    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 67600    |\n",
            "|    time_elapsed       | 2538     |\n",
            "|    total_timesteps    | 2704000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.422   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 67599    |\n",
            "|    policy_loss        | -0.0684  |\n",
            "|    value_loss         | 4.38     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 510      |\n",
            "|    ep_rew_mean        | -1.72    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 67700    |\n",
            "|    time_elapsed       | 2542     |\n",
            "|    total_timesteps    | 2708000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.331   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 67699    |\n",
            "|    policy_loss        | 0.18     |\n",
            "|    value_loss         | 4.71     |\n",
            "------------------------------------\n",
            "Num timesteps: 2712000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: 2.10\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 508      |\n",
            "|    ep_rew_mean        | 2.1      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 67800    |\n",
            "|    time_elapsed       | 2544     |\n",
            "|    total_timesteps    | 2712000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.233   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 67799    |\n",
            "|    policy_loss        | -0.127   |\n",
            "|    value_loss         | 4.33     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 493      |\n",
            "|    ep_rew_mean        | -6.09    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 67900    |\n",
            "|    time_elapsed       | 2548     |\n",
            "|    total_timesteps    | 2716000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.327   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 67899    |\n",
            "|    policy_loss        | -0.0656  |\n",
            "|    value_loss         | 2.15     |\n",
            "------------------------------------\n",
            "Num timesteps: 2720000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -12.48\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 487      |\n",
            "|    ep_rew_mean        | -12.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 68000    |\n",
            "|    time_elapsed       | 2552     |\n",
            "|    total_timesteps    | 2720000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.25    |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 67999    |\n",
            "|    policy_loss        | 0.0327   |\n",
            "|    value_loss         | 2.77     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 497      |\n",
            "|    ep_rew_mean        | -11.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 68100    |\n",
            "|    time_elapsed       | 2555     |\n",
            "|    total_timesteps    | 2724000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.305   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 68099    |\n",
            "|    policy_loss        | 0.286    |\n",
            "|    value_loss         | 3.13     |\n",
            "------------------------------------\n",
            "Num timesteps: 2728000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -15.57\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 504      |\n",
            "|    ep_rew_mean        | -15.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 68200    |\n",
            "|    time_elapsed       | 2558     |\n",
            "|    total_timesteps    | 2728000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.391   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 68199    |\n",
            "|    policy_loss        | 0.0404   |\n",
            "|    value_loss         | 1.1      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 499      |\n",
            "|    ep_rew_mean        | -19.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 68300    |\n",
            "|    time_elapsed       | 2562     |\n",
            "|    total_timesteps    | 2732000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.346   |\n",
            "|    explained_variance | 0.684    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 68299    |\n",
            "|    policy_loss        | -2.68    |\n",
            "|    value_loss         | 404      |\n",
            "------------------------------------\n",
            "Num timesteps: 2736000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -19.75\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 523      |\n",
            "|    ep_rew_mean        | -19.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 68400    |\n",
            "|    time_elapsed       | 2567     |\n",
            "|    total_timesteps    | 2736000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.359   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 68399    |\n",
            "|    policy_loss        | 0.0657   |\n",
            "|    value_loss         | 5.62     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 521      |\n",
            "|    ep_rew_mean        | -23.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 68500    |\n",
            "|    time_elapsed       | 2571     |\n",
            "|    total_timesteps    | 2740000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.362   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 68499    |\n",
            "|    policy_loss        | -0.388   |\n",
            "|    value_loss         | 8.17     |\n",
            "------------------------------------\n",
            "Num timesteps: 2744000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -31.03\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 498      |\n",
            "|    ep_rew_mean        | -31      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 68600    |\n",
            "|    time_elapsed       | 2573     |\n",
            "|    total_timesteps    | 2744000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.264   |\n",
            "|    explained_variance | 0.953    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 68599    |\n",
            "|    policy_loss        | -0.0762  |\n",
            "|    value_loss         | 4.13     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 494      |\n",
            "|    ep_rew_mean        | -37.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 68700    |\n",
            "|    time_elapsed       | 2578     |\n",
            "|    total_timesteps    | 2748000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.232   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 68699    |\n",
            "|    policy_loss        | 0.144    |\n",
            "|    value_loss         | 2.71     |\n",
            "------------------------------------\n",
            "Num timesteps: 2752000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -43.76\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 500      |\n",
            "|    ep_rew_mean        | -43.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 68800    |\n",
            "|    time_elapsed       | 2581     |\n",
            "|    total_timesteps    | 2752000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.219   |\n",
            "|    explained_variance | 0.93     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 68799    |\n",
            "|    policy_loss        | -0.197   |\n",
            "|    value_loss         | 40.1     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 508      |\n",
            "|    ep_rew_mean        | -44.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 68900    |\n",
            "|    time_elapsed       | 2586     |\n",
            "|    total_timesteps    | 2756000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.221   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 68899    |\n",
            "|    policy_loss        | 0.383    |\n",
            "|    value_loss         | 3.4      |\n",
            "------------------------------------\n",
            "Num timesteps: 2760000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -43.92\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 510      |\n",
            "|    ep_rew_mean        | -43.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 69000    |\n",
            "|    time_elapsed       | 2589     |\n",
            "|    total_timesteps    | 2760000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.307   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 68999    |\n",
            "|    policy_loss        | -0.273   |\n",
            "|    value_loss         | 1.24     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 513      |\n",
            "|    ep_rew_mean        | -44.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 69100    |\n",
            "|    time_elapsed       | 2592     |\n",
            "|    total_timesteps    | 2764000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.214   |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 69099    |\n",
            "|    policy_loss        | 0.0587   |\n",
            "|    value_loss         | 3.66     |\n",
            "------------------------------------\n",
            "Num timesteps: 2768000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -41.92\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 506      |\n",
            "|    ep_rew_mean        | -41.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 69200    |\n",
            "|    time_elapsed       | 2595     |\n",
            "|    total_timesteps    | 2768000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.323   |\n",
            "|    explained_variance | 0.98     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 69199    |\n",
            "|    policy_loss        | -0.193   |\n",
            "|    value_loss         | 4.07     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 528      |\n",
            "|    ep_rew_mean        | -42.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 69300    |\n",
            "|    time_elapsed       | 2600     |\n",
            "|    total_timesteps    | 2772000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.401   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 69299    |\n",
            "|    policy_loss        | 0.127    |\n",
            "|    value_loss         | 4.15     |\n",
            "------------------------------------\n",
            "Num timesteps: 2776000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -43.61\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 515      |\n",
            "|    ep_rew_mean        | -43.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 69400    |\n",
            "|    time_elapsed       | 2603     |\n",
            "|    total_timesteps    | 2776000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.307   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 69399    |\n",
            "|    policy_loss        | 0.131    |\n",
            "|    value_loss         | 1.69     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 509      |\n",
            "|    ep_rew_mean        | -46.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 69500    |\n",
            "|    time_elapsed       | 2608     |\n",
            "|    total_timesteps    | 2780000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.223   |\n",
            "|    explained_variance | 0.961    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 69499    |\n",
            "|    policy_loss        | -0.119   |\n",
            "|    value_loss         | 16.6     |\n",
            "------------------------------------\n",
            "Num timesteps: 2784000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -46.24\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 497      |\n",
            "|    ep_rew_mean        | -46.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 69600    |\n",
            "|    time_elapsed       | 2611     |\n",
            "|    total_timesteps    | 2784000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.223   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 69599    |\n",
            "|    policy_loss        | -0.00869 |\n",
            "|    value_loss         | 2.44     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 494      |\n",
            "|    ep_rew_mean        | -47      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 69700    |\n",
            "|    time_elapsed       | 2615     |\n",
            "|    total_timesteps    | 2788000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.273   |\n",
            "|    explained_variance | 0.97     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 69699    |\n",
            "|    policy_loss        | -0.178   |\n",
            "|    value_loss         | 5.12     |\n",
            "------------------------------------\n",
            "Num timesteps: 2792000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -42.99\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 501      |\n",
            "|    ep_rew_mean        | -43      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 69800    |\n",
            "|    time_elapsed       | 2619     |\n",
            "|    total_timesteps    | 2792000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.191   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 69799    |\n",
            "|    policy_loss        | 0.199    |\n",
            "|    value_loss         | 5.09     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 512      |\n",
            "|    ep_rew_mean        | -38.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 69900    |\n",
            "|    time_elapsed       | 2622     |\n",
            "|    total_timesteps    | 2796000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.32    |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 69899    |\n",
            "|    policy_loss        | 0.0747   |\n",
            "|    value_loss         | 3.15     |\n",
            "------------------------------------\n",
            "Num timesteps: 2800000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -35.65\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 482      |\n",
            "|    ep_rew_mean        | -35.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 70000    |\n",
            "|    time_elapsed       | 2625     |\n",
            "|    total_timesteps    | 2800000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.316   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 69999    |\n",
            "|    policy_loss        | 0.231    |\n",
            "|    value_loss         | 2.41     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 476      |\n",
            "|    ep_rew_mean        | -37.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 70100    |\n",
            "|    time_elapsed       | 2628     |\n",
            "|    total_timesteps    | 2804000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.231   |\n",
            "|    explained_variance | 0.933    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 70099    |\n",
            "|    policy_loss        | -0.0693  |\n",
            "|    value_loss         | 26.2     |\n",
            "------------------------------------\n",
            "Num timesteps: 2808000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -39.04\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 480      |\n",
            "|    ep_rew_mean        | -39      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 70200    |\n",
            "|    time_elapsed       | 2633     |\n",
            "|    total_timesteps    | 2808000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.37    |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 70199    |\n",
            "|    policy_loss        | -0.315   |\n",
            "|    value_loss         | 4.86     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 479      |\n",
            "|    ep_rew_mean        | -41.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 70300    |\n",
            "|    time_elapsed       | 2636     |\n",
            "|    total_timesteps    | 2812000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.355   |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 70299    |\n",
            "|    policy_loss        | 0.119    |\n",
            "|    value_loss         | 5.8      |\n",
            "------------------------------------\n",
            "Num timesteps: 2816000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -42.24\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 479      |\n",
            "|    ep_rew_mean        | -42.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 70400    |\n",
            "|    time_elapsed       | 2639     |\n",
            "|    total_timesteps    | 2816000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.29    |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 70399    |\n",
            "|    policy_loss        | 0.144    |\n",
            "|    value_loss         | 3.27     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 472      |\n",
            "|    ep_rew_mean        | -39.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 70500    |\n",
            "|    time_elapsed       | 2643     |\n",
            "|    total_timesteps    | 2820000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.325   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 70499    |\n",
            "|    policy_loss        | -0.0888  |\n",
            "|    value_loss         | 5.48     |\n",
            "------------------------------------\n",
            "Num timesteps: 2824000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -43.87\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 468      |\n",
            "|    ep_rew_mean        | -43.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 70600    |\n",
            "|    time_elapsed       | 2646     |\n",
            "|    total_timesteps    | 2824000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.236   |\n",
            "|    explained_variance | 0.977    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 70599    |\n",
            "|    policy_loss        | -0.0653  |\n",
            "|    value_loss         | 6.32     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 478      |\n",
            "|    ep_rew_mean        | -39.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 70700    |\n",
            "|    time_elapsed       | 2650     |\n",
            "|    total_timesteps    | 2828000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.286   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 70699    |\n",
            "|    policy_loss        | 0.244    |\n",
            "|    value_loss         | 4.94     |\n",
            "------------------------------------\n",
            "Num timesteps: 2832000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -37.89\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 456      |\n",
            "|    ep_rew_mean        | -37.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1067     |\n",
            "|    iterations         | 70800    |\n",
            "|    time_elapsed       | 2653     |\n",
            "|    total_timesteps    | 2832000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.336   |\n",
            "|    explained_variance | 0.977    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 70799    |\n",
            "|    policy_loss        | 0.163    |\n",
            "|    value_loss         | 15.8     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 431      |\n",
            "|    ep_rew_mean        | -33      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1067     |\n",
            "|    iterations         | 70900    |\n",
            "|    time_elapsed       | 2656     |\n",
            "|    total_timesteps    | 2836000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.299   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 70899    |\n",
            "|    policy_loss        | -0.353   |\n",
            "|    value_loss         | 3.46     |\n",
            "------------------------------------\n",
            "Num timesteps: 2840000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -23.60\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 435      |\n",
            "|    ep_rew_mean        | -23.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1067     |\n",
            "|    iterations         | 71000    |\n",
            "|    time_elapsed       | 2659     |\n",
            "|    total_timesteps    | 2840000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.377   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 70999    |\n",
            "|    policy_loss        | 0.113    |\n",
            "|    value_loss         | 7.12     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 434      |\n",
            "|    ep_rew_mean        | -27.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1068     |\n",
            "|    iterations         | 71100    |\n",
            "|    time_elapsed       | 2662     |\n",
            "|    total_timesteps    | 2844000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.26    |\n",
            "|    explained_variance | 0.964    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 71099    |\n",
            "|    policy_loss        | 0.144    |\n",
            "|    value_loss         | 2.55     |\n",
            "------------------------------------\n",
            "Num timesteps: 2848000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -25.14\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 443      |\n",
            "|    ep_rew_mean        | -25.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1067     |\n",
            "|    iterations         | 71200    |\n",
            "|    time_elapsed       | 2666     |\n",
            "|    total_timesteps    | 2848000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.226   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 71199    |\n",
            "|    policy_loss        | -0.00724 |\n",
            "|    value_loss         | 1.4      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 464      |\n",
            "|    ep_rew_mean        | -30.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1068     |\n",
            "|    iterations         | 71300    |\n",
            "|    time_elapsed       | 2670     |\n",
            "|    total_timesteps    | 2852000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.328   |\n",
            "|    explained_variance | 0.686    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 71299    |\n",
            "|    policy_loss        | -0.0387  |\n",
            "|    value_loss         | 113      |\n",
            "------------------------------------\n",
            "Num timesteps: 2856000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -23.72\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 445      |\n",
            "|    ep_rew_mean        | -23.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1067     |\n",
            "|    iterations         | 71400    |\n",
            "|    time_elapsed       | 2674     |\n",
            "|    total_timesteps    | 2856000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.307   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 71399    |\n",
            "|    policy_loss        | 0.181    |\n",
            "|    value_loss         | 2.63     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 466      |\n",
            "|    ep_rew_mean        | -23.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1067     |\n",
            "|    iterations         | 71500    |\n",
            "|    time_elapsed       | 2678     |\n",
            "|    total_timesteps    | 2860000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.378   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 71499    |\n",
            "|    policy_loss        | -0.118   |\n",
            "|    value_loss         | 6.99     |\n",
            "------------------------------------\n",
            "Num timesteps: 2864000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -26.23\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 442      |\n",
            "|    ep_rew_mean        | -26.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1068     |\n",
            "|    iterations         | 71600    |\n",
            "|    time_elapsed       | 2681     |\n",
            "|    total_timesteps    | 2864000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.189   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 71599    |\n",
            "|    policy_loss        | 0.0576   |\n",
            "|    value_loss         | 3.06     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 447      |\n",
            "|    ep_rew_mean        | -19      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1068     |\n",
            "|    iterations         | 71700    |\n",
            "|    time_elapsed       | 2684     |\n",
            "|    total_timesteps    | 2868000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.301   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 71699    |\n",
            "|    policy_loss        | -0.0197  |\n",
            "|    value_loss         | 2.5      |\n",
            "------------------------------------\n",
            "Num timesteps: 2872000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -17.32\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 454      |\n",
            "|    ep_rew_mean        | -17.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1067     |\n",
            "|    iterations         | 71800    |\n",
            "|    time_elapsed       | 2689     |\n",
            "|    total_timesteps    | 2872000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.257   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 71799    |\n",
            "|    policy_loss        | 0.139    |\n",
            "|    value_loss         | 1.71     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 465      |\n",
            "|    ep_rew_mean        | -19.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1067     |\n",
            "|    iterations         | 71900    |\n",
            "|    time_elapsed       | 2693     |\n",
            "|    total_timesteps    | 2876000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.378   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 71899    |\n",
            "|    policy_loss        | 0.395    |\n",
            "|    value_loss         | 2.05     |\n",
            "------------------------------------\n",
            "Num timesteps: 2880000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -21.57\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 493      |\n",
            "|    ep_rew_mean        | -21.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1067     |\n",
            "|    iterations         | 72000    |\n",
            "|    time_elapsed       | 2698     |\n",
            "|    total_timesteps    | 2880000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.16    |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 71999    |\n",
            "|    policy_loss        | 0.0458   |\n",
            "|    value_loss         | 1.16     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 506      |\n",
            "|    ep_rew_mean        | -22.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1067     |\n",
            "|    iterations         | 72100    |\n",
            "|    time_elapsed       | 2702     |\n",
            "|    total_timesteps    | 2884000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.29    |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 72099    |\n",
            "|    policy_loss        | -0.464   |\n",
            "|    value_loss         | 5.13     |\n",
            "------------------------------------\n",
            "Num timesteps: 2888000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -26.93\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 511      |\n",
            "|    ep_rew_mean        | -26.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1067     |\n",
            "|    iterations         | 72200    |\n",
            "|    time_elapsed       | 2706     |\n",
            "|    total_timesteps    | 2888000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.257   |\n",
            "|    explained_variance | 0.959    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 72199    |\n",
            "|    policy_loss        | 0.01     |\n",
            "|    value_loss         | 1.33     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 528      |\n",
            "|    ep_rew_mean        | -35.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 72300    |\n",
            "|    time_elapsed       | 2710     |\n",
            "|    total_timesteps    | 2892000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.284   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 72299    |\n",
            "|    policy_loss        | 0.211    |\n",
            "|    value_loss         | 4.92     |\n",
            "------------------------------------\n",
            "Num timesteps: 2896000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -39.65\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 519      |\n",
            "|    ep_rew_mean        | -39.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1067     |\n",
            "|    iterations         | 72400    |\n",
            "|    time_elapsed       | 2713     |\n",
            "|    total_timesteps    | 2896000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.2     |\n",
            "|    explained_variance | 0.973    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 72399    |\n",
            "|    policy_loss        | 0.256    |\n",
            "|    value_loss         | 9.87     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 510      |\n",
            "|    ep_rew_mean        | -41      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1067     |\n",
            "|    iterations         | 72500    |\n",
            "|    time_elapsed       | 2717     |\n",
            "|    total_timesteps    | 2900000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.252   |\n",
            "|    explained_variance | 0.958    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 72499    |\n",
            "|    policy_loss        | -0.125   |\n",
            "|    value_loss         | 1.94     |\n",
            "------------------------------------\n",
            "Num timesteps: 2904000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -40.43\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 522      |\n",
            "|    ep_rew_mean        | -40.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 72600    |\n",
            "|    time_elapsed       | 2721     |\n",
            "|    total_timesteps    | 2904000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.234   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 72599    |\n",
            "|    policy_loss        | 0.0166   |\n",
            "|    value_loss         | 1.01     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 541      |\n",
            "|    ep_rew_mean        | -42.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 72700    |\n",
            "|    time_elapsed       | 2725     |\n",
            "|    total_timesteps    | 2908000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.358   |\n",
            "|    explained_variance | 0.7      |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 72699    |\n",
            "|    policy_loss        | 0.152    |\n",
            "|    value_loss         | 41.2     |\n",
            "------------------------------------\n",
            "Num timesteps: 2912000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -44.01\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 552      |\n",
            "|    ep_rew_mean        | -44      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 72800    |\n",
            "|    time_elapsed       | 2730     |\n",
            "|    total_timesteps    | 2912000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.278   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 72799    |\n",
            "|    policy_loss        | -0.13    |\n",
            "|    value_loss         | 2.61     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 565      |\n",
            "|    ep_rew_mean        | -43.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 72900    |\n",
            "|    time_elapsed       | 2734     |\n",
            "|    total_timesteps    | 2916000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.389   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 72899    |\n",
            "|    policy_loss        | 0.0266   |\n",
            "|    value_loss         | 2.74     |\n",
            "------------------------------------\n",
            "Num timesteps: 2920000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -45.81\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 584      |\n",
            "|    ep_rew_mean        | -45.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 73000    |\n",
            "|    time_elapsed       | 2739     |\n",
            "|    total_timesteps    | 2920000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.318   |\n",
            "|    explained_variance | 0.969    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 72999    |\n",
            "|    policy_loss        | 0.00182  |\n",
            "|    value_loss         | 4.71     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 598      |\n",
            "|    ep_rew_mean        | -48.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 73100    |\n",
            "|    time_elapsed       | 2743     |\n",
            "|    total_timesteps    | 2924000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.288   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 73099    |\n",
            "|    policy_loss        | 0.0224   |\n",
            "|    value_loss         | 4.17     |\n",
            "------------------------------------\n",
            "Num timesteps: 2928000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -56.01\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 603      |\n",
            "|    ep_rew_mean        | -56      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 73200    |\n",
            "|    time_elapsed       | 2747     |\n",
            "|    total_timesteps    | 2928000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.317   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 73199    |\n",
            "|    policy_loss        | 0.422    |\n",
            "|    value_loss         | 3.44     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 604      |\n",
            "|    ep_rew_mean        | -58.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 73300    |\n",
            "|    time_elapsed       | 2751     |\n",
            "|    total_timesteps    | 2932000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.28    |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 73299    |\n",
            "|    policy_loss        | 0.119    |\n",
            "|    value_loss         | 3.31     |\n",
            "------------------------------------\n",
            "Num timesteps: 2936000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -64.41\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 586      |\n",
            "|    ep_rew_mean        | -64.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 73400    |\n",
            "|    time_elapsed       | 2755     |\n",
            "|    total_timesteps    | 2936000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.369   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 73399    |\n",
            "|    policy_loss        | -0.0544  |\n",
            "|    value_loss         | 1.84     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 567      |\n",
            "|    ep_rew_mean        | -66.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 73500    |\n",
            "|    time_elapsed       | 2759     |\n",
            "|    total_timesteps    | 2940000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.43    |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 73499    |\n",
            "|    policy_loss        | 0.14     |\n",
            "|    value_loss         | 4.47     |\n",
            "------------------------------------\n",
            "Num timesteps: 2944000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -68.43\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 554      |\n",
            "|    ep_rew_mean        | -68.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 73600    |\n",
            "|    time_elapsed       | 2762     |\n",
            "|    total_timesteps    | 2944000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.283   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 73599    |\n",
            "|    policy_loss        | -0.122   |\n",
            "|    value_loss         | 4.21     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 535      |\n",
            "|    ep_rew_mean        | -68.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 73700    |\n",
            "|    time_elapsed       | 2764     |\n",
            "|    total_timesteps    | 2948000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.275   |\n",
            "|    explained_variance | 0.946    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 73699    |\n",
            "|    policy_loss        | -0.0416  |\n",
            "|    value_loss         | 47.3     |\n",
            "------------------------------------\n",
            "Num timesteps: 2952000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -64.75\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 550      |\n",
            "|    ep_rew_mean        | -64.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 73800    |\n",
            "|    time_elapsed       | 2767     |\n",
            "|    total_timesteps    | 2952000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.342   |\n",
            "|    explained_variance | 0.967    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 73799    |\n",
            "|    policy_loss        | -0.00649 |\n",
            "|    value_loss         | 12.2     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 502      |\n",
            "|    ep_rew_mean        | -68.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 73900    |\n",
            "|    time_elapsed       | 2771     |\n",
            "|    total_timesteps    | 2956000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.28    |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 73899    |\n",
            "|    policy_loss        | -0.0772  |\n",
            "|    value_loss         | 4.39     |\n",
            "------------------------------------\n",
            "Num timesteps: 2960000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -72.71\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 502      |\n",
            "|    ep_rew_mean        | -72.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 74000    |\n",
            "|    time_elapsed       | 2774     |\n",
            "|    total_timesteps    | 2960000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.407   |\n",
            "|    explained_variance | 0.979    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 73999    |\n",
            "|    policy_loss        | 0.313    |\n",
            "|    value_loss         | 4.6      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 477      |\n",
            "|    ep_rew_mean        | -68.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 74100    |\n",
            "|    time_elapsed       | 2778     |\n",
            "|    total_timesteps    | 2964000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.195   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 74099    |\n",
            "|    policy_loss        | 0.0657   |\n",
            "|    value_loss         | 0.632    |\n",
            "------------------------------------\n",
            "Num timesteps: 2968000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -67.35\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 460      |\n",
            "|    ep_rew_mean        | -67.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 74200    |\n",
            "|    time_elapsed       | 2782     |\n",
            "|    total_timesteps    | 2968000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.325   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 74199    |\n",
            "|    policy_loss        | -0.505   |\n",
            "|    value_loss         | 4.33     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 447      |\n",
            "|    ep_rew_mean        | -64.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 74300    |\n",
            "|    time_elapsed       | 2785     |\n",
            "|    total_timesteps    | 2972000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.266   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 74299    |\n",
            "|    policy_loss        | -0.134   |\n",
            "|    value_loss         | 1.78     |\n",
            "------------------------------------\n",
            "Num timesteps: 2976000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -64.35\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 441      |\n",
            "|    ep_rew_mean        | -64.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 74400    |\n",
            "|    time_elapsed       | 2789     |\n",
            "|    total_timesteps    | 2976000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.251   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 74399    |\n",
            "|    policy_loss        | -0.431   |\n",
            "|    value_loss         | 6.69     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 424      |\n",
            "|    ep_rew_mean        | -59.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1067     |\n",
            "|    iterations         | 74500    |\n",
            "|    time_elapsed       | 2792     |\n",
            "|    total_timesteps    | 2980000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.237   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 74499    |\n",
            "|    policy_loss        | 0.0724   |\n",
            "|    value_loss         | 3.64     |\n",
            "------------------------------------\n",
            "Num timesteps: 2984000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -54.73\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 431      |\n",
            "|    ep_rew_mean        | -54.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 74600    |\n",
            "|    time_elapsed       | 2797     |\n",
            "|    total_timesteps    | 2984000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.259   |\n",
            "|    explained_variance | 0.975    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 74599    |\n",
            "|    policy_loss        | -0.168   |\n",
            "|    value_loss         | 2.8      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 456      |\n",
            "|    ep_rew_mean        | -58.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 74700    |\n",
            "|    time_elapsed       | 2801     |\n",
            "|    total_timesteps    | 2988000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.418   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 74699    |\n",
            "|    policy_loss        | -0.0963  |\n",
            "|    value_loss         | 5.42     |\n",
            "------------------------------------\n",
            "Num timesteps: 2992000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -57.48\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 480      |\n",
            "|    ep_rew_mean        | -57.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 74800    |\n",
            "|    time_elapsed       | 2805     |\n",
            "|    total_timesteps    | 2992000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.259   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 74799    |\n",
            "|    policy_loss        | 0.0932   |\n",
            "|    value_loss         | 2.15     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 473      |\n",
            "|    ep_rew_mean        | -49      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 74900    |\n",
            "|    time_elapsed       | 2808     |\n",
            "|    total_timesteps    | 2996000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.289   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 74899    |\n",
            "|    policy_loss        | -0.0356  |\n",
            "|    value_loss         | 2.59     |\n",
            "------------------------------------\n",
            "Num timesteps: 3000000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -54.50\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 480      |\n",
            "|    ep_rew_mean        | -54.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 75000    |\n",
            "|    time_elapsed       | 2812     |\n",
            "|    total_timesteps    | 3000000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.199   |\n",
            "|    explained_variance | 0.978    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 74999    |\n",
            "|    policy_loss        | 0.136    |\n",
            "|    value_loss         | 17       |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 496      |\n",
            "|    ep_rew_mean        | -52.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 75100    |\n",
            "|    time_elapsed       | 2816     |\n",
            "|    total_timesteps    | 3004000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.348   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 75099    |\n",
            "|    policy_loss        | 0.0845   |\n",
            "|    value_loss         | 3.31     |\n",
            "------------------------------------\n",
            "Num timesteps: 3008000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -46.24\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 494      |\n",
            "|    ep_rew_mean        | -46.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 75200    |\n",
            "|    time_elapsed       | 2819     |\n",
            "|    total_timesteps    | 3008000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.349   |\n",
            "|    explained_variance | 0.975    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 75199    |\n",
            "|    policy_loss        | -0.0937  |\n",
            "|    value_loss         | 7.66     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 507      |\n",
            "|    ep_rew_mean        | -36.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 75300    |\n",
            "|    time_elapsed       | 2823     |\n",
            "|    total_timesteps    | 3012000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.233   |\n",
            "|    explained_variance | 0.231    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 75299    |\n",
            "|    policy_loss        | 2.03     |\n",
            "|    value_loss         | 327      |\n",
            "------------------------------------\n",
            "Num timesteps: 3016000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -30.46\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 508      |\n",
            "|    ep_rew_mean        | -30.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 75400    |\n",
            "|    time_elapsed       | 2827     |\n",
            "|    total_timesteps    | 3016000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.204   |\n",
            "|    explained_variance | 0.293    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 75399    |\n",
            "|    policy_loss        | -0.147   |\n",
            "|    value_loss         | 806      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 529      |\n",
            "|    ep_rew_mean        | -32.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 75500    |\n",
            "|    time_elapsed       | 2833     |\n",
            "|    total_timesteps    | 3020000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.304   |\n",
            "|    explained_variance | 0.953    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 75499    |\n",
            "|    policy_loss        | 0.616    |\n",
            "|    value_loss         | 11.8     |\n",
            "------------------------------------\n",
            "Num timesteps: 3024000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -29.28\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 537      |\n",
            "|    ep_rew_mean        | -29.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 75600    |\n",
            "|    time_elapsed       | 2836     |\n",
            "|    total_timesteps    | 3024000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.232   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 75599    |\n",
            "|    policy_loss        | 0.0431   |\n",
            "|    value_loss         | 1.07     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 530      |\n",
            "|    ep_rew_mean        | -23      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 75700    |\n",
            "|    time_elapsed       | 2840     |\n",
            "|    total_timesteps    | 3028000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.257   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 75699    |\n",
            "|    policy_loss        | 0.107    |\n",
            "|    value_loss         | 3.5      |\n",
            "------------------------------------\n",
            "Num timesteps: 3032000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -17.25\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 547      |\n",
            "|    ep_rew_mean        | -17.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 75800    |\n",
            "|    time_elapsed       | 2844     |\n",
            "|    total_timesteps    | 3032000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.236   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 75799    |\n",
            "|    policy_loss        | 0.0417   |\n",
            "|    value_loss         | 2.01     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 559      |\n",
            "|    ep_rew_mean        | -20.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 75900    |\n",
            "|    time_elapsed       | 2848     |\n",
            "|    total_timesteps    | 3036000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.289   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 75899    |\n",
            "|    policy_loss        | -0.106   |\n",
            "|    value_loss         | 1.09     |\n",
            "------------------------------------\n",
            "Num timesteps: 3040000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -20.76\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 571      |\n",
            "|    ep_rew_mean        | -20.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 76000    |\n",
            "|    time_elapsed       | 2852     |\n",
            "|    total_timesteps    | 3040000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.285   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 75999    |\n",
            "|    policy_loss        | -0.312   |\n",
            "|    value_loss         | 3.84     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 576      |\n",
            "|    ep_rew_mean        | -16.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 76100    |\n",
            "|    time_elapsed       | 2857     |\n",
            "|    total_timesteps    | 3044000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.361   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 76099    |\n",
            "|    policy_loss        | 0.0816   |\n",
            "|    value_loss         | 2.43     |\n",
            "------------------------------------\n",
            "Num timesteps: 3048000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -17.59\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 582      |\n",
            "|    ep_rew_mean        | -17.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 76200    |\n",
            "|    time_elapsed       | 2861     |\n",
            "|    total_timesteps    | 3048000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.231   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 76199    |\n",
            "|    policy_loss        | -0.0558  |\n",
            "|    value_loss         | 3.54     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 591      |\n",
            "|    ep_rew_mean        | -16      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 76300    |\n",
            "|    time_elapsed       | 2865     |\n",
            "|    total_timesteps    | 3052000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.359   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 76299    |\n",
            "|    policy_loss        | -0.126   |\n",
            "|    value_loss         | 1.05     |\n",
            "------------------------------------\n",
            "Num timesteps: 3056000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -17.02\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 610      |\n",
            "|    ep_rew_mean        | -17      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 76400    |\n",
            "|    time_elapsed       | 2869     |\n",
            "|    total_timesteps    | 3056000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.345   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 76399    |\n",
            "|    policy_loss        | 0.0642   |\n",
            "|    value_loss         | 1.79     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 602      |\n",
            "|    ep_rew_mean        | -9.96    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 76500    |\n",
            "|    time_elapsed       | 2872     |\n",
            "|    total_timesteps    | 3060000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.304   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 76499    |\n",
            "|    policy_loss        | -0.0618  |\n",
            "|    value_loss         | 3.89     |\n",
            "------------------------------------\n",
            "Num timesteps: 3064000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -12.59\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 570      |\n",
            "|    ep_rew_mean        | -12.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 76600    |\n",
            "|    time_elapsed       | 2877     |\n",
            "|    total_timesteps    | 3064000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.164   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 76599    |\n",
            "|    policy_loss        | 0.297    |\n",
            "|    value_loss         | 3.57     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 582      |\n",
            "|    ep_rew_mean        | -11.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 76700    |\n",
            "|    time_elapsed       | 2880     |\n",
            "|    total_timesteps    | 3068000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.302   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 76699    |\n",
            "|    policy_loss        | 0.0307   |\n",
            "|    value_loss         | 3.6      |\n",
            "------------------------------------\n",
            "Num timesteps: 3072000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -11.46\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 588      |\n",
            "|    ep_rew_mean        | -11.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 76800    |\n",
            "|    time_elapsed       | 2885     |\n",
            "|    total_timesteps    | 3072000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.288   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 76799    |\n",
            "|    policy_loss        | -0.0941  |\n",
            "|    value_loss         | 1.73     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 595      |\n",
            "|    ep_rew_mean        | -20.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 76900    |\n",
            "|    time_elapsed       | 2889     |\n",
            "|    total_timesteps    | 3076000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.34    |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 76899    |\n",
            "|    policy_loss        | -0.0267  |\n",
            "|    value_loss         | 3.77     |\n",
            "------------------------------------\n",
            "Num timesteps: 3080000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -18.26\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 585      |\n",
            "|    ep_rew_mean        | -18.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 77000    |\n",
            "|    time_elapsed       | 2893     |\n",
            "|    total_timesteps    | 3080000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.393   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 76999    |\n",
            "|    policy_loss        | 0.0462   |\n",
            "|    value_loss         | 2.44     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 596      |\n",
            "|    ep_rew_mean        | -19.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 77100    |\n",
            "|    time_elapsed       | 2897     |\n",
            "|    total_timesteps    | 3084000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.374   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 77099    |\n",
            "|    policy_loss        | 0.066    |\n",
            "|    value_loss         | 3.01     |\n",
            "------------------------------------\n",
            "Num timesteps: 3088000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -26.80\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 596      |\n",
            "|    ep_rew_mean        | -26.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 77200    |\n",
            "|    time_elapsed       | 2901     |\n",
            "|    total_timesteps    | 3088000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.392   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 77199    |\n",
            "|    policy_loss        | 0.0568   |\n",
            "|    value_loss         | 2.01     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 590      |\n",
            "|    ep_rew_mean        | -30.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 77300    |\n",
            "|    time_elapsed       | 2905     |\n",
            "|    total_timesteps    | 3092000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.308   |\n",
            "|    explained_variance | 0.898    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 77299    |\n",
            "|    policy_loss        | 0.142    |\n",
            "|    value_loss         | 24.1     |\n",
            "------------------------------------\n",
            "Num timesteps: 3096000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -30.25\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 598      |\n",
            "|    ep_rew_mean        | -30.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 77400    |\n",
            "|    time_elapsed       | 2909     |\n",
            "|    total_timesteps    | 3096000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.296   |\n",
            "|    explained_variance | 0.977    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 77399    |\n",
            "|    policy_loss        | -0.248   |\n",
            "|    value_loss         | 4.87     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 594      |\n",
            "|    ep_rew_mean        | -31.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1063     |\n",
            "|    iterations         | 77500    |\n",
            "|    time_elapsed       | 2913     |\n",
            "|    total_timesteps    | 3100000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.202   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 77499    |\n",
            "|    policy_loss        | 0.0673   |\n",
            "|    value_loss         | 3.85     |\n",
            "------------------------------------\n",
            "Num timesteps: 3104000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -34.55\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 584      |\n",
            "|    ep_rew_mean        | -34.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1063     |\n",
            "|    iterations         | 77600    |\n",
            "|    time_elapsed       | 2918     |\n",
            "|    total_timesteps    | 3104000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.403   |\n",
            "|    explained_variance | 0.963    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 77599    |\n",
            "|    policy_loss        | 0.205    |\n",
            "|    value_loss         | 11.6     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 578      |\n",
            "|    ep_rew_mean        | -33.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1063     |\n",
            "|    iterations         | 77700    |\n",
            "|    time_elapsed       | 2923     |\n",
            "|    total_timesteps    | 3108000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.26    |\n",
            "|    explained_variance | 0.959    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 77699    |\n",
            "|    policy_loss        | -0.246   |\n",
            "|    value_loss         | 4.17     |\n",
            "------------------------------------\n",
            "Num timesteps: 3112000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -39.40\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 579      |\n",
            "|    ep_rew_mean        | -39.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 77800    |\n",
            "|    time_elapsed       | 2928     |\n",
            "|    total_timesteps    | 3112000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.292   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 77799    |\n",
            "|    policy_loss        | 0.0127   |\n",
            "|    value_loss         | 6.39     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 561      |\n",
            "|    ep_rew_mean        | -42.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 77900    |\n",
            "|    time_elapsed       | 2932     |\n",
            "|    total_timesteps    | 3116000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.282   |\n",
            "|    explained_variance | 0.958    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 77899    |\n",
            "|    policy_loss        | -0.139   |\n",
            "|    value_loss         | 2.14     |\n",
            "------------------------------------\n",
            "Num timesteps: 3120000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -43.07\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 582      |\n",
            "|    ep_rew_mean        | -43.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 78000    |\n",
            "|    time_elapsed       | 2936     |\n",
            "|    total_timesteps    | 3120000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.305   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 77999    |\n",
            "|    policy_loss        | -0.11    |\n",
            "|    value_loss         | 4.92     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 604      |\n",
            "|    ep_rew_mean        | -48.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 78100    |\n",
            "|    time_elapsed       | 2939     |\n",
            "|    total_timesteps    | 3124000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.295   |\n",
            "|    explained_variance | 0.954    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 78099    |\n",
            "|    policy_loss        | 0.431    |\n",
            "|    value_loss         | 5.65     |\n",
            "------------------------------------\n",
            "Num timesteps: 3128000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -55.90\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 604      |\n",
            "|    ep_rew_mean        | -55.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 78200    |\n",
            "|    time_elapsed       | 2943     |\n",
            "|    total_timesteps    | 3128000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.272   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 78199    |\n",
            "|    policy_loss        | -0.0431  |\n",
            "|    value_loss         | 1.78     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 594      |\n",
            "|    ep_rew_mean        | -59.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 78300    |\n",
            "|    time_elapsed       | 2947     |\n",
            "|    total_timesteps    | 3132000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.198   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 78299    |\n",
            "|    policy_loss        | -0.354   |\n",
            "|    value_loss         | 4.22     |\n",
            "------------------------------------\n",
            "Num timesteps: 3136000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -57.84\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 599      |\n",
            "|    ep_rew_mean        | -57.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 78400    |\n",
            "|    time_elapsed       | 2951     |\n",
            "|    total_timesteps    | 3136000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.245   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 78399    |\n",
            "|    policy_loss        | -0.473   |\n",
            "|    value_loss         | 2.99     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 603      |\n",
            "|    ep_rew_mean        | -61.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 78500    |\n",
            "|    time_elapsed       | 2956     |\n",
            "|    total_timesteps    | 3140000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.351   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 78499    |\n",
            "|    policy_loss        | -0.254   |\n",
            "|    value_loss         | 3.77     |\n",
            "------------------------------------\n",
            "Num timesteps: 3144000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -62.78\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 602      |\n",
            "|    ep_rew_mean        | -62.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 78600    |\n",
            "|    time_elapsed       | 2959     |\n",
            "|    total_timesteps    | 3144000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.21    |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 78599    |\n",
            "|    policy_loss        | 0.439    |\n",
            "|    value_loss         | 8.95     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 584      |\n",
            "|    ep_rew_mean        | -59.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 78700    |\n",
            "|    time_elapsed       | 2962     |\n",
            "|    total_timesteps    | 3148000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.331   |\n",
            "|    explained_variance | 0.595    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 78699    |\n",
            "|    policy_loss        | -8.21    |\n",
            "|    value_loss         | 381      |\n",
            "------------------------------------\n",
            "Num timesteps: 3152000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -61.19\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 583      |\n",
            "|    ep_rew_mean        | -61.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 78800    |\n",
            "|    time_elapsed       | 2966     |\n",
            "|    total_timesteps    | 3152000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.289   |\n",
            "|    explained_variance | 0.951    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 78799    |\n",
            "|    policy_loss        | -0.00805 |\n",
            "|    value_loss         | 5.94     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 577      |\n",
            "|    ep_rew_mean        | -62.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 78900    |\n",
            "|    time_elapsed       | 2969     |\n",
            "|    total_timesteps    | 3156000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.272   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 78899    |\n",
            "|    policy_loss        | -0.0587  |\n",
            "|    value_loss         | 3.94     |\n",
            "------------------------------------\n",
            "Num timesteps: 3160000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -62.33\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 573      |\n",
            "|    ep_rew_mean        | -62.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 79000    |\n",
            "|    time_elapsed       | 2973     |\n",
            "|    total_timesteps    | 3160000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.394   |\n",
            "|    explained_variance | 0.978    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 78999    |\n",
            "|    policy_loss        | 0.0636   |\n",
            "|    value_loss         | 4.3      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 561      |\n",
            "|    ep_rew_mean        | -63.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 79100    |\n",
            "|    time_elapsed       | 2978     |\n",
            "|    total_timesteps    | 3164000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.296   |\n",
            "|    explained_variance | 0.976    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 79099    |\n",
            "|    policy_loss        | 0.143    |\n",
            "|    value_loss         | 6.71     |\n",
            "------------------------------------\n",
            "Num timesteps: 3168000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -64.80\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 543      |\n",
            "|    ep_rew_mean        | -64.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 79200    |\n",
            "|    time_elapsed       | 2981     |\n",
            "|    total_timesteps    | 3168000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.207   |\n",
            "|    explained_variance | 0.959    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 79199    |\n",
            "|    policy_loss        | 0.146    |\n",
            "|    value_loss         | 3.3      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 557      |\n",
            "|    ep_rew_mean        | -64      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 79300    |\n",
            "|    time_elapsed       | 2985     |\n",
            "|    total_timesteps    | 3172000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.285   |\n",
            "|    explained_variance | 0.978    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 79299    |\n",
            "|    policy_loss        | 0.165    |\n",
            "|    value_loss         | 2.6      |\n",
            "------------------------------------\n",
            "Num timesteps: 3176000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -65.29\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 560      |\n",
            "|    ep_rew_mean        | -65.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 79400    |\n",
            "|    time_elapsed       | 2989     |\n",
            "|    total_timesteps    | 3176000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.292   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 79399    |\n",
            "|    policy_loss        | -0.312   |\n",
            "|    value_loss         | 3.59     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 563      |\n",
            "|    ep_rew_mean        | -67      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 79500    |\n",
            "|    time_elapsed       | 2993     |\n",
            "|    total_timesteps    | 3180000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.369   |\n",
            "|    explained_variance | 0.929    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 79499    |\n",
            "|    policy_loss        | 0.325    |\n",
            "|    value_loss         | 14.4     |\n",
            "------------------------------------\n",
            "Num timesteps: 3184000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -70.86\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 557      |\n",
            "|    ep_rew_mean        | -70.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 79600    |\n",
            "|    time_elapsed       | 2997     |\n",
            "|    total_timesteps    | 3184000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.284   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 79599    |\n",
            "|    policy_loss        | -0.0243  |\n",
            "|    value_loss         | 5.36     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 563      |\n",
            "|    ep_rew_mean        | -70.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 79700    |\n",
            "|    time_elapsed       | 3001     |\n",
            "|    total_timesteps    | 3188000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.268   |\n",
            "|    explained_variance | 0.977    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 79699    |\n",
            "|    policy_loss        | 0.118    |\n",
            "|    value_loss         | 2.3      |\n",
            "------------------------------------\n",
            "Num timesteps: 3192000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -68.19\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 568      |\n",
            "|    ep_rew_mean        | -68.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1061     |\n",
            "|    iterations         | 79800    |\n",
            "|    time_elapsed       | 3006     |\n",
            "|    total_timesteps    | 3192000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.236   |\n",
            "|    explained_variance | 0.953    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 79799    |\n",
            "|    policy_loss        | -0.0153  |\n",
            "|    value_loss         | 5.53     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 563      |\n",
            "|    ep_rew_mean        | -70.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1061     |\n",
            "|    iterations         | 79900    |\n",
            "|    time_elapsed       | 3009     |\n",
            "|    total_timesteps    | 3196000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.197   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 79899    |\n",
            "|    policy_loss        | -0.201   |\n",
            "|    value_loss         | 3.02     |\n",
            "------------------------------------\n",
            "Num timesteps: 3200000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -71.40\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 566      |\n",
            "|    ep_rew_mean        | -71.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1061     |\n",
            "|    iterations         | 80000    |\n",
            "|    time_elapsed       | 3014     |\n",
            "|    total_timesteps    | 3200000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.262   |\n",
            "|    explained_variance | 0.903    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 79999    |\n",
            "|    policy_loss        | 0.249    |\n",
            "|    value_loss         | 6.27     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 568      |\n",
            "|    ep_rew_mean        | -75.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1061     |\n",
            "|    iterations         | 80100    |\n",
            "|    time_elapsed       | 3017     |\n",
            "|    total_timesteps    | 3204000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.276   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 80099    |\n",
            "|    policy_loss        | -0.165   |\n",
            "|    value_loss         | 3.06     |\n",
            "------------------------------------\n",
            "Num timesteps: 3208000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -75.39\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 577      |\n",
            "|    ep_rew_mean        | -75.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1061     |\n",
            "|    iterations         | 80200    |\n",
            "|    time_elapsed       | 3021     |\n",
            "|    total_timesteps    | 3208000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.311   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 80199    |\n",
            "|    policy_loss        | 0.148    |\n",
            "|    value_loss         | 3.26     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 574      |\n",
            "|    ep_rew_mean        | -75.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1061     |\n",
            "|    iterations         | 80300    |\n",
            "|    time_elapsed       | 3025     |\n",
            "|    total_timesteps    | 3212000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.254   |\n",
            "|    explained_variance | 0.973    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 80299    |\n",
            "|    policy_loss        | -0.219   |\n",
            "|    value_loss         | 3.68     |\n",
            "------------------------------------\n",
            "Num timesteps: 3216000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -74.54\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 567      |\n",
            "|    ep_rew_mean        | -74.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1061     |\n",
            "|    iterations         | 80400    |\n",
            "|    time_elapsed       | 3029     |\n",
            "|    total_timesteps    | 3216000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.21    |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 80399    |\n",
            "|    policy_loss        | -0.00791 |\n",
            "|    value_loss         | 2.3      |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        | 572       |\n",
            "|    ep_rew_mean        | -72.5     |\n",
            "| time/                 |           |\n",
            "|    fps                | 1061      |\n",
            "|    iterations         | 80500     |\n",
            "|    time_elapsed       | 3034      |\n",
            "|    total_timesteps    | 3220000   |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.278    |\n",
            "|    explained_variance | 0.974     |\n",
            "|    learning_rate      | 0.00083   |\n",
            "|    n_updates          | 80499     |\n",
            "|    policy_loss        | -0.000957 |\n",
            "|    value_loss         | 2.88      |\n",
            "-------------------------------------\n",
            "Num timesteps: 3224000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -74.68\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 576      |\n",
            "|    ep_rew_mean        | -74.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1061     |\n",
            "|    iterations         | 80600    |\n",
            "|    time_elapsed       | 3037     |\n",
            "|    total_timesteps    | 3224000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.38    |\n",
            "|    explained_variance | 0.951    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 80599    |\n",
            "|    policy_loss        | -0.351   |\n",
            "|    value_loss         | 42.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 573      |\n",
            "|    ep_rew_mean        | -74.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1061     |\n",
            "|    iterations         | 80700    |\n",
            "|    time_elapsed       | 3041     |\n",
            "|    total_timesteps    | 3228000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.242   |\n",
            "|    explained_variance | 0.944    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 80699    |\n",
            "|    policy_loss        | 1.1      |\n",
            "|    value_loss         | 18.6     |\n",
            "------------------------------------\n",
            "Num timesteps: 3232000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -78.69\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 552      |\n",
            "|    ep_rew_mean        | -78.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1061     |\n",
            "|    iterations         | 80800    |\n",
            "|    time_elapsed       | 3045     |\n",
            "|    total_timesteps    | 3232000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.334   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 80799    |\n",
            "|    policy_loss        | 0.302    |\n",
            "|    value_loss         | 3.12     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 547      |\n",
            "|    ep_rew_mean        | -76.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1061     |\n",
            "|    iterations         | 80900    |\n",
            "|    time_elapsed       | 3049     |\n",
            "|    total_timesteps    | 3236000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.213   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 80899    |\n",
            "|    policy_loss        | -0.202   |\n",
            "|    value_loss         | 1.89     |\n",
            "------------------------------------\n",
            "Num timesteps: 3240000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -73.76\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 560      |\n",
            "|    ep_rew_mean        | -73.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1061     |\n",
            "|    iterations         | 81000    |\n",
            "|    time_elapsed       | 3053     |\n",
            "|    total_timesteps    | 3240000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.304   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 80999    |\n",
            "|    policy_loss        | 0.0265   |\n",
            "|    value_loss         | 2.59     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 541      |\n",
            "|    ep_rew_mean        | -74.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1061     |\n",
            "|    iterations         | 81100    |\n",
            "|    time_elapsed       | 3056     |\n",
            "|    total_timesteps    | 3244000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.212   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 81099    |\n",
            "|    policy_loss        | -0.0194  |\n",
            "|    value_loss         | 4.37     |\n",
            "------------------------------------\n",
            "Num timesteps: 3248000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -69.03\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 525      |\n",
            "|    ep_rew_mean        | -69      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1061     |\n",
            "|    iterations         | 81200    |\n",
            "|    time_elapsed       | 3060     |\n",
            "|    total_timesteps    | 3248000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.298   |\n",
            "|    explained_variance | 0.978    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 81199    |\n",
            "|    policy_loss        | -0.0833  |\n",
            "|    value_loss         | 3.93     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 522      |\n",
            "|    ep_rew_mean        | -67.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1061     |\n",
            "|    iterations         | 81300    |\n",
            "|    time_elapsed       | 3063     |\n",
            "|    total_timesteps    | 3252000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.192   |\n",
            "|    explained_variance | 0.965    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 81299    |\n",
            "|    policy_loss        | 0.0722   |\n",
            "|    value_loss         | 1.63     |\n",
            "------------------------------------\n",
            "Num timesteps: 3256000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -66.75\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 530      |\n",
            "|    ep_rew_mean        | -66.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1061     |\n",
            "|    iterations         | 81400    |\n",
            "|    time_elapsed       | 3068     |\n",
            "|    total_timesteps    | 3256000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.317   |\n",
            "|    explained_variance | 0.966    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 81399    |\n",
            "|    policy_loss        | -0.37    |\n",
            "|    value_loss         | 0.648    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 550      |\n",
            "|    ep_rew_mean        | -63.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1061     |\n",
            "|    iterations         | 81500    |\n",
            "|    time_elapsed       | 3072     |\n",
            "|    total_timesteps    | 3260000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.396   |\n",
            "|    explained_variance | 0.974    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 81499    |\n",
            "|    policy_loss        | -0.0388  |\n",
            "|    value_loss         | 6.83     |\n",
            "------------------------------------\n",
            "Num timesteps: 3264000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -64.52\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 532      |\n",
            "|    ep_rew_mean        | -64.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1061     |\n",
            "|    iterations         | 81600    |\n",
            "|    time_elapsed       | 3075     |\n",
            "|    total_timesteps    | 3264000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.332   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 81599    |\n",
            "|    policy_loss        | 0.0459   |\n",
            "|    value_loss         | 3.73     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 531      |\n",
            "|    ep_rew_mean        | -64.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1060     |\n",
            "|    iterations         | 81700    |\n",
            "|    time_elapsed       | 3080     |\n",
            "|    total_timesteps    | 3268000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.216   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 81699    |\n",
            "|    policy_loss        | 0.137    |\n",
            "|    value_loss         | 7.61     |\n",
            "------------------------------------\n",
            "Num timesteps: 3272000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -67.27\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 526      |\n",
            "|    ep_rew_mean        | -67.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1060     |\n",
            "|    iterations         | 81800    |\n",
            "|    time_elapsed       | 3084     |\n",
            "|    total_timesteps    | 3272000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.262   |\n",
            "|    explained_variance | 0.767    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 81799    |\n",
            "|    policy_loss        | 0.041    |\n",
            "|    value_loss         | 202      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 520      |\n",
            "|    ep_rew_mean        | -69.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1060     |\n",
            "|    iterations         | 81900    |\n",
            "|    time_elapsed       | 3088     |\n",
            "|    total_timesteps    | 3276000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.24    |\n",
            "|    explained_variance | 0.972    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 81899    |\n",
            "|    policy_loss        | 0.16     |\n",
            "|    value_loss         | 6.78     |\n",
            "------------------------------------\n",
            "Num timesteps: 3280000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -68.26\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 525      |\n",
            "|    ep_rew_mean        | -68.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1060     |\n",
            "|    iterations         | 82000    |\n",
            "|    time_elapsed       | 3092     |\n",
            "|    total_timesteps    | 3280000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.338   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 81999    |\n",
            "|    policy_loss        | -0.724   |\n",
            "|    value_loss         | 2.32     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 530      |\n",
            "|    ep_rew_mean        | -67.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1060     |\n",
            "|    iterations         | 82100    |\n",
            "|    time_elapsed       | 3096     |\n",
            "|    total_timesteps    | 3284000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.267   |\n",
            "|    explained_variance | 0.979    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 82099    |\n",
            "|    policy_loss        | 0.3      |\n",
            "|    value_loss         | 9.55     |\n",
            "------------------------------------\n",
            "Num timesteps: 3288000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -68.81\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 526      |\n",
            "|    ep_rew_mean        | -68.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1060     |\n",
            "|    iterations         | 82200    |\n",
            "|    time_elapsed       | 3100     |\n",
            "|    total_timesteps    | 3288000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.284   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 82199    |\n",
            "|    policy_loss        | 0.0571   |\n",
            "|    value_loss         | 2.86     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 513      |\n",
            "|    ep_rew_mean        | -67.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1060     |\n",
            "|    iterations         | 82300    |\n",
            "|    time_elapsed       | 3104     |\n",
            "|    total_timesteps    | 3292000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.267   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 82299    |\n",
            "|    policy_loss        | 0.0361   |\n",
            "|    value_loss         | 5.57     |\n",
            "------------------------------------\n",
            "Num timesteps: 3296000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -70.74\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 521      |\n",
            "|    ep_rew_mean        | -70.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1060     |\n",
            "|    iterations         | 82400    |\n",
            "|    time_elapsed       | 3108     |\n",
            "|    total_timesteps    | 3296000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.341   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 82399    |\n",
            "|    policy_loss        | -0.359   |\n",
            "|    value_loss         | 5.61     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 519      |\n",
            "|    ep_rew_mean        | -72.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1060     |\n",
            "|    iterations         | 82500    |\n",
            "|    time_elapsed       | 3111     |\n",
            "|    total_timesteps    | 3300000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.365   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 82499    |\n",
            "|    policy_loss        | 0.036    |\n",
            "|    value_loss         | 2.21     |\n",
            "------------------------------------\n",
            "Num timesteps: 3304000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -74.94\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 531      |\n",
            "|    ep_rew_mean        | -74.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1060     |\n",
            "|    iterations         | 82600    |\n",
            "|    time_elapsed       | 3115     |\n",
            "|    total_timesteps    | 3304000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.202   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 82599    |\n",
            "|    policy_loss        | 0.0675   |\n",
            "|    value_loss         | 1.53     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 497      |\n",
            "|    ep_rew_mean        | -75.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1060     |\n",
            "|    iterations         | 82700    |\n",
            "|    time_elapsed       | 3119     |\n",
            "|    total_timesteps    | 3308000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.273   |\n",
            "|    explained_variance | 0.962    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 82699    |\n",
            "|    policy_loss        | 0.0786   |\n",
            "|    value_loss         | 5.48     |\n",
            "------------------------------------\n",
            "Num timesteps: 3312000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -78.24\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 504      |\n",
            "|    ep_rew_mean        | -78.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1060     |\n",
            "|    iterations         | 82800    |\n",
            "|    time_elapsed       | 3122     |\n",
            "|    total_timesteps    | 3312000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.289   |\n",
            "|    explained_variance | 0.943    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 82799    |\n",
            "|    policy_loss        | -0.144   |\n",
            "|    value_loss         | 6.11     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 520      |\n",
            "|    ep_rew_mean        | -80      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1060     |\n",
            "|    iterations         | 82900    |\n",
            "|    time_elapsed       | 3127     |\n",
            "|    total_timesteps    | 3316000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.223   |\n",
            "|    explained_variance | 0.798    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 82899    |\n",
            "|    policy_loss        | 0.0577   |\n",
            "|    value_loss         | 33.8     |\n",
            "------------------------------------\n",
            "Num timesteps: 3320000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -78.45\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 536      |\n",
            "|    ep_rew_mean        | -78.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1060     |\n",
            "|    iterations         | 83000    |\n",
            "|    time_elapsed       | 3131     |\n",
            "|    total_timesteps    | 3320000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.252   |\n",
            "|    explained_variance | 0.953    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 82999    |\n",
            "|    policy_loss        | -0.0932  |\n",
            "|    value_loss         | 3.3      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 535      |\n",
            "|    ep_rew_mean        | -79.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1060     |\n",
            "|    iterations         | 83100    |\n",
            "|    time_elapsed       | 3135     |\n",
            "|    total_timesteps    | 3324000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.3     |\n",
            "|    explained_variance | 0.69     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 83099    |\n",
            "|    policy_loss        | -0.412   |\n",
            "|    value_loss         | 137      |\n",
            "------------------------------------\n",
            "Num timesteps: 3328000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -76.96\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 539      |\n",
            "|    ep_rew_mean        | -77      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1060     |\n",
            "|    iterations         | 83200    |\n",
            "|    time_elapsed       | 3139     |\n",
            "|    total_timesteps    | 3328000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.306   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 83199    |\n",
            "|    policy_loss        | -0.0549  |\n",
            "|    value_loss         | 1.6      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 548      |\n",
            "|    ep_rew_mean        | -75.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1059     |\n",
            "|    iterations         | 83300    |\n",
            "|    time_elapsed       | 3145     |\n",
            "|    total_timesteps    | 3332000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.264   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 83299    |\n",
            "|    policy_loss        | 0.234    |\n",
            "|    value_loss         | 4.2      |\n",
            "------------------------------------\n",
            "Num timesteps: 3336000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -78.26\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 578      |\n",
            "|    ep_rew_mean        | -78.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1059     |\n",
            "|    iterations         | 83400    |\n",
            "|    time_elapsed       | 3149     |\n",
            "|    total_timesteps    | 3336000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.294   |\n",
            "|    explained_variance | 0.977    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 83399    |\n",
            "|    policy_loss        | 0.101    |\n",
            "|    value_loss         | 4.47     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 578      |\n",
            "|    ep_rew_mean        | -75.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1059     |\n",
            "|    iterations         | 83500    |\n",
            "|    time_elapsed       | 3153     |\n",
            "|    total_timesteps    | 3340000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.257   |\n",
            "|    explained_variance | 0.962    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 83499    |\n",
            "|    policy_loss        | -0.121   |\n",
            "|    value_loss         | 6.81     |\n",
            "------------------------------------\n",
            "Num timesteps: 3344000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -77.01\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 577      |\n",
            "|    ep_rew_mean        | -77      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1059     |\n",
            "|    iterations         | 83600    |\n",
            "|    time_elapsed       | 3156     |\n",
            "|    total_timesteps    | 3344000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.263   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 83599    |\n",
            "|    policy_loss        | -0.272   |\n",
            "|    value_loss         | 5.6      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 570      |\n",
            "|    ep_rew_mean        | -76.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1059     |\n",
            "|    iterations         | 83700    |\n",
            "|    time_elapsed       | 3160     |\n",
            "|    total_timesteps    | 3348000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.265   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 83699    |\n",
            "|    policy_loss        | -0.215   |\n",
            "|    value_loss         | 4.31     |\n",
            "------------------------------------\n",
            "Num timesteps: 3352000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -76.10\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 600      |\n",
            "|    ep_rew_mean        | -76.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1059     |\n",
            "|    iterations         | 83800    |\n",
            "|    time_elapsed       | 3165     |\n",
            "|    total_timesteps    | 3352000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.23    |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 83799    |\n",
            "|    policy_loss        | -0.0713  |\n",
            "|    value_loss         | 2.92     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 601      |\n",
            "|    ep_rew_mean        | -72.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1058     |\n",
            "|    iterations         | 83900    |\n",
            "|    time_elapsed       | 3169     |\n",
            "|    total_timesteps    | 3356000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.299   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 83899    |\n",
            "|    policy_loss        | 0.000768 |\n",
            "|    value_loss         | 0.899    |\n",
            "------------------------------------\n",
            "Num timesteps: 3360000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -70.41\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 616      |\n",
            "|    ep_rew_mean        | -70.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1058     |\n",
            "|    iterations         | 84000    |\n",
            "|    time_elapsed       | 3173     |\n",
            "|    total_timesteps    | 3360000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.346   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 83999    |\n",
            "|    policy_loss        | 0.288    |\n",
            "|    value_loss         | 1.99     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 631      |\n",
            "|    ep_rew_mean        | -67.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1058     |\n",
            "|    iterations         | 84100    |\n",
            "|    time_elapsed       | 3178     |\n",
            "|    total_timesteps    | 3364000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.342   |\n",
            "|    explained_variance | 0.967    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 84099    |\n",
            "|    policy_loss        | -0.117   |\n",
            "|    value_loss         | 9.98     |\n",
            "------------------------------------\n",
            "Num timesteps: 3368000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -66.96\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 634      |\n",
            "|    ep_rew_mean        | -67      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1058     |\n",
            "|    iterations         | 84200    |\n",
            "|    time_elapsed       | 3182     |\n",
            "|    total_timesteps    | 3368000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.269   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 84199    |\n",
            "|    policy_loss        | -0.162   |\n",
            "|    value_loss         | 1.66     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 643      |\n",
            "|    ep_rew_mean        | -59.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1058     |\n",
            "|    iterations         | 84300    |\n",
            "|    time_elapsed       | 3186     |\n",
            "|    total_timesteps    | 3372000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.327   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 84299    |\n",
            "|    policy_loss        | -0.0335  |\n",
            "|    value_loss         | 2.61     |\n",
            "------------------------------------\n",
            "Num timesteps: 3376000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -57.99\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 631      |\n",
            "|    ep_rew_mean        | -58      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1058     |\n",
            "|    iterations         | 84400    |\n",
            "|    time_elapsed       | 3189     |\n",
            "|    total_timesteps    | 3376000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.239   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 84399    |\n",
            "|    policy_loss        | 0.245    |\n",
            "|    value_loss         | 5.63     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 625      |\n",
            "|    ep_rew_mean        | -62.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1058     |\n",
            "|    iterations         | 84500    |\n",
            "|    time_elapsed       | 3193     |\n",
            "|    total_timesteps    | 3380000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.374   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 84499    |\n",
            "|    policy_loss        | 0.118    |\n",
            "|    value_loss         | 5.3      |\n",
            "------------------------------------\n",
            "Num timesteps: 3384000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -63.30\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 616      |\n",
            "|    ep_rew_mean        | -63.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1058     |\n",
            "|    iterations         | 84600    |\n",
            "|    time_elapsed       | 3196     |\n",
            "|    total_timesteps    | 3384000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.306   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 84599    |\n",
            "|    policy_loss        | -0.0607  |\n",
            "|    value_loss         | 0.594    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 627      |\n",
            "|    ep_rew_mean        | -60.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1058     |\n",
            "|    iterations         | 84700    |\n",
            "|    time_elapsed       | 3202     |\n",
            "|    total_timesteps    | 3388000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.391   |\n",
            "|    explained_variance | 0.98     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 84699    |\n",
            "|    policy_loss        | 0.298    |\n",
            "|    value_loss         | 6.4      |\n",
            "------------------------------------\n",
            "Num timesteps: 3392000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -60.68\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 605      |\n",
            "|    ep_rew_mean        | -60.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1058     |\n",
            "|    iterations         | 84800    |\n",
            "|    time_elapsed       | 3205     |\n",
            "|    total_timesteps    | 3392000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.304   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 84799    |\n",
            "|    policy_loss        | -0.0442  |\n",
            "|    value_loss         | 3.68     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 583      |\n",
            "|    ep_rew_mean        | -63.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1058     |\n",
            "|    iterations         | 84900    |\n",
            "|    time_elapsed       | 3209     |\n",
            "|    total_timesteps    | 3396000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.276   |\n",
            "|    explained_variance | 0.938    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 84899    |\n",
            "|    policy_loss        | 0.0997   |\n",
            "|    value_loss         | 53.2     |\n",
            "------------------------------------\n",
            "Num timesteps: 3400000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -67.08\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 598      |\n",
            "|    ep_rew_mean        | -67.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1058     |\n",
            "|    iterations         | 85000    |\n",
            "|    time_elapsed       | 3213     |\n",
            "|    total_timesteps    | 3400000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.42    |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 84999    |\n",
            "|    policy_loss        | 0.0345   |\n",
            "|    value_loss         | 6.54     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 582      |\n",
            "|    ep_rew_mean        | -69.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1058     |\n",
            "|    iterations         | 85100    |\n",
            "|    time_elapsed       | 3216     |\n",
            "|    total_timesteps    | 3404000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.368   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 85099    |\n",
            "|    policy_loss        | -0.261   |\n",
            "|    value_loss         | 1.23     |\n",
            "------------------------------------\n",
            "Num timesteps: 3408000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -68.04\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 576      |\n",
            "|    ep_rew_mean        | -68      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1058     |\n",
            "|    iterations         | 85200    |\n",
            "|    time_elapsed       | 3219     |\n",
            "|    total_timesteps    | 3408000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.274   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 85199    |\n",
            "|    policy_loss        | -0.171   |\n",
            "|    value_loss         | 1.67     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 579      |\n",
            "|    ep_rew_mean        | -67.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1057     |\n",
            "|    iterations         | 85300    |\n",
            "|    time_elapsed       | 3225     |\n",
            "|    total_timesteps    | 3412000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.302   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 85299    |\n",
            "|    policy_loss        | -0.0323  |\n",
            "|    value_loss         | 0.879    |\n",
            "------------------------------------\n",
            "Num timesteps: 3416000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -69.44\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 576      |\n",
            "|    ep_rew_mean        | -69.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1058     |\n",
            "|    iterations         | 85400    |\n",
            "|    time_elapsed       | 3228     |\n",
            "|    total_timesteps    | 3416000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.336   |\n",
            "|    explained_variance | 0.969    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 85399    |\n",
            "|    policy_loss        | 0.00889  |\n",
            "|    value_loss         | 2.72     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 571      |\n",
            "|    ep_rew_mean        | -75      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1057     |\n",
            "|    iterations         | 85500    |\n",
            "|    time_elapsed       | 3232     |\n",
            "|    total_timesteps    | 3420000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.239   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 85499    |\n",
            "|    policy_loss        | 0.0261   |\n",
            "|    value_loss         | 3.78     |\n",
            "------------------------------------\n",
            "Num timesteps: 3424000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -76.50\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 577      |\n",
            "|    ep_rew_mean        | -76.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1057     |\n",
            "|    iterations         | 85600    |\n",
            "|    time_elapsed       | 3236     |\n",
            "|    total_timesteps    | 3424000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.348   |\n",
            "|    explained_variance | 0.968    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 85599    |\n",
            "|    policy_loss        | 0.0195   |\n",
            "|    value_loss         | 1.21     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 592      |\n",
            "|    ep_rew_mean        | -76.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1057     |\n",
            "|    iterations         | 85700    |\n",
            "|    time_elapsed       | 3241     |\n",
            "|    total_timesteps    | 3428000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.325   |\n",
            "|    explained_variance | 0.959    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 85699    |\n",
            "|    policy_loss        | 0.0393   |\n",
            "|    value_loss         | 1.67     |\n",
            "------------------------------------\n",
            "Num timesteps: 3432000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -81.17\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 602      |\n",
            "|    ep_rew_mean        | -81.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1057     |\n",
            "|    iterations         | 85800    |\n",
            "|    time_elapsed       | 3245     |\n",
            "|    total_timesteps    | 3432000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.378   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 85799    |\n",
            "|    policy_loss        | -0.27    |\n",
            "|    value_loss         | 8.71     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 599      |\n",
            "|    ep_rew_mean        | -84.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1057     |\n",
            "|    iterations         | 85900    |\n",
            "|    time_elapsed       | 3249     |\n",
            "|    total_timesteps    | 3436000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.385   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 85899    |\n",
            "|    policy_loss        | 0.0646   |\n",
            "|    value_loss         | 3.16     |\n",
            "------------------------------------\n",
            "Num timesteps: 3440000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -82.54\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 605      |\n",
            "|    ep_rew_mean        | -82.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1057     |\n",
            "|    iterations         | 86000    |\n",
            "|    time_elapsed       | 3252     |\n",
            "|    total_timesteps    | 3440000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.257   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 85999    |\n",
            "|    policy_loss        | 0.0283   |\n",
            "|    value_loss         | 1.49     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 619      |\n",
            "|    ep_rew_mean        | -82.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1057     |\n",
            "|    iterations         | 86100    |\n",
            "|    time_elapsed       | 3256     |\n",
            "|    total_timesteps    | 3444000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.247   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 86099    |\n",
            "|    policy_loss        | -0.00831 |\n",
            "|    value_loss         | 1.81     |\n",
            "------------------------------------\n",
            "Num timesteps: 3448000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -84.85\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 603      |\n",
            "|    ep_rew_mean        | -84.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1057     |\n",
            "|    iterations         | 86200    |\n",
            "|    time_elapsed       | 3260     |\n",
            "|    total_timesteps    | 3448000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.325   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 86199    |\n",
            "|    policy_loss        | 0.116    |\n",
            "|    value_loss         | 1.94     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 610      |\n",
            "|    ep_rew_mean        | -84      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1056     |\n",
            "|    iterations         | 86300    |\n",
            "|    time_elapsed       | 3266     |\n",
            "|    total_timesteps    | 3452000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.263   |\n",
            "|    explained_variance | 0.934    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 86299    |\n",
            "|    policy_loss        | 0.135    |\n",
            "|    value_loss         | 3.18     |\n",
            "------------------------------------\n",
            "Num timesteps: 3456000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -78.45\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 615      |\n",
            "|    ep_rew_mean        | -78.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1056     |\n",
            "|    iterations         | 86400    |\n",
            "|    time_elapsed       | 3269     |\n",
            "|    total_timesteps    | 3456000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.244   |\n",
            "|    explained_variance | 0.961    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 86399    |\n",
            "|    policy_loss        | -0.158   |\n",
            "|    value_loss         | 5.5      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 601      |\n",
            "|    ep_rew_mean        | -81.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1056     |\n",
            "|    iterations         | 86500    |\n",
            "|    time_elapsed       | 3273     |\n",
            "|    total_timesteps    | 3460000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.207   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 86499    |\n",
            "|    policy_loss        | -0.0813  |\n",
            "|    value_loss         | 2.86     |\n",
            "------------------------------------\n",
            "Num timesteps: 3464000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -79.87\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 620      |\n",
            "|    ep_rew_mean        | -79.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1056     |\n",
            "|    iterations         | 86600    |\n",
            "|    time_elapsed       | 3277     |\n",
            "|    total_timesteps    | 3464000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.322   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 86599    |\n",
            "|    policy_loss        | -0.127   |\n",
            "|    value_loss         | 5.56     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 612      |\n",
            "|    ep_rew_mean        | -81.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1056     |\n",
            "|    iterations         | 86700    |\n",
            "|    time_elapsed       | 3282     |\n",
            "|    total_timesteps    | 3468000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.221   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 86699    |\n",
            "|    policy_loss        | 0.00865  |\n",
            "|    value_loss         | 2.3      |\n",
            "------------------------------------\n",
            "Num timesteps: 3472000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -80.24\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 638      |\n",
            "|    ep_rew_mean        | -80.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1056     |\n",
            "|    iterations         | 86800    |\n",
            "|    time_elapsed       | 3286     |\n",
            "|    total_timesteps    | 3472000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.302   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 86799    |\n",
            "|    policy_loss        | 0.0898   |\n",
            "|    value_loss         | 1.28     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 636      |\n",
            "|    ep_rew_mean        | -81.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1056     |\n",
            "|    iterations         | 86900    |\n",
            "|    time_elapsed       | 3290     |\n",
            "|    total_timesteps    | 3476000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.229   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 86899    |\n",
            "|    policy_loss        | 0.173    |\n",
            "|    value_loss         | 3.48     |\n",
            "------------------------------------\n",
            "Num timesteps: 3480000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -82.33\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 637      |\n",
            "|    ep_rew_mean        | -82.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1056     |\n",
            "|    iterations         | 87000    |\n",
            "|    time_elapsed       | 3294     |\n",
            "|    total_timesteps    | 3480000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.24    |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 86999    |\n",
            "|    policy_loss        | 0.165    |\n",
            "|    value_loss         | 1.58     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 644      |\n",
            "|    ep_rew_mean        | -74.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1055     |\n",
            "|    iterations         | 87100    |\n",
            "|    time_elapsed       | 3299     |\n",
            "|    total_timesteps    | 3484000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.319   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 87099    |\n",
            "|    policy_loss        | -0.0751  |\n",
            "|    value_loss         | 2.76     |\n",
            "------------------------------------\n",
            "Num timesteps: 3488000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -74.44\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 621      |\n",
            "|    ep_rew_mean        | -74.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1056     |\n",
            "|    iterations         | 87200    |\n",
            "|    time_elapsed       | 3302     |\n",
            "|    total_timesteps    | 3488000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.262   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 87199    |\n",
            "|    policy_loss        | -0.284   |\n",
            "|    value_loss         | 7.51     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 598      |\n",
            "|    ep_rew_mean        | -73      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1056     |\n",
            "|    iterations         | 87300    |\n",
            "|    time_elapsed       | 3306     |\n",
            "|    total_timesteps    | 3492000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.261   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 87299    |\n",
            "|    policy_loss        | 0.225    |\n",
            "|    value_loss         | 2.55     |\n",
            "------------------------------------\n",
            "Num timesteps: 3496000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -68.34\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 613      |\n",
            "|    ep_rew_mean        | -68.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1056     |\n",
            "|    iterations         | 87400    |\n",
            "|    time_elapsed       | 3310     |\n",
            "|    total_timesteps    | 3496000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.331   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 87399    |\n",
            "|    policy_loss        | 0.214    |\n",
            "|    value_loss         | 1.8      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 625      |\n",
            "|    ep_rew_mean        | -71.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1055     |\n",
            "|    iterations         | 87500    |\n",
            "|    time_elapsed       | 3314     |\n",
            "|    total_timesteps    | 3500000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.338   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 87499    |\n",
            "|    policy_loss        | 0.102    |\n",
            "|    value_loss         | 2.56     |\n",
            "------------------------------------\n",
            "Num timesteps: 3504000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -68.14\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 612      |\n",
            "|    ep_rew_mean        | -68.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1055     |\n",
            "|    iterations         | 87600    |\n",
            "|    time_elapsed       | 3318     |\n",
            "|    total_timesteps    | 3504000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.248   |\n",
            "|    explained_variance | 0.971    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 87599    |\n",
            "|    policy_loss        | -1.84    |\n",
            "|    value_loss         | 51.8     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 584      |\n",
            "|    ep_rew_mean        | -69.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1056     |\n",
            "|    iterations         | 87700    |\n",
            "|    time_elapsed       | 3321     |\n",
            "|    total_timesteps    | 3508000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.283   |\n",
            "|    explained_variance | 0.978    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 87699    |\n",
            "|    policy_loss        | -0.00819 |\n",
            "|    value_loss         | 7.05     |\n",
            "------------------------------------\n",
            "Num timesteps: 3512000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -68.09\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 589      |\n",
            "|    ep_rew_mean        | -68.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1056     |\n",
            "|    iterations         | 87800    |\n",
            "|    time_elapsed       | 3325     |\n",
            "|    total_timesteps    | 3512000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.285   |\n",
            "|    explained_variance | 0.926    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 87799    |\n",
            "|    policy_loss        | -1.37    |\n",
            "|    value_loss         | 99.9     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 602      |\n",
            "|    ep_rew_mean        | -69.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1056     |\n",
            "|    iterations         | 87900    |\n",
            "|    time_elapsed       | 3329     |\n",
            "|    total_timesteps    | 3516000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.27    |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 87899    |\n",
            "|    policy_loss        | 0.0916   |\n",
            "|    value_loss         | 6.91     |\n",
            "------------------------------------\n",
            "Num timesteps: 3520000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -66.42\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 608      |\n",
            "|    ep_rew_mean        | -66.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1056     |\n",
            "|    iterations         | 88000    |\n",
            "|    time_elapsed       | 3332     |\n",
            "|    total_timesteps    | 3520000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.313   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 87999    |\n",
            "|    policy_loss        | 0.1      |\n",
            "|    value_loss         | 5.67     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 595      |\n",
            "|    ep_rew_mean        | -70.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1056     |\n",
            "|    iterations         | 88100    |\n",
            "|    time_elapsed       | 3335     |\n",
            "|    total_timesteps    | 3524000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.372   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 88099    |\n",
            "|    policy_loss        | -0.0355  |\n",
            "|    value_loss         | 1.87     |\n",
            "------------------------------------\n",
            "Num timesteps: 3528000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -62.59\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 609      |\n",
            "|    ep_rew_mean        | -62.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1056     |\n",
            "|    iterations         | 88200    |\n",
            "|    time_elapsed       | 3340     |\n",
            "|    total_timesteps    | 3528000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.213   |\n",
            "|    explained_variance | 0.841    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 88199    |\n",
            "|    policy_loss        | -0.5     |\n",
            "|    value_loss         | 57.9     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 591      |\n",
            "|    ep_rew_mean        | -51.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1056     |\n",
            "|    iterations         | 88300    |\n",
            "|    time_elapsed       | 3344     |\n",
            "|    total_timesteps    | 3532000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.23    |\n",
            "|    explained_variance | -0.123   |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 88299    |\n",
            "|    policy_loss        | 0.257    |\n",
            "|    value_loss         | 416      |\n",
            "------------------------------------\n",
            "Num timesteps: 3536000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -43.81\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 586      |\n",
            "|    ep_rew_mean        | -43.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1056     |\n",
            "|    iterations         | 88400    |\n",
            "|    time_elapsed       | 3347     |\n",
            "|    total_timesteps    | 3536000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.307   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 88399    |\n",
            "|    policy_loss        | -0.121   |\n",
            "|    value_loss         | 1.86     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 568      |\n",
            "|    ep_rew_mean        | -38.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1056     |\n",
            "|    iterations         | 88500    |\n",
            "|    time_elapsed       | 3351     |\n",
            "|    total_timesteps    | 3540000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.288   |\n",
            "|    explained_variance | 0.979    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 88499    |\n",
            "|    policy_loss        | -0.0988  |\n",
            "|    value_loss         | 4.03     |\n",
            "------------------------------------\n",
            "Num timesteps: 3544000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -37.46\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 562      |\n",
            "|    ep_rew_mean        | -37.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1056     |\n",
            "|    iterations         | 88600    |\n",
            "|    time_elapsed       | 3355     |\n",
            "|    total_timesteps    | 3544000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.386   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 88599    |\n",
            "|    policy_loss        | -0.217   |\n",
            "|    value_loss         | 4.86     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 576      |\n",
            "|    ep_rew_mean        | -29.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1056     |\n",
            "|    iterations         | 88700    |\n",
            "|    time_elapsed       | 3359     |\n",
            "|    total_timesteps    | 3548000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.326   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 88699    |\n",
            "|    policy_loss        | 0.00117  |\n",
            "|    value_loss         | 0.92     |\n",
            "------------------------------------\n",
            "Num timesteps: 3552000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -23.68\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 578      |\n",
            "|    ep_rew_mean        | -23.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1056     |\n",
            "|    iterations         | 88800    |\n",
            "|    time_elapsed       | 3362     |\n",
            "|    total_timesteps    | 3552000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.341   |\n",
            "|    explained_variance | 0.967    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 88799    |\n",
            "|    policy_loss        | -0.0883  |\n",
            "|    value_loss         | 1.74     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 570      |\n",
            "|    ep_rew_mean        | -24.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1056     |\n",
            "|    iterations         | 88900    |\n",
            "|    time_elapsed       | 3366     |\n",
            "|    total_timesteps    | 3556000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.355   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 88899    |\n",
            "|    policy_loss        | -0.38    |\n",
            "|    value_loss         | 3.11     |\n",
            "------------------------------------\n",
            "Num timesteps: 3560000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -23.39\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 572      |\n",
            "|    ep_rew_mean        | -23.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1056     |\n",
            "|    iterations         | 89000    |\n",
            "|    time_elapsed       | 3370     |\n",
            "|    total_timesteps    | 3560000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.274   |\n",
            "|    explained_variance | 0.934    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 88999    |\n",
            "|    policy_loss        | 0.177    |\n",
            "|    value_loss         | 1.19     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 589      |\n",
            "|    ep_rew_mean        | -25.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1055     |\n",
            "|    iterations         | 89100    |\n",
            "|    time_elapsed       | 3375     |\n",
            "|    total_timesteps    | 3564000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.283   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 89099    |\n",
            "|    policy_loss        | -0.205   |\n",
            "|    value_loss         | 11.9     |\n",
            "------------------------------------\n",
            "Num timesteps: 3568000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -24.89\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 593      |\n",
            "|    ep_rew_mean        | -24.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1056     |\n",
            "|    iterations         | 89200    |\n",
            "|    time_elapsed       | 3378     |\n",
            "|    total_timesteps    | 3568000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.293   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 89199    |\n",
            "|    policy_loss        | 0.00247  |\n",
            "|    value_loss         | 1        |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 593      |\n",
            "|    ep_rew_mean        | -23.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1055     |\n",
            "|    iterations         | 89300    |\n",
            "|    time_elapsed       | 3383     |\n",
            "|    total_timesteps    | 3572000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.282   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 89299    |\n",
            "|    policy_loss        | 0.0389   |\n",
            "|    value_loss         | 3.82     |\n",
            "------------------------------------\n",
            "Num timesteps: 3576000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -20.71\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 615      |\n",
            "|    ep_rew_mean        | -20.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1055     |\n",
            "|    iterations         | 89400    |\n",
            "|    time_elapsed       | 3387     |\n",
            "|    total_timesteps    | 3576000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.333   |\n",
            "|    explained_variance | 0.978    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 89399    |\n",
            "|    policy_loss        | 0.407    |\n",
            "|    value_loss         | 16.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 616      |\n",
            "|    ep_rew_mean        | -17      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1055     |\n",
            "|    iterations         | 89500    |\n",
            "|    time_elapsed       | 3391     |\n",
            "|    total_timesteps    | 3580000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.302   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 89499    |\n",
            "|    policy_loss        | 0.00237  |\n",
            "|    value_loss         | 4.76     |\n",
            "------------------------------------\n",
            "Num timesteps: 3584000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -15.29\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 626      |\n",
            "|    ep_rew_mean        | -15.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1055     |\n",
            "|    iterations         | 89600    |\n",
            "|    time_elapsed       | 3396     |\n",
            "|    total_timesteps    | 3584000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.269   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 89599    |\n",
            "|    policy_loss        | 0.145    |\n",
            "|    value_loss         | 4.13     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 629      |\n",
            "|    ep_rew_mean        | -12.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1054     |\n",
            "|    iterations         | 89700    |\n",
            "|    time_elapsed       | 3401     |\n",
            "|    total_timesteps    | 3588000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.276   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 89699    |\n",
            "|    policy_loss        | -0.0422  |\n",
            "|    value_loss         | 3.47     |\n",
            "------------------------------------\n",
            "Num timesteps: 3592000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -25.15\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 628      |\n",
            "|    ep_rew_mean        | -25.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1054     |\n",
            "|    iterations         | 89800    |\n",
            "|    time_elapsed       | 3405     |\n",
            "|    total_timesteps    | 3592000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.202   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 89799    |\n",
            "|    policy_loss        | -0.0781  |\n",
            "|    value_loss         | 3.23     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 626      |\n",
            "|    ep_rew_mean        | -36.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1054     |\n",
            "|    iterations         | 89900    |\n",
            "|    time_elapsed       | 3409     |\n",
            "|    total_timesteps    | 3596000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.27    |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 89899    |\n",
            "|    policy_loss        | -0.0227  |\n",
            "|    value_loss         | 3.12     |\n",
            "------------------------------------\n",
            "Num timesteps: 3600000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -40.26\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 643      |\n",
            "|    ep_rew_mean        | -40.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1054     |\n",
            "|    iterations         | 90000    |\n",
            "|    time_elapsed       | 3414     |\n",
            "|    total_timesteps    | 3600000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.282   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 89999    |\n",
            "|    policy_loss        | 0.299    |\n",
            "|    value_loss         | 3.92     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 649      |\n",
            "|    ep_rew_mean        | -47.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1054     |\n",
            "|    iterations         | 90100    |\n",
            "|    time_elapsed       | 3418     |\n",
            "|    total_timesteps    | 3604000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.357   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 90099    |\n",
            "|    policy_loss        | -0.186   |\n",
            "|    value_loss         | 1.81     |\n",
            "------------------------------------\n",
            "Num timesteps: 3608000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -52.46\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 655      |\n",
            "|    ep_rew_mean        | -52.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1054     |\n",
            "|    iterations         | 90200    |\n",
            "|    time_elapsed       | 3422     |\n",
            "|    total_timesteps    | 3608000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.339   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 90199    |\n",
            "|    policy_loss        | 0.0676   |\n",
            "|    value_loss         | 7.43     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 649      |\n",
            "|    ep_rew_mean        | -59.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1054     |\n",
            "|    iterations         | 90300    |\n",
            "|    time_elapsed       | 3426     |\n",
            "|    total_timesteps    | 3612000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.325   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 90299    |\n",
            "|    policy_loss        | -0.181   |\n",
            "|    value_loss         | 2.37     |\n",
            "------------------------------------\n",
            "Num timesteps: 3616000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -64.43\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 654      |\n",
            "|    ep_rew_mean        | -64.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1053     |\n",
            "|    iterations         | 90400    |\n",
            "|    time_elapsed       | 3431     |\n",
            "|    total_timesteps    | 3616000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.237   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 90399    |\n",
            "|    policy_loss        | -0.0317  |\n",
            "|    value_loss         | 4.27     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 648      |\n",
            "|    ep_rew_mean        | -68.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1053     |\n",
            "|    iterations         | 90500    |\n",
            "|    time_elapsed       | 3436     |\n",
            "|    total_timesteps    | 3620000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.333   |\n",
            "|    explained_variance | 0.883    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 90499    |\n",
            "|    policy_loss        | -0.277   |\n",
            "|    value_loss         | 144      |\n",
            "------------------------------------\n",
            "Num timesteps: 3624000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -67.78\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 629      |\n",
            "|    ep_rew_mean        | -67.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1053     |\n",
            "|    iterations         | 90600    |\n",
            "|    time_elapsed       | 3439     |\n",
            "|    total_timesteps    | 3624000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.351   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 90599    |\n",
            "|    policy_loss        | -0.0776  |\n",
            "|    value_loss         | 1.96     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 623      |\n",
            "|    ep_rew_mean        | -71      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1053     |\n",
            "|    iterations         | 90700    |\n",
            "|    time_elapsed       | 3443     |\n",
            "|    total_timesteps    | 3628000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.246   |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 90699    |\n",
            "|    policy_loss        | 0.0422   |\n",
            "|    value_loss         | 6.38     |\n",
            "------------------------------------\n",
            "Num timesteps: 3632000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -75.17\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 644      |\n",
            "|    ep_rew_mean        | -75.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1053     |\n",
            "|    iterations         | 90800    |\n",
            "|    time_elapsed       | 3448     |\n",
            "|    total_timesteps    | 3632000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.295   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 90799    |\n",
            "|    policy_loss        | 0.114    |\n",
            "|    value_loss         | 2.81     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 650      |\n",
            "|    ep_rew_mean        | -75.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1052     |\n",
            "|    iterations         | 90900    |\n",
            "|    time_elapsed       | 3453     |\n",
            "|    total_timesteps    | 3636000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.259   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 90899    |\n",
            "|    policy_loss        | 0.254    |\n",
            "|    value_loss         | 1.09     |\n",
            "------------------------------------\n",
            "Num timesteps: 3640000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -78.24\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 649      |\n",
            "|    ep_rew_mean        | -78.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1052     |\n",
            "|    iterations         | 91000    |\n",
            "|    time_elapsed       | 3457     |\n",
            "|    total_timesteps    | 3640000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.418   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 90999    |\n",
            "|    policy_loss        | 0.176    |\n",
            "|    value_loss         | 3.53     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 636      |\n",
            "|    ep_rew_mean        | -83.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1052     |\n",
            "|    iterations         | 91100    |\n",
            "|    time_elapsed       | 3462     |\n",
            "|    total_timesteps    | 3644000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.326   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 91099    |\n",
            "|    policy_loss        | -0.302   |\n",
            "|    value_loss         | 2.53     |\n",
            "------------------------------------\n",
            "Num timesteps: 3648000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -83.38\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 637      |\n",
            "|    ep_rew_mean        | -83.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1051     |\n",
            "|    iterations         | 91200    |\n",
            "|    time_elapsed       | 3468     |\n",
            "|    total_timesteps    | 3648000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.288   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 91199    |\n",
            "|    policy_loss        | 0.0811   |\n",
            "|    value_loss         | 2.29     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 654      |\n",
            "|    ep_rew_mean        | -77.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1051     |\n",
            "|    iterations         | 91300    |\n",
            "|    time_elapsed       | 3473     |\n",
            "|    total_timesteps    | 3652000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.334   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 91299    |\n",
            "|    policy_loss        | -0.00578 |\n",
            "|    value_loss         | 2.7      |\n",
            "------------------------------------\n",
            "Num timesteps: 3656000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -74.51\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 651      |\n",
            "|    ep_rew_mean        | -74.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1051     |\n",
            "|    iterations         | 91400    |\n",
            "|    time_elapsed       | 3477     |\n",
            "|    total_timesteps    | 3656000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.318   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 91399    |\n",
            "|    policy_loss        | -0.207   |\n",
            "|    value_loss         | 4.79     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 652      |\n",
            "|    ep_rew_mean        | -70.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1051     |\n",
            "|    iterations         | 91500    |\n",
            "|    time_elapsed       | 3481     |\n",
            "|    total_timesteps    | 3660000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.222   |\n",
            "|    explained_variance | 0.959    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 91499    |\n",
            "|    policy_loss        | 0.179    |\n",
            "|    value_loss         | 28.4     |\n",
            "------------------------------------\n",
            "Num timesteps: 3664000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -63.12\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 645      |\n",
            "|    ep_rew_mean        | -63.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1051     |\n",
            "|    iterations         | 91600    |\n",
            "|    time_elapsed       | 3485     |\n",
            "|    total_timesteps    | 3664000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.274   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 91599    |\n",
            "|    policy_loss        | 0.169    |\n",
            "|    value_loss         | 2.82     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 650      |\n",
            "|    ep_rew_mean        | -56      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1050     |\n",
            "|    iterations         | 91700    |\n",
            "|    time_elapsed       | 3490     |\n",
            "|    total_timesteps    | 3668000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.213   |\n",
            "|    explained_variance | 0.97     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 91699    |\n",
            "|    policy_loss        | 0.0141   |\n",
            "|    value_loss         | 7.75     |\n",
            "------------------------------------\n",
            "Num timesteps: 3672000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -50.12\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 660      |\n",
            "|    ep_rew_mean        | -50.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1050     |\n",
            "|    iterations         | 91800    |\n",
            "|    time_elapsed       | 3494     |\n",
            "|    total_timesteps    | 3672000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.247   |\n",
            "|    explained_variance | 0.908    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 91799    |\n",
            "|    policy_loss        | -0.0808  |\n",
            "|    value_loss         | 4.67     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 669      |\n",
            "|    ep_rew_mean        | -48.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1049     |\n",
            "|    iterations         | 91900    |\n",
            "|    time_elapsed       | 3501     |\n",
            "|    total_timesteps    | 3676000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.34    |\n",
            "|    explained_variance | 0.977    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 91899    |\n",
            "|    policy_loss        | 0.2      |\n",
            "|    value_loss         | 1.39     |\n",
            "------------------------------------\n",
            "Num timesteps: 3680000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -48.79\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 668      |\n",
            "|    ep_rew_mean        | -48.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1049     |\n",
            "|    iterations         | 92000    |\n",
            "|    time_elapsed       | 3505     |\n",
            "|    total_timesteps    | 3680000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.254   |\n",
            "|    explained_variance | 0.951    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 91999    |\n",
            "|    policy_loss        | -0.417   |\n",
            "|    value_loss         | 27       |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 670      |\n",
            "|    ep_rew_mean        | -44.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1049     |\n",
            "|    iterations         | 92100    |\n",
            "|    time_elapsed       | 3509     |\n",
            "|    total_timesteps    | 3684000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.325   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 92099    |\n",
            "|    policy_loss        | -0.188   |\n",
            "|    value_loss         | 8.74     |\n",
            "------------------------------------\n",
            "Num timesteps: 3688000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -42.99\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 663      |\n",
            "|    ep_rew_mean        | -43      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1049     |\n",
            "|    iterations         | 92200    |\n",
            "|    time_elapsed       | 3514     |\n",
            "|    total_timesteps    | 3688000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.266   |\n",
            "|    explained_variance | 0.846    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 92199    |\n",
            "|    policy_loss        | 0.343    |\n",
            "|    value_loss         | 177      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 670      |\n",
            "|    ep_rew_mean        | -42.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1049     |\n",
            "|    iterations         | 92300    |\n",
            "|    time_elapsed       | 3518     |\n",
            "|    total_timesteps    | 3692000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.387   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 92299    |\n",
            "|    policy_loss        | 0.0666   |\n",
            "|    value_loss         | 0.721    |\n",
            "------------------------------------\n",
            "Num timesteps: 3696000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -38.94\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 662      |\n",
            "|    ep_rew_mean        | -38.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1049     |\n",
            "|    iterations         | 92400    |\n",
            "|    time_elapsed       | 3523     |\n",
            "|    total_timesteps    | 3696000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.33    |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 92399    |\n",
            "|    policy_loss        | 0.103    |\n",
            "|    value_loss         | 6.03     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 670      |\n",
            "|    ep_rew_mean        | -37.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1048     |\n",
            "|    iterations         | 92500    |\n",
            "|    time_elapsed       | 3528     |\n",
            "|    total_timesteps    | 3700000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.304   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 92499    |\n",
            "|    policy_loss        | -0.105   |\n",
            "|    value_loss         | 2.18     |\n",
            "------------------------------------\n",
            "Num timesteps: 3704000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -39.86\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 666      |\n",
            "|    ep_rew_mean        | -39.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1048     |\n",
            "|    iterations         | 92600    |\n",
            "|    time_elapsed       | 3533     |\n",
            "|    total_timesteps    | 3704000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.338   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 92599    |\n",
            "|    policy_loss        | 0.567    |\n",
            "|    value_loss         | 3.55     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 664      |\n",
            "|    ep_rew_mean        | -31.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1048     |\n",
            "|    iterations         | 92700    |\n",
            "|    time_elapsed       | 3536     |\n",
            "|    total_timesteps    | 3708000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.309   |\n",
            "|    explained_variance | 0.98     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 92699    |\n",
            "|    policy_loss        | -0.016   |\n",
            "|    value_loss         | 2.98     |\n",
            "------------------------------------\n",
            "Num timesteps: 3712000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -29.11\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 673      |\n",
            "|    ep_rew_mean        | -29.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1048     |\n",
            "|    iterations         | 92800    |\n",
            "|    time_elapsed       | 3541     |\n",
            "|    total_timesteps    | 3712000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.195   |\n",
            "|    explained_variance | 0.511    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 92799    |\n",
            "|    policy_loss        | -0.00198 |\n",
            "|    value_loss         | 232      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 663      |\n",
            "|    ep_rew_mean        | -33.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1048     |\n",
            "|    iterations         | 92900    |\n",
            "|    time_elapsed       | 3544     |\n",
            "|    total_timesteps    | 3716000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.279   |\n",
            "|    explained_variance | 0.972    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 92899    |\n",
            "|    policy_loss        | 0.073    |\n",
            "|    value_loss         | 3.73     |\n",
            "------------------------------------\n",
            "Num timesteps: 3720000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -32.49\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 664      |\n",
            "|    ep_rew_mean        | -32.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1048     |\n",
            "|    iterations         | 93000    |\n",
            "|    time_elapsed       | 3548     |\n",
            "|    total_timesteps    | 3720000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.29    |\n",
            "|    explained_variance | 0.976    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 92999    |\n",
            "|    policy_loss        | 0.128    |\n",
            "|    value_loss         | 3.82     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 640      |\n",
            "|    ep_rew_mean        | -37.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1048     |\n",
            "|    iterations         | 93100    |\n",
            "|    time_elapsed       | 3552     |\n",
            "|    total_timesteps    | 3724000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.272   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 93099    |\n",
            "|    policy_loss        | 0.214    |\n",
            "|    value_loss         | 1.82     |\n",
            "------------------------------------\n",
            "Num timesteps: 3728000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -44.23\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 642      |\n",
            "|    ep_rew_mean        | -44.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1048     |\n",
            "|    iterations         | 93200    |\n",
            "|    time_elapsed       | 3556     |\n",
            "|    total_timesteps    | 3728000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.235   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 93199    |\n",
            "|    policy_loss        | 0.0741   |\n",
            "|    value_loss         | 1.04     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 631      |\n",
            "|    ep_rew_mean        | -50.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1048     |\n",
            "|    iterations         | 93300    |\n",
            "|    time_elapsed       | 3560     |\n",
            "|    total_timesteps    | 3732000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.306   |\n",
            "|    explained_variance | 0.979    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 93299    |\n",
            "|    policy_loss        | -0.14    |\n",
            "|    value_loss         | 2.49     |\n",
            "------------------------------------\n",
            "Num timesteps: 3736000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -63.53\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 597      |\n",
            "|    ep_rew_mean        | -63.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1048     |\n",
            "|    iterations         | 93400    |\n",
            "|    time_elapsed       | 3563     |\n",
            "|    total_timesteps    | 3736000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.35    |\n",
            "|    explained_variance | 0.974    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 93399    |\n",
            "|    policy_loss        | -0.131   |\n",
            "|    value_loss         | 8.3      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 563      |\n",
            "|    ep_rew_mean        | -69.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1048     |\n",
            "|    iterations         | 93500    |\n",
            "|    time_elapsed       | 3566     |\n",
            "|    total_timesteps    | 3740000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.364   |\n",
            "|    explained_variance | 0.946    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 93499    |\n",
            "|    policy_loss        | -1.79    |\n",
            "|    value_loss         | 49.7     |\n",
            "------------------------------------\n",
            "Num timesteps: 3744000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -71.56\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 570      |\n",
            "|    ep_rew_mean        | -71.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1048     |\n",
            "|    iterations         | 93600    |\n",
            "|    time_elapsed       | 3570     |\n",
            "|    total_timesteps    | 3744000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.298   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 93599    |\n",
            "|    policy_loss        | -0.0377  |\n",
            "|    value_loss         | 4.76     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 573      |\n",
            "|    ep_rew_mean        | -72.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1048     |\n",
            "|    iterations         | 93700    |\n",
            "|    time_elapsed       | 3575     |\n",
            "|    total_timesteps    | 3748000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.239   |\n",
            "|    explained_variance | 0.98     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 93699    |\n",
            "|    policy_loss        | 0.357    |\n",
            "|    value_loss         | 8.51     |\n",
            "------------------------------------\n",
            "Num timesteps: 3752000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -69.51\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 586      |\n",
            "|    ep_rew_mean        | -69.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1047     |\n",
            "|    iterations         | 93800    |\n",
            "|    time_elapsed       | 3580     |\n",
            "|    total_timesteps    | 3752000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.284   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 93799    |\n",
            "|    policy_loss        | 0.599    |\n",
            "|    value_loss         | 7.36     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 559      |\n",
            "|    ep_rew_mean        | -70.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1047     |\n",
            "|    iterations         | 93900    |\n",
            "|    time_elapsed       | 3584     |\n",
            "|    total_timesteps    | 3756000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.377   |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 93899    |\n",
            "|    policy_loss        | 0.0236   |\n",
            "|    value_loss         | 2.03     |\n",
            "------------------------------------\n",
            "Num timesteps: 3760000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -65.98\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 566      |\n",
            "|    ep_rew_mean        | -66      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1047     |\n",
            "|    iterations         | 94000    |\n",
            "|    time_elapsed       | 3589     |\n",
            "|    total_timesteps    | 3760000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.202   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 93999    |\n",
            "|    policy_loss        | -0.0728  |\n",
            "|    value_loss         | 4.3      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 565      |\n",
            "|    ep_rew_mean        | -68.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1047     |\n",
            "|    iterations         | 94100    |\n",
            "|    time_elapsed       | 3593     |\n",
            "|    total_timesteps    | 3764000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.207   |\n",
            "|    explained_variance | 0.879    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 94099    |\n",
            "|    policy_loss        | 0.167    |\n",
            "|    value_loss         | 42.2     |\n",
            "------------------------------------\n",
            "Num timesteps: 3768000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -75.56\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 550      |\n",
            "|    ep_rew_mean        | -75.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1047     |\n",
            "|    iterations         | 94200    |\n",
            "|    time_elapsed       | 3598     |\n",
            "|    total_timesteps    | 3768000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.254   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 94199    |\n",
            "|    policy_loss        | -0.081   |\n",
            "|    value_loss         | 2.17     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 544      |\n",
            "|    ep_rew_mean        | -70.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1047     |\n",
            "|    iterations         | 94300    |\n",
            "|    time_elapsed       | 3602     |\n",
            "|    total_timesteps    | 3772000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.285   |\n",
            "|    explained_variance | 0.968    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 94299    |\n",
            "|    policy_loss        | 0.0793   |\n",
            "|    value_loss         | 39.8     |\n",
            "------------------------------------\n",
            "Num timesteps: 3776000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -62.68\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 543      |\n",
            "|    ep_rew_mean        | -62.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1047     |\n",
            "|    iterations         | 94400    |\n",
            "|    time_elapsed       | 3605     |\n",
            "|    total_timesteps    | 3776000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.289   |\n",
            "|    explained_variance | 0.975    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 94399    |\n",
            "|    policy_loss        | -0.146   |\n",
            "|    value_loss         | 29.1     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 541      |\n",
            "|    ep_rew_mean        | -61      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1047     |\n",
            "|    iterations         | 94500    |\n",
            "|    time_elapsed       | 3610     |\n",
            "|    total_timesteps    | 3780000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.272   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 94499    |\n",
            "|    policy_loss        | -0.0516  |\n",
            "|    value_loss         | 1.69     |\n",
            "------------------------------------\n",
            "Num timesteps: 3784000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -59.25\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 540      |\n",
            "|    ep_rew_mean        | -59.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1047     |\n",
            "|    iterations         | 94600    |\n",
            "|    time_elapsed       | 3613     |\n",
            "|    total_timesteps    | 3784000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.304   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 94599    |\n",
            "|    policy_loss        | 0.0345   |\n",
            "|    value_loss         | 5.33     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 540      |\n",
            "|    ep_rew_mean        | -59.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1047     |\n",
            "|    iterations         | 94700    |\n",
            "|    time_elapsed       | 3617     |\n",
            "|    total_timesteps    | 3788000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.241   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 94699    |\n",
            "|    policy_loss        | 0.213    |\n",
            "|    value_loss         | 3.29     |\n",
            "------------------------------------\n",
            "Num timesteps: 3792000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -52.39\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 558      |\n",
            "|    ep_rew_mean        | -52.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1046     |\n",
            "|    iterations         | 94800    |\n",
            "|    time_elapsed       | 3623     |\n",
            "|    total_timesteps    | 3792000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.326   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 94799    |\n",
            "|    policy_loss        | -0.62    |\n",
            "|    value_loss         | 5.85     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 573      |\n",
            "|    ep_rew_mean        | -51.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1046     |\n",
            "|    iterations         | 94900    |\n",
            "|    time_elapsed       | 3628     |\n",
            "|    total_timesteps    | 3796000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.33    |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 94899    |\n",
            "|    policy_loss        | 0.0514   |\n",
            "|    value_loss         | 3.91     |\n",
            "------------------------------------\n",
            "Num timesteps: 3800000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -48.14\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 586      |\n",
            "|    ep_rew_mean        | -48.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1046     |\n",
            "|    iterations         | 95000    |\n",
            "|    time_elapsed       | 3631     |\n",
            "|    total_timesteps    | 3800000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.352   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 94999    |\n",
            "|    policy_loss        | -0.232   |\n",
            "|    value_loss         | 3.45     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 595      |\n",
            "|    ep_rew_mean        | -35.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1046     |\n",
            "|    iterations         | 95100    |\n",
            "|    time_elapsed       | 3635     |\n",
            "|    total_timesteps    | 3804000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.235   |\n",
            "|    explained_variance | 0.966    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 95099    |\n",
            "|    policy_loss        | -0.25    |\n",
            "|    value_loss         | 38.1     |\n",
            "------------------------------------\n",
            "Num timesteps: 3808000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -32.54\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 570      |\n",
            "|    ep_rew_mean        | -32.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1046     |\n",
            "|    iterations         | 95200    |\n",
            "|    time_elapsed       | 3639     |\n",
            "|    total_timesteps    | 3808000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.382   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 95199    |\n",
            "|    policy_loss        | 0.0427   |\n",
            "|    value_loss         | 3.11     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 585      |\n",
            "|    ep_rew_mean        | -18.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1046     |\n",
            "|    iterations         | 95300    |\n",
            "|    time_elapsed       | 3642     |\n",
            "|    total_timesteps    | 3812000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.338   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 95299    |\n",
            "|    policy_loss        | 0.282    |\n",
            "|    value_loss         | 1.84     |\n",
            "------------------------------------\n",
            "Num timesteps: 3816000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: -6.10\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 551      |\n",
            "|    ep_rew_mean        | -6.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1046     |\n",
            "|    iterations         | 95400    |\n",
            "|    time_elapsed       | 3645     |\n",
            "|    total_timesteps    | 3816000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.344   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 95399    |\n",
            "|    policy_loss        | -0.0951  |\n",
            "|    value_loss         | 2.5      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 516      |\n",
            "|    ep_rew_mean        | 10.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1047     |\n",
            "|    iterations         | 95500    |\n",
            "|    time_elapsed       | 3648     |\n",
            "|    total_timesteps    | 3820000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.27    |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 95499    |\n",
            "|    policy_loss        | -0.101   |\n",
            "|    value_loss         | 3.55     |\n",
            "------------------------------------\n",
            "Num timesteps: 3824000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: 17.11\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 503      |\n",
            "|    ep_rew_mean        | 17.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1047     |\n",
            "|    iterations         | 95600    |\n",
            "|    time_elapsed       | 3651     |\n",
            "|    total_timesteps    | 3824000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.31    |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 95599    |\n",
            "|    policy_loss        | -0.0697  |\n",
            "|    value_loss         | 3.89     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 503      |\n",
            "|    ep_rew_mean        | 30.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1047     |\n",
            "|    iterations         | 95700    |\n",
            "|    time_elapsed       | 3654     |\n",
            "|    total_timesteps    | 3828000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.362   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 95699    |\n",
            "|    policy_loss        | -0.243   |\n",
            "|    value_loss         | 1.71     |\n",
            "------------------------------------\n",
            "Num timesteps: 3832000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: 50.92\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 489      |\n",
            "|    ep_rew_mean        | 50.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1047     |\n",
            "|    iterations         | 95800    |\n",
            "|    time_elapsed       | 3657     |\n",
            "|    total_timesteps    | 3832000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.228   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 95799    |\n",
            "|    policy_loss        | -0.00495 |\n",
            "|    value_loss         | 1.81     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 473      |\n",
            "|    ep_rew_mean        | 59.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1047     |\n",
            "|    iterations         | 95900    |\n",
            "|    time_elapsed       | 3660     |\n",
            "|    total_timesteps    | 3836000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.269   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 95899    |\n",
            "|    policy_loss        | 0.0131   |\n",
            "|    value_loss         | 2.48     |\n",
            "------------------------------------\n",
            "Num timesteps: 3840000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: 72.58\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 429      |\n",
            "|    ep_rew_mean        | 72.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1048     |\n",
            "|    iterations         | 96000    |\n",
            "|    time_elapsed       | 3663     |\n",
            "|    total_timesteps    | 3840000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.313   |\n",
            "|    explained_variance | 0.767    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 95999    |\n",
            "|    policy_loss        | 0.103    |\n",
            "|    value_loss         | 294      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 424      |\n",
            "|    ep_rew_mean        | 73.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1048     |\n",
            "|    iterations         | 96100    |\n",
            "|    time_elapsed       | 3666     |\n",
            "|    total_timesteps    | 3844000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.289   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 96099    |\n",
            "|    policy_loss        | 0.0241   |\n",
            "|    value_loss         | 2.49     |\n",
            "------------------------------------\n",
            "Num timesteps: 3848000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: 85.69\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 422      |\n",
            "|    ep_rew_mean        | 85.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1048     |\n",
            "|    iterations         | 96200    |\n",
            "|    time_elapsed       | 3669     |\n",
            "|    total_timesteps    | 3848000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.21    |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 96199    |\n",
            "|    policy_loss        | 0.145    |\n",
            "|    value_loss         | 1.38     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 406      |\n",
            "|    ep_rew_mean        | 79.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1048     |\n",
            "|    iterations         | 96300    |\n",
            "|    time_elapsed       | 3672     |\n",
            "|    total_timesteps    | 3852000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.32    |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 96299    |\n",
            "|    policy_loss        | 0.189    |\n",
            "|    value_loss         | 0.837    |\n",
            "------------------------------------\n",
            "Num timesteps: 3856000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: 83.33\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 414      |\n",
            "|    ep_rew_mean        | 83.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1049     |\n",
            "|    iterations         | 96400    |\n",
            "|    time_elapsed       | 3675     |\n",
            "|    total_timesteps    | 3856000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.367   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 96399    |\n",
            "|    policy_loss        | 0.892    |\n",
            "|    value_loss         | 7.29     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 409      |\n",
            "|    ep_rew_mean        | 80.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1049     |\n",
            "|    iterations         | 96500    |\n",
            "|    time_elapsed       | 3679     |\n",
            "|    total_timesteps    | 3860000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.37    |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 96499    |\n",
            "|    policy_loss        | -0.226   |\n",
            "|    value_loss         | 0.714    |\n",
            "------------------------------------\n",
            "Num timesteps: 3864000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: 76.05\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 427      |\n",
            "|    ep_rew_mean        | 76.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1049     |\n",
            "|    iterations         | 96600    |\n",
            "|    time_elapsed       | 3682     |\n",
            "|    total_timesteps    | 3864000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.266   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 96599    |\n",
            "|    policy_loss        | 0.637    |\n",
            "|    value_loss         | 4.23     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 420      |\n",
            "|    ep_rew_mean        | 61.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1049     |\n",
            "|    iterations         | 96700    |\n",
            "|    time_elapsed       | 3684     |\n",
            "|    total_timesteps    | 3868000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.396   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 96699    |\n",
            "|    policy_loss        | -0.198   |\n",
            "|    value_loss         | 3.05     |\n",
            "------------------------------------\n",
            "Num timesteps: 3872000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: 58.83\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 423      |\n",
            "|    ep_rew_mean        | 58.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1049     |\n",
            "|    iterations         | 96800    |\n",
            "|    time_elapsed       | 3688     |\n",
            "|    total_timesteps    | 3872000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.313   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 96799    |\n",
            "|    policy_loss        | 0.014    |\n",
            "|    value_loss         | 0.906    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 432      |\n",
            "|    ep_rew_mean        | 54.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1049     |\n",
            "|    iterations         | 96900    |\n",
            "|    time_elapsed       | 3693     |\n",
            "|    total_timesteps    | 3876000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.387   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 96899    |\n",
            "|    policy_loss        | -0.15    |\n",
            "|    value_loss         | 2.65     |\n",
            "------------------------------------\n",
            "Num timesteps: 3880000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: 50.47\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 441      |\n",
            "|    ep_rew_mean        | 50.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1049     |\n",
            "|    iterations         | 97000    |\n",
            "|    time_elapsed       | 3696     |\n",
            "|    total_timesteps    | 3880000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.307   |\n",
            "|    explained_variance | 0.64     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 96999    |\n",
            "|    policy_loss        | 0.0762   |\n",
            "|    value_loss         | 258      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 446      |\n",
            "|    ep_rew_mean        | 48.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1049     |\n",
            "|    iterations         | 97100    |\n",
            "|    time_elapsed       | 3699     |\n",
            "|    total_timesteps    | 3884000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.338   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 97099    |\n",
            "|    policy_loss        | 0.0243   |\n",
            "|    value_loss         | 0.979    |\n",
            "------------------------------------\n",
            "Num timesteps: 3888000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: 44.42\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 467      |\n",
            "|    ep_rew_mean        | 44.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1049     |\n",
            "|    iterations         | 97200    |\n",
            "|    time_elapsed       | 3703     |\n",
            "|    total_timesteps    | 3888000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.297   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 97199    |\n",
            "|    policy_loss        | -0.194   |\n",
            "|    value_loss         | 4.29     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 461      |\n",
            "|    ep_rew_mean        | 35.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1049     |\n",
            "|    iterations         | 97300    |\n",
            "|    time_elapsed       | 3706     |\n",
            "|    total_timesteps    | 3892000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.357   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 97299    |\n",
            "|    policy_loss        | 0.116    |\n",
            "|    value_loss         | 5.99     |\n",
            "------------------------------------\n",
            "Num timesteps: 3896000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: 33.88\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 466      |\n",
            "|    ep_rew_mean        | 33.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1050     |\n",
            "|    iterations         | 97400    |\n",
            "|    time_elapsed       | 3709     |\n",
            "|    total_timesteps    | 3896000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.402   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 97399    |\n",
            "|    policy_loss        | 0.113    |\n",
            "|    value_loss         | 1.74     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 472      |\n",
            "|    ep_rew_mean        | 33.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1050     |\n",
            "|    iterations         | 97500    |\n",
            "|    time_elapsed       | 3713     |\n",
            "|    total_timesteps    | 3900000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.388   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 97499    |\n",
            "|    policy_loss        | -0.114   |\n",
            "|    value_loss         | 2.79     |\n",
            "------------------------------------\n",
            "Num timesteps: 3904000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: 19.92\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 479      |\n",
            "|    ep_rew_mean        | 19.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1050     |\n",
            "|    iterations         | 97600    |\n",
            "|    time_elapsed       | 3717     |\n",
            "|    total_timesteps    | 3904000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.386   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 97599    |\n",
            "|    policy_loss        | -0.187   |\n",
            "|    value_loss         | 4.01     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 495      |\n",
            "|    ep_rew_mean        | 20.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1050     |\n",
            "|    iterations         | 97700    |\n",
            "|    time_elapsed       | 3721     |\n",
            "|    total_timesteps    | 3908000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.346   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 97699    |\n",
            "|    policy_loss        | 0.102    |\n",
            "|    value_loss         | 1.15     |\n",
            "------------------------------------\n",
            "Num timesteps: 3912000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: 12.66\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 482      |\n",
            "|    ep_rew_mean        | 12.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1050     |\n",
            "|    iterations         | 97800    |\n",
            "|    time_elapsed       | 3725     |\n",
            "|    total_timesteps    | 3912000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.286   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 97799    |\n",
            "|    policy_loss        | -0.255   |\n",
            "|    value_loss         | 1.15     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 508      |\n",
            "|    ep_rew_mean        | 21       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1050     |\n",
            "|    iterations         | 97900    |\n",
            "|    time_elapsed       | 3729     |\n",
            "|    total_timesteps    | 3916000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.285   |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 97899    |\n",
            "|    policy_loss        | -0.125   |\n",
            "|    value_loss         | 2.38     |\n",
            "------------------------------------\n",
            "Num timesteps: 3920000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: 26.45\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 520      |\n",
            "|    ep_rew_mean        | 26.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1049     |\n",
            "|    iterations         | 98000    |\n",
            "|    time_elapsed       | 3733     |\n",
            "|    total_timesteps    | 3920000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.284   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 97999    |\n",
            "|    policy_loss        | -0.181   |\n",
            "|    value_loss         | 2.31     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 541      |\n",
            "|    ep_rew_mean        | 26       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1050     |\n",
            "|    iterations         | 98100    |\n",
            "|    time_elapsed       | 3737     |\n",
            "|    total_timesteps    | 3924000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.307   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 98099    |\n",
            "|    policy_loss        | 0.264    |\n",
            "|    value_loss         | 1.96     |\n",
            "------------------------------------\n",
            "Num timesteps: 3928000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: 29.37\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 542      |\n",
            "|    ep_rew_mean        | 29.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1050     |\n",
            "|    iterations         | 98200    |\n",
            "|    time_elapsed       | 3740     |\n",
            "|    total_timesteps    | 3928000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.181   |\n",
            "|    explained_variance | 0.767    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 98199    |\n",
            "|    policy_loss        | 0.0106   |\n",
            "|    value_loss         | 50.6     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 540      |\n",
            "|    ep_rew_mean        | 21.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1050     |\n",
            "|    iterations         | 98300    |\n",
            "|    time_elapsed       | 3744     |\n",
            "|    total_timesteps    | 3932000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.293   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 98299    |\n",
            "|    policy_loss        | -0.247   |\n",
            "|    value_loss         | 3.56     |\n",
            "------------------------------------\n",
            "Num timesteps: 3936000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: 11.43\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 553      |\n",
            "|    ep_rew_mean        | 11.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1050     |\n",
            "|    iterations         | 98400    |\n",
            "|    time_elapsed       | 3748     |\n",
            "|    total_timesteps    | 3936000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.332   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 98399    |\n",
            "|    policy_loss        | -0.106   |\n",
            "|    value_loss         | 1.83     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 549      |\n",
            "|    ep_rew_mean        | 6.74     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1050     |\n",
            "|    iterations         | 98500    |\n",
            "|    time_elapsed       | 3751     |\n",
            "|    total_timesteps    | 3940000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.221   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 98499    |\n",
            "|    policy_loss        | 1.05     |\n",
            "|    value_loss         | 3.7      |\n",
            "------------------------------------\n",
            "Num timesteps: 3944000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: 8.06\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 543      |\n",
            "|    ep_rew_mean        | 8.06     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1050     |\n",
            "|    iterations         | 98600    |\n",
            "|    time_elapsed       | 3754     |\n",
            "|    total_timesteps    | 3944000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.271   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 98599    |\n",
            "|    policy_loss        | -0.113   |\n",
            "|    value_loss         | 4.83     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 541      |\n",
            "|    ep_rew_mean        | 10.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1050     |\n",
            "|    iterations         | 98700    |\n",
            "|    time_elapsed       | 3758     |\n",
            "|    total_timesteps    | 3948000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.305   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 98699    |\n",
            "|    policy_loss        | 0.0315   |\n",
            "|    value_loss         | 4.86     |\n",
            "------------------------------------\n",
            "Num timesteps: 3952000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: 11.62\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 530      |\n",
            "|    ep_rew_mean        | 11.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1050     |\n",
            "|    iterations         | 98800    |\n",
            "|    time_elapsed       | 3760     |\n",
            "|    total_timesteps    | 3952000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.244   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 98799    |\n",
            "|    policy_loss        | -0.0612  |\n",
            "|    value_loss         | 1.66     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 527      |\n",
            "|    ep_rew_mean        | 15.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1051     |\n",
            "|    iterations         | 98900    |\n",
            "|    time_elapsed       | 3763     |\n",
            "|    total_timesteps    | 3956000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.342   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 98899    |\n",
            "|    policy_loss        | 0.0114   |\n",
            "|    value_loss         | 5.48     |\n",
            "------------------------------------\n",
            "Num timesteps: 3960000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: 15.81\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 516      |\n",
            "|    ep_rew_mean        | 15.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1051     |\n",
            "|    iterations         | 99000    |\n",
            "|    time_elapsed       | 3766     |\n",
            "|    total_timesteps    | 3960000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.336   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 98999    |\n",
            "|    policy_loss        | 0.234    |\n",
            "|    value_loss         | 2.49     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 508      |\n",
            "|    ep_rew_mean        | 19.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1051     |\n",
            "|    iterations         | 99100    |\n",
            "|    time_elapsed       | 3769     |\n",
            "|    total_timesteps    | 3964000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.214   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 99099    |\n",
            "|    policy_loss        | -0.108   |\n",
            "|    value_loss         | 0.992    |\n",
            "------------------------------------\n",
            "Num timesteps: 3968000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: 22.61\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 497      |\n",
            "|    ep_rew_mean        | 22.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1051     |\n",
            "|    iterations         | 99200    |\n",
            "|    time_elapsed       | 3773     |\n",
            "|    total_timesteps    | 3968000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.31    |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 99199    |\n",
            "|    policy_loss        | -0.13    |\n",
            "|    value_loss         | 2.34     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 476      |\n",
            "|    ep_rew_mean        | 23.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1051     |\n",
            "|    iterations         | 99300    |\n",
            "|    time_elapsed       | 3777     |\n",
            "|    total_timesteps    | 3972000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.282   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 99299    |\n",
            "|    policy_loss        | -0.664   |\n",
            "|    value_loss         | 7.47     |\n",
            "------------------------------------\n",
            "Num timesteps: 3976000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: 31.20\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 460      |\n",
            "|    ep_rew_mean        | 31.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1051     |\n",
            "|    iterations         | 99400    |\n",
            "|    time_elapsed       | 3780     |\n",
            "|    total_timesteps    | 3976000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.347   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 99399    |\n",
            "|    policy_loss        | -0.112   |\n",
            "|    value_loss         | 1.58     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 427      |\n",
            "|    ep_rew_mean        | 37.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1052     |\n",
            "|    iterations         | 99500    |\n",
            "|    time_elapsed       | 3783     |\n",
            "|    total_timesteps    | 3980000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.183   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 99499    |\n",
            "|    policy_loss        | 0.0341   |\n",
            "|    value_loss         | 1.58     |\n",
            "------------------------------------\n",
            "Num timesteps: 3984000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: 48.66\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 427      |\n",
            "|    ep_rew_mean        | 48.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1052     |\n",
            "|    iterations         | 99600    |\n",
            "|    time_elapsed       | 3786     |\n",
            "|    total_timesteps    | 3984000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.244   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 99599    |\n",
            "|    policy_loss        | 0.0388   |\n",
            "|    value_loss         | 1.13     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 423      |\n",
            "|    ep_rew_mean        | 59.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1052     |\n",
            "|    iterations         | 99700    |\n",
            "|    time_elapsed       | 3789     |\n",
            "|    total_timesteps    | 3988000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.29    |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 99699    |\n",
            "|    policy_loss        | -0.0971  |\n",
            "|    value_loss         | 0.798    |\n",
            "------------------------------------\n",
            "Num timesteps: 3992000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: 71.66\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 420      |\n",
            "|    ep_rew_mean        | 71.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1052     |\n",
            "|    iterations         | 99800    |\n",
            "|    time_elapsed       | 3792     |\n",
            "|    total_timesteps    | 3992000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.287   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 99799    |\n",
            "|    policy_loss        | -0.459   |\n",
            "|    value_loss         | 2.67     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 406      |\n",
            "|    ep_rew_mean        | 70.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1052     |\n",
            "|    iterations         | 99900    |\n",
            "|    time_elapsed       | 3795     |\n",
            "|    total_timesteps    | 3996000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.272   |\n",
            "|    explained_variance | 0.949    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 99899    |\n",
            "|    policy_loss        | 0.384    |\n",
            "|    value_loss         | 34       |\n",
            "------------------------------------\n",
            "Num timesteps: 4000000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: 76.05\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 403      |\n",
            "|    ep_rew_mean        | 76.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1053     |\n",
            "|    iterations         | 100000   |\n",
            "|    time_elapsed       | 3798     |\n",
            "|    total_timesteps    | 4000000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.208   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 99999    |\n",
            "|    policy_loss        | 0.0904   |\n",
            "|    value_loss         | 3.29     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 409      |\n",
            "|    ep_rew_mean        | 78.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1053     |\n",
            "|    iterations         | 100100   |\n",
            "|    time_elapsed       | 3801     |\n",
            "|    total_timesteps    | 4004000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.408   |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 100099   |\n",
            "|    policy_loss        | -0.128   |\n",
            "|    value_loss         | 11.5     |\n",
            "------------------------------------\n",
            "Num timesteps: 4008000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: 85.54\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 405      |\n",
            "|    ep_rew_mean        | 85.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1053     |\n",
            "|    iterations         | 100200   |\n",
            "|    time_elapsed       | 3805     |\n",
            "|    total_timesteps    | 4008000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.321   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 100199   |\n",
            "|    policy_loss        | 0.0834   |\n",
            "|    value_loss         | 0.666    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 401      |\n",
            "|    ep_rew_mean        | 79.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1053     |\n",
            "|    iterations         | 100300   |\n",
            "|    time_elapsed       | 3808     |\n",
            "|    total_timesteps    | 4012000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.411   |\n",
            "|    explained_variance | 0.642    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 100299   |\n",
            "|    policy_loss        | -1.59    |\n",
            "|    value_loss         | 185      |\n",
            "------------------------------------\n",
            "Num timesteps: 4016000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: 82.77\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 414      |\n",
            "|    ep_rew_mean        | 82.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1053     |\n",
            "|    iterations         | 100400   |\n",
            "|    time_elapsed       | 3811     |\n",
            "|    total_timesteps    | 4016000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.283   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 100399   |\n",
            "|    policy_loss        | -0.267   |\n",
            "|    value_loss         | 4.89     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 416      |\n",
            "|    ep_rew_mean        | 76.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1053     |\n",
            "|    iterations         | 100500   |\n",
            "|    time_elapsed       | 3814     |\n",
            "|    total_timesteps    | 4020000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.304   |\n",
            "|    explained_variance | 0.957    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 100499   |\n",
            "|    policy_loss        | 0.189    |\n",
            "|    value_loss         | 4.1      |\n",
            "------------------------------------\n",
            "Num timesteps: 4024000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: 75.94\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 421      |\n",
            "|    ep_rew_mean        | 75.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1053     |\n",
            "|    iterations         | 100600   |\n",
            "|    time_elapsed       | 3818     |\n",
            "|    total_timesteps    | 4024000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.231   |\n",
            "|    explained_variance | 0.884    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 100599   |\n",
            "|    policy_loss        | 0.114    |\n",
            "|    value_loss         | 43.6     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 434      |\n",
            "|    ep_rew_mean        | 61.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1053     |\n",
            "|    iterations         | 100700   |\n",
            "|    time_elapsed       | 3822     |\n",
            "|    total_timesteps    | 4028000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.172   |\n",
            "|    explained_variance | 0.858    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 100699   |\n",
            "|    policy_loss        | 0.3      |\n",
            "|    value_loss         | 294      |\n",
            "------------------------------------\n",
            "Num timesteps: 4032000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: 42.88\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 412      |\n",
            "|    ep_rew_mean        | 42.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1054     |\n",
            "|    iterations         | 100800   |\n",
            "|    time_elapsed       | 3824     |\n",
            "|    total_timesteps    | 4032000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.436   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 100799   |\n",
            "|    policy_loss        | 0.273    |\n",
            "|    value_loss         | 5.64     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 419      |\n",
            "|    ep_rew_mean        | 46.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1054     |\n",
            "|    iterations         | 100900   |\n",
            "|    time_elapsed       | 3827     |\n",
            "|    total_timesteps    | 4036000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.401   |\n",
            "|    explained_variance | 0.473    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 100899   |\n",
            "|    policy_loss        | 0.184    |\n",
            "|    value_loss         | 641      |\n",
            "------------------------------------\n",
            "Num timesteps: 4040000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: 46.98\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 407      |\n",
            "|    ep_rew_mean        | 47       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1054     |\n",
            "|    iterations         | 101000   |\n",
            "|    time_elapsed       | 3830     |\n",
            "|    total_timesteps    | 4040000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.378   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 100999   |\n",
            "|    policy_loss        | 0.0326   |\n",
            "|    value_loss         | 3.32     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 390      |\n",
            "|    ep_rew_mean        | 52.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1055     |\n",
            "|    iterations         | 101100   |\n",
            "|    time_elapsed       | 3832     |\n",
            "|    total_timesteps    | 4044000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.271   |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 101099   |\n",
            "|    policy_loss        | -0.334   |\n",
            "|    value_loss         | 12.1     |\n",
            "------------------------------------\n",
            "Num timesteps: 4048000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: 51.99\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 366      |\n",
            "|    ep_rew_mean        | 52       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1055     |\n",
            "|    iterations         | 101200   |\n",
            "|    time_elapsed       | 3835     |\n",
            "|    total_timesteps    | 4048000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.283   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 101199   |\n",
            "|    policy_loss        | -0.161   |\n",
            "|    value_loss         | 1.14     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 353      |\n",
            "|    ep_rew_mean        | 37.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1055     |\n",
            "|    iterations         | 101300   |\n",
            "|    time_elapsed       | 3838     |\n",
            "|    total_timesteps    | 4052000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.328   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 101299   |\n",
            "|    policy_loss        | -0.317   |\n",
            "|    value_loss         | 1.63     |\n",
            "------------------------------------\n",
            "Num timesteps: 4056000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: 40.59\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 355      |\n",
            "|    ep_rew_mean        | 40.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1055     |\n",
            "|    iterations         | 101400   |\n",
            "|    time_elapsed       | 3841     |\n",
            "|    total_timesteps    | 4056000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.336   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 101399   |\n",
            "|    policy_loss        | -0.0948  |\n",
            "|    value_loss         | 3.81     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 354      |\n",
            "|    ep_rew_mean        | 43.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1056     |\n",
            "|    iterations         | 101500   |\n",
            "|    time_elapsed       | 3844     |\n",
            "|    total_timesteps    | 4060000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.274   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 101499   |\n",
            "|    policy_loss        | -0.0411  |\n",
            "|    value_loss         | 3.63     |\n",
            "------------------------------------\n",
            "Num timesteps: 4064000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: 47.91\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 358      |\n",
            "|    ep_rew_mean        | 47.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1055     |\n",
            "|    iterations         | 101600   |\n",
            "|    time_elapsed       | 3849     |\n",
            "|    total_timesteps    | 4064000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.306   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 101599   |\n",
            "|    policy_loss        | 0.171    |\n",
            "|    value_loss         | 1.46     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 385      |\n",
            "|    ep_rew_mean        | 56       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1055     |\n",
            "|    iterations         | 101700   |\n",
            "|    time_elapsed       | 3853     |\n",
            "|    total_timesteps    | 4068000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.402   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 101699   |\n",
            "|    policy_loss        | 0.21     |\n",
            "|    value_loss         | 2.35     |\n",
            "------------------------------------\n",
            "Num timesteps: 4072000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: 61.79\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 399      |\n",
            "|    ep_rew_mean        | 61.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1055     |\n",
            "|    iterations         | 101800   |\n",
            "|    time_elapsed       | 3856     |\n",
            "|    total_timesteps    | 4072000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.304   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 101799   |\n",
            "|    policy_loss        | -0.0229  |\n",
            "|    value_loss         | 0.502    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 394      |\n",
            "|    ep_rew_mean        | 45.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1055     |\n",
            "|    iterations         | 101900   |\n",
            "|    time_elapsed       | 3860     |\n",
            "|    total_timesteps    | 4076000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.259   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 101899   |\n",
            "|    policy_loss        | -0.162   |\n",
            "|    value_loss         | 2.78     |\n",
            "------------------------------------\n",
            "Num timesteps: 4080000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: 46.73\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 406      |\n",
            "|    ep_rew_mean        | 46.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1056     |\n",
            "|    iterations         | 102000   |\n",
            "|    time_elapsed       | 3863     |\n",
            "|    total_timesteps    | 4080000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.242   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 101999   |\n",
            "|    policy_loss        | -0.134   |\n",
            "|    value_loss         | 6.63     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 418      |\n",
            "|    ep_rew_mean        | 41.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1056     |\n",
            "|    iterations         | 102100   |\n",
            "|    time_elapsed       | 3867     |\n",
            "|    total_timesteps    | 4084000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.336   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 102099   |\n",
            "|    policy_loss        | -0.166   |\n",
            "|    value_loss         | 2.22     |\n",
            "------------------------------------\n",
            "Num timesteps: 4088000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: 45.41\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 433      |\n",
            "|    ep_rew_mean        | 45.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1056     |\n",
            "|    iterations         | 102200   |\n",
            "|    time_elapsed       | 3870     |\n",
            "|    total_timesteps    | 4088000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.242   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 102199   |\n",
            "|    policy_loss        | 0.14     |\n",
            "|    value_loss         | 4.39     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 447      |\n",
            "|    ep_rew_mean        | 62.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1056     |\n",
            "|    iterations         | 102300   |\n",
            "|    time_elapsed       | 3872     |\n",
            "|    total_timesteps    | 4092000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.253   |\n",
            "|    explained_variance | 0.899    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 102299   |\n",
            "|    policy_loss        | -0.498   |\n",
            "|    value_loss         | 228      |\n",
            "------------------------------------\n",
            "Num timesteps: 4096000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: 65.83\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 436      |\n",
            "|    ep_rew_mean        | 65.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1056     |\n",
            "|    iterations         | 102400   |\n",
            "|    time_elapsed       | 3875     |\n",
            "|    total_timesteps    | 4096000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.321   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 102399   |\n",
            "|    policy_loss        | 0.0869   |\n",
            "|    value_loss         | 1.87     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 420      |\n",
            "|    ep_rew_mean        | 72.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1057     |\n",
            "|    iterations         | 102500   |\n",
            "|    time_elapsed       | 3877     |\n",
            "|    total_timesteps    | 4100000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.187   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 102499   |\n",
            "|    policy_loss        | 0.153    |\n",
            "|    value_loss         | 6.77     |\n",
            "------------------------------------\n",
            "Num timesteps: 4104000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: 66.31\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 384      |\n",
            "|    ep_rew_mean        | 66.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1057     |\n",
            "|    iterations         | 102600   |\n",
            "|    time_elapsed       | 3879     |\n",
            "|    total_timesteps    | 4104000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.388   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 102599   |\n",
            "|    policy_loss        | -0.00286 |\n",
            "|    value_loss         | 0.814    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 370      |\n",
            "|    ep_rew_mean        | 67.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1058     |\n",
            "|    iterations         | 102700   |\n",
            "|    time_elapsed       | 3882     |\n",
            "|    total_timesteps    | 4108000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.191   |\n",
            "|    explained_variance | 0.976    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 102699   |\n",
            "|    policy_loss        | -0.0876  |\n",
            "|    value_loss         | 39       |\n",
            "------------------------------------\n",
            "Num timesteps: 4112000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: 76.48\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 373      |\n",
            "|    ep_rew_mean        | 76.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1058     |\n",
            "|    iterations         | 102800   |\n",
            "|    time_elapsed       | 3885     |\n",
            "|    total_timesteps    | 4112000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.277   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 102799   |\n",
            "|    policy_loss        | 0.0792   |\n",
            "|    value_loss         | 1.25     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 367      |\n",
            "|    ep_rew_mean        | 94.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1058     |\n",
            "|    iterations         | 102900   |\n",
            "|    time_elapsed       | 3887     |\n",
            "|    total_timesteps    | 4116000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.32    |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 102899   |\n",
            "|    policy_loss        | -0.0904  |\n",
            "|    value_loss         | 2.62     |\n",
            "------------------------------------\n",
            "Num timesteps: 4120000\n",
            "Best mean reward: 92.49 - Last mean reward per episode: 97.51\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 364      |\n",
            "|    ep_rew_mean        | 97.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1058     |\n",
            "|    iterations         | 103000   |\n",
            "|    time_elapsed       | 3890     |\n",
            "|    total_timesteps    | 4120000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.345   |\n",
            "|    explained_variance | 0.696    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 102999   |\n",
            "|    policy_loss        | -0.103   |\n",
            "|    value_loss         | 149      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 358      |\n",
            "|    ep_rew_mean        | 81.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1059     |\n",
            "|    iterations         | 103100   |\n",
            "|    time_elapsed       | 3894     |\n",
            "|    total_timesteps    | 4124000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.288   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 103099   |\n",
            "|    policy_loss        | -0.112   |\n",
            "|    value_loss         | 1.46     |\n",
            "------------------------------------\n",
            "Num timesteps: 4128000\n",
            "Best mean reward: 97.51 - Last mean reward per episode: 82.90\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 363      |\n",
            "|    ep_rew_mean        | 82.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1059     |\n",
            "|    iterations         | 103200   |\n",
            "|    time_elapsed       | 3897     |\n",
            "|    total_timesteps    | 4128000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.27    |\n",
            "|    explained_variance | 0.832    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 103199   |\n",
            "|    policy_loss        | 0.458    |\n",
            "|    value_loss         | 211      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 366      |\n",
            "|    ep_rew_mean        | 78       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1059     |\n",
            "|    iterations         | 103300   |\n",
            "|    time_elapsed       | 3900     |\n",
            "|    total_timesteps    | 4132000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.26    |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 103299   |\n",
            "|    policy_loss        | -0.0938  |\n",
            "|    value_loss         | 0.639    |\n",
            "------------------------------------\n",
            "Num timesteps: 4136000\n",
            "Best mean reward: 97.51 - Last mean reward per episode: 78.46\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 379      |\n",
            "|    ep_rew_mean        | 78.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1059     |\n",
            "|    iterations         | 103400   |\n",
            "|    time_elapsed       | 3903     |\n",
            "|    total_timesteps    | 4136000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.368   |\n",
            "|    explained_variance | 0.968    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 103399   |\n",
            "|    policy_loss        | 0.05     |\n",
            "|    value_loss         | 5.45     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 400      |\n",
            "|    ep_rew_mean        | 82.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1059     |\n",
            "|    iterations         | 103500   |\n",
            "|    time_elapsed       | 3906     |\n",
            "|    total_timesteps    | 4140000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.233   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 103499   |\n",
            "|    policy_loss        | -0.742   |\n",
            "|    value_loss         | 13.5     |\n",
            "------------------------------------\n",
            "Num timesteps: 4144000\n",
            "Best mean reward: 97.51 - Last mean reward per episode: 91.03\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 403      |\n",
            "|    ep_rew_mean        | 91       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1060     |\n",
            "|    iterations         | 103600   |\n",
            "|    time_elapsed       | 3909     |\n",
            "|    total_timesteps    | 4144000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.344   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 103599   |\n",
            "|    policy_loss        | -0.203   |\n",
            "|    value_loss         | 1.47     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 395      |\n",
            "|    ep_rew_mean        | 80.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1060     |\n",
            "|    iterations         | 103700   |\n",
            "|    time_elapsed       | 3911     |\n",
            "|    total_timesteps    | 4148000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.384   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 103699   |\n",
            "|    policy_loss        | 0.332    |\n",
            "|    value_loss         | 2.64     |\n",
            "------------------------------------\n",
            "Num timesteps: 4152000\n",
            "Best mean reward: 97.51 - Last mean reward per episode: 73.94\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 380      |\n",
            "|    ep_rew_mean        | 73.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1060     |\n",
            "|    iterations         | 103800   |\n",
            "|    time_elapsed       | 3914     |\n",
            "|    total_timesteps    | 4152000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.319   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 103799   |\n",
            "|    policy_loss        | -0.00632 |\n",
            "|    value_loss         | 2.32     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 388      |\n",
            "|    ep_rew_mean        | 74.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1060     |\n",
            "|    iterations         | 103900   |\n",
            "|    time_elapsed       | 3917     |\n",
            "|    total_timesteps    | 4156000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.328   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 103899   |\n",
            "|    policy_loss        | -0.222   |\n",
            "|    value_loss         | 5.06     |\n",
            "------------------------------------\n",
            "Num timesteps: 4160000\n",
            "Best mean reward: 97.51 - Last mean reward per episode: 73.58\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 392      |\n",
            "|    ep_rew_mean        | 73.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1060     |\n",
            "|    iterations         | 104000   |\n",
            "|    time_elapsed       | 3920     |\n",
            "|    total_timesteps    | 4160000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.336   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 103999   |\n",
            "|    policy_loss        | -0.0431  |\n",
            "|    value_loss         | 0.449    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 392      |\n",
            "|    ep_rew_mean        | 71.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1061     |\n",
            "|    iterations         | 104100   |\n",
            "|    time_elapsed       | 3923     |\n",
            "|    total_timesteps    | 4164000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.316   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 104099   |\n",
            "|    policy_loss        | -0.103   |\n",
            "|    value_loss         | 3.65     |\n",
            "------------------------------------\n",
            "Num timesteps: 4168000\n",
            "Best mean reward: 97.51 - Last mean reward per episode: 77.74\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 391      |\n",
            "|    ep_rew_mean        | 77.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1061     |\n",
            "|    iterations         | 104200   |\n",
            "|    time_elapsed       | 3927     |\n",
            "|    total_timesteps    | 4168000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.314   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 104199   |\n",
            "|    policy_loss        | 0.152    |\n",
            "|    value_loss         | 3.96     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 399      |\n",
            "|    ep_rew_mean        | 78.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1061     |\n",
            "|    iterations         | 104300   |\n",
            "|    time_elapsed       | 3930     |\n",
            "|    total_timesteps    | 4172000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.254   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 104299   |\n",
            "|    policy_loss        | -0.0304  |\n",
            "|    value_loss         | 3.59     |\n",
            "------------------------------------\n",
            "Num timesteps: 4176000\n",
            "Best mean reward: 97.51 - Last mean reward per episode: 87.50\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 390      |\n",
            "|    ep_rew_mean        | 87.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1061     |\n",
            "|    iterations         | 104400   |\n",
            "|    time_elapsed       | 3932     |\n",
            "|    total_timesteps    | 4176000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.273   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 104399   |\n",
            "|    policy_loss        | -0.0475  |\n",
            "|    value_loss         | 1.79     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 382      |\n",
            "|    ep_rew_mean        | 84.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 104500   |\n",
            "|    time_elapsed       | 3935     |\n",
            "|    total_timesteps    | 4180000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.174   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 104499   |\n",
            "|    policy_loss        | -0.195   |\n",
            "|    value_loss         | 1.54     |\n",
            "------------------------------------\n",
            "Num timesteps: 4184000\n",
            "Best mean reward: 97.51 - Last mean reward per episode: 84.59\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 386      |\n",
            "|    ep_rew_mean        | 84.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 104600   |\n",
            "|    time_elapsed       | 3939     |\n",
            "|    total_timesteps    | 4184000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.341   |\n",
            "|    explained_variance | 0.969    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 104599   |\n",
            "|    policy_loss        | 0.0613   |\n",
            "|    value_loss         | 7.67     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 405      |\n",
            "|    ep_rew_mean        | 87.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 104700   |\n",
            "|    time_elapsed       | 3942     |\n",
            "|    total_timesteps    | 4188000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.312   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 104699   |\n",
            "|    policy_loss        | 0.128    |\n",
            "|    value_loss         | 2.4      |\n",
            "------------------------------------\n",
            "Num timesteps: 4192000\n",
            "Best mean reward: 97.51 - Last mean reward per episode: 101.37\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 412      |\n",
            "|    ep_rew_mean        | 101      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 104800   |\n",
            "|    time_elapsed       | 3945     |\n",
            "|    total_timesteps    | 4192000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.259   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 104799   |\n",
            "|    policy_loss        | 0.0424   |\n",
            "|    value_loss         | 4.22     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 407      |\n",
            "|    ep_rew_mean        | 89.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 104900   |\n",
            "|    time_elapsed       | 3948     |\n",
            "|    total_timesteps    | 4196000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.394   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 104899   |\n",
            "|    policy_loss        | -0.0836  |\n",
            "|    value_loss         | 5.13     |\n",
            "------------------------------------\n",
            "Num timesteps: 4200000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 82.55\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 405      |\n",
            "|    ep_rew_mean        | 82.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 105000   |\n",
            "|    time_elapsed       | 3951     |\n",
            "|    total_timesteps    | 4200000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.397   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 104999   |\n",
            "|    policy_loss        | -0.363   |\n",
            "|    value_loss         | 6.36     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 392      |\n",
            "|    ep_rew_mean        | 89.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1063     |\n",
            "|    iterations         | 105100   |\n",
            "|    time_elapsed       | 3953     |\n",
            "|    total_timesteps    | 4204000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.245   |\n",
            "|    explained_variance | 0.111    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 105099   |\n",
            "|    policy_loss        | -2.32    |\n",
            "|    value_loss         | 1.3e+03  |\n",
            "------------------------------------\n",
            "Num timesteps: 4208000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 82.18\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 401      |\n",
            "|    ep_rew_mean        | 82.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1063     |\n",
            "|    iterations         | 105200   |\n",
            "|    time_elapsed       | 3956     |\n",
            "|    total_timesteps    | 4208000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.267   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 105199   |\n",
            "|    policy_loss        | 0.0886   |\n",
            "|    value_loss         | 2.56     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 392      |\n",
            "|    ep_rew_mean        | 77.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1063     |\n",
            "|    iterations         | 105300   |\n",
            "|    time_elapsed       | 3959     |\n",
            "|    total_timesteps    | 4212000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.33    |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 105299   |\n",
            "|    policy_loss        | 0.0771   |\n",
            "|    value_loss         | 2.28     |\n",
            "------------------------------------\n",
            "Num timesteps: 4216000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 57.97\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 394      |\n",
            "|    ep_rew_mean        | 58       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1063     |\n",
            "|    iterations         | 105400   |\n",
            "|    time_elapsed       | 3962     |\n",
            "|    total_timesteps    | 4216000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.188   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 105399   |\n",
            "|    policy_loss        | 0.0272   |\n",
            "|    value_loss         | 1.3      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 392      |\n",
            "|    ep_rew_mean        | 47.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 105500   |\n",
            "|    time_elapsed       | 3965     |\n",
            "|    total_timesteps    | 4220000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.234   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 105499   |\n",
            "|    policy_loss        | 0.0563   |\n",
            "|    value_loss         | 3.18     |\n",
            "------------------------------------\n",
            "Num timesteps: 4224000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 34.02\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 404      |\n",
            "|    ep_rew_mean        | 34       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 105600   |\n",
            "|    time_elapsed       | 3969     |\n",
            "|    total_timesteps    | 4224000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.354   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 105599   |\n",
            "|    policy_loss        | -0.0812  |\n",
            "|    value_loss         | 3.39     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 391      |\n",
            "|    ep_rew_mean        | 29.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 105700   |\n",
            "|    time_elapsed       | 3971     |\n",
            "|    total_timesteps    | 4228000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.3     |\n",
            "|    explained_variance | 0.959    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 105699   |\n",
            "|    policy_loss        | -0.0638  |\n",
            "|    value_loss         | 26.5     |\n",
            "------------------------------------\n",
            "Num timesteps: 4232000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 26.83\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 395      |\n",
            "|    ep_rew_mean        | 26.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 105800   |\n",
            "|    time_elapsed       | 3974     |\n",
            "|    total_timesteps    | 4232000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.318   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 105799   |\n",
            "|    policy_loss        | 0.0808   |\n",
            "|    value_loss         | 5.45     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 401      |\n",
            "|    ep_rew_mean        | 16       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 105900   |\n",
            "|    time_elapsed       | 3978     |\n",
            "|    total_timesteps    | 4236000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.299   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 105899   |\n",
            "|    policy_loss        | 0.0928   |\n",
            "|    value_loss         | 5.52     |\n",
            "------------------------------------\n",
            "Num timesteps: 4240000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 23.48\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 406      |\n",
            "|    ep_rew_mean        | 23.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1064     |\n",
            "|    iterations         | 106000   |\n",
            "|    time_elapsed       | 3981     |\n",
            "|    total_timesteps    | 4240000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.388   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 105999   |\n",
            "|    policy_loss        | 0.456    |\n",
            "|    value_loss         | 5.45     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 409      |\n",
            "|    ep_rew_mean        | 39.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 106100   |\n",
            "|    time_elapsed       | 3983     |\n",
            "|    total_timesteps    | 4244000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.271   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 106099   |\n",
            "|    policy_loss        | 0.00388  |\n",
            "|    value_loss         | 2.87     |\n",
            "------------------------------------\n",
            "Num timesteps: 4248000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 41.62\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 406      |\n",
            "|    ep_rew_mean        | 41.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1065     |\n",
            "|    iterations         | 106200   |\n",
            "|    time_elapsed       | 3986     |\n",
            "|    total_timesteps    | 4248000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.36    |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 106199   |\n",
            "|    policy_loss        | 0.611    |\n",
            "|    value_loss         | 4.82     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 393      |\n",
            "|    ep_rew_mean        | 49.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 106300   |\n",
            "|    time_elapsed       | 3988     |\n",
            "|    total_timesteps    | 4252000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.25    |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 106299   |\n",
            "|    policy_loss        | -0.0962  |\n",
            "|    value_loss         | 1.39     |\n",
            "------------------------------------\n",
            "Num timesteps: 4256000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 60.42\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 390      |\n",
            "|    ep_rew_mean        | 60.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 106400   |\n",
            "|    time_elapsed       | 3991     |\n",
            "|    total_timesteps    | 4256000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.396   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 106399   |\n",
            "|    policy_loss        | 0.364    |\n",
            "|    value_loss         | 4.92     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 401      |\n",
            "|    ep_rew_mean        | 67.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 106500   |\n",
            "|    time_elapsed       | 3995     |\n",
            "|    total_timesteps    | 4260000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.22    |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 106499   |\n",
            "|    policy_loss        | 0.12     |\n",
            "|    value_loss         | 2.51     |\n",
            "------------------------------------\n",
            "Num timesteps: 4264000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 60.78\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 395      |\n",
            "|    ep_rew_mean        | 60.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 106600   |\n",
            "|    time_elapsed       | 3998     |\n",
            "|    total_timesteps    | 4264000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.209   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 106599   |\n",
            "|    policy_loss        | -0.269   |\n",
            "|    value_loss         | 3.93     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 396      |\n",
            "|    ep_rew_mean        | 66.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 106700   |\n",
            "|    time_elapsed       | 4001     |\n",
            "|    total_timesteps    | 4268000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.335   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 106699   |\n",
            "|    policy_loss        | -0.191   |\n",
            "|    value_loss         | 5.77     |\n",
            "------------------------------------\n",
            "Num timesteps: 4272000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 64.07\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 395      |\n",
            "|    ep_rew_mean        | 64.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 106800   |\n",
            "|    time_elapsed       | 4003     |\n",
            "|    total_timesteps    | 4272000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.331   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 106799   |\n",
            "|    policy_loss        | -0.0447  |\n",
            "|    value_loss         | 2.78     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 398      |\n",
            "|    ep_rew_mean        | 67.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1067     |\n",
            "|    iterations         | 106900   |\n",
            "|    time_elapsed       | 4007     |\n",
            "|    total_timesteps    | 4276000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.324   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 106899   |\n",
            "|    policy_loss        | 0.215    |\n",
            "|    value_loss         | 6.45     |\n",
            "------------------------------------\n",
            "Num timesteps: 4280000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 60.22\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 391      |\n",
            "|    ep_rew_mean        | 60.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1067     |\n",
            "|    iterations         | 107000   |\n",
            "|    time_elapsed       | 4009     |\n",
            "|    total_timesteps    | 4280000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.276   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 106999   |\n",
            "|    policy_loss        | -0.0965  |\n",
            "|    value_loss         | 8.32     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 396      |\n",
            "|    ep_rew_mean        | 51.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1067     |\n",
            "|    iterations         | 107100   |\n",
            "|    time_elapsed       | 4013     |\n",
            "|    total_timesteps    | 4284000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.231   |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 107099   |\n",
            "|    policy_loss        | -0.0193  |\n",
            "|    value_loss         | 4.28     |\n",
            "------------------------------------\n",
            "Num timesteps: 4288000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 47.59\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 406      |\n",
            "|    ep_rew_mean        | 47.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1067     |\n",
            "|    iterations         | 107200   |\n",
            "|    time_elapsed       | 4016     |\n",
            "|    total_timesteps    | 4288000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.355   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 107199   |\n",
            "|    policy_loss        | 0.0567   |\n",
            "|    value_loss         | 1.46     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 418      |\n",
            "|    ep_rew_mean        | 46.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1067     |\n",
            "|    iterations         | 107300   |\n",
            "|    time_elapsed       | 4020     |\n",
            "|    total_timesteps    | 4292000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.384   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 107299   |\n",
            "|    policy_loss        | 0.0993   |\n",
            "|    value_loss         | 1.25     |\n",
            "------------------------------------\n",
            "Num timesteps: 4296000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 38.38\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 427      |\n",
            "|    ep_rew_mean        | 38.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1067     |\n",
            "|    iterations         | 107400   |\n",
            "|    time_elapsed       | 4024     |\n",
            "|    total_timesteps    | 4296000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.258   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 107399   |\n",
            "|    policy_loss        | -0.187   |\n",
            "|    value_loss         | 3.07     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 442      |\n",
            "|    ep_rew_mean        | 30.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1067     |\n",
            "|    iterations         | 107500   |\n",
            "|    time_elapsed       | 4028     |\n",
            "|    total_timesteps    | 4300000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.328   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 107499   |\n",
            "|    policy_loss        | 0.164    |\n",
            "|    value_loss         | 4.59     |\n",
            "------------------------------------\n",
            "Num timesteps: 4304000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 33.99\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 421      |\n",
            "|    ep_rew_mean        | 34       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1067     |\n",
            "|    iterations         | 107600   |\n",
            "|    time_elapsed       | 4031     |\n",
            "|    total_timesteps    | 4304000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.458   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 107599   |\n",
            "|    policy_loss        | 0.33     |\n",
            "|    value_loss         | 11.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 422      |\n",
            "|    ep_rew_mean        | 38       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1067     |\n",
            "|    iterations         | 107700   |\n",
            "|    time_elapsed       | 4033     |\n",
            "|    total_timesteps    | 4308000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.405   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 107699   |\n",
            "|    policy_loss        | -0.207   |\n",
            "|    value_loss         | 5.54     |\n",
            "------------------------------------\n",
            "Num timesteps: 4312000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 32.32\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 408      |\n",
            "|    ep_rew_mean        | 32.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1067     |\n",
            "|    iterations         | 107800   |\n",
            "|    time_elapsed       | 4038     |\n",
            "|    total_timesteps    | 4312000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.344   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 107799   |\n",
            "|    policy_loss        | -0.251   |\n",
            "|    value_loss         | 1.6      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 425      |\n",
            "|    ep_rew_mean        | 29.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1067     |\n",
            "|    iterations         | 107900   |\n",
            "|    time_elapsed       | 4041     |\n",
            "|    total_timesteps    | 4316000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.252   |\n",
            "|    explained_variance | 0.194    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 107899   |\n",
            "|    policy_loss        | -0.202   |\n",
            "|    value_loss         | 1.45e+03 |\n",
            "------------------------------------\n",
            "Num timesteps: 4320000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 37.19\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 409      |\n",
            "|    ep_rew_mean        | 37.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1068     |\n",
            "|    iterations         | 108000   |\n",
            "|    time_elapsed       | 4044     |\n",
            "|    total_timesteps    | 4320000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.17    |\n",
            "|    explained_variance | 0.957    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 107999   |\n",
            "|    policy_loss        | -0.0302  |\n",
            "|    value_loss         | 10.9     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 409      |\n",
            "|    ep_rew_mean        | 32.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1068     |\n",
            "|    iterations         | 108100   |\n",
            "|    time_elapsed       | 4048     |\n",
            "|    total_timesteps    | 4324000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.318   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 108099   |\n",
            "|    policy_loss        | -0.0123  |\n",
            "|    value_loss         | 7.03     |\n",
            "------------------------------------\n",
            "Num timesteps: 4328000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 42.97\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 395      |\n",
            "|    ep_rew_mean        | 43       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1068     |\n",
            "|    iterations         | 108200   |\n",
            "|    time_elapsed       | 4050     |\n",
            "|    total_timesteps    | 4328000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.437   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 108199   |\n",
            "|    policy_loss        | -0.136   |\n",
            "|    value_loss         | 1.43     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 388      |\n",
            "|    ep_rew_mean        | 49.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1068     |\n",
            "|    iterations         | 108300   |\n",
            "|    time_elapsed       | 4053     |\n",
            "|    total_timesteps    | 4332000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.335   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 108299   |\n",
            "|    policy_loss        | 0.00383  |\n",
            "|    value_loss         | 1.44     |\n",
            "------------------------------------\n",
            "Num timesteps: 4336000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 57.46\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 375      |\n",
            "|    ep_rew_mean        | 57.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1068     |\n",
            "|    iterations         | 108400   |\n",
            "|    time_elapsed       | 4056     |\n",
            "|    total_timesteps    | 4336000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.192   |\n",
            "|    explained_variance | 0.941    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 108399   |\n",
            "|    policy_loss        | 0.191    |\n",
            "|    value_loss         | 67       |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 384      |\n",
            "|    ep_rew_mean        | 64.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1069     |\n",
            "|    iterations         | 108500   |\n",
            "|    time_elapsed       | 4059     |\n",
            "|    total_timesteps    | 4340000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.247   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 108499   |\n",
            "|    policy_loss        | 0.475    |\n",
            "|    value_loss         | 3.11     |\n",
            "------------------------------------\n",
            "Num timesteps: 4344000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 74.91\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 381      |\n",
            "|    ep_rew_mean        | 74.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1069     |\n",
            "|    iterations         | 108600   |\n",
            "|    time_elapsed       | 4062     |\n",
            "|    total_timesteps    | 4344000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.265   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 108599   |\n",
            "|    policy_loss        | 0.0559   |\n",
            "|    value_loss         | 5.22     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 391      |\n",
            "|    ep_rew_mean        | 64.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1069     |\n",
            "|    iterations         | 108700   |\n",
            "|    time_elapsed       | 4065     |\n",
            "|    total_timesteps    | 4348000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.287   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 108699   |\n",
            "|    policy_loss        | 0.135    |\n",
            "|    value_loss         | 1.74     |\n",
            "------------------------------------\n",
            "Num timesteps: 4352000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 77.65\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 366      |\n",
            "|    ep_rew_mean        | 77.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1069     |\n",
            "|    iterations         | 108800   |\n",
            "|    time_elapsed       | 4067     |\n",
            "|    total_timesteps    | 4352000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.348   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 108799   |\n",
            "|    policy_loss        | 0.367    |\n",
            "|    value_loss         | 5.46     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 360      |\n",
            "|    ep_rew_mean        | 71       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1070     |\n",
            "|    iterations         | 108900   |\n",
            "|    time_elapsed       | 4070     |\n",
            "|    total_timesteps    | 4356000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.303   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 108899   |\n",
            "|    policy_loss        | -0.114   |\n",
            "|    value_loss         | 5.37     |\n",
            "------------------------------------\n",
            "Num timesteps: 4360000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 67.14\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 358      |\n",
            "|    ep_rew_mean        | 67.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1070     |\n",
            "|    iterations         | 109000   |\n",
            "|    time_elapsed       | 4072     |\n",
            "|    total_timesteps    | 4360000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.394   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 108999   |\n",
            "|    policy_loss        | -0.214   |\n",
            "|    value_loss         | 2.13     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 356      |\n",
            "|    ep_rew_mean        | 52.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1070     |\n",
            "|    iterations         | 109100   |\n",
            "|    time_elapsed       | 4075     |\n",
            "|    total_timesteps    | 4364000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.428   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 109099   |\n",
            "|    policy_loss        | -0.355   |\n",
            "|    value_loss         | 8.15     |\n",
            "------------------------------------\n",
            "Num timesteps: 4368000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 43.96\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 337      |\n",
            "|    ep_rew_mean        | 44       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1071     |\n",
            "|    iterations         | 109200   |\n",
            "|    time_elapsed       | 4076     |\n",
            "|    total_timesteps    | 4368000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.311   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 109199   |\n",
            "|    policy_loss        | -0.182   |\n",
            "|    value_loss         | 2.79     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 330      |\n",
            "|    ep_rew_mean        | 33.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1071     |\n",
            "|    iterations         | 109300   |\n",
            "|    time_elapsed       | 4079     |\n",
            "|    total_timesteps    | 4372000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.188   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 109299   |\n",
            "|    policy_loss        | 0.0747   |\n",
            "|    value_loss         | 2.09     |\n",
            "------------------------------------\n",
            "Num timesteps: 4376000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 32.34\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 331      |\n",
            "|    ep_rew_mean        | 32.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1071     |\n",
            "|    iterations         | 109400   |\n",
            "|    time_elapsed       | 4082     |\n",
            "|    total_timesteps    | 4376000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.322   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 109399   |\n",
            "|    policy_loss        | -0.00429 |\n",
            "|    value_loss         | 3.75     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 321      |\n",
            "|    ep_rew_mean        | 44.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1072     |\n",
            "|    iterations         | 109500   |\n",
            "|    time_elapsed       | 4085     |\n",
            "|    total_timesteps    | 4380000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.354   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 109499   |\n",
            "|    policy_loss        | 0.144    |\n",
            "|    value_loss         | 5.39     |\n",
            "------------------------------------\n",
            "Num timesteps: 4384000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 38.02\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 343      |\n",
            "|    ep_rew_mean        | 38       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1071     |\n",
            "|    iterations         | 109600   |\n",
            "|    time_elapsed       | 4090     |\n",
            "|    total_timesteps    | 4384000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.224   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 109599   |\n",
            "|    policy_loss        | -0.106   |\n",
            "|    value_loss         | 4.17     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 353      |\n",
            "|    ep_rew_mean        | 33.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1072     |\n",
            "|    iterations         | 109700   |\n",
            "|    time_elapsed       | 4092     |\n",
            "|    total_timesteps    | 4388000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.293   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 109699   |\n",
            "|    policy_loss        | -0.154   |\n",
            "|    value_loss         | 4.47     |\n",
            "------------------------------------\n",
            "Num timesteps: 4392000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 32.18\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 362      |\n",
            "|    ep_rew_mean        | 32.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1072     |\n",
            "|    iterations         | 109800   |\n",
            "|    time_elapsed       | 4095     |\n",
            "|    total_timesteps    | 4392000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.355   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 109799   |\n",
            "|    policy_loss        | -0.116   |\n",
            "|    value_loss         | 1.53     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 358      |\n",
            "|    ep_rew_mean        | 41.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1072     |\n",
            "|    iterations         | 109900   |\n",
            "|    time_elapsed       | 4097     |\n",
            "|    total_timesteps    | 4396000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.442   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 109899   |\n",
            "|    policy_loss        | -0.0298  |\n",
            "|    value_loss         | 4.46     |\n",
            "------------------------------------\n",
            "Num timesteps: 4400000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 32.83\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 355      |\n",
            "|    ep_rew_mean        | 32.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1073     |\n",
            "|    iterations         | 110000   |\n",
            "|    time_elapsed       | 4100     |\n",
            "|    total_timesteps    | 4400000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.319   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 109999   |\n",
            "|    policy_loss        | -0.0901  |\n",
            "|    value_loss         | 4.22     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 364      |\n",
            "|    ep_rew_mean        | 39.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1073     |\n",
            "|    iterations         | 110100   |\n",
            "|    time_elapsed       | 4102     |\n",
            "|    total_timesteps    | 4404000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.309   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 110099   |\n",
            "|    policy_loss        | -0.0826  |\n",
            "|    value_loss         | 2.82     |\n",
            "------------------------------------\n",
            "Num timesteps: 4408000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 35.89\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 373      |\n",
            "|    ep_rew_mean        | 35.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1073     |\n",
            "|    iterations         | 110200   |\n",
            "|    time_elapsed       | 4105     |\n",
            "|    total_timesteps    | 4408000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.322   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 110199   |\n",
            "|    policy_loss        | 0.101    |\n",
            "|    value_loss         | 1.1      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 362      |\n",
            "|    ep_rew_mean        | 41       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1074     |\n",
            "|    iterations         | 110300   |\n",
            "|    time_elapsed       | 4107     |\n",
            "|    total_timesteps    | 4412000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.269   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 110299   |\n",
            "|    policy_loss        | -0.0145  |\n",
            "|    value_loss         | 3.92     |\n",
            "------------------------------------\n",
            "Num timesteps: 4416000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 33.52\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 353      |\n",
            "|    ep_rew_mean        | 33.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1074     |\n",
            "|    iterations         | 110400   |\n",
            "|    time_elapsed       | 4110     |\n",
            "|    total_timesteps    | 4416000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.302   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 110399   |\n",
            "|    policy_loss        | -0.628   |\n",
            "|    value_loss         | 2.18     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 349      |\n",
            "|    ep_rew_mean        | 40.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1074     |\n",
            "|    iterations         | 110500   |\n",
            "|    time_elapsed       | 4114     |\n",
            "|    total_timesteps    | 4420000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.196   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 110499   |\n",
            "|    policy_loss        | -0.0742  |\n",
            "|    value_loss         | 1.4      |\n",
            "------------------------------------\n",
            "Num timesteps: 4424000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 57.32\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 349      |\n",
            "|    ep_rew_mean        | 57.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1074     |\n",
            "|    iterations         | 110600   |\n",
            "|    time_elapsed       | 4116     |\n",
            "|    total_timesteps    | 4424000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.361   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 110599   |\n",
            "|    policy_loss        | -0.258   |\n",
            "|    value_loss         | 4.89     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 336      |\n",
            "|    ep_rew_mean        | 55.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1075     |\n",
            "|    iterations         | 110700   |\n",
            "|    time_elapsed       | 4119     |\n",
            "|    total_timesteps    | 4428000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.247   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 110699   |\n",
            "|    policy_loss        | 0.224    |\n",
            "|    value_loss         | 7.57     |\n",
            "------------------------------------\n",
            "Num timesteps: 4432000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 64.94\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 329      |\n",
            "|    ep_rew_mean        | 64.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1075     |\n",
            "|    iterations         | 110800   |\n",
            "|    time_elapsed       | 4121     |\n",
            "|    total_timesteps    | 4432000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.374   |\n",
            "|    explained_variance | 0.873    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 110799   |\n",
            "|    policy_loss        | 0.456    |\n",
            "|    value_loss         | 175      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 322      |\n",
            "|    ep_rew_mean        | 65.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1075     |\n",
            "|    iterations         | 110900   |\n",
            "|    time_elapsed       | 4123     |\n",
            "|    total_timesteps    | 4436000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.318   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 110899   |\n",
            "|    policy_loss        | -0.319   |\n",
            "|    value_loss         | 2.71     |\n",
            "------------------------------------\n",
            "Num timesteps: 4440000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 82.92\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 329      |\n",
            "|    ep_rew_mean        | 82.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1076     |\n",
            "|    iterations         | 111000   |\n",
            "|    time_elapsed       | 4126     |\n",
            "|    total_timesteps    | 4440000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.317   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 110999   |\n",
            "|    policy_loss        | 0.0104   |\n",
            "|    value_loss         | 4.24     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 326      |\n",
            "|    ep_rew_mean        | 75.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1076     |\n",
            "|    iterations         | 111100   |\n",
            "|    time_elapsed       | 4128     |\n",
            "|    total_timesteps    | 4444000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.387   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 111099   |\n",
            "|    policy_loss        | -0.149   |\n",
            "|    value_loss         | 2.27     |\n",
            "------------------------------------\n",
            "Num timesteps: 4448000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 86.28\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 327      |\n",
            "|    ep_rew_mean        | 86.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1076     |\n",
            "|    iterations         | 111200   |\n",
            "|    time_elapsed       | 4130     |\n",
            "|    total_timesteps    | 4448000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.382   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 111199   |\n",
            "|    policy_loss        | -0.448   |\n",
            "|    value_loss         | 2.49     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 325      |\n",
            "|    ep_rew_mean        | 77.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1076     |\n",
            "|    iterations         | 111300   |\n",
            "|    time_elapsed       | 4134     |\n",
            "|    total_timesteps    | 4452000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.364   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 111299   |\n",
            "|    policy_loss        | 0.216    |\n",
            "|    value_loss         | 3.09     |\n",
            "------------------------------------\n",
            "Num timesteps: 4456000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 82.58\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 328      |\n",
            "|    ep_rew_mean        | 82.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1076     |\n",
            "|    iterations         | 111400   |\n",
            "|    time_elapsed       | 4137     |\n",
            "|    total_timesteps    | 4456000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.36    |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 111399   |\n",
            "|    policy_loss        | 0.0582   |\n",
            "|    value_loss         | 0.89     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 334      |\n",
            "|    ep_rew_mean        | 78.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1077     |\n",
            "|    iterations         | 111500   |\n",
            "|    time_elapsed       | 4139     |\n",
            "|    total_timesteps    | 4460000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.317   |\n",
            "|    explained_variance | 0.953    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 111499   |\n",
            "|    policy_loss        | 0.161    |\n",
            "|    value_loss         | 33.7     |\n",
            "------------------------------------\n",
            "Num timesteps: 4464000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 98.13\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 340      |\n",
            "|    ep_rew_mean        | 98.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1077     |\n",
            "|    iterations         | 111600   |\n",
            "|    time_elapsed       | 4142     |\n",
            "|    total_timesteps    | 4464000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.331   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 111599   |\n",
            "|    policy_loss        | -0.321   |\n",
            "|    value_loss         | 2.81     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 346      |\n",
            "|    ep_rew_mean        | 96.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1077     |\n",
            "|    iterations         | 111700   |\n",
            "|    time_elapsed       | 4144     |\n",
            "|    total_timesteps    | 4468000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.24    |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 111699   |\n",
            "|    policy_loss        | -0.0828  |\n",
            "|    value_loss         | 5.87     |\n",
            "------------------------------------\n",
            "Num timesteps: 4472000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 99.74\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 357      |\n",
            "|    ep_rew_mean        | 99.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1078     |\n",
            "|    iterations         | 111800   |\n",
            "|    time_elapsed       | 4147     |\n",
            "|    total_timesteps    | 4472000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.29    |\n",
            "|    explained_variance | 0.959    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 111799   |\n",
            "|    policy_loss        | -0.032   |\n",
            "|    value_loss         | 34.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 365      |\n",
            "|    ep_rew_mean        | 91.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1078     |\n",
            "|    iterations         | 111900   |\n",
            "|    time_elapsed       | 4150     |\n",
            "|    total_timesteps    | 4476000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.388   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 111899   |\n",
            "|    policy_loss        | -0.153   |\n",
            "|    value_loss         | 2.45     |\n",
            "------------------------------------\n",
            "Num timesteps: 4480000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 89.21\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 362      |\n",
            "|    ep_rew_mean        | 89.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1078     |\n",
            "|    iterations         | 112000   |\n",
            "|    time_elapsed       | 4153     |\n",
            "|    total_timesteps    | 4480000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.46    |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 111999   |\n",
            "|    policy_loss        | -0.0866  |\n",
            "|    value_loss         | 1.96     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 383      |\n",
            "|    ep_rew_mean        | 83.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1078     |\n",
            "|    iterations         | 112100   |\n",
            "|    time_elapsed       | 4156     |\n",
            "|    total_timesteps    | 4484000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.295   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 112099   |\n",
            "|    policy_loss        | 0.294    |\n",
            "|    value_loss         | 4.33     |\n",
            "------------------------------------\n",
            "Num timesteps: 4488000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 89.01\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 394      |\n",
            "|    ep_rew_mean        | 89       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1078     |\n",
            "|    iterations         | 112200   |\n",
            "|    time_elapsed       | 4159     |\n",
            "|    total_timesteps    | 4488000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.282   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 112199   |\n",
            "|    policy_loss        | -0.056   |\n",
            "|    value_loss         | 1.05     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 384      |\n",
            "|    ep_rew_mean        | 96       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1079     |\n",
            "|    iterations         | 112300   |\n",
            "|    time_elapsed       | 4163     |\n",
            "|    total_timesteps    | 4492000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.375   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 112299   |\n",
            "|    policy_loss        | -0.061   |\n",
            "|    value_loss         | 1.06     |\n",
            "------------------------------------\n",
            "Num timesteps: 4496000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 87.29\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 390      |\n",
            "|    ep_rew_mean        | 87.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1079     |\n",
            "|    iterations         | 112400   |\n",
            "|    time_elapsed       | 4166     |\n",
            "|    total_timesteps    | 4496000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.271   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 112399   |\n",
            "|    policy_loss        | -0.0676  |\n",
            "|    value_loss         | 3.34     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 389      |\n",
            "|    ep_rew_mean        | 74.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1079     |\n",
            "|    iterations         | 112500   |\n",
            "|    time_elapsed       | 4168     |\n",
            "|    total_timesteps    | 4500000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.298   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 112499   |\n",
            "|    policy_loss        | 1.03     |\n",
            "|    value_loss         | 12.4     |\n",
            "------------------------------------\n",
            "Num timesteps: 4504000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 72.39\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 390      |\n",
            "|    ep_rew_mean        | 72.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1079     |\n",
            "|    iterations         | 112600   |\n",
            "|    time_elapsed       | 4170     |\n",
            "|    total_timesteps    | 4504000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.257   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 112599   |\n",
            "|    policy_loss        | 0.0375   |\n",
            "|    value_loss         | 5.76     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 384      |\n",
            "|    ep_rew_mean        | 74.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1080     |\n",
            "|    iterations         | 112700   |\n",
            "|    time_elapsed       | 4172     |\n",
            "|    total_timesteps    | 4508000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.364   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 112699   |\n",
            "|    policy_loss        | 0.224    |\n",
            "|    value_loss         | 2.23     |\n",
            "------------------------------------\n",
            "Num timesteps: 4512000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 86.74\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 390      |\n",
            "|    ep_rew_mean        | 86.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1080     |\n",
            "|    iterations         | 112800   |\n",
            "|    time_elapsed       | 4175     |\n",
            "|    total_timesteps    | 4512000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.245   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 112799   |\n",
            "|    policy_loss        | 0.0201   |\n",
            "|    value_loss         | 1.95     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 381      |\n",
            "|    ep_rew_mean        | 89.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1080     |\n",
            "|    iterations         | 112900   |\n",
            "|    time_elapsed       | 4178     |\n",
            "|    total_timesteps    | 4516000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.301   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 112899   |\n",
            "|    policy_loss        | -0.186   |\n",
            "|    value_loss         | 9.48     |\n",
            "------------------------------------\n",
            "Num timesteps: 4520000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 97.46\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 361      |\n",
            "|    ep_rew_mean        | 97.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1081     |\n",
            "|    iterations         | 113000   |\n",
            "|    time_elapsed       | 4180     |\n",
            "|    total_timesteps    | 4520000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.347   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 112999   |\n",
            "|    policy_loss        | -0.0313  |\n",
            "|    value_loss         | 1.44     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 355      |\n",
            "|    ep_rew_mean        | 96.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1081     |\n",
            "|    iterations         | 113100   |\n",
            "|    time_elapsed       | 4183     |\n",
            "|    total_timesteps    | 4524000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.288   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 113099   |\n",
            "|    policy_loss        | 0.0917   |\n",
            "|    value_loss         | 2.59     |\n",
            "------------------------------------\n",
            "Num timesteps: 4528000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 85.30\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 361      |\n",
            "|    ep_rew_mean        | 85.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1081     |\n",
            "|    iterations         | 113200   |\n",
            "|    time_elapsed       | 4186     |\n",
            "|    total_timesteps    | 4528000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.372   |\n",
            "|    explained_variance | 0.965    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 113199   |\n",
            "|    policy_loss        | -0.0925  |\n",
            "|    value_loss         | 53.2     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 348      |\n",
            "|    ep_rew_mean        | 94       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1081     |\n",
            "|    iterations         | 113300   |\n",
            "|    time_elapsed       | 4189     |\n",
            "|    total_timesteps    | 4532000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.184   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 113299   |\n",
            "|    policy_loss        | -0.263   |\n",
            "|    value_loss         | 5.81     |\n",
            "------------------------------------\n",
            "Num timesteps: 4536000\n",
            "Best mean reward: 101.37 - Last mean reward per episode: 105.84\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 351      |\n",
            "|    ep_rew_mean        | 106      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1082     |\n",
            "|    iterations         | 113400   |\n",
            "|    time_elapsed       | 4191     |\n",
            "|    total_timesteps    | 4536000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.314   |\n",
            "|    explained_variance | 0.825    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 113399   |\n",
            "|    policy_loss        | -0.0418  |\n",
            "|    value_loss         | 69.8     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 348      |\n",
            "|    ep_rew_mean        | 115      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1082     |\n",
            "|    iterations         | 113500   |\n",
            "|    time_elapsed       | 4194     |\n",
            "|    total_timesteps    | 4540000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.27    |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 113499   |\n",
            "|    policy_loss        | -0.153   |\n",
            "|    value_loss         | 2.94     |\n",
            "------------------------------------\n",
            "Num timesteps: 4544000\n",
            "Best mean reward: 105.84 - Last mean reward per episode: 109.65\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 358      |\n",
            "|    ep_rew_mean        | 110      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1082     |\n",
            "|    iterations         | 113600   |\n",
            "|    time_elapsed       | 4197     |\n",
            "|    total_timesteps    | 4544000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.469   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 113599   |\n",
            "|    policy_loss        | 0.22     |\n",
            "|    value_loss         | 3.43     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 368      |\n",
            "|    ep_rew_mean        | 98.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1082     |\n",
            "|    iterations         | 113700   |\n",
            "|    time_elapsed       | 4200     |\n",
            "|    total_timesteps    | 4548000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.336   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 113699   |\n",
            "|    policy_loss        | 0.305    |\n",
            "|    value_loss         | 3.88     |\n",
            "------------------------------------\n",
            "Num timesteps: 4552000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 89.25\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 386      |\n",
            "|    ep_rew_mean        | 89.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1082     |\n",
            "|    iterations         | 113800   |\n",
            "|    time_elapsed       | 4203     |\n",
            "|    total_timesteps    | 4552000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.29    |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 113799   |\n",
            "|    policy_loss        | -0.0627  |\n",
            "|    value_loss         | 7.42     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 383      |\n",
            "|    ep_rew_mean        | 78.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1082     |\n",
            "|    iterations         | 113900   |\n",
            "|    time_elapsed       | 4206     |\n",
            "|    total_timesteps    | 4556000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.228   |\n",
            "|    explained_variance | 0.872    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 113899   |\n",
            "|    policy_loss        | -0.972   |\n",
            "|    value_loss         | 159      |\n",
            "------------------------------------\n",
            "Num timesteps: 4560000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 62.45\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 398      |\n",
            "|    ep_rew_mean        | 62.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1082     |\n",
            "|    iterations         | 114000   |\n",
            "|    time_elapsed       | 4210     |\n",
            "|    total_timesteps    | 4560000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.325   |\n",
            "|    explained_variance | 0.978    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 113999   |\n",
            "|    policy_loss        | -0.44    |\n",
            "|    value_loss         | 42.9     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 412      |\n",
            "|    ep_rew_mean        | 59       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1083     |\n",
            "|    iterations         | 114100   |\n",
            "|    time_elapsed       | 4213     |\n",
            "|    total_timesteps    | 4564000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.307   |\n",
            "|    explained_variance | 0.656    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 114099   |\n",
            "|    policy_loss        | -0.051   |\n",
            "|    value_loss         | 674      |\n",
            "------------------------------------\n",
            "Num timesteps: 4568000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 63.27\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 406      |\n",
            "|    ep_rew_mean        | 63.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1083     |\n",
            "|    iterations         | 114200   |\n",
            "|    time_elapsed       | 4216     |\n",
            "|    total_timesteps    | 4568000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.371   |\n",
            "|    explained_variance | 0.333    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 114199   |\n",
            "|    policy_loss        | 0.00475  |\n",
            "|    value_loss         | 1.11e+03 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 400      |\n",
            "|    ep_rew_mean        | 50.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1083     |\n",
            "|    iterations         | 114300   |\n",
            "|    time_elapsed       | 4218     |\n",
            "|    total_timesteps    | 4572000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.487   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 114299   |\n",
            "|    policy_loss        | -0.0956  |\n",
            "|    value_loss         | 2.26     |\n",
            "------------------------------------\n",
            "Num timesteps: 4576000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 39.78\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 402      |\n",
            "|    ep_rew_mean        | 39.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1083     |\n",
            "|    iterations         | 114400   |\n",
            "|    time_elapsed       | 4221     |\n",
            "|    total_timesteps    | 4576000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.338   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 114399   |\n",
            "|    policy_loss        | 0.146    |\n",
            "|    value_loss         | 3.15     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 408      |\n",
            "|    ep_rew_mean        | 14.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1084     |\n",
            "|    iterations         | 114500   |\n",
            "|    time_elapsed       | 4224     |\n",
            "|    total_timesteps    | 4580000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.394   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 114499   |\n",
            "|    policy_loss        | 0.0764   |\n",
            "|    value_loss         | 3.79     |\n",
            "------------------------------------\n",
            "Num timesteps: 4584000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 16.49\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 409      |\n",
            "|    ep_rew_mean        | 16.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1084     |\n",
            "|    iterations         | 114600   |\n",
            "|    time_elapsed       | 4227     |\n",
            "|    total_timesteps    | 4584000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.289   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 114599   |\n",
            "|    policy_loss        | -0.0915  |\n",
            "|    value_loss         | 2.71     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 404      |\n",
            "|    ep_rew_mean        | 24.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1084     |\n",
            "|    iterations         | 114700   |\n",
            "|    time_elapsed       | 4230     |\n",
            "|    total_timesteps    | 4588000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.355   |\n",
            "|    explained_variance | 0.673    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 114699   |\n",
            "|    policy_loss        | -0.0016  |\n",
            "|    value_loss         | 404      |\n",
            "------------------------------------\n",
            "Num timesteps: 4592000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 11.82\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 389      |\n",
            "|    ep_rew_mean        | 11.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1084     |\n",
            "|    iterations         | 114800   |\n",
            "|    time_elapsed       | 4233     |\n",
            "|    total_timesteps    | 4592000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.298   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 114799   |\n",
            "|    policy_loss        | -0.0102  |\n",
            "|    value_loss         | 1.78     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 387      |\n",
            "|    ep_rew_mean        | 20.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1084     |\n",
            "|    iterations         | 114900   |\n",
            "|    time_elapsed       | 4236     |\n",
            "|    total_timesteps    | 4596000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.419   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 114899   |\n",
            "|    policy_loss        | 0.0901   |\n",
            "|    value_loss         | 0.668    |\n",
            "------------------------------------\n",
            "Num timesteps: 4600000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 22.80\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 382      |\n",
            "|    ep_rew_mean        | 22.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1085     |\n",
            "|    iterations         | 115000   |\n",
            "|    time_elapsed       | 4239     |\n",
            "|    total_timesteps    | 4600000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.328   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 114999   |\n",
            "|    policy_loss        | -0.67    |\n",
            "|    value_loss         | 6.32     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 385      |\n",
            "|    ep_rew_mean        | 16.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1085     |\n",
            "|    iterations         | 115100   |\n",
            "|    time_elapsed       | 4242     |\n",
            "|    total_timesteps    | 4604000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.158   |\n",
            "|    explained_variance | 0.975    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 115099   |\n",
            "|    policy_loss        | 0.0225   |\n",
            "|    value_loss         | 21       |\n",
            "------------------------------------\n",
            "Num timesteps: 4608000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 12.72\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 388      |\n",
            "|    ep_rew_mean        | 12.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1085     |\n",
            "|    iterations         | 115200   |\n",
            "|    time_elapsed       | 4245     |\n",
            "|    total_timesteps    | 4608000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.285   |\n",
            "|    explained_variance | 0.871    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 115199   |\n",
            "|    policy_loss        | -0.453   |\n",
            "|    value_loss         | 123      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 398      |\n",
            "|    ep_rew_mean        | 7.06     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1085     |\n",
            "|    iterations         | 115300   |\n",
            "|    time_elapsed       | 4248     |\n",
            "|    total_timesteps    | 4612000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.379   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 115299   |\n",
            "|    policy_loss        | 0.106    |\n",
            "|    value_loss         | 1.15     |\n",
            "------------------------------------\n",
            "Num timesteps: 4616000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 13.68\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 403      |\n",
            "|    ep_rew_mean        | 13.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1085     |\n",
            "|    iterations         | 115400   |\n",
            "|    time_elapsed       | 4250     |\n",
            "|    total_timesteps    | 4616000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.264   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 115399   |\n",
            "|    policy_loss        | -0.0966  |\n",
            "|    value_loss         | 3.47     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 395      |\n",
            "|    ep_rew_mean        | 21       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1086     |\n",
            "|    iterations         | 115500   |\n",
            "|    time_elapsed       | 4253     |\n",
            "|    total_timesteps    | 4620000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.282   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 115499   |\n",
            "|    policy_loss        | 0.000869 |\n",
            "|    value_loss         | 1.21     |\n",
            "------------------------------------\n",
            "Num timesteps: 4624000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 20.10\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 381      |\n",
            "|    ep_rew_mean        | 20.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1086     |\n",
            "|    iterations         | 115600   |\n",
            "|    time_elapsed       | 4255     |\n",
            "|    total_timesteps    | 4624000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.342   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 115599   |\n",
            "|    policy_loss        | 0.212    |\n",
            "|    value_loss         | 1.93     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 363      |\n",
            "|    ep_rew_mean        | 22.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1086     |\n",
            "|    iterations         | 115700   |\n",
            "|    time_elapsed       | 4257     |\n",
            "|    total_timesteps    | 4628000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.287   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 115699   |\n",
            "|    policy_loss        | -0.747   |\n",
            "|    value_loss         | 6.52     |\n",
            "------------------------------------\n",
            "Num timesteps: 4632000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 21.50\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 368      |\n",
            "|    ep_rew_mean        | 21.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1087     |\n",
            "|    iterations         | 115800   |\n",
            "|    time_elapsed       | 4260     |\n",
            "|    total_timesteps    | 4632000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.321   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 115799   |\n",
            "|    policy_loss        | -0.0511  |\n",
            "|    value_loss         | 1.17     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 361      |\n",
            "|    ep_rew_mean        | 35.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1087     |\n",
            "|    iterations         | 115900   |\n",
            "|    time_elapsed       | 4262     |\n",
            "|    total_timesteps    | 4636000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.458   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 115899   |\n",
            "|    policy_loss        | -0.473   |\n",
            "|    value_loss         | 6.65     |\n",
            "------------------------------------\n",
            "Num timesteps: 4640000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 33.93\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 350      |\n",
            "|    ep_rew_mean        | 33.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1087     |\n",
            "|    iterations         | 116000   |\n",
            "|    time_elapsed       | 4265     |\n",
            "|    total_timesteps    | 4640000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.312   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 115999   |\n",
            "|    policy_loss        | -0.169   |\n",
            "|    value_loss         | 3.19     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 335      |\n",
            "|    ep_rew_mean        | 44.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1088     |\n",
            "|    iterations         | 116100   |\n",
            "|    time_elapsed       | 4268     |\n",
            "|    total_timesteps    | 4644000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.294   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 116099   |\n",
            "|    policy_loss        | -0.131   |\n",
            "|    value_loss         | 1.59     |\n",
            "------------------------------------\n",
            "Num timesteps: 4648000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 48.02\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 338      |\n",
            "|    ep_rew_mean        | 48       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1088     |\n",
            "|    iterations         | 116200   |\n",
            "|    time_elapsed       | 4271     |\n",
            "|    total_timesteps    | 4648000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.33    |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 116199   |\n",
            "|    policy_loss        | -0.128   |\n",
            "|    value_loss         | 1.99     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 332      |\n",
            "|    ep_rew_mean        | 54.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1088     |\n",
            "|    iterations         | 116300   |\n",
            "|    time_elapsed       | 4273     |\n",
            "|    total_timesteps    | 4652000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.377   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 116299   |\n",
            "|    policy_loss        | -0.0282  |\n",
            "|    value_loss         | 4.06     |\n",
            "------------------------------------\n",
            "Num timesteps: 4656000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 45.03\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 328      |\n",
            "|    ep_rew_mean        | 45       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1088     |\n",
            "|    iterations         | 116400   |\n",
            "|    time_elapsed       | 4276     |\n",
            "|    total_timesteps    | 4656000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.273   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 116399   |\n",
            "|    policy_loss        | 0.251    |\n",
            "|    value_loss         | 7.21     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 347      |\n",
            "|    ep_rew_mean        | 53       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1089     |\n",
            "|    iterations         | 116500   |\n",
            "|    time_elapsed       | 4278     |\n",
            "|    total_timesteps    | 4660000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.332   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 116499   |\n",
            "|    policy_loss        | -0.542   |\n",
            "|    value_loss         | 9        |\n",
            "------------------------------------\n",
            "Num timesteps: 4664000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 58.68\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 353      |\n",
            "|    ep_rew_mean        | 58.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1089     |\n",
            "|    iterations         | 116600   |\n",
            "|    time_elapsed       | 4281     |\n",
            "|    total_timesteps    | 4664000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.397   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 116599   |\n",
            "|    policy_loss        | -0.47    |\n",
            "|    value_loss         | 3.26     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 339      |\n",
            "|    ep_rew_mean        | 59.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1089     |\n",
            "|    iterations         | 116700   |\n",
            "|    time_elapsed       | 4285     |\n",
            "|    total_timesteps    | 4668000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.246   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 116699   |\n",
            "|    policy_loss        | 0.325    |\n",
            "|    value_loss         | 4.3      |\n",
            "------------------------------------\n",
            "Num timesteps: 4672000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 36.37\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 355      |\n",
            "|    ep_rew_mean        | 36.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1089     |\n",
            "|    iterations         | 116800   |\n",
            "|    time_elapsed       | 4289     |\n",
            "|    total_timesteps    | 4672000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.242   |\n",
            "|    explained_variance | 0.844    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 116799   |\n",
            "|    policy_loss        | -4.34    |\n",
            "|    value_loss         | 242      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 364      |\n",
            "|    ep_rew_mean        | 45.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1089     |\n",
            "|    iterations         | 116900   |\n",
            "|    time_elapsed       | 4291     |\n",
            "|    total_timesteps    | 4676000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.265   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 116899   |\n",
            "|    policy_loss        | 0.129    |\n",
            "|    value_loss         | 2.08     |\n",
            "------------------------------------\n",
            "Num timesteps: 4680000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 50.63\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 373      |\n",
            "|    ep_rew_mean        | 50.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1089     |\n",
            "|    iterations         | 117000   |\n",
            "|    time_elapsed       | 4295     |\n",
            "|    total_timesteps    | 4680000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.42    |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 116999   |\n",
            "|    policy_loss        | -0.0614  |\n",
            "|    value_loss         | 2.33     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 378      |\n",
            "|    ep_rew_mean        | 56.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1089     |\n",
            "|    iterations         | 117100   |\n",
            "|    time_elapsed       | 4298     |\n",
            "|    total_timesteps    | 4684000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.325   |\n",
            "|    explained_variance | 0.586    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 117099   |\n",
            "|    policy_loss        | -0.338   |\n",
            "|    value_loss         | 703      |\n",
            "------------------------------------\n",
            "Num timesteps: 4688000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 52.87\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 387      |\n",
            "|    ep_rew_mean        | 52.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1089     |\n",
            "|    iterations         | 117200   |\n",
            "|    time_elapsed       | 4301     |\n",
            "|    total_timesteps    | 4688000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.263   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 117199   |\n",
            "|    policy_loss        | -0.0178  |\n",
            "|    value_loss         | 3.26     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 405      |\n",
            "|    ep_rew_mean        | 53.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1090     |\n",
            "|    iterations         | 117300   |\n",
            "|    time_elapsed       | 4304     |\n",
            "|    total_timesteps    | 4692000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.373   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 117299   |\n",
            "|    policy_loss        | 0.053    |\n",
            "|    value_loss         | 2.72     |\n",
            "------------------------------------\n",
            "Num timesteps: 4696000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 52.42\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 416      |\n",
            "|    ep_rew_mean        | 52.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1090     |\n",
            "|    iterations         | 117400   |\n",
            "|    time_elapsed       | 4308     |\n",
            "|    total_timesteps    | 4696000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.244   |\n",
            "|    explained_variance | 0.911    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 117399   |\n",
            "|    policy_loss        | -0.0933  |\n",
            "|    value_loss         | 103      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 427      |\n",
            "|    ep_rew_mean        | 55.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1090     |\n",
            "|    iterations         | 117500   |\n",
            "|    time_elapsed       | 4310     |\n",
            "|    total_timesteps    | 4700000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.142   |\n",
            "|    explained_variance | 0.909    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 117499   |\n",
            "|    policy_loss        | 1.66     |\n",
            "|    value_loss         | 186      |\n",
            "------------------------------------\n",
            "Num timesteps: 4704000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 59.18\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 415      |\n",
            "|    ep_rew_mean        | 59.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1090     |\n",
            "|    iterations         | 117600   |\n",
            "|    time_elapsed       | 4313     |\n",
            "|    total_timesteps    | 4704000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.312   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 117599   |\n",
            "|    policy_loss        | 0.178    |\n",
            "|    value_loss         | 5.41     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 425      |\n",
            "|    ep_rew_mean        | 64.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1090     |\n",
            "|    iterations         | 117700   |\n",
            "|    time_elapsed       | 4315     |\n",
            "|    total_timesteps    | 4708000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.347   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 117699   |\n",
            "|    policy_loss        | -0.203   |\n",
            "|    value_loss         | 1.84     |\n",
            "------------------------------------\n",
            "Num timesteps: 4712000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 86.40\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 408      |\n",
            "|    ep_rew_mean        | 86.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1091     |\n",
            "|    iterations         | 117800   |\n",
            "|    time_elapsed       | 4318     |\n",
            "|    total_timesteps    | 4712000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.306   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 117799   |\n",
            "|    policy_loss        | 0.0157   |\n",
            "|    value_loss         | 3.05     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 407      |\n",
            "|    ep_rew_mean        | 100      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1091     |\n",
            "|    iterations         | 117900   |\n",
            "|    time_elapsed       | 4321     |\n",
            "|    total_timesteps    | 4716000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.317   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 117899   |\n",
            "|    policy_loss        | -0.0354  |\n",
            "|    value_loss         | 1.59     |\n",
            "------------------------------------\n",
            "Num timesteps: 4720000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 93.53\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 396      |\n",
            "|    ep_rew_mean        | 93.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1091     |\n",
            "|    iterations         | 118000   |\n",
            "|    time_elapsed       | 4324     |\n",
            "|    total_timesteps    | 4720000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.382   |\n",
            "|    explained_variance | 0.977    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 117999   |\n",
            "|    policy_loss        | -0.714   |\n",
            "|    value_loss         | 52.8     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 382      |\n",
            "|    ep_rew_mean        | 87.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1091     |\n",
            "|    iterations         | 118100   |\n",
            "|    time_elapsed       | 4326     |\n",
            "|    total_timesteps    | 4724000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.24    |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 118099   |\n",
            "|    policy_loss        | -0.0387  |\n",
            "|    value_loss         | 1.59     |\n",
            "------------------------------------\n",
            "Num timesteps: 4728000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 92.76\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 361      |\n",
            "|    ep_rew_mean        | 92.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1092     |\n",
            "|    iterations         | 118200   |\n",
            "|    time_elapsed       | 4328     |\n",
            "|    total_timesteps    | 4728000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.235   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 118199   |\n",
            "|    policy_loss        | 0.158    |\n",
            "|    value_loss         | 4.33     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 332      |\n",
            "|    ep_rew_mean        | 92.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1092     |\n",
            "|    iterations         | 118300   |\n",
            "|    time_elapsed       | 4330     |\n",
            "|    total_timesteps    | 4732000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.373   |\n",
            "|    explained_variance | 0.748    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 118299   |\n",
            "|    policy_loss        | 2.99     |\n",
            "|    value_loss         | 632      |\n",
            "------------------------------------\n",
            "Num timesteps: 4736000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 94.69\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 329      |\n",
            "|    ep_rew_mean        | 94.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1092     |\n",
            "|    iterations         | 118400   |\n",
            "|    time_elapsed       | 4333     |\n",
            "|    total_timesteps    | 4736000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.356   |\n",
            "|    explained_variance | 0.954    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 118399   |\n",
            "|    policy_loss        | -1.23    |\n",
            "|    value_loss         | 104      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 329      |\n",
            "|    ep_rew_mean        | 86       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1093     |\n",
            "|    iterations         | 118500   |\n",
            "|    time_elapsed       | 4336     |\n",
            "|    total_timesteps    | 4740000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.199   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 118499   |\n",
            "|    policy_loss        | 0.0347   |\n",
            "|    value_loss         | 2.11     |\n",
            "------------------------------------\n",
            "Num timesteps: 4744000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 78.07\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 321      |\n",
            "|    ep_rew_mean        | 78.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1093     |\n",
            "|    iterations         | 118600   |\n",
            "|    time_elapsed       | 4339     |\n",
            "|    total_timesteps    | 4744000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.281   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 118599   |\n",
            "|    policy_loss        | -0.0314  |\n",
            "|    value_loss         | 4.64     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 332      |\n",
            "|    ep_rew_mean        | 61.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1093     |\n",
            "|    iterations         | 118700   |\n",
            "|    time_elapsed       | 4342     |\n",
            "|    total_timesteps    | 4748000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.494   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 118699   |\n",
            "|    policy_loss        | 0.27     |\n",
            "|    value_loss         | 2.21     |\n",
            "------------------------------------\n",
            "Num timesteps: 4752000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 51.91\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 332      |\n",
            "|    ep_rew_mean        | 51.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1093     |\n",
            "|    iterations         | 118800   |\n",
            "|    time_elapsed       | 4344     |\n",
            "|    total_timesteps    | 4752000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.35    |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 118799   |\n",
            "|    policy_loss        | -0.02    |\n",
            "|    value_loss         | 5.39     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 322      |\n",
            "|    ep_rew_mean        | 51.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1093     |\n",
            "|    iterations         | 118900   |\n",
            "|    time_elapsed       | 4347     |\n",
            "|    total_timesteps    | 4756000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.392   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 118899   |\n",
            "|    policy_loss        | 0.18     |\n",
            "|    value_loss         | 8.85     |\n",
            "------------------------------------\n",
            "Num timesteps: 4760000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 38.53\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 330      |\n",
            "|    ep_rew_mean        | 38.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 119000   |\n",
            "|    time_elapsed       | 4350     |\n",
            "|    total_timesteps    | 4760000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.444   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 118999   |\n",
            "|    policy_loss        | -0.0945  |\n",
            "|    value_loss         | 3.91     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 351      |\n",
            "|    ep_rew_mean        | 45.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 119100   |\n",
            "|    time_elapsed       | 4353     |\n",
            "|    total_timesteps    | 4764000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.382   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 119099   |\n",
            "|    policy_loss        | 0.0702   |\n",
            "|    value_loss         | 1.85     |\n",
            "------------------------------------\n",
            "Num timesteps: 4768000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 52.96\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 367      |\n",
            "|    ep_rew_mean        | 53       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 119200   |\n",
            "|    time_elapsed       | 4356     |\n",
            "|    total_timesteps    | 4768000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.305   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 119199   |\n",
            "|    policy_loss        | -0.0211  |\n",
            "|    value_loss         | 5.89     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 371      |\n",
            "|    ep_rew_mean        | 35.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 119300   |\n",
            "|    time_elapsed       | 4359     |\n",
            "|    total_timesteps    | 4772000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.389   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 119299   |\n",
            "|    policy_loss        | -0.0454  |\n",
            "|    value_loss         | 6.24     |\n",
            "------------------------------------\n",
            "Num timesteps: 4776000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 28.43\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 365      |\n",
            "|    ep_rew_mean        | 28.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 119400   |\n",
            "|    time_elapsed       | 4362     |\n",
            "|    total_timesteps    | 4776000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.268   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 119399   |\n",
            "|    policy_loss        | -0.161   |\n",
            "|    value_loss         | 2.53     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 366      |\n",
            "|    ep_rew_mean        | 18.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 119500   |\n",
            "|    time_elapsed       | 4364     |\n",
            "|    total_timesteps    | 4780000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.267   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 119499   |\n",
            "|    policy_loss        | 0.494    |\n",
            "|    value_loss         | 11.6     |\n",
            "------------------------------------\n",
            "Num timesteps: 4784000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 25.25\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 360      |\n",
            "|    ep_rew_mean        | 25.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 119600   |\n",
            "|    time_elapsed       | 4367     |\n",
            "|    total_timesteps    | 4784000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.3     |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 119599   |\n",
            "|    policy_loss        | 0.09     |\n",
            "|    value_loss         | 2.25     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 356      |\n",
            "|    ep_rew_mean        | 29.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 119700   |\n",
            "|    time_elapsed       | 4369     |\n",
            "|    total_timesteps    | 4788000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.198   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 119699   |\n",
            "|    policy_loss        | 0.179    |\n",
            "|    value_loss         | 4.36     |\n",
            "------------------------------------\n",
            "Num timesteps: 4792000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 42.60\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 364      |\n",
            "|    ep_rew_mean        | 42.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 119800   |\n",
            "|    time_elapsed       | 4372     |\n",
            "|    total_timesteps    | 4792000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.331   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 119799   |\n",
            "|    policy_loss        | -0.0173  |\n",
            "|    value_loss         | 2.21     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 359      |\n",
            "|    ep_rew_mean        | 54.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 119900   |\n",
            "|    time_elapsed       | 4375     |\n",
            "|    total_timesteps    | 4796000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.335   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 119899   |\n",
            "|    policy_loss        | 0.198    |\n",
            "|    value_loss         | 20.5     |\n",
            "------------------------------------\n",
            "Num timesteps: 4800000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 46.11\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 337      |\n",
            "|    ep_rew_mean        | 46.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 120000   |\n",
            "|    time_elapsed       | 4377     |\n",
            "|    total_timesteps    | 4800000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.257   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 119999   |\n",
            "|    policy_loss        | -0.319   |\n",
            "|    value_loss         | 5.24     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 322      |\n",
            "|    ep_rew_mean        | 48.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 120100   |\n",
            "|    time_elapsed       | 4380     |\n",
            "|    total_timesteps    | 4804000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.313   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 120099   |\n",
            "|    policy_loss        | 0.0601   |\n",
            "|    value_loss         | 2.18     |\n",
            "------------------------------------\n",
            "Num timesteps: 4808000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 55.99\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 333      |\n",
            "|    ep_rew_mean        | 56       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 120200   |\n",
            "|    time_elapsed       | 4383     |\n",
            "|    total_timesteps    | 4808000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.197   |\n",
            "|    explained_variance | 0.971    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 120199   |\n",
            "|    policy_loss        | -0.0859  |\n",
            "|    value_loss         | 9.82     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 360      |\n",
            "|    ep_rew_mean        | 69       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 120300   |\n",
            "|    time_elapsed       | 4386     |\n",
            "|    total_timesteps    | 4812000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.411   |\n",
            "|    explained_variance | 0.602    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 120299   |\n",
            "|    policy_loss        | 3        |\n",
            "|    value_loss         | 340      |\n",
            "------------------------------------\n",
            "Num timesteps: 4816000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 74.56\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 358      |\n",
            "|    ep_rew_mean        | 74.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 120400   |\n",
            "|    time_elapsed       | 4389     |\n",
            "|    total_timesteps    | 4816000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.306   |\n",
            "|    explained_variance | 0.946    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 120399   |\n",
            "|    policy_loss        | 0.135    |\n",
            "|    value_loss         | 101      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 362      |\n",
            "|    ep_rew_mean        | 72.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 120500   |\n",
            "|    time_elapsed       | 4392     |\n",
            "|    total_timesteps    | 4820000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.315   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 120499   |\n",
            "|    policy_loss        | 0.0887   |\n",
            "|    value_loss         | 3.64     |\n",
            "------------------------------------\n",
            "Num timesteps: 4824000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 81.11\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 369      |\n",
            "|    ep_rew_mean        | 81.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 120600   |\n",
            "|    time_elapsed       | 4395     |\n",
            "|    total_timesteps    | 4824000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.393   |\n",
            "|    explained_variance | 0.98     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 120599   |\n",
            "|    policy_loss        | 0.419    |\n",
            "|    value_loss         | 6.28     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 377      |\n",
            "|    ep_rew_mean        | 81.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 120700   |\n",
            "|    time_elapsed       | 4398     |\n",
            "|    total_timesteps    | 4828000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.404   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 120699   |\n",
            "|    policy_loss        | 0.0674   |\n",
            "|    value_loss         | 0.903    |\n",
            "------------------------------------\n",
            "Num timesteps: 4832000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 85.41\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 381      |\n",
            "|    ep_rew_mean        | 85.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 120800   |\n",
            "|    time_elapsed       | 4401     |\n",
            "|    total_timesteps    | 4832000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.312   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 120799   |\n",
            "|    policy_loss        | 0.0519   |\n",
            "|    value_loss         | 2.99     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 390      |\n",
            "|    ep_rew_mean        | 96.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 120900   |\n",
            "|    time_elapsed       | 4403     |\n",
            "|    total_timesteps    | 4836000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.421   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 120899   |\n",
            "|    policy_loss        | -0.209   |\n",
            "|    value_loss         | 9.06     |\n",
            "------------------------------------\n",
            "Num timesteps: 4840000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 92.15\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 392      |\n",
            "|    ep_rew_mean        | 92.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 121000   |\n",
            "|    time_elapsed       | 4406     |\n",
            "|    total_timesteps    | 4840000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.278   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 120999   |\n",
            "|    policy_loss        | 0.0497   |\n",
            "|    value_loss         | 3.28     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 402      |\n",
            "|    ep_rew_mean        | 105      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 121100   |\n",
            "|    time_elapsed       | 4409     |\n",
            "|    total_timesteps    | 4844000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.394   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 121099   |\n",
            "|    policy_loss        | -0.126   |\n",
            "|    value_loss         | 2.29     |\n",
            "------------------------------------\n",
            "Num timesteps: 4848000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 97.51\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 374      |\n",
            "|    ep_rew_mean        | 97.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 121200   |\n",
            "|    time_elapsed       | 4411     |\n",
            "|    total_timesteps    | 4848000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.146   |\n",
            "|    explained_variance | 0.877    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 121199   |\n",
            "|    policy_loss        | -0.0164  |\n",
            "|    value_loss         | 339      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 371      |\n",
            "|    ep_rew_mean        | 87.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 121300   |\n",
            "|    time_elapsed       | 4414     |\n",
            "|    total_timesteps    | 4852000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.361   |\n",
            "|    explained_variance | 0.712    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 121299   |\n",
            "|    policy_loss        | -0.94    |\n",
            "|    value_loss         | 610      |\n",
            "------------------------------------\n",
            "Num timesteps: 4856000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 84.37\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 357      |\n",
            "|    ep_rew_mean        | 84.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 121400   |\n",
            "|    time_elapsed       | 4417     |\n",
            "|    total_timesteps    | 4856000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.307   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 121399   |\n",
            "|    policy_loss        | -0.429   |\n",
            "|    value_loss         | 4.08     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 349      |\n",
            "|    ep_rew_mean        | 77.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 121500   |\n",
            "|    time_elapsed       | 4419     |\n",
            "|    total_timesteps    | 4860000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.312   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 121499   |\n",
            "|    policy_loss        | 0.0988   |\n",
            "|    value_loss         | 5.19     |\n",
            "------------------------------------\n",
            "Num timesteps: 4864000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 57.24\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 332      |\n",
            "|    ep_rew_mean        | 57.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 121600   |\n",
            "|    time_elapsed       | 4422     |\n",
            "|    total_timesteps    | 4864000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.318   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 121599   |\n",
            "|    policy_loss        | 0.104    |\n",
            "|    value_loss         | 1.67     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 337      |\n",
            "|    ep_rew_mean        | 53.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 121700   |\n",
            "|    time_elapsed       | 4425     |\n",
            "|    total_timesteps    | 4868000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.281   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 121699   |\n",
            "|    policy_loss        | 0.177    |\n",
            "|    value_loss         | 6.97     |\n",
            "------------------------------------\n",
            "Num timesteps: 4872000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 45.10\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 331      |\n",
            "|    ep_rew_mean        | 45.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 121800   |\n",
            "|    time_elapsed       | 4427     |\n",
            "|    total_timesteps    | 4872000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.226   |\n",
            "|    explained_variance | 0.969    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 121799   |\n",
            "|    policy_loss        | 0.193    |\n",
            "|    value_loss         | 61.4     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 333      |\n",
            "|    ep_rew_mean        | 53.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 121900   |\n",
            "|    time_elapsed       | 4430     |\n",
            "|    total_timesteps    | 4876000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.412   |\n",
            "|    explained_variance | 0.894    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 121899   |\n",
            "|    policy_loss        | 0.374    |\n",
            "|    value_loss         | 328      |\n",
            "------------------------------------\n",
            "Num timesteps: 4880000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 59.97\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 325      |\n",
            "|    ep_rew_mean        | 60       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 122000   |\n",
            "|    time_elapsed       | 4432     |\n",
            "|    total_timesteps    | 4880000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.253   |\n",
            "|    explained_variance | 0.931    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 121999   |\n",
            "|    policy_loss        | -0.033   |\n",
            "|    value_loss         | 108      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 319      |\n",
            "|    ep_rew_mean        | 64.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 122100   |\n",
            "|    time_elapsed       | 4434     |\n",
            "|    total_timesteps    | 4884000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.419   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 122099   |\n",
            "|    policy_loss        | 0.193    |\n",
            "|    value_loss         | 1.74     |\n",
            "------------------------------------\n",
            "Num timesteps: 4888000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 73.53\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 328      |\n",
            "|    ep_rew_mean        | 73.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 122200   |\n",
            "|    time_elapsed       | 4437     |\n",
            "|    total_timesteps    | 4888000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.279   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 122199   |\n",
            "|    policy_loss        | -0.349   |\n",
            "|    value_loss         | 3.93     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 330      |\n",
            "|    ep_rew_mean        | 68.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 122300   |\n",
            "|    time_elapsed       | 4439     |\n",
            "|    total_timesteps    | 4892000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.319   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 122299   |\n",
            "|    policy_loss        | 0.194    |\n",
            "|    value_loss         | 1.9      |\n",
            "------------------------------------\n",
            "Num timesteps: 4896000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 66.70\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 328      |\n",
            "|    ep_rew_mean        | 66.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 122400   |\n",
            "|    time_elapsed       | 4442     |\n",
            "|    total_timesteps    | 4896000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.255   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 122399   |\n",
            "|    policy_loss        | 0.676    |\n",
            "|    value_loss         | 5.1      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 334      |\n",
            "|    ep_rew_mean        | 61.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 122500   |\n",
            "|    time_elapsed       | 4445     |\n",
            "|    total_timesteps    | 4900000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.31    |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 122499   |\n",
            "|    policy_loss        | -0.0874  |\n",
            "|    value_loss         | 4.01     |\n",
            "------------------------------------\n",
            "Num timesteps: 4904000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 63.05\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 336      |\n",
            "|    ep_rew_mean        | 63.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 122600   |\n",
            "|    time_elapsed       | 4447     |\n",
            "|    total_timesteps    | 4904000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.22    |\n",
            "|    explained_variance | 0.694    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 122599   |\n",
            "|    policy_loss        | 0.714    |\n",
            "|    value_loss         | 389      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 335      |\n",
            "|    ep_rew_mean        | 58.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 122700   |\n",
            "|    time_elapsed       | 4450     |\n",
            "|    total_timesteps    | 4908000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.165   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 122699   |\n",
            "|    policy_loss        | 0.0394   |\n",
            "|    value_loss         | 7.24     |\n",
            "------------------------------------\n",
            "Num timesteps: 4912000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 61.71\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 339      |\n",
            "|    ep_rew_mean        | 61.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 122800   |\n",
            "|    time_elapsed       | 4453     |\n",
            "|    total_timesteps    | 4912000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.344   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 122799   |\n",
            "|    policy_loss        | 0.0125   |\n",
            "|    value_loss         | 4.94     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 349      |\n",
            "|    ep_rew_mean        | 56.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1103     |\n",
            "|    iterations         | 122900   |\n",
            "|    time_elapsed       | 4456     |\n",
            "|    total_timesteps    | 4916000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.252   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 122899   |\n",
            "|    policy_loss        | -0.0838  |\n",
            "|    value_loss         | 2.51     |\n",
            "------------------------------------\n",
            "Num timesteps: 4920000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 49.29\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 362      |\n",
            "|    ep_rew_mean        | 49.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1103     |\n",
            "|    iterations         | 123000   |\n",
            "|    time_elapsed       | 4458     |\n",
            "|    total_timesteps    | 4920000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.283   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 122999   |\n",
            "|    policy_loss        | -0.0676  |\n",
            "|    value_loss         | 0.67     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 346      |\n",
            "|    ep_rew_mean        | 46.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1103     |\n",
            "|    iterations         | 123100   |\n",
            "|    time_elapsed       | 4461     |\n",
            "|    total_timesteps    | 4924000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.25    |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 123099   |\n",
            "|    policy_loss        | -0.271   |\n",
            "|    value_loss         | 1.44     |\n",
            "------------------------------------\n",
            "Num timesteps: 4928000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 52.95\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 340      |\n",
            "|    ep_rew_mean        | 53       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1103     |\n",
            "|    iterations         | 123200   |\n",
            "|    time_elapsed       | 4464     |\n",
            "|    total_timesteps    | 4928000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.326   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 123199   |\n",
            "|    policy_loss        | -0.293   |\n",
            "|    value_loss         | 2.4      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 352      |\n",
            "|    ep_rew_mean        | 54.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1104     |\n",
            "|    iterations         | 123300   |\n",
            "|    time_elapsed       | 4467     |\n",
            "|    total_timesteps    | 4932000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.357   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 123299   |\n",
            "|    policy_loss        | -0.234   |\n",
            "|    value_loss         | 2.14     |\n",
            "------------------------------------\n",
            "Num timesteps: 4936000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 55.47\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 357      |\n",
            "|    ep_rew_mean        | 55.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1104     |\n",
            "|    iterations         | 123400   |\n",
            "|    time_elapsed       | 4470     |\n",
            "|    total_timesteps    | 4936000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.335   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 123399   |\n",
            "|    policy_loss        | 0.0131   |\n",
            "|    value_loss         | 3.46     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 361      |\n",
            "|    ep_rew_mean        | 55.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1104     |\n",
            "|    iterations         | 123500   |\n",
            "|    time_elapsed       | 4472     |\n",
            "|    total_timesteps    | 4940000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.314   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 123499   |\n",
            "|    policy_loss        | 0.0803   |\n",
            "|    value_loss         | 2.92     |\n",
            "------------------------------------\n",
            "Num timesteps: 4944000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 57.40\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 368      |\n",
            "|    ep_rew_mean        | 57.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1104     |\n",
            "|    iterations         | 123600   |\n",
            "|    time_elapsed       | 4475     |\n",
            "|    total_timesteps    | 4944000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.307   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 123599   |\n",
            "|    policy_loss        | -0.0732  |\n",
            "|    value_loss         | 3.91     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 356      |\n",
            "|    ep_rew_mean        | 42.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1104     |\n",
            "|    iterations         | 123700   |\n",
            "|    time_elapsed       | 4478     |\n",
            "|    total_timesteps    | 4948000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.209   |\n",
            "|    explained_variance | 0.629    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 123699   |\n",
            "|    policy_loss        | 0.186    |\n",
            "|    value_loss         | 676      |\n",
            "------------------------------------\n",
            "Num timesteps: 4952000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 40.45\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 363      |\n",
            "|    ep_rew_mean        | 40.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1105     |\n",
            "|    iterations         | 123800   |\n",
            "|    time_elapsed       | 4481     |\n",
            "|    total_timesteps    | 4952000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.379   |\n",
            "|    explained_variance | 0.932    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 123799   |\n",
            "|    policy_loss        | -0.0148  |\n",
            "|    value_loss         | 23.7     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 349      |\n",
            "|    ep_rew_mean        | 37.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1105     |\n",
            "|    iterations         | 123900   |\n",
            "|    time_elapsed       | 4483     |\n",
            "|    total_timesteps    | 4956000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.349   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 123899   |\n",
            "|    policy_loss        | -0.0758  |\n",
            "|    value_loss         | 4.59     |\n",
            "------------------------------------\n",
            "Num timesteps: 4960000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 32.93\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 353      |\n",
            "|    ep_rew_mean        | 32.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1105     |\n",
            "|    iterations         | 124000   |\n",
            "|    time_elapsed       | 4486     |\n",
            "|    total_timesteps    | 4960000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.248   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 123999   |\n",
            "|    policy_loss        | 0.225    |\n",
            "|    value_loss         | 3.86     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 348      |\n",
            "|    ep_rew_mean        | 38.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1105     |\n",
            "|    iterations         | 124100   |\n",
            "|    time_elapsed       | 4488     |\n",
            "|    total_timesteps    | 4964000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.403   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 124099   |\n",
            "|    policy_loss        | 0.395    |\n",
            "|    value_loss         | 6.9      |\n",
            "------------------------------------\n",
            "Num timesteps: 4968000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 41.65\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 332      |\n",
            "|    ep_rew_mean        | 41.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1106     |\n",
            "|    iterations         | 124200   |\n",
            "|    time_elapsed       | 4490     |\n",
            "|    total_timesteps    | 4968000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.276   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 124199   |\n",
            "|    policy_loss        | 0.2      |\n",
            "|    value_loss         | 7.85     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 309      |\n",
            "|    ep_rew_mean        | 41.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1106     |\n",
            "|    iterations         | 124300   |\n",
            "|    time_elapsed       | 4493     |\n",
            "|    total_timesteps    | 4972000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.253   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 124299   |\n",
            "|    policy_loss        | -0.0876  |\n",
            "|    value_loss         | 9.13     |\n",
            "------------------------------------\n",
            "Num timesteps: 4976000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 43.25\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 297      |\n",
            "|    ep_rew_mean        | 43.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1106     |\n",
            "|    iterations         | 124400   |\n",
            "|    time_elapsed       | 4495     |\n",
            "|    total_timesteps    | 4976000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.271   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 124399   |\n",
            "|    policy_loss        | 0.123    |\n",
            "|    value_loss         | 3.97     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 298      |\n",
            "|    ep_rew_mean        | 34.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1107     |\n",
            "|    iterations         | 124500   |\n",
            "|    time_elapsed       | 4497     |\n",
            "|    total_timesteps    | 4980000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.263   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 124499   |\n",
            "|    policy_loss        | -0.143   |\n",
            "|    value_loss         | 2.67     |\n",
            "------------------------------------\n",
            "Num timesteps: 4984000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 32.27\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 298      |\n",
            "|    ep_rew_mean        | 32.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1107     |\n",
            "|    iterations         | 124600   |\n",
            "|    time_elapsed       | 4500     |\n",
            "|    total_timesteps    | 4984000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.332   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 124599   |\n",
            "|    policy_loss        | -0.00499 |\n",
            "|    value_loss         | 1.54     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 296      |\n",
            "|    ep_rew_mean        | 38.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1107     |\n",
            "|    iterations         | 124700   |\n",
            "|    time_elapsed       | 4503     |\n",
            "|    total_timesteps    | 4988000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.327   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 124699   |\n",
            "|    policy_loss        | -0.115   |\n",
            "|    value_loss         | 2.56     |\n",
            "------------------------------------\n",
            "Num timesteps: 4992000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 41.53\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 311      |\n",
            "|    ep_rew_mean        | 41.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1107     |\n",
            "|    iterations         | 124800   |\n",
            "|    time_elapsed       | 4506     |\n",
            "|    total_timesteps    | 4992000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.191   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 124799   |\n",
            "|    policy_loss        | -0.0212  |\n",
            "|    value_loss         | 7.06     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 323      |\n",
            "|    ep_rew_mean        | 42.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1107     |\n",
            "|    iterations         | 124900   |\n",
            "|    time_elapsed       | 4509     |\n",
            "|    total_timesteps    | 4996000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.364   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 124899   |\n",
            "|    policy_loss        | -0.0353  |\n",
            "|    value_loss         | 2.62     |\n",
            "------------------------------------\n",
            "Num timesteps: 5000000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 50.82\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 332      |\n",
            "|    ep_rew_mean        | 50.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1108     |\n",
            "|    iterations         | 125000   |\n",
            "|    time_elapsed       | 4512     |\n",
            "|    total_timesteps    | 5000000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.305   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 124999   |\n",
            "|    policy_loss        | 0.0471   |\n",
            "|    value_loss         | 4.93     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 334      |\n",
            "|    ep_rew_mean        | 46.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1108     |\n",
            "|    iterations         | 125100   |\n",
            "|    time_elapsed       | 4514     |\n",
            "|    total_timesteps    | 5004000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.417   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 125099   |\n",
            "|    policy_loss        | -0.153   |\n",
            "|    value_loss         | 2.18     |\n",
            "------------------------------------\n",
            "Num timesteps: 5008000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 47.72\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 334      |\n",
            "|    ep_rew_mean        | 47.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1108     |\n",
            "|    iterations         | 125200   |\n",
            "|    time_elapsed       | 4516     |\n",
            "|    total_timesteps    | 5008000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.306   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 125199   |\n",
            "|    policy_loss        | -0.0598  |\n",
            "|    value_loss         | 1.03     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 337      |\n",
            "|    ep_rew_mean        | 45.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 125300   |\n",
            "|    time_elapsed       | 4518     |\n",
            "|    total_timesteps    | 5012000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.264   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 125299   |\n",
            "|    policy_loss        | 0.0351   |\n",
            "|    value_loss         | 2.82     |\n",
            "------------------------------------\n",
            "Num timesteps: 5016000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 55.21\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 328      |\n",
            "|    ep_rew_mean        | 55.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 125400   |\n",
            "|    time_elapsed       | 4521     |\n",
            "|    total_timesteps    | 5016000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.313   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 125399   |\n",
            "|    policy_loss        | 0.117    |\n",
            "|    value_loss         | 2.82     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 342      |\n",
            "|    ep_rew_mean        | 62.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 125500   |\n",
            "|    time_elapsed       | 4524     |\n",
            "|    total_timesteps    | 5020000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.369   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 125499   |\n",
            "|    policy_loss        | -0.261   |\n",
            "|    value_loss         | 1.03     |\n",
            "------------------------------------\n",
            "Num timesteps: 5024000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 72.71\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 338      |\n",
            "|    ep_rew_mean        | 72.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 125600   |\n",
            "|    time_elapsed       | 4526     |\n",
            "|    total_timesteps    | 5024000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.201   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 125599   |\n",
            "|    policy_loss        | -0.142   |\n",
            "|    value_loss         | 1.69     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 323      |\n",
            "|    ep_rew_mean        | 78.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1110     |\n",
            "|    iterations         | 125700   |\n",
            "|    time_elapsed       | 4528     |\n",
            "|    total_timesteps    | 5028000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.206   |\n",
            "|    explained_variance | 0.928    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 125699   |\n",
            "|    policy_loss        | 2.8      |\n",
            "|    value_loss         | 183      |\n",
            "------------------------------------\n",
            "Num timesteps: 5032000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 83.25\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 309      |\n",
            "|    ep_rew_mean        | 83.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1110     |\n",
            "|    iterations         | 125800   |\n",
            "|    time_elapsed       | 4530     |\n",
            "|    total_timesteps    | 5032000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.276   |\n",
            "|    explained_variance | 0.902    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 125799   |\n",
            "|    policy_loss        | 0.0392   |\n",
            "|    value_loss         | 63.9     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 308      |\n",
            "|    ep_rew_mean        | 88.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1110     |\n",
            "|    iterations         | 125900   |\n",
            "|    time_elapsed       | 4532     |\n",
            "|    total_timesteps    | 5036000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.314   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 125899   |\n",
            "|    policy_loss        | -0.092   |\n",
            "|    value_loss         | 4.11     |\n",
            "------------------------------------\n",
            "Num timesteps: 5040000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 101.32\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 308      |\n",
            "|    ep_rew_mean        | 101      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1111     |\n",
            "|    iterations         | 126000   |\n",
            "|    time_elapsed       | 4535     |\n",
            "|    total_timesteps    | 5040000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.408   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 125999   |\n",
            "|    policy_loss        | -0.156   |\n",
            "|    value_loss         | 2.94     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 309      |\n",
            "|    ep_rew_mean        | 110      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1111     |\n",
            "|    iterations         | 126100   |\n",
            "|    time_elapsed       | 4537     |\n",
            "|    total_timesteps    | 5044000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.304   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 126099   |\n",
            "|    policy_loss        | -0.0917  |\n",
            "|    value_loss         | 2.02     |\n",
            "------------------------------------\n",
            "Num timesteps: 5048000\n",
            "Best mean reward: 109.65 - Last mean reward per episode: 115.69\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 309      |\n",
            "|    ep_rew_mean        | 116      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1111     |\n",
            "|    iterations         | 126200   |\n",
            "|    time_elapsed       | 4539     |\n",
            "|    total_timesteps    | 5048000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.398   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 126199   |\n",
            "|    policy_loss        | -0.0489  |\n",
            "|    value_loss         | 3.32     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 297      |\n",
            "|    ep_rew_mean        | 104      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1112     |\n",
            "|    iterations         | 126300   |\n",
            "|    time_elapsed       | 4542     |\n",
            "|    total_timesteps    | 5052000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.259   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 126299   |\n",
            "|    policy_loss        | -0.116   |\n",
            "|    value_loss         | 1.48     |\n",
            "------------------------------------\n",
            "Num timesteps: 5056000\n",
            "Best mean reward: 115.69 - Last mean reward per episode: 98.52\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 291      |\n",
            "|    ep_rew_mean        | 98.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1112     |\n",
            "|    iterations         | 126400   |\n",
            "|    time_elapsed       | 4544     |\n",
            "|    total_timesteps    | 5056000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.275   |\n",
            "|    explained_variance | 0.977    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 126399   |\n",
            "|    policy_loss        | -0.113   |\n",
            "|    value_loss         | 11.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 304      |\n",
            "|    ep_rew_mean        | 98.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1112     |\n",
            "|    iterations         | 126500   |\n",
            "|    time_elapsed       | 4546     |\n",
            "|    total_timesteps    | 5060000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.442   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 126499   |\n",
            "|    policy_loss        | -0.672   |\n",
            "|    value_loss         | 11.9     |\n",
            "------------------------------------\n",
            "Num timesteps: 5064000\n",
            "Best mean reward: 115.69 - Last mean reward per episode: 79.70\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 304      |\n",
            "|    ep_rew_mean        | 79.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1113     |\n",
            "|    iterations         | 126600   |\n",
            "|    time_elapsed       | 4549     |\n",
            "|    total_timesteps    | 5064000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.415   |\n",
            "|    explained_variance | 0.814    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 126599   |\n",
            "|    policy_loss        | -0.158   |\n",
            "|    value_loss         | 52.4     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 319      |\n",
            "|    ep_rew_mean        | 81.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1113     |\n",
            "|    iterations         | 126700   |\n",
            "|    time_elapsed       | 4552     |\n",
            "|    total_timesteps    | 5068000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.321   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 126699   |\n",
            "|    policy_loss        | -0.00726 |\n",
            "|    value_loss         | 1.61     |\n",
            "------------------------------------\n",
            "Num timesteps: 5072000\n",
            "Best mean reward: 115.69 - Last mean reward per episode: 66.18\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 325      |\n",
            "|    ep_rew_mean        | 66.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1113     |\n",
            "|    iterations         | 126800   |\n",
            "|    time_elapsed       | 4555     |\n",
            "|    total_timesteps    | 5072000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.421   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 126799   |\n",
            "|    policy_loss        | -0.0539  |\n",
            "|    value_loss         | 1.83     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 323      |\n",
            "|    ep_rew_mean        | 62.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1113     |\n",
            "|    iterations         | 126900   |\n",
            "|    time_elapsed       | 4557     |\n",
            "|    total_timesteps    | 5076000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.408   |\n",
            "|    explained_variance | 0.969    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 126899   |\n",
            "|    policy_loss        | 0.562    |\n",
            "|    value_loss         | 16.2     |\n",
            "------------------------------------\n",
            "Num timesteps: 5080000\n",
            "Best mean reward: 115.69 - Last mean reward per episode: 69.29\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 325      |\n",
            "|    ep_rew_mean        | 69.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1114     |\n",
            "|    iterations         | 127000   |\n",
            "|    time_elapsed       | 4559     |\n",
            "|    total_timesteps    | 5080000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.253   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 126999   |\n",
            "|    policy_loss        | -0.319   |\n",
            "|    value_loss         | 4.62     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 323      |\n",
            "|    ep_rew_mean        | 62.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1114     |\n",
            "|    iterations         | 127100   |\n",
            "|    time_elapsed       | 4562     |\n",
            "|    total_timesteps    | 5084000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.275   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 127099   |\n",
            "|    policy_loss        | -0.0534  |\n",
            "|    value_loss         | 1.87     |\n",
            "------------------------------------\n",
            "Num timesteps: 5088000\n",
            "Best mean reward: 115.69 - Last mean reward per episode: 57.65\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 325      |\n",
            "|    ep_rew_mean        | 57.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1114     |\n",
            "|    iterations         | 127200   |\n",
            "|    time_elapsed       | 4564     |\n",
            "|    total_timesteps    | 5088000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.252   |\n",
            "|    explained_variance | 0.897    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 127199   |\n",
            "|    policy_loss        | -3.36    |\n",
            "|    value_loss         | 193      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 329      |\n",
            "|    ep_rew_mean        | 65.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1114     |\n",
            "|    iterations         | 127300   |\n",
            "|    time_elapsed       | 4567     |\n",
            "|    total_timesteps    | 5092000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.319   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 127299   |\n",
            "|    policy_loss        | -0.294   |\n",
            "|    value_loss         | 2.97     |\n",
            "------------------------------------\n",
            "Num timesteps: 5096000\n",
            "Best mean reward: 115.69 - Last mean reward per episode: 77.15\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 331      |\n",
            "|    ep_rew_mean        | 77.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1115     |\n",
            "|    iterations         | 127400   |\n",
            "|    time_elapsed       | 4569     |\n",
            "|    total_timesteps    | 5096000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.478   |\n",
            "|    explained_variance | 0.952    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 127399   |\n",
            "|    policy_loss        | -0.328   |\n",
            "|    value_loss         | 44.8     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 318      |\n",
            "|    ep_rew_mean        | 85.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1115     |\n",
            "|    iterations         | 127500   |\n",
            "|    time_elapsed       | 4571     |\n",
            "|    total_timesteps    | 5100000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.177   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 127499   |\n",
            "|    policy_loss        | 1.02     |\n",
            "|    value_loss         | 11.4     |\n",
            "------------------------------------\n",
            "Num timesteps: 5104000\n",
            "Best mean reward: 115.69 - Last mean reward per episode: 98.87\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 314      |\n",
            "|    ep_rew_mean        | 98.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1115     |\n",
            "|    iterations         | 127600   |\n",
            "|    time_elapsed       | 4574     |\n",
            "|    total_timesteps    | 5104000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.489   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 127599   |\n",
            "|    policy_loss        | -0.00969 |\n",
            "|    value_loss         | 1.7      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 314      |\n",
            "|    ep_rew_mean        | 99.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1116     |\n",
            "|    iterations         | 127700   |\n",
            "|    time_elapsed       | 4576     |\n",
            "|    total_timesteps    | 5108000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.306   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 127699   |\n",
            "|    policy_loss        | 0.361    |\n",
            "|    value_loss         | 3.82     |\n",
            "------------------------------------\n",
            "Num timesteps: 5112000\n",
            "Best mean reward: 115.69 - Last mean reward per episode: 86.83\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 313      |\n",
            "|    ep_rew_mean        | 86.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1116     |\n",
            "|    iterations         | 127800   |\n",
            "|    time_elapsed       | 4578     |\n",
            "|    total_timesteps    | 5112000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.358   |\n",
            "|    explained_variance | 0.973    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 127799   |\n",
            "|    policy_loss        | 0.228    |\n",
            "|    value_loss         | 13.4     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 312      |\n",
            "|    ep_rew_mean        | 81.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1116     |\n",
            "|    iterations         | 127900   |\n",
            "|    time_elapsed       | 4582     |\n",
            "|    total_timesteps    | 5116000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.296   |\n",
            "|    explained_variance | 0.961    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 127899   |\n",
            "|    policy_loss        | 0.171    |\n",
            "|    value_loss         | 8.57     |\n",
            "------------------------------------\n",
            "Num timesteps: 5120000\n",
            "Best mean reward: 115.69 - Last mean reward per episode: 85.15\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 320      |\n",
            "|    ep_rew_mean        | 85.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1116     |\n",
            "|    iterations         | 128000   |\n",
            "|    time_elapsed       | 4585     |\n",
            "|    total_timesteps    | 5120000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.344   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 127999   |\n",
            "|    policy_loss        | 0.399    |\n",
            "|    value_loss         | 1.84     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 317      |\n",
            "|    ep_rew_mean        | 63.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1116     |\n",
            "|    iterations         | 128100   |\n",
            "|    time_elapsed       | 4587     |\n",
            "|    total_timesteps    | 5124000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.328   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 128099   |\n",
            "|    policy_loss        | -0.365   |\n",
            "|    value_loss         | 3.42     |\n",
            "------------------------------------\n",
            "Num timesteps: 5128000\n",
            "Best mean reward: 115.69 - Last mean reward per episode: 49.10\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 313      |\n",
            "|    ep_rew_mean        | 49.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 128200   |\n",
            "|    time_elapsed       | 4589     |\n",
            "|    total_timesteps    | 5128000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.462   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 128199   |\n",
            "|    policy_loss        | -0.679   |\n",
            "|    value_loss         | 7.68     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 311      |\n",
            "|    ep_rew_mean        | 43.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 128300   |\n",
            "|    time_elapsed       | 4592     |\n",
            "|    total_timesteps    | 5132000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.305   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 128299   |\n",
            "|    policy_loss        | -0.139   |\n",
            "|    value_loss         | 1.29     |\n",
            "------------------------------------\n",
            "Num timesteps: 5136000\n",
            "Best mean reward: 115.69 - Last mean reward per episode: 39.38\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 313      |\n",
            "|    ep_rew_mean        | 39.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 128400   |\n",
            "|    time_elapsed       | 4594     |\n",
            "|    total_timesteps    | 5136000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.179   |\n",
            "|    explained_variance | 0.821    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 128399   |\n",
            "|    policy_loss        | 0.243    |\n",
            "|    value_loss         | 261      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 323      |\n",
            "|    ep_rew_mean        | 35.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 128500   |\n",
            "|    time_elapsed       | 4597     |\n",
            "|    total_timesteps    | 5140000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.334   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 128499   |\n",
            "|    policy_loss        | -0.0408  |\n",
            "|    value_loss         | 3.44     |\n",
            "------------------------------------\n",
            "Num timesteps: 5144000\n",
            "Best mean reward: 115.69 - Last mean reward per episode: 47.13\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 327      |\n",
            "|    ep_rew_mean        | 47.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1118     |\n",
            "|    iterations         | 128600   |\n",
            "|    time_elapsed       | 4600     |\n",
            "|    total_timesteps    | 5144000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.339   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 128599   |\n",
            "|    policy_loss        | -0.0949  |\n",
            "|    value_loss         | 2.18     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 327      |\n",
            "|    ep_rew_mean        | 70.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1118     |\n",
            "|    iterations         | 128700   |\n",
            "|    time_elapsed       | 4603     |\n",
            "|    total_timesteps    | 5148000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.348   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 128699   |\n",
            "|    policy_loss        | -0.181   |\n",
            "|    value_loss         | 17.6     |\n",
            "------------------------------------\n",
            "Num timesteps: 5152000\n",
            "Best mean reward: 115.69 - Last mean reward per episode: 71.38\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 323      |\n",
            "|    ep_rew_mean        | 71.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1118     |\n",
            "|    iterations         | 128800   |\n",
            "|    time_elapsed       | 4605     |\n",
            "|    total_timesteps    | 5152000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.4     |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 128799   |\n",
            "|    policy_loss        | 0.254    |\n",
            "|    value_loss         | 4.15     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 316      |\n",
            "|    ep_rew_mean        | 82.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1118     |\n",
            "|    iterations         | 128900   |\n",
            "|    time_elapsed       | 4608     |\n",
            "|    total_timesteps    | 5156000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.354   |\n",
            "|    explained_variance | 0.695    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 128899   |\n",
            "|    policy_loss        | -0.25    |\n",
            "|    value_loss         | 416      |\n",
            "------------------------------------\n",
            "Num timesteps: 5160000\n",
            "Best mean reward: 115.69 - Last mean reward per episode: 89.52\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 318      |\n",
            "|    ep_rew_mean        | 89.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1119     |\n",
            "|    iterations         | 129000   |\n",
            "|    time_elapsed       | 4610     |\n",
            "|    total_timesteps    | 5160000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.313   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 128999   |\n",
            "|    policy_loss        | 0.157    |\n",
            "|    value_loss         | 19.7     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 314      |\n",
            "|    ep_rew_mean        | 88.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1119     |\n",
            "|    iterations         | 129100   |\n",
            "|    time_elapsed       | 4612     |\n",
            "|    total_timesteps    | 5164000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.401   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 129099   |\n",
            "|    policy_loss        | 0.0793   |\n",
            "|    value_loss         | 4.4      |\n",
            "------------------------------------\n",
            "Num timesteps: 5168000\n",
            "Best mean reward: 115.69 - Last mean reward per episode: 78.68\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 302      |\n",
            "|    ep_rew_mean        | 78.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1119     |\n",
            "|    iterations         | 129200   |\n",
            "|    time_elapsed       | 4614     |\n",
            "|    total_timesteps    | 5168000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.377   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 129199   |\n",
            "|    policy_loss        | 0.0837   |\n",
            "|    value_loss         | 2.9      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 297      |\n",
            "|    ep_rew_mean        | 77.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1120     |\n",
            "|    iterations         | 129300   |\n",
            "|    time_elapsed       | 4617     |\n",
            "|    total_timesteps    | 5172000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.237   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 129299   |\n",
            "|    policy_loss        | -0.00192 |\n",
            "|    value_loss         | 2.02     |\n",
            "------------------------------------\n",
            "Num timesteps: 5176000\n",
            "Best mean reward: 115.69 - Last mean reward per episode: 68.58\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 281      |\n",
            "|    ep_rew_mean        | 68.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1120     |\n",
            "|    iterations         | 129400   |\n",
            "|    time_elapsed       | 4619     |\n",
            "|    total_timesteps    | 5176000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.322   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 129399   |\n",
            "|    policy_loss        | -0.106   |\n",
            "|    value_loss         | 2.57     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 279      |\n",
            "|    ep_rew_mean        | 66.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1120     |\n",
            "|    iterations         | 129500   |\n",
            "|    time_elapsed       | 4621     |\n",
            "|    total_timesteps    | 5180000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.384   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 129499   |\n",
            "|    policy_loss        | -0.395   |\n",
            "|    value_loss         | 7.98     |\n",
            "------------------------------------\n",
            "Num timesteps: 5184000\n",
            "Best mean reward: 115.69 - Last mean reward per episode: 71.63\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 278      |\n",
            "|    ep_rew_mean        | 71.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1121     |\n",
            "|    iterations         | 129600   |\n",
            "|    time_elapsed       | 4623     |\n",
            "|    total_timesteps    | 5184000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.435   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 129599   |\n",
            "|    policy_loss        | -0.1     |\n",
            "|    value_loss         | 1.18     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 287      |\n",
            "|    ep_rew_mean        | 85.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1121     |\n",
            "|    iterations         | 129700   |\n",
            "|    time_elapsed       | 4625     |\n",
            "|    total_timesteps    | 5188000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.352   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 129699   |\n",
            "|    policy_loss        | 0.0932   |\n",
            "|    value_loss         | 6.92     |\n",
            "------------------------------------\n",
            "Num timesteps: 5192000\n",
            "Best mean reward: 115.69 - Last mean reward per episode: 91.91\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 289      |\n",
            "|    ep_rew_mean        | 91.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1121     |\n",
            "|    iterations         | 129800   |\n",
            "|    time_elapsed       | 4627     |\n",
            "|    total_timesteps    | 5192000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.244   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 129799   |\n",
            "|    policy_loss        | 0.065    |\n",
            "|    value_loss         | 10.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 288      |\n",
            "|    ep_rew_mean        | 93.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1122     |\n",
            "|    iterations         | 129900   |\n",
            "|    time_elapsed       | 4629     |\n",
            "|    total_timesteps    | 5196000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.374   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 129899   |\n",
            "|    policy_loss        | -0.072   |\n",
            "|    value_loss         | 4.35     |\n",
            "------------------------------------\n",
            "Num timesteps: 5200000\n",
            "Best mean reward: 115.69 - Last mean reward per episode: 97.41\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 280      |\n",
            "|    ep_rew_mean        | 97.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1122     |\n",
            "|    iterations         | 130000   |\n",
            "|    time_elapsed       | 4631     |\n",
            "|    total_timesteps    | 5200000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.355   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 129999   |\n",
            "|    policy_loss        | -0.0489  |\n",
            "|    value_loss         | 0.941    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 278      |\n",
            "|    ep_rew_mean        | 91.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1123     |\n",
            "|    iterations         | 130100   |\n",
            "|    time_elapsed       | 4633     |\n",
            "|    total_timesteps    | 5204000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.385   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 130099   |\n",
            "|    policy_loss        | 0.277    |\n",
            "|    value_loss         | 27.2     |\n",
            "------------------------------------\n",
            "Num timesteps: 5208000\n",
            "Best mean reward: 115.69 - Last mean reward per episode: 93.35\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 276      |\n",
            "|    ep_rew_mean        | 93.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1123     |\n",
            "|    iterations         | 130200   |\n",
            "|    time_elapsed       | 4635     |\n",
            "|    total_timesteps    | 5208000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.263   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 130199   |\n",
            "|    policy_loss        | -0.295   |\n",
            "|    value_loss         | 3.05     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 277      |\n",
            "|    ep_rew_mean        | 91.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1123     |\n",
            "|    iterations         | 130300   |\n",
            "|    time_elapsed       | 4638     |\n",
            "|    total_timesteps    | 5212000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.324   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 130299   |\n",
            "|    policy_loss        | 0.0105   |\n",
            "|    value_loss         | 14.7     |\n",
            "------------------------------------\n",
            "Num timesteps: 5216000\n",
            "Best mean reward: 115.69 - Last mean reward per episode: 92.94\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 266      |\n",
            "|    ep_rew_mean        | 92.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1124     |\n",
            "|    iterations         | 130400   |\n",
            "|    time_elapsed       | 4640     |\n",
            "|    total_timesteps    | 5216000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.292   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 130399   |\n",
            "|    policy_loss        | 0.309    |\n",
            "|    value_loss         | 10.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 263      |\n",
            "|    ep_rew_mean        | 98.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1124     |\n",
            "|    iterations         | 130500   |\n",
            "|    time_elapsed       | 4642     |\n",
            "|    total_timesteps    | 5220000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.402   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 130499   |\n",
            "|    policy_loss        | -0.138   |\n",
            "|    value_loss         | 1.8      |\n",
            "------------------------------------\n",
            "Num timesteps: 5224000\n",
            "Best mean reward: 115.69 - Last mean reward per episode: 93.68\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 276      |\n",
            "|    ep_rew_mean        | 93.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1124     |\n",
            "|    iterations         | 130600   |\n",
            "|    time_elapsed       | 4645     |\n",
            "|    total_timesteps    | 5224000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.335   |\n",
            "|    explained_variance | 0.553    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 130599   |\n",
            "|    policy_loss        | -0.0781  |\n",
            "|    value_loss         | 313      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 290      |\n",
            "|    ep_rew_mean        | 82.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1124     |\n",
            "|    iterations         | 130700   |\n",
            "|    time_elapsed       | 4647     |\n",
            "|    total_timesteps    | 5228000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.29    |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 130699   |\n",
            "|    policy_loss        | 0.119    |\n",
            "|    value_loss         | 33.5     |\n",
            "------------------------------------\n",
            "Num timesteps: 5232000\n",
            "Best mean reward: 115.69 - Last mean reward per episode: 69.53\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 287      |\n",
            "|    ep_rew_mean        | 69.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1125     |\n",
            "|    iterations         | 130800   |\n",
            "|    time_elapsed       | 4649     |\n",
            "|    total_timesteps    | 5232000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.271   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 130799   |\n",
            "|    policy_loss        | -0.485   |\n",
            "|    value_loss         | 5.03     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 280      |\n",
            "|    ep_rew_mean        | 63.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1125     |\n",
            "|    iterations         | 130900   |\n",
            "|    time_elapsed       | 4651     |\n",
            "|    total_timesteps    | 5236000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.352   |\n",
            "|    explained_variance | 0.925    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 130899   |\n",
            "|    policy_loss        | -0.112   |\n",
            "|    value_loss         | 68       |\n",
            "------------------------------------\n",
            "Num timesteps: 5240000\n",
            "Best mean reward: 115.69 - Last mean reward per episode: 68.75\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 274      |\n",
            "|    ep_rew_mean        | 68.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1126     |\n",
            "|    iterations         | 131000   |\n",
            "|    time_elapsed       | 4653     |\n",
            "|    total_timesteps    | 5240000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.27    |\n",
            "|    explained_variance | 0.957    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 130999   |\n",
            "|    policy_loss        | -0.159   |\n",
            "|    value_loss         | 49.9     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 274      |\n",
            "|    ep_rew_mean        | 60.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1126     |\n",
            "|    iterations         | 131100   |\n",
            "|    time_elapsed       | 4655     |\n",
            "|    total_timesteps    | 5244000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.448   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 131099   |\n",
            "|    policy_loss        | -0.0422  |\n",
            "|    value_loss         | 2.65     |\n",
            "------------------------------------\n",
            "Num timesteps: 5248000\n",
            "Best mean reward: 115.69 - Last mean reward per episode: 57.05\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 282      |\n",
            "|    ep_rew_mean        | 57       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1126     |\n",
            "|    iterations         | 131200   |\n",
            "|    time_elapsed       | 4658     |\n",
            "|    total_timesteps    | 5248000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.44    |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 131199   |\n",
            "|    policy_loss        | -0.0414  |\n",
            "|    value_loss         | 3.42     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 268      |\n",
            "|    ep_rew_mean        | 55.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1126     |\n",
            "|    iterations         | 131300   |\n",
            "|    time_elapsed       | 4660     |\n",
            "|    total_timesteps    | 5252000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.342   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 131299   |\n",
            "|    policy_loss        | -0.00642 |\n",
            "|    value_loss         | 3.63     |\n",
            "------------------------------------\n",
            "Num timesteps: 5256000\n",
            "Best mean reward: 115.69 - Last mean reward per episode: 57.34\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 262      |\n",
            "|    ep_rew_mean        | 57.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1127     |\n",
            "|    iterations         | 131400   |\n",
            "|    time_elapsed       | 4662     |\n",
            "|    total_timesteps    | 5256000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.316   |\n",
            "|    explained_variance | 0.703    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 131399   |\n",
            "|    policy_loss        | 0.0988   |\n",
            "|    value_loss         | 779      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 270      |\n",
            "|    ep_rew_mean        | 79.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1127     |\n",
            "|    iterations         | 131500   |\n",
            "|    time_elapsed       | 4664     |\n",
            "|    total_timesteps    | 5260000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.414   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 131499   |\n",
            "|    policy_loss        | 0.0133   |\n",
            "|    value_loss         | 4.93     |\n",
            "------------------------------------\n",
            "Num timesteps: 5264000\n",
            "Best mean reward: 115.69 - Last mean reward per episode: 85.85\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 277      |\n",
            "|    ep_rew_mean        | 85.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1128     |\n",
            "|    iterations         | 131600   |\n",
            "|    time_elapsed       | 4666     |\n",
            "|    total_timesteps    | 5264000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.326   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 131599   |\n",
            "|    policy_loss        | 0.487    |\n",
            "|    value_loss         | 8.73     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 280      |\n",
            "|    ep_rew_mean        | 81.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1128     |\n",
            "|    iterations         | 131700   |\n",
            "|    time_elapsed       | 4668     |\n",
            "|    total_timesteps    | 5268000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.379   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 131699   |\n",
            "|    policy_loss        | -0.0849  |\n",
            "|    value_loss         | 6.27     |\n",
            "------------------------------------\n",
            "Num timesteps: 5272000\n",
            "Best mean reward: 115.69 - Last mean reward per episode: 91.33\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 285      |\n",
            "|    ep_rew_mean        | 91.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1128     |\n",
            "|    iterations         | 131800   |\n",
            "|    time_elapsed       | 4670     |\n",
            "|    total_timesteps    | 5272000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.39    |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 131799   |\n",
            "|    policy_loss        | 0.119    |\n",
            "|    value_loss         | 1.37     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 276      |\n",
            "|    ep_rew_mean        | 87.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1129     |\n",
            "|    iterations         | 131900   |\n",
            "|    time_elapsed       | 4672     |\n",
            "|    total_timesteps    | 5276000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.275   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 131899   |\n",
            "|    policy_loss        | -0.0901  |\n",
            "|    value_loss         | 8.16     |\n",
            "------------------------------------\n",
            "Num timesteps: 5280000\n",
            "Best mean reward: 115.69 - Last mean reward per episode: 86.46\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 272      |\n",
            "|    ep_rew_mean        | 86.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1129     |\n",
            "|    iterations         | 132000   |\n",
            "|    time_elapsed       | 4674     |\n",
            "|    total_timesteps    | 5280000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.289   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 131999   |\n",
            "|    policy_loss        | -0.0924  |\n",
            "|    value_loss         | 0.838    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 278      |\n",
            "|    ep_rew_mean        | 92.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1129     |\n",
            "|    iterations         | 132100   |\n",
            "|    time_elapsed       | 4676     |\n",
            "|    total_timesteps    | 5284000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.313   |\n",
            "|    explained_variance | 0.915    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 132099   |\n",
            "|    policy_loss        | -0.333   |\n",
            "|    value_loss         | 57.8     |\n",
            "------------------------------------\n",
            "Num timesteps: 5288000\n",
            "Best mean reward: 115.69 - Last mean reward per episode: 90.89\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 281      |\n",
            "|    ep_rew_mean        | 90.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1130     |\n",
            "|    iterations         | 132200   |\n",
            "|    time_elapsed       | 4679     |\n",
            "|    total_timesteps    | 5288000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.291   |\n",
            "|    explained_variance | 0.932    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 132199   |\n",
            "|    policy_loss        | -0.304   |\n",
            "|    value_loss         | 97.4     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 282      |\n",
            "|    ep_rew_mean        | 89.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1130     |\n",
            "|    iterations         | 132300   |\n",
            "|    time_elapsed       | 4681     |\n",
            "|    total_timesteps    | 5292000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.316   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 132299   |\n",
            "|    policy_loss        | 0.0802   |\n",
            "|    value_loss         | 3.6      |\n",
            "------------------------------------\n",
            "Num timesteps: 5296000\n",
            "Best mean reward: 115.69 - Last mean reward per episode: 89.23\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 291      |\n",
            "|    ep_rew_mean        | 89.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1130     |\n",
            "|    iterations         | 132400   |\n",
            "|    time_elapsed       | 4683     |\n",
            "|    total_timesteps    | 5296000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.399   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 132399   |\n",
            "|    policy_loss        | 0.0533   |\n",
            "|    value_loss         | 2.42     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 293      |\n",
            "|    ep_rew_mean        | 81.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1131     |\n",
            "|    iterations         | 132500   |\n",
            "|    time_elapsed       | 4686     |\n",
            "|    total_timesteps    | 5300000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.322   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 132499   |\n",
            "|    policy_loss        | 0.00733  |\n",
            "|    value_loss         | 0.981    |\n",
            "------------------------------------\n",
            "Num timesteps: 5304000\n",
            "Best mean reward: 115.69 - Last mean reward per episode: 80.00\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 292      |\n",
            "|    ep_rew_mean        | 80       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1131     |\n",
            "|    iterations         | 132600   |\n",
            "|    time_elapsed       | 4688     |\n",
            "|    total_timesteps    | 5304000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.294   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 132599   |\n",
            "|    policy_loss        | 0.328    |\n",
            "|    value_loss         | 4.1      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 298      |\n",
            "|    ep_rew_mean        | 82.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1131     |\n",
            "|    iterations         | 132700   |\n",
            "|    time_elapsed       | 4690     |\n",
            "|    total_timesteps    | 5308000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.226   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 132699   |\n",
            "|    policy_loss        | 0.0337   |\n",
            "|    value_loss         | 1.65     |\n",
            "------------------------------------\n",
            "Num timesteps: 5312000\n",
            "Best mean reward: 115.69 - Last mean reward per episode: 87.65\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 313      |\n",
            "|    ep_rew_mean        | 87.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1131     |\n",
            "|    iterations         | 132800   |\n",
            "|    time_elapsed       | 4693     |\n",
            "|    total_timesteps    | 5312000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.258   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 132799   |\n",
            "|    policy_loss        | 0.00135  |\n",
            "|    value_loss         | 16.8     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 321      |\n",
            "|    ep_rew_mean        | 84.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1131     |\n",
            "|    iterations         | 132900   |\n",
            "|    time_elapsed       | 4696     |\n",
            "|    total_timesteps    | 5316000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.243   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 132899   |\n",
            "|    policy_loss        | -0.0252  |\n",
            "|    value_loss         | 1.26     |\n",
            "------------------------------------\n",
            "Num timesteps: 5320000\n",
            "Best mean reward: 115.69 - Last mean reward per episode: 97.49\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 326      |\n",
            "|    ep_rew_mean        | 97.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1132     |\n",
            "|    iterations         | 133000   |\n",
            "|    time_elapsed       | 4698     |\n",
            "|    total_timesteps    | 5320000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.401   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 132999   |\n",
            "|    policy_loss        | -0.166   |\n",
            "|    value_loss         | 1.68     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 338      |\n",
            "|    ep_rew_mean        | 97.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1132     |\n",
            "|    iterations         | 133100   |\n",
            "|    time_elapsed       | 4701     |\n",
            "|    total_timesteps    | 5324000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.29    |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 133099   |\n",
            "|    policy_loss        | 0.178    |\n",
            "|    value_loss         | 2.31     |\n",
            "------------------------------------\n",
            "Num timesteps: 5328000\n",
            "Best mean reward: 115.69 - Last mean reward per episode: 106.41\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 353      |\n",
            "|    ep_rew_mean        | 106      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1132     |\n",
            "|    iterations         | 133200   |\n",
            "|    time_elapsed       | 4703     |\n",
            "|    total_timesteps    | 5328000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.368   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 133199   |\n",
            "|    policy_loss        | 0.336    |\n",
            "|    value_loss         | 2.92     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 348      |\n",
            "|    ep_rew_mean        | 116      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1133     |\n",
            "|    iterations         | 133300   |\n",
            "|    time_elapsed       | 4706     |\n",
            "|    total_timesteps    | 5332000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.294   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 133299   |\n",
            "|    policy_loss        | -0.449   |\n",
            "|    value_loss         | 1.89     |\n",
            "------------------------------------\n",
            "Num timesteps: 5336000\n",
            "Best mean reward: 115.69 - Last mean reward per episode: 116.71\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 358      |\n",
            "|    ep_rew_mean        | 117      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1133     |\n",
            "|    iterations         | 133400   |\n",
            "|    time_elapsed       | 4709     |\n",
            "|    total_timesteps    | 5336000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.353   |\n",
            "|    explained_variance | 0.684    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 133399   |\n",
            "|    policy_loss        | 3.18     |\n",
            "|    value_loss         | 409      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 367      |\n",
            "|    ep_rew_mean        | 119      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1133     |\n",
            "|    iterations         | 133500   |\n",
            "|    time_elapsed       | 4711     |\n",
            "|    total_timesteps    | 5340000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.326   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 133499   |\n",
            "|    policy_loss        | 0.26     |\n",
            "|    value_loss         | 4.2      |\n",
            "------------------------------------\n",
            "Num timesteps: 5344000\n",
            "Best mean reward: 116.71 - Last mean reward per episode: 116.54\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 379      |\n",
            "|    ep_rew_mean        | 117      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1133     |\n",
            "|    iterations         | 133600   |\n",
            "|    time_elapsed       | 4714     |\n",
            "|    total_timesteps    | 5344000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.34    |\n",
            "|    explained_variance | 0.796    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 133599   |\n",
            "|    policy_loss        | 0.428    |\n",
            "|    value_loss         | 263      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 365      |\n",
            "|    ep_rew_mean        | 109      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1133     |\n",
            "|    iterations         | 133700   |\n",
            "|    time_elapsed       | 4717     |\n",
            "|    total_timesteps    | 5348000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.309   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 133699   |\n",
            "|    policy_loss        | -0.0519  |\n",
            "|    value_loss         | 2.46     |\n",
            "------------------------------------\n",
            "Num timesteps: 5352000\n",
            "Best mean reward: 116.71 - Last mean reward per episode: 109.77\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 365      |\n",
            "|    ep_rew_mean        | 110      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1133     |\n",
            "|    iterations         | 133800   |\n",
            "|    time_elapsed       | 4719     |\n",
            "|    total_timesteps    | 5352000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.314   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 133799   |\n",
            "|    policy_loss        | 0.12     |\n",
            "|    value_loss         | 0.948    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 367      |\n",
            "|    ep_rew_mean        | 99.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1133     |\n",
            "|    iterations         | 133900   |\n",
            "|    time_elapsed       | 4723     |\n",
            "|    total_timesteps    | 5356000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.32    |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 133899   |\n",
            "|    policy_loss        | -0.347   |\n",
            "|    value_loss         | 3.31     |\n",
            "------------------------------------\n",
            "Num timesteps: 5360000\n",
            "Best mean reward: 116.71 - Last mean reward per episode: 92.30\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 385      |\n",
            "|    ep_rew_mean        | 92.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1134     |\n",
            "|    iterations         | 134000   |\n",
            "|    time_elapsed       | 4726     |\n",
            "|    total_timesteps    | 5360000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.192   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 133999   |\n",
            "|    policy_loss        | 0.115    |\n",
            "|    value_loss         | 2.92     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 375      |\n",
            "|    ep_rew_mean        | 81       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1134     |\n",
            "|    iterations         | 134100   |\n",
            "|    time_elapsed       | 4729     |\n",
            "|    total_timesteps    | 5364000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.246   |\n",
            "|    explained_variance | 0.939    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 134099   |\n",
            "|    policy_loss        | 0.464    |\n",
            "|    value_loss         | 58       |\n",
            "------------------------------------\n",
            "Num timesteps: 5368000\n",
            "Best mean reward: 116.71 - Last mean reward per episode: 78.09\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 385      |\n",
            "|    ep_rew_mean        | 78.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1134     |\n",
            "|    iterations         | 134200   |\n",
            "|    time_elapsed       | 4731     |\n",
            "|    total_timesteps    | 5368000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.299   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 134199   |\n",
            "|    policy_loss        | -0.167   |\n",
            "|    value_loss         | 2.38     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 387      |\n",
            "|    ep_rew_mean        | 81.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1134     |\n",
            "|    iterations         | 134300   |\n",
            "|    time_elapsed       | 4734     |\n",
            "|    total_timesteps    | 5372000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.365   |\n",
            "|    explained_variance | 0.957    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 134299   |\n",
            "|    policy_loss        | -0.0386  |\n",
            "|    value_loss         | 3.73     |\n",
            "------------------------------------\n",
            "Num timesteps: 5376000\n",
            "Best mean reward: 116.71 - Last mean reward per episode: 82.69\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 377      |\n",
            "|    ep_rew_mean        | 82.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1134     |\n",
            "|    iterations         | 134400   |\n",
            "|    time_elapsed       | 4736     |\n",
            "|    total_timesteps    | 5376000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.36    |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 134399   |\n",
            "|    policy_loss        | -0.0175  |\n",
            "|    value_loss         | 6.37     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 360      |\n",
            "|    ep_rew_mean        | 80.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1135     |\n",
            "|    iterations         | 134500   |\n",
            "|    time_elapsed       | 4738     |\n",
            "|    total_timesteps    | 5380000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.369   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 134499   |\n",
            "|    policy_loss        | -0.322   |\n",
            "|    value_loss         | 16.3     |\n",
            "------------------------------------\n",
            "Num timesteps: 5384000\n",
            "Best mean reward: 116.71 - Last mean reward per episode: 80.32\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 361      |\n",
            "|    ep_rew_mean        | 80.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1135     |\n",
            "|    iterations         | 134600   |\n",
            "|    time_elapsed       | 4741     |\n",
            "|    total_timesteps    | 5384000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.441   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 134599   |\n",
            "|    policy_loss        | -0.106   |\n",
            "|    value_loss         | 4.67     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 360      |\n",
            "|    ep_rew_mean        | 77.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1135     |\n",
            "|    iterations         | 134700   |\n",
            "|    time_elapsed       | 4743     |\n",
            "|    total_timesteps    | 5388000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.193   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 134699   |\n",
            "|    policy_loss        | -0.216   |\n",
            "|    value_loss         | 3.43     |\n",
            "------------------------------------\n",
            "Num timesteps: 5392000\n",
            "Best mean reward: 116.71 - Last mean reward per episode: 73.87\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 338      |\n",
            "|    ep_rew_mean        | 73.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1136     |\n",
            "|    iterations         | 134800   |\n",
            "|    time_elapsed       | 4745     |\n",
            "|    total_timesteps    | 5392000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.236   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 134799   |\n",
            "|    policy_loss        | -0.257   |\n",
            "|    value_loss         | 8.83     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 329      |\n",
            "|    ep_rew_mean        | 73.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1136     |\n",
            "|    iterations         | 134900   |\n",
            "|    time_elapsed       | 4748     |\n",
            "|    total_timesteps    | 5396000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.312   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 134899   |\n",
            "|    policy_loss        | 0.217    |\n",
            "|    value_loss         | 5.89     |\n",
            "------------------------------------\n",
            "Num timesteps: 5400000\n",
            "Best mean reward: 116.71 - Last mean reward per episode: 66.85\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 328      |\n",
            "|    ep_rew_mean        | 66.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1136     |\n",
            "|    iterations         | 135000   |\n",
            "|    time_elapsed       | 4751     |\n",
            "|    total_timesteps    | 5400000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.343   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 134999   |\n",
            "|    policy_loss        | 0.399    |\n",
            "|    value_loss         | 8.93     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 316      |\n",
            "|    ep_rew_mean        | 59.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1136     |\n",
            "|    iterations         | 135100   |\n",
            "|    time_elapsed       | 4753     |\n",
            "|    total_timesteps    | 5404000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.285   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 135099   |\n",
            "|    policy_loss        | 0.0683   |\n",
            "|    value_loss         | 1.86     |\n",
            "------------------------------------\n",
            "Num timesteps: 5408000\n",
            "Best mean reward: 116.71 - Last mean reward per episode: 54.12\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 319      |\n",
            "|    ep_rew_mean        | 54.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1137     |\n",
            "|    iterations         | 135200   |\n",
            "|    time_elapsed       | 4756     |\n",
            "|    total_timesteps    | 5408000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.31    |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 135199   |\n",
            "|    policy_loss        | 0.0941   |\n",
            "|    value_loss         | 5.16     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 323      |\n",
            "|    ep_rew_mean        | 59.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1137     |\n",
            "|    iterations         | 135300   |\n",
            "|    time_elapsed       | 4758     |\n",
            "|    total_timesteps    | 5412000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.308   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 135299   |\n",
            "|    policy_loss        | -0.208   |\n",
            "|    value_loss         | 11.8     |\n",
            "------------------------------------\n",
            "Num timesteps: 5416000\n",
            "Best mean reward: 116.71 - Last mean reward per episode: 73.80\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 322      |\n",
            "|    ep_rew_mean        | 73.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1137     |\n",
            "|    iterations         | 135400   |\n",
            "|    time_elapsed       | 4760     |\n",
            "|    total_timesteps    | 5416000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.205   |\n",
            "|    explained_variance | 0.242    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 135399   |\n",
            "|    policy_loss        | -3.79    |\n",
            "|    value_loss         | 1.17e+03 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 311      |\n",
            "|    ep_rew_mean        | 71.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1137     |\n",
            "|    iterations         | 135500   |\n",
            "|    time_elapsed       | 4762     |\n",
            "|    total_timesteps    | 5420000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.364   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 135499   |\n",
            "|    policy_loss        | -0.0907  |\n",
            "|    value_loss         | 31.7     |\n",
            "------------------------------------\n",
            "Num timesteps: 5424000\n",
            "Best mean reward: 116.71 - Last mean reward per episode: 82.30\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 317      |\n",
            "|    ep_rew_mean        | 82.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1138     |\n",
            "|    iterations         | 135600   |\n",
            "|    time_elapsed       | 4765     |\n",
            "|    total_timesteps    | 5424000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.318   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 135599   |\n",
            "|    policy_loss        | -1.59    |\n",
            "|    value_loss         | 33       |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 303      |\n",
            "|    ep_rew_mean        | 80.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1138     |\n",
            "|    iterations         | 135700   |\n",
            "|    time_elapsed       | 4767     |\n",
            "|    total_timesteps    | 5428000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.317   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 135699   |\n",
            "|    policy_loss        | -0.00507 |\n",
            "|    value_loss         | 19.9     |\n",
            "------------------------------------\n",
            "Num timesteps: 5432000\n",
            "Best mean reward: 116.71 - Last mean reward per episode: 75.10\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 290      |\n",
            "|    ep_rew_mean        | 75.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1139     |\n",
            "|    iterations         | 135800   |\n",
            "|    time_elapsed       | 4768     |\n",
            "|    total_timesteps    | 5432000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.252   |\n",
            "|    explained_variance | 0.524    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 135799   |\n",
            "|    policy_loss        | 0.265    |\n",
            "|    value_loss         | 1.26e+03 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 284      |\n",
            "|    ep_rew_mean        | 86.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1139     |\n",
            "|    iterations         | 135900   |\n",
            "|    time_elapsed       | 4771     |\n",
            "|    total_timesteps    | 5436000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.307   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 135899   |\n",
            "|    policy_loss        | -0.181   |\n",
            "|    value_loss         | 9.36     |\n",
            "------------------------------------\n",
            "Num timesteps: 5440000\n",
            "Best mean reward: 116.71 - Last mean reward per episode: 94.49\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 292      |\n",
            "|    ep_rew_mean        | 94.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1139     |\n",
            "|    iterations         | 136000   |\n",
            "|    time_elapsed       | 4773     |\n",
            "|    total_timesteps    | 5440000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.311   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 135999   |\n",
            "|    policy_loss        | 0.0102   |\n",
            "|    value_loss         | 2.38     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 285      |\n",
            "|    ep_rew_mean        | 90.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1139     |\n",
            "|    iterations         | 136100   |\n",
            "|    time_elapsed       | 4775     |\n",
            "|    total_timesteps    | 5444000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.307   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 136099   |\n",
            "|    policy_loss        | -0.0762  |\n",
            "|    value_loss         | 0.513    |\n",
            "------------------------------------\n",
            "Num timesteps: 5448000\n",
            "Best mean reward: 116.71 - Last mean reward per episode: 94.31\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 285      |\n",
            "|    ep_rew_mean        | 94.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1140     |\n",
            "|    iterations         | 136200   |\n",
            "|    time_elapsed       | 4777     |\n",
            "|    total_timesteps    | 5448000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.229   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 136199   |\n",
            "|    policy_loss        | -0.662   |\n",
            "|    value_loss         | 11.3     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 278      |\n",
            "|    ep_rew_mean        | 102      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1140     |\n",
            "|    iterations         | 136300   |\n",
            "|    time_elapsed       | 4779     |\n",
            "|    total_timesteps    | 5452000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.328   |\n",
            "|    explained_variance | 0.979    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 136299   |\n",
            "|    policy_loss        | -0.0283  |\n",
            "|    value_loss         | 43.3     |\n",
            "------------------------------------\n",
            "Num timesteps: 5456000\n",
            "Best mean reward: 116.71 - Last mean reward per episode: 119.09\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 272      |\n",
            "|    ep_rew_mean        | 119      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1141     |\n",
            "|    iterations         | 136400   |\n",
            "|    time_elapsed       | 4781     |\n",
            "|    total_timesteps    | 5456000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.315   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 136399   |\n",
            "|    policy_loss        | 0.305    |\n",
            "|    value_loss         | 5.72     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 265      |\n",
            "|    ep_rew_mean        | 120      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1141     |\n",
            "|    iterations         | 136500   |\n",
            "|    time_elapsed       | 4783     |\n",
            "|    total_timesteps    | 5460000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.356   |\n",
            "|    explained_variance | 0.889    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 136499   |\n",
            "|    policy_loss        | 0.117    |\n",
            "|    value_loss         | 262      |\n",
            "------------------------------------\n",
            "Num timesteps: 5464000\n",
            "Best mean reward: 119.09 - Last mean reward per episode: 110.44\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 256      |\n",
            "|    ep_rew_mean        | 110      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1141     |\n",
            "|    iterations         | 136600   |\n",
            "|    time_elapsed       | 4785     |\n",
            "|    total_timesteps    | 5464000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.342   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 136599   |\n",
            "|    policy_loss        | 0.563    |\n",
            "|    value_loss         | 12.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 260      |\n",
            "|    ep_rew_mean        | 111      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1142     |\n",
            "|    iterations         | 136700   |\n",
            "|    time_elapsed       | 4787     |\n",
            "|    total_timesteps    | 5468000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.32    |\n",
            "|    explained_variance | 0.314    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 136699   |\n",
            "|    policy_loss        | 0.0104   |\n",
            "|    value_loss         | 338      |\n",
            "------------------------------------\n",
            "Num timesteps: 5472000\n",
            "Best mean reward: 119.09 - Last mean reward per episode: 117.10\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 261      |\n",
            "|    ep_rew_mean        | 117      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1142     |\n",
            "|    iterations         | 136800   |\n",
            "|    time_elapsed       | 4789     |\n",
            "|    total_timesteps    | 5472000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.218   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 136799   |\n",
            "|    policy_loss        | -0.0579  |\n",
            "|    value_loss         | 2.14     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 254      |\n",
            "|    ep_rew_mean        | 107      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1142     |\n",
            "|    iterations         | 136900   |\n",
            "|    time_elapsed       | 4791     |\n",
            "|    total_timesteps    | 5476000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.193   |\n",
            "|    explained_variance | 0.577    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 136899   |\n",
            "|    policy_loss        | -0.00708 |\n",
            "|    value_loss         | 268      |\n",
            "------------------------------------\n",
            "Num timesteps: 5480000\n",
            "Best mean reward: 119.09 - Last mean reward per episode: 103.99\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 260      |\n",
            "|    ep_rew_mean        | 104      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1143     |\n",
            "|    iterations         | 137000   |\n",
            "|    time_elapsed       | 4793     |\n",
            "|    total_timesteps    | 5480000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.131   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 136999   |\n",
            "|    policy_loss        | 0.117    |\n",
            "|    value_loss         | 0.946    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 277      |\n",
            "|    ep_rew_mean        | 113      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1143     |\n",
            "|    iterations         | 137100   |\n",
            "|    time_elapsed       | 4795     |\n",
            "|    total_timesteps    | 5484000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.255   |\n",
            "|    explained_variance | 0.491    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 137099   |\n",
            "|    policy_loss        | 0.0628   |\n",
            "|    value_loss         | 1.14e+03 |\n",
            "------------------------------------\n",
            "Num timesteps: 5488000\n",
            "Best mean reward: 119.09 - Last mean reward per episode: 127.56\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 281      |\n",
            "|    ep_rew_mean        | 128      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1143     |\n",
            "|    iterations         | 137200   |\n",
            "|    time_elapsed       | 4797     |\n",
            "|    total_timesteps    | 5488000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.179   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 137199   |\n",
            "|    policy_loss        | 0.0824   |\n",
            "|    value_loss         | 2.81     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 286      |\n",
            "|    ep_rew_mean        | 131      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1144     |\n",
            "|    iterations         | 137300   |\n",
            "|    time_elapsed       | 4799     |\n",
            "|    total_timesteps    | 5492000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.302   |\n",
            "|    explained_variance | 0.825    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 137299   |\n",
            "|    policy_loss        | 0.338    |\n",
            "|    value_loss         | 296      |\n",
            "------------------------------------\n",
            "Num timesteps: 5496000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 125.08\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 273      |\n",
            "|    ep_rew_mean        | 125      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1144     |\n",
            "|    iterations         | 137400   |\n",
            "|    time_elapsed       | 4801     |\n",
            "|    total_timesteps    | 5496000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.184   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 137399   |\n",
            "|    policy_loss        | -0.217   |\n",
            "|    value_loss         | 4        |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 266      |\n",
            "|    ep_rew_mean        | 115      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1144     |\n",
            "|    iterations         | 137500   |\n",
            "|    time_elapsed       | 4803     |\n",
            "|    total_timesteps    | 5500000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.218   |\n",
            "|    explained_variance | 0.216    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 137499   |\n",
            "|    policy_loss        | -0.0843  |\n",
            "|    value_loss         | 1.23e+03 |\n",
            "------------------------------------\n",
            "Num timesteps: 5504000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 118.58\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 264      |\n",
            "|    ep_rew_mean        | 119      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1145     |\n",
            "|    iterations         | 137600   |\n",
            "|    time_elapsed       | 4805     |\n",
            "|    total_timesteps    | 5504000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.401   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 137599   |\n",
            "|    policy_loss        | -0.0399  |\n",
            "|    value_loss         | 4.3      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 275      |\n",
            "|    ep_rew_mean        | 123      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1145     |\n",
            "|    iterations         | 137700   |\n",
            "|    time_elapsed       | 4807     |\n",
            "|    total_timesteps    | 5508000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.296   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 137699   |\n",
            "|    policy_loss        | -0.0737  |\n",
            "|    value_loss         | 4.33     |\n",
            "------------------------------------\n",
            "Num timesteps: 5512000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 106.62\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 267      |\n",
            "|    ep_rew_mean        | 107      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1145     |\n",
            "|    iterations         | 137800   |\n",
            "|    time_elapsed       | 4810     |\n",
            "|    total_timesteps    | 5512000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.25    |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 137799   |\n",
            "|    policy_loss        | 0.0676   |\n",
            "|    value_loss         | 3.18     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 285      |\n",
            "|    ep_rew_mean        | 92.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1146     |\n",
            "|    iterations         | 137900   |\n",
            "|    time_elapsed       | 4812     |\n",
            "|    total_timesteps    | 5516000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.211   |\n",
            "|    explained_variance | 0.907    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 137899   |\n",
            "|    policy_loss        | -0.313   |\n",
            "|    value_loss         | 100      |\n",
            "------------------------------------\n",
            "Num timesteps: 5520000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 88.37\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 287      |\n",
            "|    ep_rew_mean        | 88.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1146     |\n",
            "|    iterations         | 138000   |\n",
            "|    time_elapsed       | 4815     |\n",
            "|    total_timesteps    | 5520000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.359   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 137999   |\n",
            "|    policy_loss        | -0.297   |\n",
            "|    value_loss         | 2.5      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 303      |\n",
            "|    ep_rew_mean        | 92.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1146     |\n",
            "|    iterations         | 138100   |\n",
            "|    time_elapsed       | 4817     |\n",
            "|    total_timesteps    | 5524000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.37    |\n",
            "|    explained_variance | 0.796    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 138099   |\n",
            "|    policy_loss        | 0.152    |\n",
            "|    value_loss         | 460      |\n",
            "------------------------------------\n",
            "Num timesteps: 5528000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 91.92\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 308      |\n",
            "|    ep_rew_mean        | 91.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1146     |\n",
            "|    iterations         | 138200   |\n",
            "|    time_elapsed       | 4820     |\n",
            "|    total_timesteps    | 5528000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.239   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 138199   |\n",
            "|    policy_loss        | -0.258   |\n",
            "|    value_loss         | 14.6     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 323      |\n",
            "|    ep_rew_mean        | 90.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1147     |\n",
            "|    iterations         | 138300   |\n",
            "|    time_elapsed       | 4822     |\n",
            "|    total_timesteps    | 5532000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.357   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 138299   |\n",
            "|    policy_loss        | -0.201   |\n",
            "|    value_loss         | 2.64     |\n",
            "------------------------------------\n",
            "Num timesteps: 5536000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 95.10\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 335      |\n",
            "|    ep_rew_mean        | 95.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1147     |\n",
            "|    iterations         | 138400   |\n",
            "|    time_elapsed       | 4825     |\n",
            "|    total_timesteps    | 5536000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.233   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 138399   |\n",
            "|    policy_loss        | 0.0947   |\n",
            "|    value_loss         | 2.93     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 349      |\n",
            "|    ep_rew_mean        | 89.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1147     |\n",
            "|    iterations         | 138500   |\n",
            "|    time_elapsed       | 4828     |\n",
            "|    total_timesteps    | 5540000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.327   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 138499   |\n",
            "|    policy_loss        | 0.119    |\n",
            "|    value_loss         | 3.72     |\n",
            "------------------------------------\n",
            "Num timesteps: 5544000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 81.53\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 350      |\n",
            "|    ep_rew_mean        | 81.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1147     |\n",
            "|    iterations         | 138600   |\n",
            "|    time_elapsed       | 4831     |\n",
            "|    total_timesteps    | 5544000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.28    |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 138599   |\n",
            "|    policy_loss        | -0.11    |\n",
            "|    value_loss         | 4.29     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 361      |\n",
            "|    ep_rew_mean        | 81.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1147     |\n",
            "|    iterations         | 138700   |\n",
            "|    time_elapsed       | 4833     |\n",
            "|    total_timesteps    | 5548000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.304   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 138699   |\n",
            "|    policy_loss        | -0.163   |\n",
            "|    value_loss         | 9.55     |\n",
            "------------------------------------\n",
            "Num timesteps: 5552000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 91.90\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 350      |\n",
            "|    ep_rew_mean        | 91.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1148     |\n",
            "|    iterations         | 138800   |\n",
            "|    time_elapsed       | 4836     |\n",
            "|    total_timesteps    | 5552000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.386   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 138799   |\n",
            "|    policy_loss        | -0.075   |\n",
            "|    value_loss         | 0.453    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 357      |\n",
            "|    ep_rew_mean        | 105      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1148     |\n",
            "|    iterations         | 138900   |\n",
            "|    time_elapsed       | 4838     |\n",
            "|    total_timesteps    | 5556000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.325   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 138899   |\n",
            "|    policy_loss        | -0.153   |\n",
            "|    value_loss         | 2.99     |\n",
            "------------------------------------\n",
            "Num timesteps: 5560000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 91.62\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 355      |\n",
            "|    ep_rew_mean        | 91.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1148     |\n",
            "|    iterations         | 139000   |\n",
            "|    time_elapsed       | 4841     |\n",
            "|    total_timesteps    | 5560000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.156   |\n",
            "|    explained_variance | 0.928    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 138999   |\n",
            "|    policy_loss        | -0.187   |\n",
            "|    value_loss         | 18.7     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 358      |\n",
            "|    ep_rew_mean        | 85.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1148     |\n",
            "|    iterations         | 139100   |\n",
            "|    time_elapsed       | 4844     |\n",
            "|    total_timesteps    | 5564000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.355   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 139099   |\n",
            "|    policy_loss        | 0.266    |\n",
            "|    value_loss         | 1.61     |\n",
            "------------------------------------\n",
            "Num timesteps: 5568000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 89.65\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 353      |\n",
            "|    ep_rew_mean        | 89.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1148     |\n",
            "|    iterations         | 139200   |\n",
            "|    time_elapsed       | 4847     |\n",
            "|    total_timesteps    | 5568000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.28    |\n",
            "|    explained_variance | 0.921    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 139199   |\n",
            "|    policy_loss        | -0.0658  |\n",
            "|    value_loss         | 60.8     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 366      |\n",
            "|    ep_rew_mean        | 89       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1148     |\n",
            "|    iterations         | 139300   |\n",
            "|    time_elapsed       | 4849     |\n",
            "|    total_timesteps    | 5572000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.33    |\n",
            "|    explained_variance | 0.929    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 139299   |\n",
            "|    policy_loss        | 0.096    |\n",
            "|    value_loss         | 95.6     |\n",
            "------------------------------------\n",
            "Num timesteps: 5576000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 79.06\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 370      |\n",
            "|    ep_rew_mean        | 79.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1148     |\n",
            "|    iterations         | 139400   |\n",
            "|    time_elapsed       | 4854     |\n",
            "|    total_timesteps    | 5576000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.234   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 139399   |\n",
            "|    policy_loss        | -0.736   |\n",
            "|    value_loss         | 3.38     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 381      |\n",
            "|    ep_rew_mean        | 79.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1148     |\n",
            "|    iterations         | 139500   |\n",
            "|    time_elapsed       | 4857     |\n",
            "|    total_timesteps    | 5580000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.244   |\n",
            "|    explained_variance | 0.954    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 139499   |\n",
            "|    policy_loss        | -0.111   |\n",
            "|    value_loss         | 83.4     |\n",
            "------------------------------------\n",
            "Num timesteps: 5584000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 81.67\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 369      |\n",
            "|    ep_rew_mean        | 81.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1149     |\n",
            "|    iterations         | 139600   |\n",
            "|    time_elapsed       | 4859     |\n",
            "|    total_timesteps    | 5584000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.367   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 139599   |\n",
            "|    policy_loss        | -0.0816  |\n",
            "|    value_loss         | 2.8      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 368      |\n",
            "|    ep_rew_mean        | 72.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1149     |\n",
            "|    iterations         | 139700   |\n",
            "|    time_elapsed       | 4861     |\n",
            "|    total_timesteps    | 5588000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.36    |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 139699   |\n",
            "|    policy_loss        | -0.197   |\n",
            "|    value_loss         | 2.69     |\n",
            "------------------------------------\n",
            "Num timesteps: 5592000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 68.42\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 358      |\n",
            "|    ep_rew_mean        | 68.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1149     |\n",
            "|    iterations         | 139800   |\n",
            "|    time_elapsed       | 4863     |\n",
            "|    total_timesteps    | 5592000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.301   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 139799   |\n",
            "|    policy_loss        | -0.13    |\n",
            "|    value_loss         | 1.72     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 340      |\n",
            "|    ep_rew_mean        | 79.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1150     |\n",
            "|    iterations         | 139900   |\n",
            "|    time_elapsed       | 4865     |\n",
            "|    total_timesteps    | 5596000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.394   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 139899   |\n",
            "|    policy_loss        | -0.0157  |\n",
            "|    value_loss         | 25.7     |\n",
            "------------------------------------\n",
            "Num timesteps: 5600000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 83.92\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 332      |\n",
            "|    ep_rew_mean        | 83.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1150     |\n",
            "|    iterations         | 140000   |\n",
            "|    time_elapsed       | 4867     |\n",
            "|    total_timesteps    | 5600000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.317   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 139999   |\n",
            "|    policy_loss        | 0.0503   |\n",
            "|    value_loss         | 1.53     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 314      |\n",
            "|    ep_rew_mean        | 81.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1150     |\n",
            "|    iterations         | 140100   |\n",
            "|    time_elapsed       | 4870     |\n",
            "|    total_timesteps    | 5604000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.264   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 140099   |\n",
            "|    policy_loss        | -0.0573  |\n",
            "|    value_loss         | 4.66     |\n",
            "------------------------------------\n",
            "Num timesteps: 5608000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 77.04\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 297      |\n",
            "|    ep_rew_mean        | 77       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1150     |\n",
            "|    iterations         | 140200   |\n",
            "|    time_elapsed       | 4872     |\n",
            "|    total_timesteps    | 5608000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.301   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 140199   |\n",
            "|    policy_loss        | -0.293   |\n",
            "|    value_loss         | 1.93     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 283      |\n",
            "|    ep_rew_mean        | 73.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1151     |\n",
            "|    iterations         | 140300   |\n",
            "|    time_elapsed       | 4874     |\n",
            "|    total_timesteps    | 5612000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.476   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 140299   |\n",
            "|    policy_loss        | -0.0151  |\n",
            "|    value_loss         | 2.16     |\n",
            "------------------------------------\n",
            "Num timesteps: 5616000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 83.74\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 287      |\n",
            "|    ep_rew_mean        | 83.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1151     |\n",
            "|    iterations         | 140400   |\n",
            "|    time_elapsed       | 4877     |\n",
            "|    total_timesteps    | 5616000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.268   |\n",
            "|    explained_variance | 0.959    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 140399   |\n",
            "|    policy_loss        | 0.0654   |\n",
            "|    value_loss         | 26.1     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 297      |\n",
            "|    ep_rew_mean        | 88.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1151     |\n",
            "|    iterations         | 140500   |\n",
            "|    time_elapsed       | 4879     |\n",
            "|    total_timesteps    | 5620000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.329   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 140499   |\n",
            "|    policy_loss        | 0.0668   |\n",
            "|    value_loss         | 1.02     |\n",
            "------------------------------------\n",
            "Num timesteps: 5624000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 78.97\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 305      |\n",
            "|    ep_rew_mean        | 79       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1151     |\n",
            "|    iterations         | 140600   |\n",
            "|    time_elapsed       | 4882     |\n",
            "|    total_timesteps    | 5624000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.359   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 140599   |\n",
            "|    policy_loss        | 0.321    |\n",
            "|    value_loss         | 2.4      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 311      |\n",
            "|    ep_rew_mean        | 73.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1152     |\n",
            "|    iterations         | 140700   |\n",
            "|    time_elapsed       | 4884     |\n",
            "|    total_timesteps    | 5628000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.342   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 140699   |\n",
            "|    policy_loss        | -0.188   |\n",
            "|    value_loss         | 2.48     |\n",
            "------------------------------------\n",
            "Num timesteps: 5632000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 64.97\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 307      |\n",
            "|    ep_rew_mean        | 65       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1152     |\n",
            "|    iterations         | 140800   |\n",
            "|    time_elapsed       | 4886     |\n",
            "|    total_timesteps    | 5632000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.262   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 140799   |\n",
            "|    policy_loss        | -0.273   |\n",
            "|    value_loss         | 1.51     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 296      |\n",
            "|    ep_rew_mean        | 72       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1152     |\n",
            "|    iterations         | 140900   |\n",
            "|    time_elapsed       | 4888     |\n",
            "|    total_timesteps    | 5636000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.288   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 140899   |\n",
            "|    policy_loss        | -0.0751  |\n",
            "|    value_loss         | 2.11     |\n",
            "------------------------------------\n",
            "Num timesteps: 5640000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 76.13\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 291      |\n",
            "|    ep_rew_mean        | 76.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1153     |\n",
            "|    iterations         | 141000   |\n",
            "|    time_elapsed       | 4890     |\n",
            "|    total_timesteps    | 5640000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.307   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 140999   |\n",
            "|    policy_loss        | -0.179   |\n",
            "|    value_loss         | 1.34     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 293      |\n",
            "|    ep_rew_mean        | 82.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1153     |\n",
            "|    iterations         | 141100   |\n",
            "|    time_elapsed       | 4893     |\n",
            "|    total_timesteps    | 5644000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.49    |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 141099   |\n",
            "|    policy_loss        | -0.305   |\n",
            "|    value_loss         | 2.79     |\n",
            "------------------------------------\n",
            "Num timesteps: 5648000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 75.51\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 294      |\n",
            "|    ep_rew_mean        | 75.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1153     |\n",
            "|    iterations         | 141200   |\n",
            "|    time_elapsed       | 4896     |\n",
            "|    total_timesteps    | 5648000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.376   |\n",
            "|    explained_variance | 0.455    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 141199   |\n",
            "|    policy_loss        | -0.704   |\n",
            "|    value_loss         | 891      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 289      |\n",
            "|    ep_rew_mean        | 66.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1153     |\n",
            "|    iterations         | 141300   |\n",
            "|    time_elapsed       | 4898     |\n",
            "|    total_timesteps    | 5652000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.335   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 141299   |\n",
            "|    policy_loss        | -0.0365  |\n",
            "|    value_loss         | 1.72     |\n",
            "------------------------------------\n",
            "Num timesteps: 5656000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 65.90\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 281      |\n",
            "|    ep_rew_mean        | 65.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1153     |\n",
            "|    iterations         | 141400   |\n",
            "|    time_elapsed       | 4901     |\n",
            "|    total_timesteps    | 5656000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.428   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 141399   |\n",
            "|    policy_loss        | 0.425    |\n",
            "|    value_loss         | 5.98     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 306      |\n",
            "|    ep_rew_mean        | 71.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1154     |\n",
            "|    iterations         | 141500   |\n",
            "|    time_elapsed       | 4903     |\n",
            "|    total_timesteps    | 5660000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.255   |\n",
            "|    explained_variance | 0.824    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 141499   |\n",
            "|    policy_loss        | -0.00132 |\n",
            "|    value_loss         | 123      |\n",
            "------------------------------------\n",
            "Num timesteps: 5664000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 77.71\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 308      |\n",
            "|    ep_rew_mean        | 77.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1154     |\n",
            "|    iterations         | 141600   |\n",
            "|    time_elapsed       | 4906     |\n",
            "|    total_timesteps    | 5664000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.309   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 141599   |\n",
            "|    policy_loss        | -0.185   |\n",
            "|    value_loss         | 2.99     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 323      |\n",
            "|    ep_rew_mean        | 88.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1154     |\n",
            "|    iterations         | 141700   |\n",
            "|    time_elapsed       | 4908     |\n",
            "|    total_timesteps    | 5668000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.387   |\n",
            "|    explained_variance | 0.0178   |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 141699   |\n",
            "|    policy_loss        | -0.0149  |\n",
            "|    value_loss         | 936      |\n",
            "------------------------------------\n",
            "Num timesteps: 5672000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 101.53\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 343      |\n",
            "|    ep_rew_mean        | 102      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1154     |\n",
            "|    iterations         | 141800   |\n",
            "|    time_elapsed       | 4911     |\n",
            "|    total_timesteps    | 5672000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.372   |\n",
            "|    explained_variance | 0.914    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 141799   |\n",
            "|    policy_loss        | -0.482   |\n",
            "|    value_loss         | 140      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 350      |\n",
            "|    ep_rew_mean        | 97.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1155     |\n",
            "|    iterations         | 141900   |\n",
            "|    time_elapsed       | 4913     |\n",
            "|    total_timesteps    | 5676000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.453   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 141899   |\n",
            "|    policy_loss        | 0.0469   |\n",
            "|    value_loss         | 0.718    |\n",
            "------------------------------------\n",
            "Num timesteps: 5680000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 105.28\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 342      |\n",
            "|    ep_rew_mean        | 105      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1155     |\n",
            "|    iterations         | 142000   |\n",
            "|    time_elapsed       | 4916     |\n",
            "|    total_timesteps    | 5680000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.341   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 141999   |\n",
            "|    policy_loss        | -0.235   |\n",
            "|    value_loss         | 5.44     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 340      |\n",
            "|    ep_rew_mean        | 99.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1155     |\n",
            "|    iterations         | 142100   |\n",
            "|    time_elapsed       | 4919     |\n",
            "|    total_timesteps    | 5684000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.337   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 142099   |\n",
            "|    policy_loss        | -0.285   |\n",
            "|    value_loss         | 1.64     |\n",
            "------------------------------------\n",
            "Num timesteps: 5688000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 101.78\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 352      |\n",
            "|    ep_rew_mean        | 102      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1155     |\n",
            "|    iterations         | 142200   |\n",
            "|    time_elapsed       | 4922     |\n",
            "|    total_timesteps    | 5688000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.356   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 142199   |\n",
            "|    policy_loss        | 0.182    |\n",
            "|    value_loss         | 2.95     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 375      |\n",
            "|    ep_rew_mean        | 113      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1155     |\n",
            "|    iterations         | 142300   |\n",
            "|    time_elapsed       | 4925     |\n",
            "|    total_timesteps    | 5692000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.303   |\n",
            "|    explained_variance | 0.979    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 142299   |\n",
            "|    policy_loss        | -0.468   |\n",
            "|    value_loss         | 14.6     |\n",
            "------------------------------------\n",
            "Num timesteps: 5696000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 119.24\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 362      |\n",
            "|    ep_rew_mean        | 119      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1155     |\n",
            "|    iterations         | 142400   |\n",
            "|    time_elapsed       | 4927     |\n",
            "|    total_timesteps    | 5696000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.223   |\n",
            "|    explained_variance | 0.921    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 142399   |\n",
            "|    policy_loss        | 0.537    |\n",
            "|    value_loss         | 27.9     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 378      |\n",
            "|    ep_rew_mean        | 119      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1156     |\n",
            "|    iterations         | 142500   |\n",
            "|    time_elapsed       | 4930     |\n",
            "|    total_timesteps    | 5700000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.379   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 142499   |\n",
            "|    policy_loss        | 0.155    |\n",
            "|    value_loss         | 1.38     |\n",
            "------------------------------------\n",
            "Num timesteps: 5704000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 97.48\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 387      |\n",
            "|    ep_rew_mean        | 97.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1156     |\n",
            "|    iterations         | 142600   |\n",
            "|    time_elapsed       | 4933     |\n",
            "|    total_timesteps    | 5704000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.353   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 142599   |\n",
            "|    policy_loss        | 0.0349   |\n",
            "|    value_loss         | 7.01     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 387      |\n",
            "|    ep_rew_mean        | 90.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1156     |\n",
            "|    iterations         | 142700   |\n",
            "|    time_elapsed       | 4936     |\n",
            "|    total_timesteps    | 5708000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.266   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 142699   |\n",
            "|    policy_loss        | 0.573    |\n",
            "|    value_loss         | 3.22     |\n",
            "------------------------------------\n",
            "Num timesteps: 5712000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 83.00\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 391      |\n",
            "|    ep_rew_mean        | 83       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1156     |\n",
            "|    iterations         | 142800   |\n",
            "|    time_elapsed       | 4939     |\n",
            "|    total_timesteps    | 5712000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.399   |\n",
            "|    explained_variance | 0.868    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 142799   |\n",
            "|    policy_loss        | 0.0233   |\n",
            "|    value_loss         | 71.7     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 397      |\n",
            "|    ep_rew_mean        | 83.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1156     |\n",
            "|    iterations         | 142900   |\n",
            "|    time_elapsed       | 4941     |\n",
            "|    total_timesteps    | 5716000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.395   |\n",
            "|    explained_variance | 0.959    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 142899   |\n",
            "|    policy_loss        | -1.25    |\n",
            "|    value_loss         | 16.3     |\n",
            "------------------------------------\n",
            "Num timesteps: 5720000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 84.59\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 419      |\n",
            "|    ep_rew_mean        | 84.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1156     |\n",
            "|    iterations         | 143000   |\n",
            "|    time_elapsed       | 4944     |\n",
            "|    total_timesteps    | 5720000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.278   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 142999   |\n",
            "|    policy_loss        | 0.226    |\n",
            "|    value_loss         | 3.37     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 421      |\n",
            "|    ep_rew_mean        | 84.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1156     |\n",
            "|    iterations         | 143100   |\n",
            "|    time_elapsed       | 4948     |\n",
            "|    total_timesteps    | 5724000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.327   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 143099   |\n",
            "|    policy_loss        | -0.196   |\n",
            "|    value_loss         | 0.784    |\n",
            "------------------------------------\n",
            "Num timesteps: 5728000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 103.36\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 432      |\n",
            "|    ep_rew_mean        | 103      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1156     |\n",
            "|    iterations         | 143200   |\n",
            "|    time_elapsed       | 4950     |\n",
            "|    total_timesteps    | 5728000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.36    |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 143199   |\n",
            "|    policy_loss        | -0.143   |\n",
            "|    value_loss         | 2.65     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 438      |\n",
            "|    ep_rew_mean        | 114      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1157     |\n",
            "|    iterations         | 143300   |\n",
            "|    time_elapsed       | 4954     |\n",
            "|    total_timesteps    | 5732000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.431   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 143299   |\n",
            "|    policy_loss        | 0.00323  |\n",
            "|    value_loss         | 1.59     |\n",
            "------------------------------------\n",
            "Num timesteps: 5736000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 113.95\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 447      |\n",
            "|    ep_rew_mean        | 114      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1156     |\n",
            "|    iterations         | 143400   |\n",
            "|    time_elapsed       | 4958     |\n",
            "|    total_timesteps    | 5736000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.425   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 143399   |\n",
            "|    policy_loss        | 0.686    |\n",
            "|    value_loss         | 7.96     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 467      |\n",
            "|    ep_rew_mean        | 111      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1156     |\n",
            "|    iterations         | 143500   |\n",
            "|    time_elapsed       | 4961     |\n",
            "|    total_timesteps    | 5740000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.291   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 143499   |\n",
            "|    policy_loss        | -0.104   |\n",
            "|    value_loss         | 1.89     |\n",
            "------------------------------------\n",
            "Num timesteps: 5744000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 111.64\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 484      |\n",
            "|    ep_rew_mean        | 112      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1156     |\n",
            "|    iterations         | 143600   |\n",
            "|    time_elapsed       | 4965     |\n",
            "|    total_timesteps    | 5744000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.292   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 143599   |\n",
            "|    policy_loss        | -0.17    |\n",
            "|    value_loss         | 0.919    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 495      |\n",
            "|    ep_rew_mean        | 93.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1156     |\n",
            "|    iterations         | 143700   |\n",
            "|    time_elapsed       | 4969     |\n",
            "|    total_timesteps    | 5748000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.296   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 143699   |\n",
            "|    policy_loss        | 0.0151   |\n",
            "|    value_loss         | 3.83     |\n",
            "------------------------------------\n",
            "Num timesteps: 5752000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 97.94\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 502      |\n",
            "|    ep_rew_mean        | 97.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1156     |\n",
            "|    iterations         | 143800   |\n",
            "|    time_elapsed       | 4973     |\n",
            "|    total_timesteps    | 5752000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.426   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 143799   |\n",
            "|    policy_loss        | -0.0994  |\n",
            "|    value_loss         | 0.61     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 493      |\n",
            "|    ep_rew_mean        | 103      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1156     |\n",
            "|    iterations         | 143900   |\n",
            "|    time_elapsed       | 4976     |\n",
            "|    total_timesteps    | 5756000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.405   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 143899   |\n",
            "|    policy_loss        | 0.00556  |\n",
            "|    value_loss         | 2.52     |\n",
            "------------------------------------\n",
            "Num timesteps: 5760000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 104.33\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 498      |\n",
            "|    ep_rew_mean        | 104      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1156     |\n",
            "|    iterations         | 144000   |\n",
            "|    time_elapsed       | 4979     |\n",
            "|    total_timesteps    | 5760000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.316   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 143999   |\n",
            "|    policy_loss        | 0.266    |\n",
            "|    value_loss         | 1.17     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 492      |\n",
            "|    ep_rew_mean        | 103      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1156     |\n",
            "|    iterations         | 144100   |\n",
            "|    time_elapsed       | 4982     |\n",
            "|    total_timesteps    | 5764000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.355   |\n",
            "|    explained_variance | 0.804    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 144099   |\n",
            "|    policy_loss        | 2.12     |\n",
            "|    value_loss         | 79.9     |\n",
            "------------------------------------\n",
            "Num timesteps: 5768000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 97.86\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 479      |\n",
            "|    ep_rew_mean        | 97.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1157     |\n",
            "|    iterations         | 144200   |\n",
            "|    time_elapsed       | 4984     |\n",
            "|    total_timesteps    | 5768000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.267   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 144199   |\n",
            "|    policy_loss        | 0.189    |\n",
            "|    value_loss         | 2.96     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 457      |\n",
            "|    ep_rew_mean        | 81       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1157     |\n",
            "|    iterations         | 144300   |\n",
            "|    time_elapsed       | 4986     |\n",
            "|    total_timesteps    | 5772000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.358   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 144299   |\n",
            "|    policy_loss        | 0.12     |\n",
            "|    value_loss         | 1.61     |\n",
            "------------------------------------\n",
            "Num timesteps: 5776000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 77.55\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 388      |\n",
            "|    ep_rew_mean        | 77.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1157     |\n",
            "|    iterations         | 144400   |\n",
            "|    time_elapsed       | 4988     |\n",
            "|    total_timesteps    | 5776000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.41    |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 144399   |\n",
            "|    policy_loss        | -0.0755  |\n",
            "|    value_loss         | 3.07     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 334      |\n",
            "|    ep_rew_mean        | 87.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1158     |\n",
            "|    iterations         | 144500   |\n",
            "|    time_elapsed       | 4990     |\n",
            "|    total_timesteps    | 5780000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.233   |\n",
            "|    explained_variance | 0.832    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 144499   |\n",
            "|    policy_loss        | -0.162   |\n",
            "|    value_loss         | 247      |\n",
            "------------------------------------\n",
            "Num timesteps: 5784000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 91.62\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 307      |\n",
            "|    ep_rew_mean        | 91.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1158     |\n",
            "|    iterations         | 144600   |\n",
            "|    time_elapsed       | 4992     |\n",
            "|    total_timesteps    | 5784000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.285   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 144599   |\n",
            "|    policy_loss        | 0.101    |\n",
            "|    value_loss         | 1.48     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 303      |\n",
            "|    ep_rew_mean        | 99.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1158     |\n",
            "|    iterations         | 144700   |\n",
            "|    time_elapsed       | 4995     |\n",
            "|    total_timesteps    | 5788000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.242   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 144699   |\n",
            "|    policy_loss        | -0.491   |\n",
            "|    value_loss         | 12.6     |\n",
            "------------------------------------\n",
            "Num timesteps: 5792000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 100.67\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 296      |\n",
            "|    ep_rew_mean        | 101      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1158     |\n",
            "|    iterations         | 144800   |\n",
            "|    time_elapsed       | 4997     |\n",
            "|    total_timesteps    | 5792000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.292   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 144799   |\n",
            "|    policy_loss        | 0.0949   |\n",
            "|    value_loss         | 3.24     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 290      |\n",
            "|    ep_rew_mean        | 107      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1159     |\n",
            "|    iterations         | 144900   |\n",
            "|    time_elapsed       | 5000     |\n",
            "|    total_timesteps    | 5796000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.31    |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 144899   |\n",
            "|    policy_loss        | -0.0521  |\n",
            "|    value_loss         | 2.51     |\n",
            "------------------------------------\n",
            "Num timesteps: 5800000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 108.71\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 288      |\n",
            "|    ep_rew_mean        | 109      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1159     |\n",
            "|    iterations         | 145000   |\n",
            "|    time_elapsed       | 5003     |\n",
            "|    total_timesteps    | 5800000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.373   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 144999   |\n",
            "|    policy_loss        | -0.159   |\n",
            "|    value_loss         | 6.95     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 292      |\n",
            "|    ep_rew_mean        | 95.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1159     |\n",
            "|    iterations         | 145100   |\n",
            "|    time_elapsed       | 5005     |\n",
            "|    total_timesteps    | 5804000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.364   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 145099   |\n",
            "|    policy_loss        | -0.155   |\n",
            "|    value_loss         | 5.62     |\n",
            "------------------------------------\n",
            "Num timesteps: 5808000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 87.32\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 295      |\n",
            "|    ep_rew_mean        | 87.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1159     |\n",
            "|    iterations         | 145200   |\n",
            "|    time_elapsed       | 5007     |\n",
            "|    total_timesteps    | 5808000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.44    |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 145199   |\n",
            "|    policy_loss        | -0.0585  |\n",
            "|    value_loss         | 1.77     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 301      |\n",
            "|    ep_rew_mean        | 82.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1160     |\n",
            "|    iterations         | 145300   |\n",
            "|    time_elapsed       | 5009     |\n",
            "|    total_timesteps    | 5812000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.442   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 145299   |\n",
            "|    policy_loss        | -0.12    |\n",
            "|    value_loss         | 1.73     |\n",
            "------------------------------------\n",
            "Num timesteps: 5816000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 73.51\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 292      |\n",
            "|    ep_rew_mean        | 73.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1160     |\n",
            "|    iterations         | 145400   |\n",
            "|    time_elapsed       | 5011     |\n",
            "|    total_timesteps    | 5816000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.364   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 145399   |\n",
            "|    policy_loss        | 0.161    |\n",
            "|    value_loss         | 1.61     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 266      |\n",
            "|    ep_rew_mean        | 65.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1160     |\n",
            "|    iterations         | 145500   |\n",
            "|    time_elapsed       | 5013     |\n",
            "|    total_timesteps    | 5820000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.414   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 145499   |\n",
            "|    policy_loss        | 0.0882   |\n",
            "|    value_loss         | 1.11     |\n",
            "------------------------------------\n",
            "Num timesteps: 5824000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 56.53\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 267      |\n",
            "|    ep_rew_mean        | 56.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1161     |\n",
            "|    iterations         | 145600   |\n",
            "|    time_elapsed       | 5014     |\n",
            "|    total_timesteps    | 5824000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.239   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 145599   |\n",
            "|    policy_loss        | 0.0559   |\n",
            "|    value_loss         | 2.03     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 254      |\n",
            "|    ep_rew_mean        | 65.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1161     |\n",
            "|    iterations         | 145700   |\n",
            "|    time_elapsed       | 5016     |\n",
            "|    total_timesteps    | 5828000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.392   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 145699   |\n",
            "|    policy_loss        | -0.0932  |\n",
            "|    value_loss         | 2.7      |\n",
            "------------------------------------\n",
            "Num timesteps: 5832000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 67.91\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 249      |\n",
            "|    ep_rew_mean        | 67.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1162     |\n",
            "|    iterations         | 145800   |\n",
            "|    time_elapsed       | 5018     |\n",
            "|    total_timesteps    | 5832000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.377   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 145799   |\n",
            "|    policy_loss        | 0.0711   |\n",
            "|    value_loss         | 3.24     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 238      |\n",
            "|    ep_rew_mean        | 68.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1162     |\n",
            "|    iterations         | 145900   |\n",
            "|    time_elapsed       | 5020     |\n",
            "|    total_timesteps    | 5836000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.386   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 145899   |\n",
            "|    policy_loss        | 0.0391   |\n",
            "|    value_loss         | 6.14     |\n",
            "------------------------------------\n",
            "Num timesteps: 5840000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 67.64\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 237      |\n",
            "|    ep_rew_mean        | 67.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1162     |\n",
            "|    iterations         | 146000   |\n",
            "|    time_elapsed       | 5022     |\n",
            "|    total_timesteps    | 5840000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.379   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 145999   |\n",
            "|    policy_loss        | 0.0708   |\n",
            "|    value_loss         | 2.08     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 242      |\n",
            "|    ep_rew_mean        | 70.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1163     |\n",
            "|    iterations         | 146100   |\n",
            "|    time_elapsed       | 5024     |\n",
            "|    total_timesteps    | 5844000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.377   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 146099   |\n",
            "|    policy_loss        | 0.157    |\n",
            "|    value_loss         | 5.75     |\n",
            "------------------------------------\n",
            "Num timesteps: 5848000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 74.38\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 242      |\n",
            "|    ep_rew_mean        | 74.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1163     |\n",
            "|    iterations         | 146200   |\n",
            "|    time_elapsed       | 5026     |\n",
            "|    total_timesteps    | 5848000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.286   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 146199   |\n",
            "|    policy_loss        | 0.195    |\n",
            "|    value_loss         | 17.7     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 257      |\n",
            "|    ep_rew_mean        | 79.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1163     |\n",
            "|    iterations         | 146300   |\n",
            "|    time_elapsed       | 5028     |\n",
            "|    total_timesteps    | 5852000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.293   |\n",
            "|    explained_variance | 0.0134   |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 146299   |\n",
            "|    policy_loss        | -0.813   |\n",
            "|    value_loss         | 1.91e+03 |\n",
            "------------------------------------\n",
            "Num timesteps: 5856000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 80.25\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 261      |\n",
            "|    ep_rew_mean        | 80.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1163     |\n",
            "|    iterations         | 146400   |\n",
            "|    time_elapsed       | 5031     |\n",
            "|    total_timesteps    | 5856000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.352   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 146399   |\n",
            "|    policy_loss        | -0.369   |\n",
            "|    value_loss         | 4.67     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 276      |\n",
            "|    ep_rew_mean        | 90.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1164     |\n",
            "|    iterations         | 146500   |\n",
            "|    time_elapsed       | 5033     |\n",
            "|    total_timesteps    | 5860000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.377   |\n",
            "|    explained_variance | 0.385    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 146499   |\n",
            "|    policy_loss        | 0.275    |\n",
            "|    value_loss         | 430      |\n",
            "------------------------------------\n",
            "Num timesteps: 5864000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 91.36\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 284      |\n",
            "|    ep_rew_mean        | 91.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1164     |\n",
            "|    iterations         | 146600   |\n",
            "|    time_elapsed       | 5035     |\n",
            "|    total_timesteps    | 5864000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.266   |\n",
            "|    explained_variance | 0.952    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 146599   |\n",
            "|    policy_loss        | -0.0732  |\n",
            "|    value_loss         | 43       |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 294      |\n",
            "|    ep_rew_mean        | 92.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1164     |\n",
            "|    iterations         | 146700   |\n",
            "|    time_elapsed       | 5037     |\n",
            "|    total_timesteps    | 5868000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.391   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 146699   |\n",
            "|    policy_loss        | 0.108    |\n",
            "|    value_loss         | 7.07     |\n",
            "------------------------------------\n",
            "Num timesteps: 5872000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 98.57\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 296      |\n",
            "|    ep_rew_mean        | 98.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1165     |\n",
            "|    iterations         | 146800   |\n",
            "|    time_elapsed       | 5039     |\n",
            "|    total_timesteps    | 5872000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.352   |\n",
            "|    explained_variance | 0.957    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 146799   |\n",
            "|    policy_loss        | 0.00177  |\n",
            "|    value_loss         | 10.8     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 296      |\n",
            "|    ep_rew_mean        | 104      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1165     |\n",
            "|    iterations         | 146900   |\n",
            "|    time_elapsed       | 5041     |\n",
            "|    total_timesteps    | 5876000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.21    |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 146899   |\n",
            "|    policy_loss        | 0.0418   |\n",
            "|    value_loss         | 1.67     |\n",
            "------------------------------------\n",
            "Num timesteps: 5880000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 99.93\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 273      |\n",
            "|    ep_rew_mean        | 99.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1165     |\n",
            "|    iterations         | 147000   |\n",
            "|    time_elapsed       | 5043     |\n",
            "|    total_timesteps    | 5880000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.317   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 146999   |\n",
            "|    policy_loss        | 0.277    |\n",
            "|    value_loss         | 5.11     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 251      |\n",
            "|    ep_rew_mean        | 77.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1166     |\n",
            "|    iterations         | 147100   |\n",
            "|    time_elapsed       | 5044     |\n",
            "|    total_timesteps    | 5884000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.341   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 147099   |\n",
            "|    policy_loss        | 0.537    |\n",
            "|    value_loss         | 16.5     |\n",
            "------------------------------------\n",
            "Num timesteps: 5888000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 76.62\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 244      |\n",
            "|    ep_rew_mean        | 76.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1166     |\n",
            "|    iterations         | 147200   |\n",
            "|    time_elapsed       | 5046     |\n",
            "|    total_timesteps    | 5888000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.331   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 147199   |\n",
            "|    policy_loss        | -0.0716  |\n",
            "|    value_loss         | 3.02     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 248      |\n",
            "|    ep_rew_mean        | 76.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1167     |\n",
            "|    iterations         | 147300   |\n",
            "|    time_elapsed       | 5048     |\n",
            "|    total_timesteps    | 5892000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.42    |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 147299   |\n",
            "|    policy_loss        | -0.0373  |\n",
            "|    value_loss         | 1.93     |\n",
            "------------------------------------\n",
            "Num timesteps: 5896000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 63.70\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 243      |\n",
            "|    ep_rew_mean        | 63.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1167     |\n",
            "|    iterations         | 147400   |\n",
            "|    time_elapsed       | 5050     |\n",
            "|    total_timesteps    | 5896000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.36    |\n",
            "|    explained_variance | 0.979    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 147399   |\n",
            "|    policy_loss        | -0.973   |\n",
            "|    value_loss         | 43.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 244      |\n",
            "|    ep_rew_mean        | 54.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1167     |\n",
            "|    iterations         | 147500   |\n",
            "|    time_elapsed       | 5052     |\n",
            "|    total_timesteps    | 5900000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.273   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 147499   |\n",
            "|    policy_loss        | -0.218   |\n",
            "|    value_loss         | 1.71     |\n",
            "------------------------------------\n",
            "Num timesteps: 5904000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 52.89\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 255      |\n",
            "|    ep_rew_mean        | 52.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1167     |\n",
            "|    iterations         | 147600   |\n",
            "|    time_elapsed       | 5055     |\n",
            "|    total_timesteps    | 5904000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.356   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 147599   |\n",
            "|    policy_loss        | 0.0343   |\n",
            "|    value_loss         | 2.18     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 264      |\n",
            "|    ep_rew_mean        | 59.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1168     |\n",
            "|    iterations         | 147700   |\n",
            "|    time_elapsed       | 5057     |\n",
            "|    total_timesteps    | 5908000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.389   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 147699   |\n",
            "|    policy_loss        | 0.263    |\n",
            "|    value_loss         | 1.93     |\n",
            "------------------------------------\n",
            "Num timesteps: 5912000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 69.99\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 270      |\n",
            "|    ep_rew_mean        | 70       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1168     |\n",
            "|    iterations         | 147800   |\n",
            "|    time_elapsed       | 5059     |\n",
            "|    total_timesteps    | 5912000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.445   |\n",
            "|    explained_variance | 0.896    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 147799   |\n",
            "|    policy_loss        | 1.98     |\n",
            "|    value_loss         | 112      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 278      |\n",
            "|    ep_rew_mean        | 68.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1168     |\n",
            "|    iterations         | 147900   |\n",
            "|    time_elapsed       | 5061     |\n",
            "|    total_timesteps    | 5916000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.485   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 147899   |\n",
            "|    policy_loss        | -0.298   |\n",
            "|    value_loss         | 5.4      |\n",
            "------------------------------------\n",
            "Num timesteps: 5920000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 64.90\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 278      |\n",
            "|    ep_rew_mean        | 64.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1169     |\n",
            "|    iterations         | 148000   |\n",
            "|    time_elapsed       | 5064     |\n",
            "|    total_timesteps    | 5920000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.355   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 147999   |\n",
            "|    policy_loss        | 0.208    |\n",
            "|    value_loss         | 3.69     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 289      |\n",
            "|    ep_rew_mean        | 57.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1169     |\n",
            "|    iterations         | 148100   |\n",
            "|    time_elapsed       | 5066     |\n",
            "|    total_timesteps    | 5924000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.327   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 148099   |\n",
            "|    policy_loss        | -0.354   |\n",
            "|    value_loss         | 2.1      |\n",
            "------------------------------------\n",
            "Num timesteps: 5928000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 65.63\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 297      |\n",
            "|    ep_rew_mean        | 65.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1169     |\n",
            "|    iterations         | 148200   |\n",
            "|    time_elapsed       | 5068     |\n",
            "|    total_timesteps    | 5928000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.316   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 148199   |\n",
            "|    policy_loss        | 0.0338   |\n",
            "|    value_loss         | 5.59     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 307      |\n",
            "|    ep_rew_mean        | 68.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1169     |\n",
            "|    iterations         | 148300   |\n",
            "|    time_elapsed       | 5071     |\n",
            "|    total_timesteps    | 5932000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.338   |\n",
            "|    explained_variance | 0.638    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 148299   |\n",
            "|    policy_loss        | -0.207   |\n",
            "|    value_loss         | 321      |\n",
            "------------------------------------\n",
            "Num timesteps: 5936000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 58.47\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 325      |\n",
            "|    ep_rew_mean        | 58.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1169     |\n",
            "|    iterations         | 148400   |\n",
            "|    time_elapsed       | 5075     |\n",
            "|    total_timesteps    | 5936000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.346   |\n",
            "|    explained_variance | 0.978    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 148399   |\n",
            "|    policy_loss        | -0.675   |\n",
            "|    value_loss         | 18.6     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 336      |\n",
            "|    ep_rew_mean        | 66       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1169     |\n",
            "|    iterations         | 148500   |\n",
            "|    time_elapsed       | 5078     |\n",
            "|    total_timesteps    | 5940000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.444   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 148499   |\n",
            "|    policy_loss        | 0.129    |\n",
            "|    value_loss         | 3.77     |\n",
            "------------------------------------\n",
            "Num timesteps: 5944000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 68.66\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 343      |\n",
            "|    ep_rew_mean        | 68.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1169     |\n",
            "|    iterations         | 148600   |\n",
            "|    time_elapsed       | 5082     |\n",
            "|    total_timesteps    | 5944000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.31    |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 148599   |\n",
            "|    policy_loss        | 0.0801   |\n",
            "|    value_loss         | 1.8      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 361      |\n",
            "|    ep_rew_mean        | 62.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1169     |\n",
            "|    iterations         | 148700   |\n",
            "|    time_elapsed       | 5085     |\n",
            "|    total_timesteps    | 5948000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.355   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 148699   |\n",
            "|    policy_loss        | -0.153   |\n",
            "|    value_loss         | 3.79     |\n",
            "------------------------------------\n",
            "Num timesteps: 5952000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 63.46\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 370      |\n",
            "|    ep_rew_mean        | 63.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1169     |\n",
            "|    iterations         | 148800   |\n",
            "|    time_elapsed       | 5088     |\n",
            "|    total_timesteps    | 5952000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.278   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 148799   |\n",
            "|    policy_loss        | -0.226   |\n",
            "|    value_loss         | 0.832    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 370      |\n",
            "|    ep_rew_mean        | 71.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1169     |\n",
            "|    iterations         | 148900   |\n",
            "|    time_elapsed       | 5091     |\n",
            "|    total_timesteps    | 5956000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.429   |\n",
            "|    explained_variance | 0.693    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 148899   |\n",
            "|    policy_loss        | -0.525   |\n",
            "|    value_loss         | 141      |\n",
            "------------------------------------\n",
            "Num timesteps: 5960000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 70.95\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 377      |\n",
            "|    ep_rew_mean        | 71       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1170     |\n",
            "|    iterations         | 149000   |\n",
            "|    time_elapsed       | 5093     |\n",
            "|    total_timesteps    | 5960000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.361   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 148999   |\n",
            "|    policy_loss        | 0.4      |\n",
            "|    value_loss         | 3.85     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 387      |\n",
            "|    ep_rew_mean        | 80.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1170     |\n",
            "|    iterations         | 149100   |\n",
            "|    time_elapsed       | 5097     |\n",
            "|    total_timesteps    | 5964000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.505   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 149099   |\n",
            "|    policy_loss        | -0.248   |\n",
            "|    value_loss         | 1.3      |\n",
            "------------------------------------\n",
            "Num timesteps: 5968000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 76.20\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 406      |\n",
            "|    ep_rew_mean        | 76.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1169     |\n",
            "|    iterations         | 149200   |\n",
            "|    time_elapsed       | 5101     |\n",
            "|    total_timesteps    | 5968000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.365   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 149199   |\n",
            "|    policy_loss        | 0.339    |\n",
            "|    value_loss         | 4.86     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 410      |\n",
            "|    ep_rew_mean        | 79.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1170     |\n",
            "|    iterations         | 149300   |\n",
            "|    time_elapsed       | 5103     |\n",
            "|    total_timesteps    | 5972000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.41    |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 149299   |\n",
            "|    policy_loss        | 0.392    |\n",
            "|    value_loss         | 7.39     |\n",
            "------------------------------------\n",
            "Num timesteps: 5976000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 83.37\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 402      |\n",
            "|    ep_rew_mean        | 83.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1170     |\n",
            "|    iterations         | 149400   |\n",
            "|    time_elapsed       | 5107     |\n",
            "|    total_timesteps    | 5976000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.417   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 149399   |\n",
            "|    policy_loss        | -0.135   |\n",
            "|    value_loss         | 0.826    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 412      |\n",
            "|    ep_rew_mean        | 83.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1170     |\n",
            "|    iterations         | 149500   |\n",
            "|    time_elapsed       | 5110     |\n",
            "|    total_timesteps    | 5980000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.421   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 149499   |\n",
            "|    policy_loss        | -0.146   |\n",
            "|    value_loss         | 2.58     |\n",
            "------------------------------------\n",
            "Num timesteps: 5984000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 84.64\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 418      |\n",
            "|    ep_rew_mean        | 84.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1170     |\n",
            "|    iterations         | 149600   |\n",
            "|    time_elapsed       | 5113     |\n",
            "|    total_timesteps    | 5984000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.277   |\n",
            "|    explained_variance | 0.479    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 149599   |\n",
            "|    policy_loss        | -0.214   |\n",
            "|    value_loss         | 640      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 425      |\n",
            "|    ep_rew_mean        | 91.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1170     |\n",
            "|    iterations         | 149700   |\n",
            "|    time_elapsed       | 5117     |\n",
            "|    total_timesteps    | 5988000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.412   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 149699   |\n",
            "|    policy_loss        | 0.286    |\n",
            "|    value_loss         | 2.25     |\n",
            "------------------------------------\n",
            "Num timesteps: 5992000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 95.90\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 442      |\n",
            "|    ep_rew_mean        | 95.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1170     |\n",
            "|    iterations         | 149800   |\n",
            "|    time_elapsed       | 5120     |\n",
            "|    total_timesteps    | 5992000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.385   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 149799   |\n",
            "|    policy_loss        | -0.168   |\n",
            "|    value_loss         | 0.373    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 467      |\n",
            "|    ep_rew_mean        | 101      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1169     |\n",
            "|    iterations         | 149900   |\n",
            "|    time_elapsed       | 5124     |\n",
            "|    total_timesteps    | 5996000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.329   |\n",
            "|    explained_variance | 0.957    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 149899   |\n",
            "|    policy_loss        | -0.174   |\n",
            "|    value_loss         | 12.1     |\n",
            "------------------------------------\n",
            "Num timesteps: 6000000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 96.78\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 460      |\n",
            "|    ep_rew_mean        | 96.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1170     |\n",
            "|    iterations         | 150000   |\n",
            "|    time_elapsed       | 5127     |\n",
            "|    total_timesteps    | 6000000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.192   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 149999   |\n",
            "|    policy_loss        | 0.0498   |\n",
            "|    value_loss         | 2.58     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 476      |\n",
            "|    ep_rew_mean        | 99.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1170     |\n",
            "|    iterations         | 150100   |\n",
            "|    time_elapsed       | 5130     |\n",
            "|    total_timesteps    | 6004000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.359   |\n",
            "|    explained_variance | 0.978    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 150099   |\n",
            "|    policy_loss        | 4.51e-05 |\n",
            "|    value_loss         | 1.68     |\n",
            "------------------------------------\n",
            "Num timesteps: 6008000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 111.93\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 489      |\n",
            "|    ep_rew_mean        | 112      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1170     |\n",
            "|    iterations         | 150200   |\n",
            "|    time_elapsed       | 5134     |\n",
            "|    total_timesteps    | 6008000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.239   |\n",
            "|    explained_variance | 0.517    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 150199   |\n",
            "|    policy_loss        | -0.0347  |\n",
            "|    value_loss         | 352      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 500      |\n",
            "|    ep_rew_mean        | 108      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1170     |\n",
            "|    iterations         | 150300   |\n",
            "|    time_elapsed       | 5137     |\n",
            "|    total_timesteps    | 6012000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.363   |\n",
            "|    explained_variance | 0.954    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 150299   |\n",
            "|    policy_loss        | -0.292   |\n",
            "|    value_loss         | 16.2     |\n",
            "------------------------------------\n",
            "Num timesteps: 6016000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 108.79\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 492      |\n",
            "|    ep_rew_mean        | 109      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1170     |\n",
            "|    iterations         | 150400   |\n",
            "|    time_elapsed       | 5140     |\n",
            "|    total_timesteps    | 6016000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.324   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 150399   |\n",
            "|    policy_loss        | -0.0496  |\n",
            "|    value_loss         | 0.633    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 503      |\n",
            "|    ep_rew_mean        | 113      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1170     |\n",
            "|    iterations         | 150500   |\n",
            "|    time_elapsed       | 5143     |\n",
            "|    total_timesteps    | 6020000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.506   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 150499   |\n",
            "|    policy_loss        | -0.471   |\n",
            "|    value_loss         | 4.77     |\n",
            "------------------------------------\n",
            "Num timesteps: 6024000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 101.71\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 500      |\n",
            "|    ep_rew_mean        | 102      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1170     |\n",
            "|    iterations         | 150600   |\n",
            "|    time_elapsed       | 5146     |\n",
            "|    total_timesteps    | 6024000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.284   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 150599   |\n",
            "|    policy_loss        | -0.00927 |\n",
            "|    value_loss         | 2.01     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 501      |\n",
            "|    ep_rew_mean        | 102      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1170     |\n",
            "|    iterations         | 150700   |\n",
            "|    time_elapsed       | 5150     |\n",
            "|    total_timesteps    | 6028000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.384   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 150699   |\n",
            "|    policy_loss        | -0.386   |\n",
            "|    value_loss         | 2.67     |\n",
            "------------------------------------\n",
            "Num timesteps: 6032000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 97.84\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 499      |\n",
            "|    ep_rew_mean        | 97.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1170     |\n",
            "|    iterations         | 150800   |\n",
            "|    time_elapsed       | 5153     |\n",
            "|    total_timesteps    | 6032000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.416   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 150799   |\n",
            "|    policy_loss        | 0.261    |\n",
            "|    value_loss         | 2.27     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 493      |\n",
            "|    ep_rew_mean        | 78.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1170     |\n",
            "|    iterations         | 150900   |\n",
            "|    time_elapsed       | 5156     |\n",
            "|    total_timesteps    | 6036000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.347   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 150899   |\n",
            "|    policy_loss        | 0.354    |\n",
            "|    value_loss         | 5.01     |\n",
            "------------------------------------\n",
            "Num timesteps: 6040000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 76.21\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 441      |\n",
            "|    ep_rew_mean        | 76.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1170     |\n",
            "|    iterations         | 151000   |\n",
            "|    time_elapsed       | 5158     |\n",
            "|    total_timesteps    | 6040000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.381   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 150999   |\n",
            "|    policy_loss        | 0.144    |\n",
            "|    value_loss         | 8.92     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 448      |\n",
            "|    ep_rew_mean        | 69.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1170     |\n",
            "|    iterations         | 151100   |\n",
            "|    time_elapsed       | 5162     |\n",
            "|    total_timesteps    | 6044000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.325   |\n",
            "|    explained_variance | 0.949    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 151099   |\n",
            "|    policy_loss        | 0.177    |\n",
            "|    value_loss         | 4.29     |\n",
            "------------------------------------\n",
            "Num timesteps: 6048000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 59.46\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 470      |\n",
            "|    ep_rew_mean        | 59.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1170     |\n",
            "|    iterations         | 151200   |\n",
            "|    time_elapsed       | 5165     |\n",
            "|    total_timesteps    | 6048000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.342   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 151199   |\n",
            "|    policy_loss        | -0.0744  |\n",
            "|    value_loss         | 0.247    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 432      |\n",
            "|    ep_rew_mean        | 60.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1170     |\n",
            "|    iterations         | 151300   |\n",
            "|    time_elapsed       | 5168     |\n",
            "|    total_timesteps    | 6052000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.421   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 151299   |\n",
            "|    policy_loss        | -0.299   |\n",
            "|    value_loss         | 1.49     |\n",
            "------------------------------------\n",
            "Num timesteps: 6056000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 61.56\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 421      |\n",
            "|    ep_rew_mean        | 61.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1171     |\n",
            "|    iterations         | 151400   |\n",
            "|    time_elapsed       | 5171     |\n",
            "|    total_timesteps    | 6056000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.302   |\n",
            "|    explained_variance | 0.88     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 151399   |\n",
            "|    policy_loss        | 0.06     |\n",
            "|    value_loss         | 57.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 425      |\n",
            "|    ep_rew_mean        | 59.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1170     |\n",
            "|    iterations         | 151500   |\n",
            "|    time_elapsed       | 5175     |\n",
            "|    total_timesteps    | 6060000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.389   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 151499   |\n",
            "|    policy_loss        | -0.269   |\n",
            "|    value_loss         | 1.29     |\n",
            "------------------------------------\n",
            "Num timesteps: 6064000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 57.99\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 418      |\n",
            "|    ep_rew_mean        | 58       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1171     |\n",
            "|    iterations         | 151600   |\n",
            "|    time_elapsed       | 5178     |\n",
            "|    total_timesteps    | 6064000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.433   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 151599   |\n",
            "|    policy_loss        | -0.0557  |\n",
            "|    value_loss         | 3.16     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 421      |\n",
            "|    ep_rew_mean        | 57       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1171     |\n",
            "|    iterations         | 151700   |\n",
            "|    time_elapsed       | 5181     |\n",
            "|    total_timesteps    | 6068000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.347   |\n",
            "|    explained_variance | 0.00209  |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 151699   |\n",
            "|    policy_loss        | -0.619   |\n",
            "|    value_loss         | 1.49e+03 |\n",
            "------------------------------------\n",
            "Num timesteps: 6072000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 53.95\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 421      |\n",
            "|    ep_rew_mean        | 53.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1171     |\n",
            "|    iterations         | 151800   |\n",
            "|    time_elapsed       | 5184     |\n",
            "|    total_timesteps    | 6072000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.406   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 151799   |\n",
            "|    policy_loss        | -0.0166  |\n",
            "|    value_loss         | 1.57     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 424      |\n",
            "|    ep_rew_mean        | 59.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1171     |\n",
            "|    iterations         | 151900   |\n",
            "|    time_elapsed       | 5187     |\n",
            "|    total_timesteps    | 6076000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.389   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 151899   |\n",
            "|    policy_loss        | 0.0841   |\n",
            "|    value_loss         | 2.54     |\n",
            "------------------------------------\n",
            "Num timesteps: 6080000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 68.08\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 439      |\n",
            "|    ep_rew_mean        | 68.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1171     |\n",
            "|    iterations         | 152000   |\n",
            "|    time_elapsed       | 5191     |\n",
            "|    total_timesteps    | 6080000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.442   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 151999   |\n",
            "|    policy_loss        | -0.385   |\n",
            "|    value_loss         | 1.33     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 443      |\n",
            "|    ep_rew_mean        | 63.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1171     |\n",
            "|    iterations         | 152100   |\n",
            "|    time_elapsed       | 5194     |\n",
            "|    total_timesteps    | 6084000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.305   |\n",
            "|    explained_variance | 0.962    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 152099   |\n",
            "|    policy_loss        | -0.0351  |\n",
            "|    value_loss         | 44.5     |\n",
            "------------------------------------\n",
            "Num timesteps: 6088000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 72.10\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 419      |\n",
            "|    ep_rew_mean        | 72.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1171     |\n",
            "|    iterations         | 152200   |\n",
            "|    time_elapsed       | 5196     |\n",
            "|    total_timesteps    | 6088000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.382   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 152199   |\n",
            "|    policy_loss        | -0.531   |\n",
            "|    value_loss         | 10.2     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 408      |\n",
            "|    ep_rew_mean        | 62.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1171     |\n",
            "|    iterations         | 152300   |\n",
            "|    time_elapsed       | 5199     |\n",
            "|    total_timesteps    | 6092000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.377   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 152299   |\n",
            "|    policy_loss        | -0.0317  |\n",
            "|    value_loss         | 0.691    |\n",
            "------------------------------------\n",
            "Num timesteps: 6096000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 70.03\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 406      |\n",
            "|    ep_rew_mean        | 70       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1172     |\n",
            "|    iterations         | 152400   |\n",
            "|    time_elapsed       | 5201     |\n",
            "|    total_timesteps    | 6096000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.343   |\n",
            "|    explained_variance | 0.911    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 152399   |\n",
            "|    policy_loss        | 0.03     |\n",
            "|    value_loss         | 22.1     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 375      |\n",
            "|    ep_rew_mean        | 68.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1172     |\n",
            "|    iterations         | 152500   |\n",
            "|    time_elapsed       | 5204     |\n",
            "|    total_timesteps    | 6100000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.339   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 152499   |\n",
            "|    policy_loss        | -0.0559  |\n",
            "|    value_loss         | 2.88     |\n",
            "------------------------------------\n",
            "Num timesteps: 6104000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 58.61\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 366      |\n",
            "|    ep_rew_mean        | 58.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1172     |\n",
            "|    iterations         | 152600   |\n",
            "|    time_elapsed       | 5206     |\n",
            "|    total_timesteps    | 6104000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.423   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 152599   |\n",
            "|    policy_loss        | 0.165    |\n",
            "|    value_loss         | 2.05     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 354      |\n",
            "|    ep_rew_mean        | 66       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1172     |\n",
            "|    iterations         | 152700   |\n",
            "|    time_elapsed       | 5209     |\n",
            "|    total_timesteps    | 6108000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.396   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 152699   |\n",
            "|    policy_loss        | 0.228    |\n",
            "|    value_loss         | 2.3      |\n",
            "------------------------------------\n",
            "Num timesteps: 6112000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 64.60\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 378      |\n",
            "|    ep_rew_mean        | 64.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1172     |\n",
            "|    iterations         | 152800   |\n",
            "|    time_elapsed       | 5213     |\n",
            "|    total_timesteps    | 6112000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.47    |\n",
            "|    explained_variance | 0.525    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 152799   |\n",
            "|    policy_loss        | -0.0114  |\n",
            "|    value_loss         | 537      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 369      |\n",
            "|    ep_rew_mean        | 58.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1172     |\n",
            "|    iterations         | 152900   |\n",
            "|    time_elapsed       | 5215     |\n",
            "|    total_timesteps    | 6116000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.468   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 152899   |\n",
            "|    policy_loss        | 0.023    |\n",
            "|    value_loss         | 3.07     |\n",
            "------------------------------------\n",
            "Num timesteps: 6120000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 69.62\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 360      |\n",
            "|    ep_rew_mean        | 69.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1172     |\n",
            "|    iterations         | 153000   |\n",
            "|    time_elapsed       | 5219     |\n",
            "|    total_timesteps    | 6120000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.396   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 152999   |\n",
            "|    policy_loss        | -0.00592 |\n",
            "|    value_loss         | 0.526    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 362      |\n",
            "|    ep_rew_mean        | 71.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1172     |\n",
            "|    iterations         | 153100   |\n",
            "|    time_elapsed       | 5221     |\n",
            "|    total_timesteps    | 6124000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.492   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 153099   |\n",
            "|    policy_loss        | -0.0508  |\n",
            "|    value_loss         | 0.93     |\n",
            "------------------------------------\n",
            "Num timesteps: 6128000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 76.89\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 351      |\n",
            "|    ep_rew_mean        | 76.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1173     |\n",
            "|    iterations         | 153200   |\n",
            "|    time_elapsed       | 5223     |\n",
            "|    total_timesteps    | 6128000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.483   |\n",
            "|    explained_variance | 0.897    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 153199   |\n",
            "|    policy_loss        | 0.292    |\n",
            "|    value_loss         | 8.82     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 360      |\n",
            "|    ep_rew_mean        | 67.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1173     |\n",
            "|    iterations         | 153300   |\n",
            "|    time_elapsed       | 5227     |\n",
            "|    total_timesteps    | 6132000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.457   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 153299   |\n",
            "|    policy_loss        | 0.0852   |\n",
            "|    value_loss         | 0.435    |\n",
            "------------------------------------\n",
            "Num timesteps: 6136000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 62.74\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 387      |\n",
            "|    ep_rew_mean        | 62.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1173     |\n",
            "|    iterations         | 153400   |\n",
            "|    time_elapsed       | 5230     |\n",
            "|    total_timesteps    | 6136000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.408   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 153399   |\n",
            "|    policy_loss        | -0.161   |\n",
            "|    value_loss         | 1.95     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 381      |\n",
            "|    ep_rew_mean        | 62.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1173     |\n",
            "|    iterations         | 153500   |\n",
            "|    time_elapsed       | 5233     |\n",
            "|    total_timesteps    | 6140000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.477   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 153499   |\n",
            "|    policy_loss        | -0.186   |\n",
            "|    value_loss         | 3.86     |\n",
            "------------------------------------\n",
            "Num timesteps: 6144000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 71.83\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 384      |\n",
            "|    ep_rew_mean        | 71.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1173     |\n",
            "|    iterations         | 153600   |\n",
            "|    time_elapsed       | 5236     |\n",
            "|    total_timesteps    | 6144000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.412   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 153599   |\n",
            "|    policy_loss        | 0.164    |\n",
            "|    value_loss         | 9.91     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 388      |\n",
            "|    ep_rew_mean        | 65.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1173     |\n",
            "|    iterations         | 153700   |\n",
            "|    time_elapsed       | 5239     |\n",
            "|    total_timesteps    | 6148000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.347   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 153699   |\n",
            "|    policy_loss        | 0.119    |\n",
            "|    value_loss         | 1.04     |\n",
            "------------------------------------\n",
            "Num timesteps: 6152000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 60.27\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 384      |\n",
            "|    ep_rew_mean        | 60.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1173     |\n",
            "|    iterations         | 153800   |\n",
            "|    time_elapsed       | 5242     |\n",
            "|    total_timesteps    | 6152000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.454   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 153799   |\n",
            "|    policy_loss        | -0.00942 |\n",
            "|    value_loss         | 0.509    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 403      |\n",
            "|    ep_rew_mean        | 61.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1173     |\n",
            "|    iterations         | 153900   |\n",
            "|    time_elapsed       | 5245     |\n",
            "|    total_timesteps    | 6156000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.355   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 153899   |\n",
            "|    policy_loss        | 0.0508   |\n",
            "|    value_loss         | 0.579    |\n",
            "------------------------------------\n",
            "Num timesteps: 6160000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 55.01\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 412      |\n",
            "|    ep_rew_mean        | 55       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1173     |\n",
            "|    iterations         | 154000   |\n",
            "|    time_elapsed       | 5248     |\n",
            "|    total_timesteps    | 6160000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.386   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 153999   |\n",
            "|    policy_loss        | 0.00281  |\n",
            "|    value_loss         | 0.653    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 398      |\n",
            "|    ep_rew_mean        | 46.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1173     |\n",
            "|    iterations         | 154100   |\n",
            "|    time_elapsed       | 5251     |\n",
            "|    total_timesteps    | 6164000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.278   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 154099   |\n",
            "|    policy_loss        | 0.0111   |\n",
            "|    value_loss         | 1.77     |\n",
            "------------------------------------\n",
            "Num timesteps: 6168000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 47.09\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 423      |\n",
            "|    ep_rew_mean        | 47.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1173     |\n",
            "|    iterations         | 154200   |\n",
            "|    time_elapsed       | 5254     |\n",
            "|    total_timesteps    | 6168000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.322   |\n",
            "|    explained_variance | 0.515    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 154199   |\n",
            "|    policy_loss        | 0.33     |\n",
            "|    value_loss         | 750      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 410      |\n",
            "|    ep_rew_mean        | 43.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1173     |\n",
            "|    iterations         | 154300   |\n",
            "|    time_elapsed       | 5257     |\n",
            "|    total_timesteps    | 6172000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.383   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 154299   |\n",
            "|    policy_loss        | 0.0177   |\n",
            "|    value_loss         | 1.31     |\n",
            "------------------------------------\n",
            "Num timesteps: 6176000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 43.86\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 400      |\n",
            "|    ep_rew_mean        | 43.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1173     |\n",
            "|    iterations         | 154400   |\n",
            "|    time_elapsed       | 5260     |\n",
            "|    total_timesteps    | 6176000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.509   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 154399   |\n",
            "|    policy_loss        | -0.094   |\n",
            "|    value_loss         | 1.58     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 416      |\n",
            "|    ep_rew_mean        | 41.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1174     |\n",
            "|    iterations         | 154500   |\n",
            "|    time_elapsed       | 5263     |\n",
            "|    total_timesteps    | 6180000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.363   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 154499   |\n",
            "|    policy_loss        | 0.0417   |\n",
            "|    value_loss         | 2.21     |\n",
            "------------------------------------\n",
            "Num timesteps: 6184000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 35.52\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 427      |\n",
            "|    ep_rew_mean        | 35.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1174     |\n",
            "|    iterations         | 154600   |\n",
            "|    time_elapsed       | 5266     |\n",
            "|    total_timesteps    | 6184000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.351   |\n",
            "|    explained_variance | 0.967    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 154599   |\n",
            "|    policy_loss        | 0.115    |\n",
            "|    value_loss         | 1.66     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 427      |\n",
            "|    ep_rew_mean        | 39.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1174     |\n",
            "|    iterations         | 154700   |\n",
            "|    time_elapsed       | 5270     |\n",
            "|    total_timesteps    | 6188000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.453   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 154699   |\n",
            "|    policy_loss        | 0.0558   |\n",
            "|    value_loss         | 1.23     |\n",
            "------------------------------------\n",
            "Num timesteps: 6192000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 42.79\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 440      |\n",
            "|    ep_rew_mean        | 42.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1174     |\n",
            "|    iterations         | 154800   |\n",
            "|    time_elapsed       | 5273     |\n",
            "|    total_timesteps    | 6192000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.346   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 154799   |\n",
            "|    policy_loss        | 0.229    |\n",
            "|    value_loss         | 1.52     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 412      |\n",
            "|    ep_rew_mean        | 48.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1174     |\n",
            "|    iterations         | 154900   |\n",
            "|    time_elapsed       | 5276     |\n",
            "|    total_timesteps    | 6196000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.399   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 154899   |\n",
            "|    policy_loss        | 0.109    |\n",
            "|    value_loss         | 2.49     |\n",
            "------------------------------------\n",
            "Num timesteps: 6200000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 47.52\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 392      |\n",
            "|    ep_rew_mean        | 47.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1174     |\n",
            "|    iterations         | 155000   |\n",
            "|    time_elapsed       | 5278     |\n",
            "|    total_timesteps    | 6200000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.421   |\n",
            "|    explained_variance | 0.913    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 154999   |\n",
            "|    policy_loss        | -0.00965 |\n",
            "|    value_loss         | 128      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 393      |\n",
            "|    ep_rew_mean        | 52.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1174     |\n",
            "|    iterations         | 155100   |\n",
            "|    time_elapsed       | 5281     |\n",
            "|    total_timesteps    | 6204000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.42    |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 155099   |\n",
            "|    policy_loss        | 0.0105   |\n",
            "|    value_loss         | 4.39     |\n",
            "------------------------------------\n",
            "Num timesteps: 6208000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 60.62\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 386      |\n",
            "|    ep_rew_mean        | 60.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1174     |\n",
            "|    iterations         | 155200   |\n",
            "|    time_elapsed       | 5284     |\n",
            "|    total_timesteps    | 6208000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.576   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 155199   |\n",
            "|    policy_loss        | -0.0357  |\n",
            "|    value_loss         | 0.588    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 408      |\n",
            "|    ep_rew_mean        | 57.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1174     |\n",
            "|    iterations         | 155300   |\n",
            "|    time_elapsed       | 5287     |\n",
            "|    total_timesteps    | 6212000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.336   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 155299   |\n",
            "|    policy_loss        | -0.11    |\n",
            "|    value_loss         | 6.42     |\n",
            "------------------------------------\n",
            "Num timesteps: 6216000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 64.70\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 420      |\n",
            "|    ep_rew_mean        | 64.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1174     |\n",
            "|    iterations         | 155400   |\n",
            "|    time_elapsed       | 5290     |\n",
            "|    total_timesteps    | 6216000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.363   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 155399   |\n",
            "|    policy_loss        | -0.185   |\n",
            "|    value_loss         | 2.28     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 422      |\n",
            "|    ep_rew_mean        | 62       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1175     |\n",
            "|    iterations         | 155500   |\n",
            "|    time_elapsed       | 5293     |\n",
            "|    total_timesteps    | 6220000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.417   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 155499   |\n",
            "|    policy_loss        | 0.392    |\n",
            "|    value_loss         | 3.01     |\n",
            "------------------------------------\n",
            "Num timesteps: 6224000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 68.43\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 431      |\n",
            "|    ep_rew_mean        | 68.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1175     |\n",
            "|    iterations         | 155600   |\n",
            "|    time_elapsed       | 5296     |\n",
            "|    total_timesteps    | 6224000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.42    |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 155599   |\n",
            "|    policy_loss        | 0.0344   |\n",
            "|    value_loss         | 0.635    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 420      |\n",
            "|    ep_rew_mean        | 75.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1175     |\n",
            "|    iterations         | 155700   |\n",
            "|    time_elapsed       | 5299     |\n",
            "|    total_timesteps    | 6228000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.419   |\n",
            "|    explained_variance | 0.977    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 155699   |\n",
            "|    policy_loss        | 0.188    |\n",
            "|    value_loss         | 1.45     |\n",
            "------------------------------------\n",
            "Num timesteps: 6232000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 71.99\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 415      |\n",
            "|    ep_rew_mean        | 72       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1175     |\n",
            "|    iterations         | 155800   |\n",
            "|    time_elapsed       | 5302     |\n",
            "|    total_timesteps    | 6232000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.445   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 155799   |\n",
            "|    policy_loss        | 0.0119   |\n",
            "|    value_loss         | 1.72     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 424      |\n",
            "|    ep_rew_mean        | 66.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1175     |\n",
            "|    iterations         | 155900   |\n",
            "|    time_elapsed       | 5306     |\n",
            "|    total_timesteps    | 6236000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.346   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 155899   |\n",
            "|    policy_loss        | -0.262   |\n",
            "|    value_loss         | 1.41     |\n",
            "------------------------------------\n",
            "Num timesteps: 6240000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 57.44\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 433      |\n",
            "|    ep_rew_mean        | 57.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1175     |\n",
            "|    iterations         | 156000   |\n",
            "|    time_elapsed       | 5309     |\n",
            "|    total_timesteps    | 6240000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.424   |\n",
            "|    explained_variance | 0.938    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 155999   |\n",
            "|    policy_loss        | -0.718   |\n",
            "|    value_loss         | 104      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 445      |\n",
            "|    ep_rew_mean        | 58.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1175     |\n",
            "|    iterations         | 156100   |\n",
            "|    time_elapsed       | 5311     |\n",
            "|    total_timesteps    | 6244000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.421   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 156099   |\n",
            "|    policy_loss        | -0.378   |\n",
            "|    value_loss         | 0.846    |\n",
            "------------------------------------\n",
            "Num timesteps: 6248000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 69.64\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 451      |\n",
            "|    ep_rew_mean        | 69.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1175     |\n",
            "|    iterations         | 156200   |\n",
            "|    time_elapsed       | 5314     |\n",
            "|    total_timesteps    | 6248000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.427   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 156199   |\n",
            "|    policy_loss        | -0.194   |\n",
            "|    value_loss         | 1.11     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 430      |\n",
            "|    ep_rew_mean        | 73.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1175     |\n",
            "|    iterations         | 156300   |\n",
            "|    time_elapsed       | 5316     |\n",
            "|    total_timesteps    | 6252000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.483   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 156299   |\n",
            "|    policy_loss        | -0.0671  |\n",
            "|    value_loss         | 1.94     |\n",
            "------------------------------------\n",
            "Num timesteps: 6256000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 79.60\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 421      |\n",
            "|    ep_rew_mean        | 79.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1175     |\n",
            "|    iterations         | 156400   |\n",
            "|    time_elapsed       | 5320     |\n",
            "|    total_timesteps    | 6256000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.35    |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 156399   |\n",
            "|    policy_loss        | -0.359   |\n",
            "|    value_loss         | 3.65     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 444      |\n",
            "|    ep_rew_mean        | 68.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1175     |\n",
            "|    iterations         | 156500   |\n",
            "|    time_elapsed       | 5323     |\n",
            "|    total_timesteps    | 6260000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.493   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 156499   |\n",
            "|    policy_loss        | 0.0307   |\n",
            "|    value_loss         | 1.16     |\n",
            "------------------------------------\n",
            "Num timesteps: 6264000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 72.04\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 442      |\n",
            "|    ep_rew_mean        | 72       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1176     |\n",
            "|    iterations         | 156600   |\n",
            "|    time_elapsed       | 5326     |\n",
            "|    total_timesteps    | 6264000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.35    |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 156599   |\n",
            "|    policy_loss        | -0.0432  |\n",
            "|    value_loss         | 0.315    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 439      |\n",
            "|    ep_rew_mean        | 66.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1176     |\n",
            "|    iterations         | 156700   |\n",
            "|    time_elapsed       | 5329     |\n",
            "|    total_timesteps    | 6268000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.555   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 156699   |\n",
            "|    policy_loss        | -0.545   |\n",
            "|    value_loss         | 15.1     |\n",
            "------------------------------------\n",
            "Num timesteps: 6272000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 75.58\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 422      |\n",
            "|    ep_rew_mean        | 75.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1176     |\n",
            "|    iterations         | 156800   |\n",
            "|    time_elapsed       | 5332     |\n",
            "|    total_timesteps    | 6272000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.408   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 156799   |\n",
            "|    policy_loss        | -0.29    |\n",
            "|    value_loss         | 3.15     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 417      |\n",
            "|    ep_rew_mean        | 73.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1176     |\n",
            "|    iterations         | 156900   |\n",
            "|    time_elapsed       | 5335     |\n",
            "|    total_timesteps    | 6276000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.39    |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 156899   |\n",
            "|    policy_loss        | 0.0861   |\n",
            "|    value_loss         | 1.36     |\n",
            "------------------------------------\n",
            "Num timesteps: 6280000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 72.73\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 415      |\n",
            "|    ep_rew_mean        | 72.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1176     |\n",
            "|    iterations         | 157000   |\n",
            "|    time_elapsed       | 5338     |\n",
            "|    total_timesteps    | 6280000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.339   |\n",
            "|    explained_variance | 0.765    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 156999   |\n",
            "|    policy_loss        | 1.22     |\n",
            "|    value_loss         | 139      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 413      |\n",
            "|    ep_rew_mean        | 80.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1176     |\n",
            "|    iterations         | 157100   |\n",
            "|    time_elapsed       | 5341     |\n",
            "|    total_timesteps    | 6284000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.439   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 157099   |\n",
            "|    policy_loss        | -0.248   |\n",
            "|    value_loss         | 5.83     |\n",
            "------------------------------------\n",
            "Num timesteps: 6288000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 80.63\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 425      |\n",
            "|    ep_rew_mean        | 80.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1176     |\n",
            "|    iterations         | 157200   |\n",
            "|    time_elapsed       | 5344     |\n",
            "|    total_timesteps    | 6288000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.328   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 157199   |\n",
            "|    policy_loss        | 0.0591   |\n",
            "|    value_loss         | 2.15     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 438      |\n",
            "|    ep_rew_mean        | 73.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1176     |\n",
            "|    iterations         | 157300   |\n",
            "|    time_elapsed       | 5347     |\n",
            "|    total_timesteps    | 6292000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.418   |\n",
            "|    explained_variance | 0.972    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 157299   |\n",
            "|    policy_loss        | -0.202   |\n",
            "|    value_loss         | 4.61     |\n",
            "------------------------------------\n",
            "Num timesteps: 6296000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 76.65\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 444      |\n",
            "|    ep_rew_mean        | 76.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1176     |\n",
            "|    iterations         | 157400   |\n",
            "|    time_elapsed       | 5350     |\n",
            "|    total_timesteps    | 6296000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.432   |\n",
            "|    explained_variance | 0.894    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 157399   |\n",
            "|    policy_loss        | -0.454   |\n",
            "|    value_loss         | 38.8     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 405      |\n",
            "|    ep_rew_mean        | 88.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1176     |\n",
            "|    iterations         | 157500   |\n",
            "|    time_elapsed       | 5352     |\n",
            "|    total_timesteps    | 6300000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.277   |\n",
            "|    explained_variance | 0.588    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 157499   |\n",
            "|    policy_loss        | -3.79    |\n",
            "|    value_loss         | 891      |\n",
            "------------------------------------\n",
            "Num timesteps: 6304000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 94.76\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 396      |\n",
            "|    ep_rew_mean        | 94.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1177     |\n",
            "|    iterations         | 157600   |\n",
            "|    time_elapsed       | 5355     |\n",
            "|    total_timesteps    | 6304000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.551   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 157599   |\n",
            "|    policy_loss        | -0.138   |\n",
            "|    value_loss         | 2.1      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 386      |\n",
            "|    ep_rew_mean        | 100      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1177     |\n",
            "|    iterations         | 157700   |\n",
            "|    time_elapsed       | 5358     |\n",
            "|    total_timesteps    | 6308000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.32    |\n",
            "|    explained_variance | 0.925    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 157699   |\n",
            "|    policy_loss        | -0.00906 |\n",
            "|    value_loss         | 29.9     |\n",
            "------------------------------------\n",
            "Num timesteps: 6312000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 99.09\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 401      |\n",
            "|    ep_rew_mean        | 99.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1177     |\n",
            "|    iterations         | 157800   |\n",
            "|    time_elapsed       | 5361     |\n",
            "|    total_timesteps    | 6312000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.401   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 157799   |\n",
            "|    policy_loss        | -0.171   |\n",
            "|    value_loss         | 1.15     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 408      |\n",
            "|    ep_rew_mean        | 105      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1177     |\n",
            "|    iterations         | 157900   |\n",
            "|    time_elapsed       | 5365     |\n",
            "|    total_timesteps    | 6316000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.347   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 157899   |\n",
            "|    policy_loss        | -0.0176  |\n",
            "|    value_loss         | 0.611    |\n",
            "------------------------------------\n",
            "Num timesteps: 6320000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 109.64\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 410      |\n",
            "|    ep_rew_mean        | 110      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1177     |\n",
            "|    iterations         | 158000   |\n",
            "|    time_elapsed       | 5368     |\n",
            "|    total_timesteps    | 6320000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.384   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 157999   |\n",
            "|    policy_loss        | -0.193   |\n",
            "|    value_loss         | 0.563    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 417      |\n",
            "|    ep_rew_mean        | 119      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1177     |\n",
            "|    iterations         | 158100   |\n",
            "|    time_elapsed       | 5372     |\n",
            "|    total_timesteps    | 6324000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.59    |\n",
            "|    explained_variance | 0.491    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 158099   |\n",
            "|    policy_loss        | 0.175    |\n",
            "|    value_loss         | 592      |\n",
            "------------------------------------\n",
            "Num timesteps: 6328000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 123.65\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 405      |\n",
            "|    ep_rew_mean        | 124      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1177     |\n",
            "|    iterations         | 158200   |\n",
            "|    time_elapsed       | 5375     |\n",
            "|    total_timesteps    | 6328000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.337   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 158199   |\n",
            "|    policy_loss        | -0.272   |\n",
            "|    value_loss         | 0.889    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 421      |\n",
            "|    ep_rew_mean        | 114      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1177     |\n",
            "|    iterations         | 158300   |\n",
            "|    time_elapsed       | 5379     |\n",
            "|    total_timesteps    | 6332000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.479   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 158299   |\n",
            "|    policy_loss        | -0.129   |\n",
            "|    value_loss         | 1.41     |\n",
            "------------------------------------\n",
            "Num timesteps: 6336000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 104.15\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 406      |\n",
            "|    ep_rew_mean        | 104      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1177     |\n",
            "|    iterations         | 158400   |\n",
            "|    time_elapsed       | 5382     |\n",
            "|    total_timesteps    | 6336000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.355   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 158399   |\n",
            "|    policy_loss        | 0.171    |\n",
            "|    value_loss         | 1.51     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 414      |\n",
            "|    ep_rew_mean        | 97.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1177     |\n",
            "|    iterations         | 158500   |\n",
            "|    time_elapsed       | 5385     |\n",
            "|    total_timesteps    | 6340000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.3     |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 158499   |\n",
            "|    policy_loss        | 0.0499   |\n",
            "|    value_loss         | 3        |\n",
            "------------------------------------\n",
            "Num timesteps: 6344000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 105.75\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 437      |\n",
            "|    ep_rew_mean        | 106      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1177     |\n",
            "|    iterations         | 158600   |\n",
            "|    time_elapsed       | 5388     |\n",
            "|    total_timesteps    | 6344000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.434   |\n",
            "|    explained_variance | 0.429    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 158599   |\n",
            "|    policy_loss        | 0.0869   |\n",
            "|    value_loss         | 196      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 449      |\n",
            "|    ep_rew_mean        | 108      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1177     |\n",
            "|    iterations         | 158700   |\n",
            "|    time_elapsed       | 5391     |\n",
            "|    total_timesteps    | 6348000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.445   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 158699   |\n",
            "|    policy_loss        | -0.0253  |\n",
            "|    value_loss         | 1.41     |\n",
            "------------------------------------\n",
            "Num timesteps: 6352000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 110.77\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 451      |\n",
            "|    ep_rew_mean        | 111      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1177     |\n",
            "|    iterations         | 158800   |\n",
            "|    time_elapsed       | 5394     |\n",
            "|    total_timesteps    | 6352000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.441   |\n",
            "|    explained_variance | 0.273    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 158799   |\n",
            "|    policy_loss        | 1.36     |\n",
            "|    value_loss         | 529      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 423      |\n",
            "|    ep_rew_mean        | 105      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1177     |\n",
            "|    iterations         | 158900   |\n",
            "|    time_elapsed       | 5397     |\n",
            "|    total_timesteps    | 6356000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.399   |\n",
            "|    explained_variance | 0.899    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 158899   |\n",
            "|    policy_loss        | 0.746    |\n",
            "|    value_loss         | 17.2     |\n",
            "------------------------------------\n",
            "Num timesteps: 6360000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 107.44\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 397      |\n",
            "|    ep_rew_mean        | 107      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1177     |\n",
            "|    iterations         | 159000   |\n",
            "|    time_elapsed       | 5399     |\n",
            "|    total_timesteps    | 6360000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.464   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 158999   |\n",
            "|    policy_loss        | -0.16    |\n",
            "|    value_loss         | 2.39     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 383      |\n",
            "|    ep_rew_mean        | 112      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1177     |\n",
            "|    iterations         | 159100   |\n",
            "|    time_elapsed       | 5403     |\n",
            "|    total_timesteps    | 6364000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.522   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 159099   |\n",
            "|    policy_loss        | -0.221   |\n",
            "|    value_loss         | 2.58     |\n",
            "------------------------------------\n",
            "Num timesteps: 6368000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 123.33\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 361      |\n",
            "|    ep_rew_mean        | 123      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1177     |\n",
            "|    iterations         | 159200   |\n",
            "|    time_elapsed       | 5405     |\n",
            "|    total_timesteps    | 6368000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.473   |\n",
            "|    explained_variance | 0.62     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 159199   |\n",
            "|    policy_loss        | -0.414   |\n",
            "|    value_loss         | 181      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 345      |\n",
            "|    ep_rew_mean        | 131      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1178     |\n",
            "|    iterations         | 159300   |\n",
            "|    time_elapsed       | 5407     |\n",
            "|    total_timesteps    | 6372000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.53    |\n",
            "|    explained_variance | 0.978    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 159299   |\n",
            "|    policy_loss        | 0.259    |\n",
            "|    value_loss         | 7.02     |\n",
            "------------------------------------\n",
            "Num timesteps: 6376000\n",
            "Best mean reward: 127.56 - Last mean reward per episode: 131.09\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 336      |\n",
            "|    ep_rew_mean        | 131      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1178     |\n",
            "|    iterations         | 159400   |\n",
            "|    time_elapsed       | 5410     |\n",
            "|    total_timesteps    | 6376000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.356   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 159399   |\n",
            "|    policy_loss        | 0.282    |\n",
            "|    value_loss         | 1.39     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 322      |\n",
            "|    ep_rew_mean        | 132      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1178     |\n",
            "|    iterations         | 159500   |\n",
            "|    time_elapsed       | 5412     |\n",
            "|    total_timesteps    | 6380000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.419   |\n",
            "|    explained_variance | 0.945    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 159499   |\n",
            "|    policy_loss        | 0.181    |\n",
            "|    value_loss         | 5.5      |\n",
            "------------------------------------\n",
            "Num timesteps: 6384000\n",
            "Best mean reward: 131.09 - Last mean reward per episode: 133.82\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 312      |\n",
            "|    ep_rew_mean        | 134      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1178     |\n",
            "|    iterations         | 159600   |\n",
            "|    time_elapsed       | 5414     |\n",
            "|    total_timesteps    | 6384000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.384   |\n",
            "|    explained_variance | 0.898    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 159599   |\n",
            "|    policy_loss        | -0.0942  |\n",
            "|    value_loss         | 34.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 304      |\n",
            "|    ep_rew_mean        | 128      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1179     |\n",
            "|    iterations         | 159700   |\n",
            "|    time_elapsed       | 5416     |\n",
            "|    total_timesteps    | 6388000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.397   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 159699   |\n",
            "|    policy_loss        | -0.267   |\n",
            "|    value_loss         | 1.82     |\n",
            "------------------------------------\n",
            "Num timesteps: 6392000\n",
            "Best mean reward: 133.82 - Last mean reward per episode: 126.37\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 306      |\n",
            "|    ep_rew_mean        | 126      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1179     |\n",
            "|    iterations         | 159800   |\n",
            "|    time_elapsed       | 5419     |\n",
            "|    total_timesteps    | 6392000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.393   |\n",
            "|    explained_variance | 0.133    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 159799   |\n",
            "|    policy_loss        | 0.333    |\n",
            "|    value_loss         | 1.79e+03 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 295      |\n",
            "|    ep_rew_mean        | 125      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1179     |\n",
            "|    iterations         | 159900   |\n",
            "|    time_elapsed       | 5422     |\n",
            "|    total_timesteps    | 6396000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.352   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 159899   |\n",
            "|    policy_loss        | -0.621   |\n",
            "|    value_loss         | 2.08     |\n",
            "------------------------------------\n",
            "Num timesteps: 6400000\n",
            "Best mean reward: 133.82 - Last mean reward per episode: 121.40\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 294      |\n",
            "|    ep_rew_mean        | 121      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1179     |\n",
            "|    iterations         | 160000   |\n",
            "|    time_elapsed       | 5424     |\n",
            "|    total_timesteps    | 6400000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.382   |\n",
            "|    explained_variance | 0.265    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 159999   |\n",
            "|    policy_loss        | 0.0325   |\n",
            "|    value_loss         | 571      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 281      |\n",
            "|    ep_rew_mean        | 113      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1180     |\n",
            "|    iterations         | 160100   |\n",
            "|    time_elapsed       | 5426     |\n",
            "|    total_timesteps    | 6404000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.347   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 160099   |\n",
            "|    policy_loss        | 0.0397   |\n",
            "|    value_loss         | 7.11     |\n",
            "------------------------------------\n",
            "Num timesteps: 6408000\n",
            "Best mean reward: 133.82 - Last mean reward per episode: 122.59\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 284      |\n",
            "|    ep_rew_mean        | 123      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1180     |\n",
            "|    iterations         | 160200   |\n",
            "|    time_elapsed       | 5428     |\n",
            "|    total_timesteps    | 6408000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.467   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 160199   |\n",
            "|    policy_loss        | -0.171   |\n",
            "|    value_loss         | 0.916    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 282      |\n",
            "|    ep_rew_mean        | 118      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1180     |\n",
            "|    iterations         | 160300   |\n",
            "|    time_elapsed       | 5430     |\n",
            "|    total_timesteps    | 6412000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.607   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 160299   |\n",
            "|    policy_loss        | -0.15    |\n",
            "|    value_loss         | 1.11     |\n",
            "------------------------------------\n",
            "Num timesteps: 6416000\n",
            "Best mean reward: 133.82 - Last mean reward per episode: 125.11\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 290      |\n",
            "|    ep_rew_mean        | 125      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1180     |\n",
            "|    iterations         | 160400   |\n",
            "|    time_elapsed       | 5433     |\n",
            "|    total_timesteps    | 6416000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.404   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 160399   |\n",
            "|    policy_loss        | -0.00105 |\n",
            "|    value_loss         | 1.18     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 284      |\n",
            "|    ep_rew_mean        | 131      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1181     |\n",
            "|    iterations         | 160500   |\n",
            "|    time_elapsed       | 5435     |\n",
            "|    total_timesteps    | 6420000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.261   |\n",
            "|    explained_variance | 0.42     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 160499   |\n",
            "|    policy_loss        | -0.343   |\n",
            "|    value_loss         | 957      |\n",
            "------------------------------------\n",
            "Num timesteps: 6424000\n",
            "Best mean reward: 133.82 - Last mean reward per episode: 137.58\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 289      |\n",
            "|    ep_rew_mean        | 138      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1181     |\n",
            "|    iterations         | 160600   |\n",
            "|    time_elapsed       | 5438     |\n",
            "|    total_timesteps    | 6424000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.288   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 160599   |\n",
            "|    policy_loss        | -0.14    |\n",
            "|    value_loss         | 0.315    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 288      |\n",
            "|    ep_rew_mean        | 140      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1181     |\n",
            "|    iterations         | 160700   |\n",
            "|    time_elapsed       | 5440     |\n",
            "|    total_timesteps    | 6428000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.491   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 160699   |\n",
            "|    policy_loss        | 0.249    |\n",
            "|    value_loss         | 0.752    |\n",
            "------------------------------------\n",
            "Num timesteps: 6432000\n",
            "Best mean reward: 137.58 - Last mean reward per episode: 143.25\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 296      |\n",
            "|    ep_rew_mean        | 143      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1181     |\n",
            "|    iterations         | 160800   |\n",
            "|    time_elapsed       | 5442     |\n",
            "|    total_timesteps    | 6432000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.593   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 160799   |\n",
            "|    policy_loss        | -0.191   |\n",
            "|    value_loss         | 1.18     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 285      |\n",
            "|    ep_rew_mean        | 134      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1181     |\n",
            "|    iterations         | 160900   |\n",
            "|    time_elapsed       | 5445     |\n",
            "|    total_timesteps    | 6436000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.322   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 160899   |\n",
            "|    policy_loss        | 0.0241   |\n",
            "|    value_loss         | 3.38     |\n",
            "------------------------------------\n",
            "Num timesteps: 6440000\n",
            "Best mean reward: 143.25 - Last mean reward per episode: 138.91\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 297      |\n",
            "|    ep_rew_mean        | 139      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1182     |\n",
            "|    iterations         | 161000   |\n",
            "|    time_elapsed       | 5447     |\n",
            "|    total_timesteps    | 6440000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.435   |\n",
            "|    explained_variance | 0.872    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 160999   |\n",
            "|    policy_loss        | 1.33     |\n",
            "|    value_loss         | 46.1     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 285      |\n",
            "|    ep_rew_mean        | 128      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1182     |\n",
            "|    iterations         | 161100   |\n",
            "|    time_elapsed       | 5449     |\n",
            "|    total_timesteps    | 6444000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.327   |\n",
            "|    explained_variance | 0.567    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 161099   |\n",
            "|    policy_loss        | -0.636   |\n",
            "|    value_loss         | 67.5     |\n",
            "------------------------------------\n",
            "Num timesteps: 6448000\n",
            "Best mean reward: 143.25 - Last mean reward per episode: 134.51\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 293      |\n",
            "|    ep_rew_mean        | 135      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1182     |\n",
            "|    iterations         | 161200   |\n",
            "|    time_elapsed       | 5451     |\n",
            "|    total_timesteps    | 6448000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.432   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 161199   |\n",
            "|    policy_loss        | -0.0857  |\n",
            "|    value_loss         | 0.86     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 278      |\n",
            "|    ep_rew_mean        | 126      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1183     |\n",
            "|    iterations         | 161300   |\n",
            "|    time_elapsed       | 5453     |\n",
            "|    total_timesteps    | 6452000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.517   |\n",
            "|    explained_variance | 0.894    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 161299   |\n",
            "|    policy_loss        | 1.34     |\n",
            "|    value_loss         | 21.7     |\n",
            "------------------------------------\n",
            "Num timesteps: 6456000\n",
            "Best mean reward: 143.25 - Last mean reward per episode: 124.74\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 272      |\n",
            "|    ep_rew_mean        | 125      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1183     |\n",
            "|    iterations         | 161400   |\n",
            "|    time_elapsed       | 5455     |\n",
            "|    total_timesteps    | 6456000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.253   |\n",
            "|    explained_variance | 0.156    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 161399   |\n",
            "|    policy_loss        | 1.43     |\n",
            "|    value_loss         | 819      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 269      |\n",
            "|    ep_rew_mean        | 123      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1183     |\n",
            "|    iterations         | 161500   |\n",
            "|    time_elapsed       | 5457     |\n",
            "|    total_timesteps    | 6460000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.277   |\n",
            "|    explained_variance | 0.467    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 161499   |\n",
            "|    policy_loss        | -1.3     |\n",
            "|    value_loss         | 31.6     |\n",
            "------------------------------------\n",
            "Num timesteps: 6464000\n",
            "Best mean reward: 143.25 - Last mean reward per episode: 122.89\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 261      |\n",
            "|    ep_rew_mean        | 123      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1183     |\n",
            "|    iterations         | 161600   |\n",
            "|    time_elapsed       | 5459     |\n",
            "|    total_timesteps    | 6464000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.558   |\n",
            "|    explained_variance | 0.967    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 161599   |\n",
            "|    policy_loss        | -0.313   |\n",
            "|    value_loss         | 10.1     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 258      |\n",
            "|    ep_rew_mean        | 121      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1184     |\n",
            "|    iterations         | 161700   |\n",
            "|    time_elapsed       | 5462     |\n",
            "|    total_timesteps    | 6468000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.43    |\n",
            "|    explained_variance | 0.973    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 161699   |\n",
            "|    policy_loss        | -0.115   |\n",
            "|    value_loss         | 7.42     |\n",
            "------------------------------------\n",
            "Num timesteps: 6472000\n",
            "Best mean reward: 143.25 - Last mean reward per episode: 112.62\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 255      |\n",
            "|    ep_rew_mean        | 113      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1184     |\n",
            "|    iterations         | 161800   |\n",
            "|    time_elapsed       | 5464     |\n",
            "|    total_timesteps    | 6472000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.407   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 161799   |\n",
            "|    policy_loss        | 0.32     |\n",
            "|    value_loss         | 2.2      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 244      |\n",
            "|    ep_rew_mean        | 98.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1184     |\n",
            "|    iterations         | 161900   |\n",
            "|    time_elapsed       | 5466     |\n",
            "|    total_timesteps    | 6476000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.414   |\n",
            "|    explained_variance | 0.393    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 161899   |\n",
            "|    policy_loss        | -0.00154 |\n",
            "|    value_loss         | 427      |\n",
            "------------------------------------\n",
            "Num timesteps: 6480000\n",
            "Best mean reward: 143.25 - Last mean reward per episode: 95.34\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 239      |\n",
            "|    ep_rew_mean        | 95.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1184     |\n",
            "|    iterations         | 162000   |\n",
            "|    time_elapsed       | 5468     |\n",
            "|    total_timesteps    | 6480000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.385   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 161999   |\n",
            "|    policy_loss        | 0.0339   |\n",
            "|    value_loss         | 0.509    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 244      |\n",
            "|    ep_rew_mean        | 91.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1185     |\n",
            "|    iterations         | 162100   |\n",
            "|    time_elapsed       | 5470     |\n",
            "|    total_timesteps    | 6484000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.383   |\n",
            "|    explained_variance | 0.915    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 162099   |\n",
            "|    policy_loss        | 0.313    |\n",
            "|    value_loss         | 181      |\n",
            "------------------------------------\n",
            "Num timesteps: 6488000\n",
            "Best mean reward: 143.25 - Last mean reward per episode: 83.05\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 241      |\n",
            "|    ep_rew_mean        | 83.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1185     |\n",
            "|    iterations         | 162200   |\n",
            "|    time_elapsed       | 5472     |\n",
            "|    total_timesteps    | 6488000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.4     |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 162199   |\n",
            "|    policy_loss        | 0.103    |\n",
            "|    value_loss         | 1.06     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 238      |\n",
            "|    ep_rew_mean        | 79.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1185     |\n",
            "|    iterations         | 162300   |\n",
            "|    time_elapsed       | 5475     |\n",
            "|    total_timesteps    | 6492000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.377   |\n",
            "|    explained_variance | 0.945    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 162299   |\n",
            "|    policy_loss        | -0.126   |\n",
            "|    value_loss         | 46.1     |\n",
            "------------------------------------\n",
            "Num timesteps: 6496000\n",
            "Best mean reward: 143.25 - Last mean reward per episode: 75.97\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 248      |\n",
            "|    ep_rew_mean        | 76       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1185     |\n",
            "|    iterations         | 162400   |\n",
            "|    time_elapsed       | 5477     |\n",
            "|    total_timesteps    | 6496000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.597   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 162399   |\n",
            "|    policy_loss        | 0.04     |\n",
            "|    value_loss         | 1.07     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 253      |\n",
            "|    ep_rew_mean        | 83.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1186     |\n",
            "|    iterations         | 162500   |\n",
            "|    time_elapsed       | 5479     |\n",
            "|    total_timesteps    | 6500000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.498   |\n",
            "|    explained_variance | 0.626    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 162499   |\n",
            "|    policy_loss        | 0.0489   |\n",
            "|    value_loss         | 276      |\n",
            "------------------------------------\n",
            "Num timesteps: 6504000\n",
            "Best mean reward: 143.25 - Last mean reward per episode: 78.03\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 252      |\n",
            "|    ep_rew_mean        | 78       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1186     |\n",
            "|    iterations         | 162600   |\n",
            "|    time_elapsed       | 5482     |\n",
            "|    total_timesteps    | 6504000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.35    |\n",
            "|    explained_variance | 0.322    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 162599   |\n",
            "|    policy_loss        | -0.115   |\n",
            "|    value_loss         | 785      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 258      |\n",
            "|    ep_rew_mean        | 78.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1186     |\n",
            "|    iterations         | 162700   |\n",
            "|    time_elapsed       | 5484     |\n",
            "|    total_timesteps    | 6508000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.311   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 162699   |\n",
            "|    policy_loss        | 0.222    |\n",
            "|    value_loss         | 3.08     |\n",
            "------------------------------------\n",
            "Num timesteps: 6512000\n",
            "Best mean reward: 143.25 - Last mean reward per episode: 75.47\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 261      |\n",
            "|    ep_rew_mean        | 75.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1186     |\n",
            "|    iterations         | 162800   |\n",
            "|    time_elapsed       | 5487     |\n",
            "|    total_timesteps    | 6512000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.452   |\n",
            "|    explained_variance | 0.904    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 162799   |\n",
            "|    policy_loss        | 0.315    |\n",
            "|    value_loss         | 53.2     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 260      |\n",
            "|    ep_rew_mean        | 76       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1187     |\n",
            "|    iterations         | 162900   |\n",
            "|    time_elapsed       | 5489     |\n",
            "|    total_timesteps    | 6516000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.415   |\n",
            "|    explained_variance | 0.699    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 162899   |\n",
            "|    policy_loss        | -3.87    |\n",
            "|    value_loss         | 686      |\n",
            "------------------------------------\n",
            "Num timesteps: 6520000\n",
            "Best mean reward: 143.25 - Last mean reward per episode: 81.51\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 264      |\n",
            "|    ep_rew_mean        | 81.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1187     |\n",
            "|    iterations         | 163000   |\n",
            "|    time_elapsed       | 5491     |\n",
            "|    total_timesteps    | 6520000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.354   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 162999   |\n",
            "|    policy_loss        | 0.171    |\n",
            "|    value_loss         | 1.8      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 248      |\n",
            "|    ep_rew_mean        | 82       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1187     |\n",
            "|    iterations         | 163100   |\n",
            "|    time_elapsed       | 5493     |\n",
            "|    total_timesteps    | 6524000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.471   |\n",
            "|    explained_variance | 0.8      |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 163099   |\n",
            "|    policy_loss        | -1.1     |\n",
            "|    value_loss         | 242      |\n",
            "------------------------------------\n",
            "Num timesteps: 6528000\n",
            "Best mean reward: 143.25 - Last mean reward per episode: 83.75\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 245      |\n",
            "|    ep_rew_mean        | 83.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1187     |\n",
            "|    iterations         | 163200   |\n",
            "|    time_elapsed       | 5495     |\n",
            "|    total_timesteps    | 6528000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.557   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 163199   |\n",
            "|    policy_loss        | -0.0631  |\n",
            "|    value_loss         | 1.44     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 251      |\n",
            "|    ep_rew_mean        | 78.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1188     |\n",
            "|    iterations         | 163300   |\n",
            "|    time_elapsed       | 5498     |\n",
            "|    total_timesteps    | 6532000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.537   |\n",
            "|    explained_variance | 0.795    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 163299   |\n",
            "|    policy_loss        | -5.11    |\n",
            "|    value_loss         | 263      |\n",
            "------------------------------------\n",
            "Num timesteps: 6536000\n",
            "Best mean reward: 143.25 - Last mean reward per episode: 73.94\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 235      |\n",
            "|    ep_rew_mean        | 73.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1188     |\n",
            "|    iterations         | 163400   |\n",
            "|    time_elapsed       | 5499     |\n",
            "|    total_timesteps    | 6536000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.392   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 163399   |\n",
            "|    policy_loss        | 0.201    |\n",
            "|    value_loss         | 15.2     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 225      |\n",
            "|    ep_rew_mean        | 70.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1188     |\n",
            "|    iterations         | 163500   |\n",
            "|    time_elapsed       | 5501     |\n",
            "|    total_timesteps    | 6540000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.515   |\n",
            "|    explained_variance | 0.809    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 163499   |\n",
            "|    policy_loss        | -1.02    |\n",
            "|    value_loss         | 195      |\n",
            "------------------------------------\n",
            "Num timesteps: 6544000\n",
            "Best mean reward: 143.25 - Last mean reward per episode: 57.59\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 222      |\n",
            "|    ep_rew_mean        | 57.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1188     |\n",
            "|    iterations         | 163600   |\n",
            "|    time_elapsed       | 5504     |\n",
            "|    total_timesteps    | 6544000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.315   |\n",
            "|    explained_variance | 0.714    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 163599   |\n",
            "|    policy_loss        | 1.31     |\n",
            "|    value_loss         | 369      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 237      |\n",
            "|    ep_rew_mean        | 55.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1189     |\n",
            "|    iterations         | 163700   |\n",
            "|    time_elapsed       | 5506     |\n",
            "|    total_timesteps    | 6548000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.206   |\n",
            "|    explained_variance | 0.922    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 163699   |\n",
            "|    policy_loss        | 0.0631   |\n",
            "|    value_loss         | 70.5     |\n",
            "------------------------------------\n",
            "Num timesteps: 6552000\n",
            "Best mean reward: 143.25 - Last mean reward per episode: 57.83\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 213      |\n",
            "|    ep_rew_mean        | 57.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1189     |\n",
            "|    iterations         | 163800   |\n",
            "|    time_elapsed       | 5508     |\n",
            "|    total_timesteps    | 6552000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.569   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 163799   |\n",
            "|    policy_loss        | -0.204   |\n",
            "|    value_loss         | 2.5      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 210      |\n",
            "|    ep_rew_mean        | 52.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1189     |\n",
            "|    iterations         | 163900   |\n",
            "|    time_elapsed       | 5511     |\n",
            "|    total_timesteps    | 6556000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.345   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 163899   |\n",
            "|    policy_loss        | 0.036    |\n",
            "|    value_loss         | 0.309    |\n",
            "------------------------------------\n",
            "Num timesteps: 6560000\n",
            "Best mean reward: 143.25 - Last mean reward per episode: 64.82\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 232      |\n",
            "|    ep_rew_mean        | 64.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1189     |\n",
            "|    iterations         | 164000   |\n",
            "|    time_elapsed       | 5513     |\n",
            "|    total_timesteps    | 6560000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.386   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 163999   |\n",
            "|    policy_loss        | 0.127    |\n",
            "|    value_loss         | 0.683    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 248      |\n",
            "|    ep_rew_mean        | 75       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1189     |\n",
            "|    iterations         | 164100   |\n",
            "|    time_elapsed       | 5516     |\n",
            "|    total_timesteps    | 6564000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.348   |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 164099   |\n",
            "|    policy_loss        | 0.0843   |\n",
            "|    value_loss         | 5.76     |\n",
            "------------------------------------\n",
            "Num timesteps: 6568000\n",
            "Best mean reward: 143.25 - Last mean reward per episode: 76.79\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 265      |\n",
            "|    ep_rew_mean        | 76.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1190     |\n",
            "|    iterations         | 164200   |\n",
            "|    time_elapsed       | 5518     |\n",
            "|    total_timesteps    | 6568000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.418   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 164199   |\n",
            "|    policy_loss        | 0.1      |\n",
            "|    value_loss         | 2.42     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 259      |\n",
            "|    ep_rew_mean        | 70.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1190     |\n",
            "|    iterations         | 164300   |\n",
            "|    time_elapsed       | 5520     |\n",
            "|    total_timesteps    | 6572000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.471   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 164299   |\n",
            "|    policy_loss        | -0.099   |\n",
            "|    value_loss         | 1.56     |\n",
            "------------------------------------\n",
            "Num timesteps: 6576000\n",
            "Best mean reward: 143.25 - Last mean reward per episode: 85.93\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 264      |\n",
            "|    ep_rew_mean        | 85.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1190     |\n",
            "|    iterations         | 164400   |\n",
            "|    time_elapsed       | 5523     |\n",
            "|    total_timesteps    | 6576000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.483   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 164399   |\n",
            "|    policy_loss        | 0.165    |\n",
            "|    value_loss         | 1.72     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 271      |\n",
            "|    ep_rew_mean        | 91.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1190     |\n",
            "|    iterations         | 164500   |\n",
            "|    time_elapsed       | 5525     |\n",
            "|    total_timesteps    | 6580000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.28    |\n",
            "|    explained_variance | 0.723    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 164499   |\n",
            "|    policy_loss        | -0.0824  |\n",
            "|    value_loss         | 206      |\n",
            "------------------------------------\n",
            "Num timesteps: 6584000\n",
            "Best mean reward: 143.25 - Last mean reward per episode: 93.65\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 261      |\n",
            "|    ep_rew_mean        | 93.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 164600   |\n",
            "|    time_elapsed       | 5527     |\n",
            "|    total_timesteps    | 6584000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.432   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 164599   |\n",
            "|    policy_loss        | -0.0949  |\n",
            "|    value_loss         | 0.757    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 252      |\n",
            "|    ep_rew_mean        | 88.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 164700   |\n",
            "|    time_elapsed       | 5529     |\n",
            "|    total_timesteps    | 6588000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.298   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 164699   |\n",
            "|    policy_loss        | 0.019    |\n",
            "|    value_loss         | 1.68     |\n",
            "------------------------------------\n",
            "Num timesteps: 6592000\n",
            "Best mean reward: 143.25 - Last mean reward per episode: 101.80\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 248      |\n",
            "|    ep_rew_mean        | 102      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 164800   |\n",
            "|    time_elapsed       | 5532     |\n",
            "|    total_timesteps    | 6592000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.4     |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 164799   |\n",
            "|    policy_loss        | 0.0949   |\n",
            "|    value_loss         | 0.737    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 258      |\n",
            "|    ep_rew_mean        | 110      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 164900   |\n",
            "|    time_elapsed       | 5534     |\n",
            "|    total_timesteps    | 6596000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.471   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 164899   |\n",
            "|    policy_loss        | -0.189   |\n",
            "|    value_loss         | 2.58     |\n",
            "------------------------------------\n",
            "Num timesteps: 6600000\n",
            "Best mean reward: 143.25 - Last mean reward per episode: 110.57\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 256      |\n",
            "|    ep_rew_mean        | 111      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 165000   |\n",
            "|    time_elapsed       | 5537     |\n",
            "|    total_timesteps    | 6600000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.307   |\n",
            "|    explained_variance | 0.656    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 164999   |\n",
            "|    policy_loss        | -0.248   |\n",
            "|    value_loss         | 705      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 262      |\n",
            "|    ep_rew_mean        | 115      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 165100   |\n",
            "|    time_elapsed       | 5539     |\n",
            "|    total_timesteps    | 6604000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.335   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 165099   |\n",
            "|    policy_loss        | -0.00909 |\n",
            "|    value_loss         | 0.686    |\n",
            "------------------------------------\n",
            "Num timesteps: 6608000\n",
            "Best mean reward: 143.25 - Last mean reward per episode: 105.54\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 269      |\n",
            "|    ep_rew_mean        | 106      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 165200   |\n",
            "|    time_elapsed       | 5542     |\n",
            "|    total_timesteps    | 6608000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.505   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 165199   |\n",
            "|    policy_loss        | 0.0833   |\n",
            "|    value_loss         | 0.867    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 284      |\n",
            "|    ep_rew_mean        | 112      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 165300   |\n",
            "|    time_elapsed       | 5545     |\n",
            "|    total_timesteps    | 6612000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.387   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 165299   |\n",
            "|    policy_loss        | -0.0217  |\n",
            "|    value_loss         | 1.01     |\n",
            "------------------------------------\n",
            "Num timesteps: 6616000\n",
            "Best mean reward: 143.25 - Last mean reward per episode: 126.51\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 308      |\n",
            "|    ep_rew_mean        | 127      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 165400   |\n",
            "|    time_elapsed       | 5548     |\n",
            "|    total_timesteps    | 6616000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.484   |\n",
            "|    explained_variance | 0.952    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 165399   |\n",
            "|    policy_loss        | 0.227    |\n",
            "|    value_loss         | 24.6     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 324      |\n",
            "|    ep_rew_mean        | 140      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 165500   |\n",
            "|    time_elapsed       | 5550     |\n",
            "|    total_timesteps    | 6620000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.308   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 165499   |\n",
            "|    policy_loss        | -0.271   |\n",
            "|    value_loss         | 1.18     |\n",
            "------------------------------------\n",
            "Num timesteps: 6624000\n",
            "Best mean reward: 143.25 - Last mean reward per episode: 141.15\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 332      |\n",
            "|    ep_rew_mean        | 141      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 165600   |\n",
            "|    time_elapsed       | 5552     |\n",
            "|    total_timesteps    | 6624000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.337   |\n",
            "|    explained_variance | 0.751    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 165599   |\n",
            "|    policy_loss        | -0.622   |\n",
            "|    value_loss         | 550      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 339      |\n",
            "|    ep_rew_mean        | 141      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 165700   |\n",
            "|    time_elapsed       | 5555     |\n",
            "|    total_timesteps    | 6628000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.413   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 165699   |\n",
            "|    policy_loss        | -0.188   |\n",
            "|    value_loss         | 1.25     |\n",
            "------------------------------------\n",
            "Num timesteps: 6632000\n",
            "Best mean reward: 143.25 - Last mean reward per episode: 145.74\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 344      |\n",
            "|    ep_rew_mean        | 146      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 165800   |\n",
            "|    time_elapsed       | 5558     |\n",
            "|    total_timesteps    | 6632000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.318   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 165799   |\n",
            "|    policy_loss        | 0.109    |\n",
            "|    value_loss         | 1.75     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 344      |\n",
            "|    ep_rew_mean        | 154      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 165900   |\n",
            "|    time_elapsed       | 5560     |\n",
            "|    total_timesteps    | 6636000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.411   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 165899   |\n",
            "|    policy_loss        | 0.259    |\n",
            "|    value_loss         | 1.25     |\n",
            "------------------------------------\n",
            "Num timesteps: 6640000\n",
            "Best mean reward: 145.74 - Last mean reward per episode: 158.07\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 349      |\n",
            "|    ep_rew_mean        | 158      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 166000   |\n",
            "|    time_elapsed       | 5562     |\n",
            "|    total_timesteps    | 6640000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.384   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 165999   |\n",
            "|    policy_loss        | -0.075   |\n",
            "|    value_loss         | 1.96     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 336      |\n",
            "|    ep_rew_mean        | 166      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 166100   |\n",
            "|    time_elapsed       | 5565     |\n",
            "|    total_timesteps    | 6644000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.204   |\n",
            "|    explained_variance | 0.962    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 166099   |\n",
            "|    policy_loss        | 0.0135   |\n",
            "|    value_loss         | 11.3     |\n",
            "------------------------------------\n",
            "Num timesteps: 6648000\n",
            "Best mean reward: 158.07 - Last mean reward per episode: 165.50\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 326      |\n",
            "|    ep_rew_mean        | 165      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 166200   |\n",
            "|    time_elapsed       | 5567     |\n",
            "|    total_timesteps    | 6648000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.299   |\n",
            "|    explained_variance | 0.429    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 166199   |\n",
            "|    policy_loss        | 0.647    |\n",
            "|    value_loss         | 206      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 323      |\n",
            "|    ep_rew_mean        | 165      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 166300   |\n",
            "|    time_elapsed       | 5570     |\n",
            "|    total_timesteps    | 6652000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.272   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 166299   |\n",
            "|    policy_loss        | 0.101    |\n",
            "|    value_loss         | 2.82     |\n",
            "------------------------------------\n",
            "Num timesteps: 6656000\n",
            "Best mean reward: 165.50 - Last mean reward per episode: 169.03\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 325      |\n",
            "|    ep_rew_mean        | 169      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 166400   |\n",
            "|    time_elapsed       | 5572     |\n",
            "|    total_timesteps    | 6656000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.312   |\n",
            "|    explained_variance | 0.623    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 166399   |\n",
            "|    policy_loss        | 0.245    |\n",
            "|    value_loss         | 156      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 326      |\n",
            "|    ep_rew_mean        | 178      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 166500   |\n",
            "|    time_elapsed       | 5575     |\n",
            "|    total_timesteps    | 6660000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.262   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 166499   |\n",
            "|    policy_loss        | -0.162   |\n",
            "|    value_loss         | 1.49     |\n",
            "------------------------------------\n",
            "Num timesteps: 6664000\n",
            "Best mean reward: 169.03 - Last mean reward per episode: 184.36\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 325      |\n",
            "|    ep_rew_mean        | 184      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 166600   |\n",
            "|    time_elapsed       | 5577     |\n",
            "|    total_timesteps    | 6664000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.477   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 166599   |\n",
            "|    policy_loss        | 0.204    |\n",
            "|    value_loss         | 1.97     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 327      |\n",
            "|    ep_rew_mean        | 185      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 166700   |\n",
            "|    time_elapsed       | 5580     |\n",
            "|    total_timesteps    | 6668000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.431   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 166699   |\n",
            "|    policy_loss        | -0.133   |\n",
            "|    value_loss         | 0.8      |\n",
            "------------------------------------\n",
            "Num timesteps: 6672000\n",
            "Best mean reward: 184.36 - Last mean reward per episode: 186.00\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 341      |\n",
            "|    ep_rew_mean        | 186      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 166800   |\n",
            "|    time_elapsed       | 5583     |\n",
            "|    total_timesteps    | 6672000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.317   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 166799   |\n",
            "|    policy_loss        | 0.0518   |\n",
            "|    value_loss         | 1.82     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 359      |\n",
            "|    ep_rew_mean        | 191      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 166900   |\n",
            "|    time_elapsed       | 5586     |\n",
            "|    total_timesteps    | 6676000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.432   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 166899   |\n",
            "|    policy_loss        | -0.171   |\n",
            "|    value_loss         | 3.06     |\n",
            "------------------------------------\n",
            "Num timesteps: 6680000\n",
            "Best mean reward: 186.00 - Last mean reward per episode: 193.19\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 369      |\n",
            "|    ep_rew_mean        | 193      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 167000   |\n",
            "|    time_elapsed       | 5589     |\n",
            "|    total_timesteps    | 6680000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.272   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 166999   |\n",
            "|    policy_loss        | 0.0668   |\n",
            "|    value_loss         | 1.81     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 367      |\n",
            "|    ep_rew_mean        | 186      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 167100   |\n",
            "|    time_elapsed       | 5591     |\n",
            "|    total_timesteps    | 6684000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.429   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 167099   |\n",
            "|    policy_loss        | -0.106   |\n",
            "|    value_loss         | 2.61     |\n",
            "------------------------------------\n",
            "Num timesteps: 6688000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 183.03\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 370      |\n",
            "|    ep_rew_mean        | 183      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 167200   |\n",
            "|    time_elapsed       | 5594     |\n",
            "|    total_timesteps    | 6688000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.273   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 167199   |\n",
            "|    policy_loss        | -0.326   |\n",
            "|    value_loss         | 1.25     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 373      |\n",
            "|    ep_rew_mean        | 182      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 167300   |\n",
            "|    time_elapsed       | 5597     |\n",
            "|    total_timesteps    | 6692000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.339   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 167299   |\n",
            "|    policy_loss        | 0.0292   |\n",
            "|    value_loss         | 2.05     |\n",
            "------------------------------------\n",
            "Num timesteps: 6696000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 182.84\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 384      |\n",
            "|    ep_rew_mean        | 183      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 167400   |\n",
            "|    time_elapsed       | 5600     |\n",
            "|    total_timesteps    | 6696000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.305   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 167399   |\n",
            "|    policy_loss        | -0.213   |\n",
            "|    value_loss         | 1.45     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 389      |\n",
            "|    ep_rew_mean        | 178      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 167500   |\n",
            "|    time_elapsed       | 5603     |\n",
            "|    total_timesteps    | 6700000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.389   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 167499   |\n",
            "|    policy_loss        | -0.263   |\n",
            "|    value_loss         | 1.21     |\n",
            "------------------------------------\n",
            "Num timesteps: 6704000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 171.77\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 395      |\n",
            "|    ep_rew_mean        | 172      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 167600   |\n",
            "|    time_elapsed       | 5606     |\n",
            "|    total_timesteps    | 6704000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.465   |\n",
            "|    explained_variance | 0.928    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 167599   |\n",
            "|    policy_loss        | -0.631   |\n",
            "|    value_loss         | 51.3     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 403      |\n",
            "|    ep_rew_mean        | 176      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 167700   |\n",
            "|    time_elapsed       | 5609     |\n",
            "|    total_timesteps    | 6708000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.388   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 167699   |\n",
            "|    policy_loss        | -0.234   |\n",
            "|    value_loss         | 2.37     |\n",
            "------------------------------------\n",
            "Num timesteps: 6712000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 173.92\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 412      |\n",
            "|    ep_rew_mean        | 174      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 167800   |\n",
            "|    time_elapsed       | 5613     |\n",
            "|    total_timesteps    | 6712000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.436   |\n",
            "|    explained_variance | 0.833    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 167799   |\n",
            "|    policy_loss        | 0.238    |\n",
            "|    value_loss         | 152      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 399      |\n",
            "|    ep_rew_mean        | 175      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 167900   |\n",
            "|    time_elapsed       | 5615     |\n",
            "|    total_timesteps    | 6716000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.345   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 167899   |\n",
            "|    policy_loss        | 0.0688   |\n",
            "|    value_loss         | 0.901    |\n",
            "------------------------------------\n",
            "Num timesteps: 6720000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 174.42\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 407      |\n",
            "|    ep_rew_mean        | 174      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 168000   |\n",
            "|    time_elapsed       | 5619     |\n",
            "|    total_timesteps    | 6720000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.345   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 167999   |\n",
            "|    policy_loss        | 0.085    |\n",
            "|    value_loss         | 0.69     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 426      |\n",
            "|    ep_rew_mean        | 171      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 168100   |\n",
            "|    time_elapsed       | 5623     |\n",
            "|    total_timesteps    | 6724000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.385   |\n",
            "|    explained_variance | 0.585    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 168099   |\n",
            "|    policy_loss        | -0.289   |\n",
            "|    value_loss         | 479      |\n",
            "------------------------------------\n",
            "Num timesteps: 6728000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 172.83\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 449      |\n",
            "|    ep_rew_mean        | 173      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 168200   |\n",
            "|    time_elapsed       | 5627     |\n",
            "|    total_timesteps    | 6728000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.297   |\n",
            "|    explained_variance | 0.778    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 168199   |\n",
            "|    policy_loss        | -0.0107  |\n",
            "|    value_loss         | 185      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 452      |\n",
            "|    ep_rew_mean        | 171      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 168300   |\n",
            "|    time_elapsed       | 5630     |\n",
            "|    total_timesteps    | 6732000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.39    |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 168299   |\n",
            "|    policy_loss        | 0.122    |\n",
            "|    value_loss         | 0.673    |\n",
            "------------------------------------\n",
            "Num timesteps: 6736000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 175.20\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 463      |\n",
            "|    ep_rew_mean        | 175      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 168400   |\n",
            "|    time_elapsed       | 5633     |\n",
            "|    total_timesteps    | 6736000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.51    |\n",
            "|    explained_variance | 0.707    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 168399   |\n",
            "|    policy_loss        | -0.0189  |\n",
            "|    value_loss         | 295      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 458      |\n",
            "|    ep_rew_mean        | 170      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 168500   |\n",
            "|    time_elapsed       | 5636     |\n",
            "|    total_timesteps    | 6740000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.264   |\n",
            "|    explained_variance | 0.528    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 168499   |\n",
            "|    policy_loss        | 0.188    |\n",
            "|    value_loss         | 263      |\n",
            "------------------------------------\n",
            "Num timesteps: 6744000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 164.28\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 449      |\n",
            "|    ep_rew_mean        | 164      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 168600   |\n",
            "|    time_elapsed       | 5639     |\n",
            "|    total_timesteps    | 6744000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.425   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 168599   |\n",
            "|    policy_loss        | -0.573   |\n",
            "|    value_loss         | 2.24     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 445      |\n",
            "|    ep_rew_mean        | 158      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 168700   |\n",
            "|    time_elapsed       | 5642     |\n",
            "|    total_timesteps    | 6748000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.358   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 168699   |\n",
            "|    policy_loss        | 0.0608   |\n",
            "|    value_loss         | 3.44     |\n",
            "------------------------------------\n",
            "Num timesteps: 6752000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 160.07\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 451      |\n",
            "|    ep_rew_mean        | 160      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 168800   |\n",
            "|    time_elapsed       | 5645     |\n",
            "|    total_timesteps    | 6752000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.448   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 168799   |\n",
            "|    policy_loss        | -0.104   |\n",
            "|    value_loss         | 0.835    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 440      |\n",
            "|    ep_rew_mean        | 162      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 168900   |\n",
            "|    time_elapsed       | 5648     |\n",
            "|    total_timesteps    | 6756000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.277   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 168899   |\n",
            "|    policy_loss        | -0.422   |\n",
            "|    value_loss         | 0.92     |\n",
            "------------------------------------\n",
            "Num timesteps: 6760000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 162.35\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 441      |\n",
            "|    ep_rew_mean        | 162      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 169000   |\n",
            "|    time_elapsed       | 5651     |\n",
            "|    total_timesteps    | 6760000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.32    |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 168999   |\n",
            "|    policy_loss        | -0.066   |\n",
            "|    value_loss         | 6.84     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 444      |\n",
            "|    ep_rew_mean        | 159      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 169100   |\n",
            "|    time_elapsed       | 5655     |\n",
            "|    total_timesteps    | 6764000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.398   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 169099   |\n",
            "|    policy_loss        | -0.0634  |\n",
            "|    value_loss         | 0.628    |\n",
            "------------------------------------\n",
            "Num timesteps: 6768000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 157.09\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 444      |\n",
            "|    ep_rew_mean        | 157      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 169200   |\n",
            "|    time_elapsed       | 5659     |\n",
            "|    total_timesteps    | 6768000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.312   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 169199   |\n",
            "|    policy_loss        | -0.0504  |\n",
            "|    value_loss         | 0.751    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 441      |\n",
            "|    ep_rew_mean        | 154      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 169300   |\n",
            "|    time_elapsed       | 5663     |\n",
            "|    total_timesteps    | 6772000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.438   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 169299   |\n",
            "|    policy_loss        | -0.0109  |\n",
            "|    value_loss         | 2.34     |\n",
            "------------------------------------\n",
            "Num timesteps: 6776000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 151.98\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 441      |\n",
            "|    ep_rew_mean        | 152      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 169400   |\n",
            "|    time_elapsed       | 5666     |\n",
            "|    total_timesteps    | 6776000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.42    |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 169399   |\n",
            "|    policy_loss        | 0.161    |\n",
            "|    value_loss         | 3.19     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 452      |\n",
            "|    ep_rew_mean        | 147      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 169500   |\n",
            "|    time_elapsed       | 5670     |\n",
            "|    total_timesteps    | 6780000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.238   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 169499   |\n",
            "|    policy_loss        | 0.793    |\n",
            "|    value_loss         | 7.74     |\n",
            "------------------------------------\n",
            "Num timesteps: 6784000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 147.84\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 449      |\n",
            "|    ep_rew_mean        | 148      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 169600   |\n",
            "|    time_elapsed       | 5673     |\n",
            "|    total_timesteps    | 6784000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.339   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 169599   |\n",
            "|    policy_loss        | 0.081    |\n",
            "|    value_loss         | 1.21     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 462      |\n",
            "|    ep_rew_mean        | 153      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 169700   |\n",
            "|    time_elapsed       | 5676     |\n",
            "|    total_timesteps    | 6788000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.395   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 169699   |\n",
            "|    policy_loss        | -0.0635  |\n",
            "|    value_loss         | 1.35     |\n",
            "------------------------------------\n",
            "Num timesteps: 6792000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 141.48\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 464      |\n",
            "|    ep_rew_mean        | 141      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 169800   |\n",
            "|    time_elapsed       | 5679     |\n",
            "|    total_timesteps    | 6792000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.51    |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 169799   |\n",
            "|    policy_loss        | -0.0465  |\n",
            "|    value_loss         | 0.816    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 461      |\n",
            "|    ep_rew_mean        | 142      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 169900   |\n",
            "|    time_elapsed       | 5682     |\n",
            "|    total_timesteps    | 6796000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.311   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 169899   |\n",
            "|    policy_loss        | -0.392   |\n",
            "|    value_loss         | 0.73     |\n",
            "------------------------------------\n",
            "Num timesteps: 6800000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 132.38\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 468      |\n",
            "|    ep_rew_mean        | 132      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 170000   |\n",
            "|    time_elapsed       | 5685     |\n",
            "|    total_timesteps    | 6800000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.603   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 169999   |\n",
            "|    policy_loss        | 0.213    |\n",
            "|    value_loss         | 2.08     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 460      |\n",
            "|    ep_rew_mean        | 127      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 170100   |\n",
            "|    time_elapsed       | 5688     |\n",
            "|    total_timesteps    | 6804000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.342   |\n",
            "|    explained_variance | 0.961    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 170099   |\n",
            "|    policy_loss        | -0.287   |\n",
            "|    value_loss         | 57.3     |\n",
            "------------------------------------\n",
            "Num timesteps: 6808000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 123.50\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 440      |\n",
            "|    ep_rew_mean        | 124      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 170200   |\n",
            "|    time_elapsed       | 5691     |\n",
            "|    total_timesteps    | 6808000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.379   |\n",
            "|    explained_variance | 0.533    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 170199   |\n",
            "|    policy_loss        | -0.0123  |\n",
            "|    value_loss         | 783      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 409      |\n",
            "|    ep_rew_mean        | 125      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 170300   |\n",
            "|    time_elapsed       | 5693     |\n",
            "|    total_timesteps    | 6812000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.364   |\n",
            "|    explained_variance | 0.926    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 170299   |\n",
            "|    policy_loss        | -0.0435  |\n",
            "|    value_loss         | 23.8     |\n",
            "------------------------------------\n",
            "Num timesteps: 6816000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 128.18\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 403      |\n",
            "|    ep_rew_mean        | 128      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 170400   |\n",
            "|    time_elapsed       | 5696     |\n",
            "|    total_timesteps    | 6816000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.313   |\n",
            "|    explained_variance | 0.977    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 170399   |\n",
            "|    policy_loss        | -0.0909  |\n",
            "|    value_loss         | 6.2      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 399      |\n",
            "|    ep_rew_mean        | 124      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 170500   |\n",
            "|    time_elapsed       | 5700     |\n",
            "|    total_timesteps    | 6820000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.473   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 170499   |\n",
            "|    policy_loss        | -0.231   |\n",
            "|    value_loss         | 2.22     |\n",
            "------------------------------------\n",
            "Num timesteps: 6824000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 127.09\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 409      |\n",
            "|    ep_rew_mean        | 127      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 170600   |\n",
            "|    time_elapsed       | 5703     |\n",
            "|    total_timesteps    | 6824000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.512   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 170599   |\n",
            "|    policy_loss        | 0.0149   |\n",
            "|    value_loss         | 0.857    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 422      |\n",
            "|    ep_rew_mean        | 128      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 170700   |\n",
            "|    time_elapsed       | 5708     |\n",
            "|    total_timesteps    | 6828000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.347   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 170699   |\n",
            "|    policy_loss        | -0.362   |\n",
            "|    value_loss         | 1.02     |\n",
            "------------------------------------\n",
            "Num timesteps: 6832000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 127.38\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 431      |\n",
            "|    ep_rew_mean        | 127      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 170800   |\n",
            "|    time_elapsed       | 5712     |\n",
            "|    total_timesteps    | 6832000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.326   |\n",
            "|    explained_variance | 0.608    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 170799   |\n",
            "|    policy_loss        | -0.119   |\n",
            "|    value_loss         | 1.1e+03  |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 432      |\n",
            "|    ep_rew_mean        | 131      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 170900   |\n",
            "|    time_elapsed       | 5715     |\n",
            "|    total_timesteps    | 6836000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.433   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 170899   |\n",
            "|    policy_loss        | -0.139   |\n",
            "|    value_loss         | 0.534    |\n",
            "------------------------------------\n",
            "Num timesteps: 6840000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 130.54\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 452      |\n",
            "|    ep_rew_mean        | 131      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 171000   |\n",
            "|    time_elapsed       | 5719     |\n",
            "|    total_timesteps    | 6840000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.43    |\n",
            "|    explained_variance | 0.544    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 170999   |\n",
            "|    policy_loss        | 3.59     |\n",
            "|    value_loss         | 142      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 451      |\n",
            "|    ep_rew_mean        | 133      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 171100   |\n",
            "|    time_elapsed       | 5721     |\n",
            "|    total_timesteps    | 6844000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.337   |\n",
            "|    explained_variance | 0.688    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 171099   |\n",
            "|    policy_loss        | 0.724    |\n",
            "|    value_loss         | 128      |\n",
            "------------------------------------\n",
            "Num timesteps: 6848000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 130.35\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 453      |\n",
            "|    ep_rew_mean        | 130      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 171200   |\n",
            "|    time_elapsed       | 5725     |\n",
            "|    total_timesteps    | 6848000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.376   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 171199   |\n",
            "|    policy_loss        | 0.0445   |\n",
            "|    value_loss         | 1.95     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 453      |\n",
            "|    ep_rew_mean        | 129      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 171300   |\n",
            "|    time_elapsed       | 5728     |\n",
            "|    total_timesteps    | 6852000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.315   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 171299   |\n",
            "|    policy_loss        | 0.0314   |\n",
            "|    value_loss         | 1.63     |\n",
            "------------------------------------\n",
            "Num timesteps: 6856000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 129.00\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 460      |\n",
            "|    ep_rew_mean        | 129      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 171400   |\n",
            "|    time_elapsed       | 5731     |\n",
            "|    total_timesteps    | 6856000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.477   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 171399   |\n",
            "|    policy_loss        | -0.196   |\n",
            "|    value_loss         | 1.93     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 469      |\n",
            "|    ep_rew_mean        | 127      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 171500   |\n",
            "|    time_elapsed       | 5735     |\n",
            "|    total_timesteps    | 6860000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.431   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 171499   |\n",
            "|    policy_loss        | -0.0203  |\n",
            "|    value_loss         | 1.11     |\n",
            "------------------------------------\n",
            "Num timesteps: 6864000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 124.88\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 474      |\n",
            "|    ep_rew_mean        | 125      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 171600   |\n",
            "|    time_elapsed       | 5738     |\n",
            "|    total_timesteps    | 6864000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.579   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 171599   |\n",
            "|    policy_loss        | 0.102    |\n",
            "|    value_loss         | 0.769    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 473      |\n",
            "|    ep_rew_mean        | 130      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 171700   |\n",
            "|    time_elapsed       | 5742     |\n",
            "|    total_timesteps    | 6868000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.233   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 171699   |\n",
            "|    policy_loss        | -0.254   |\n",
            "|    value_loss         | 0.507    |\n",
            "------------------------------------\n",
            "Num timesteps: 6872000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 132.90\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 491      |\n",
            "|    ep_rew_mean        | 133      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 171800   |\n",
            "|    time_elapsed       | 5746     |\n",
            "|    total_timesteps    | 6872000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.556   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 171799   |\n",
            "|    policy_loss        | 0.142    |\n",
            "|    value_loss         | 2.17     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 491      |\n",
            "|    ep_rew_mean        | 128      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 171900   |\n",
            "|    time_elapsed       | 5751     |\n",
            "|    total_timesteps    | 6876000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.217   |\n",
            "|    explained_variance | 0.419    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 171899   |\n",
            "|    policy_loss        | -0.0793  |\n",
            "|    value_loss         | 1.29e+03 |\n",
            "------------------------------------\n",
            "Num timesteps: 6880000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 122.76\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 470      |\n",
            "|    ep_rew_mean        | 123      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 172000   |\n",
            "|    time_elapsed       | 5754     |\n",
            "|    total_timesteps    | 6880000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.267   |\n",
            "|    explained_variance | 0.626    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 171999   |\n",
            "|    policy_loss        | -0.238   |\n",
            "|    value_loss         | 225      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 471      |\n",
            "|    ep_rew_mean        | 126      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 172100   |\n",
            "|    time_elapsed       | 5758     |\n",
            "|    total_timesteps    | 6884000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.298   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 172099   |\n",
            "|    policy_loss        | -0.184   |\n",
            "|    value_loss         | 1.55     |\n",
            "------------------------------------\n",
            "Num timesteps: 6888000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 124.57\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 452      |\n",
            "|    ep_rew_mean        | 125      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 172200   |\n",
            "|    time_elapsed       | 5761     |\n",
            "|    total_timesteps    | 6888000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.384   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 172199   |\n",
            "|    policy_loss        | -0.101   |\n",
            "|    value_loss         | 2.34     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 456      |\n",
            "|    ep_rew_mean        | 128      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 172300   |\n",
            "|    time_elapsed       | 5763     |\n",
            "|    total_timesteps    | 6892000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.327   |\n",
            "|    explained_variance | 0.898    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 172299   |\n",
            "|    policy_loss        | -0.0399  |\n",
            "|    value_loss         | 69.5     |\n",
            "------------------------------------\n",
            "Num timesteps: 6896000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 129.72\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 453      |\n",
            "|    ep_rew_mean        | 130      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 172400   |\n",
            "|    time_elapsed       | 5768     |\n",
            "|    total_timesteps    | 6896000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.409   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 172399   |\n",
            "|    policy_loss        | -0.071   |\n",
            "|    value_loss         | 3.67     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 472      |\n",
            "|    ep_rew_mean        | 135      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 172500   |\n",
            "|    time_elapsed       | 5771     |\n",
            "|    total_timesteps    | 6900000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.317   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 172499   |\n",
            "|    policy_loss        | 0.157    |\n",
            "|    value_loss         | 2.62     |\n",
            "------------------------------------\n",
            "Num timesteps: 6904000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 134.14\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 470      |\n",
            "|    ep_rew_mean        | 134      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 172600   |\n",
            "|    time_elapsed       | 5776     |\n",
            "|    total_timesteps    | 6904000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.35    |\n",
            "|    explained_variance | 0.633    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 172599   |\n",
            "|    policy_loss        | 0.0715   |\n",
            "|    value_loss         | 189      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 472      |\n",
            "|    ep_rew_mean        | 128      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 172700   |\n",
            "|    time_elapsed       | 5780     |\n",
            "|    total_timesteps    | 6908000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.363   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 172699   |\n",
            "|    policy_loss        | -0.292   |\n",
            "|    value_loss         | 1.68     |\n",
            "------------------------------------\n",
            "Num timesteps: 6912000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 121.55\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 477      |\n",
            "|    ep_rew_mean        | 122      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 172800   |\n",
            "|    time_elapsed       | 5784     |\n",
            "|    total_timesteps    | 6912000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.497   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 172799   |\n",
            "|    policy_loss        | -0.149   |\n",
            "|    value_loss         | 0.742    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 469      |\n",
            "|    ep_rew_mean        | 117      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 172900   |\n",
            "|    time_elapsed       | 5788     |\n",
            "|    total_timesteps    | 6916000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.357   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 172899   |\n",
            "|    policy_loss        | 0.0737   |\n",
            "|    value_loss         | 1.91     |\n",
            "------------------------------------\n",
            "Num timesteps: 6920000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 114.53\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 467      |\n",
            "|    ep_rew_mean        | 115      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 173000   |\n",
            "|    time_elapsed       | 5792     |\n",
            "|    total_timesteps    | 6920000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.275   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 172999   |\n",
            "|    policy_loss        | -0.0533  |\n",
            "|    value_loss         | 1.8      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 469      |\n",
            "|    ep_rew_mean        | 115      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 173100   |\n",
            "|    time_elapsed       | 5797     |\n",
            "|    total_timesteps    | 6924000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.353   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 173099   |\n",
            "|    policy_loss        | -0.0152  |\n",
            "|    value_loss         | 5.22     |\n",
            "------------------------------------\n",
            "Num timesteps: 6928000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 113.65\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 458      |\n",
            "|    ep_rew_mean        | 114      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 173200   |\n",
            "|    time_elapsed       | 5801     |\n",
            "|    total_timesteps    | 6928000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.376   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 173199   |\n",
            "|    policy_loss        | -0.282   |\n",
            "|    value_loss         | 1.16     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 467      |\n",
            "|    ep_rew_mean        | 102      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 173300   |\n",
            "|    time_elapsed       | 5804     |\n",
            "|    total_timesteps    | 6932000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.362   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 173299   |\n",
            "|    policy_loss        | 0.089    |\n",
            "|    value_loss         | 3.34     |\n",
            "------------------------------------\n",
            "Num timesteps: 6936000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 97.32\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 473      |\n",
            "|    ep_rew_mean        | 97.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 173400   |\n",
            "|    time_elapsed       | 5808     |\n",
            "|    total_timesteps    | 6936000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.397   |\n",
            "|    explained_variance | 0.949    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 173399   |\n",
            "|    policy_loss        | -0.363   |\n",
            "|    value_loss         | 8.11     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 473      |\n",
            "|    ep_rew_mean        | 85.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 173500   |\n",
            "|    time_elapsed       | 5812     |\n",
            "|    total_timesteps    | 6940000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.281   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 173499   |\n",
            "|    policy_loss        | 0.165    |\n",
            "|    value_loss         | 1.6      |\n",
            "------------------------------------\n",
            "Num timesteps: 6944000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 67.12\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 455      |\n",
            "|    ep_rew_mean        | 67.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 173600   |\n",
            "|    time_elapsed       | 5816     |\n",
            "|    total_timesteps    | 6944000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.323   |\n",
            "|    explained_variance | 0.718    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 173599   |\n",
            "|    policy_loss        | 0.0912   |\n",
            "|    value_loss         | 396      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 459      |\n",
            "|    ep_rew_mean        | 62.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 173700   |\n",
            "|    time_elapsed       | 5820     |\n",
            "|    total_timesteps    | 6948000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.403   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 173699   |\n",
            "|    policy_loss        | -0.168   |\n",
            "|    value_loss         | 0.721    |\n",
            "------------------------------------\n",
            "Num timesteps: 6952000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 61.98\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 438      |\n",
            "|    ep_rew_mean        | 62       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 173800   |\n",
            "|    time_elapsed       | 5822     |\n",
            "|    total_timesteps    | 6952000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.412   |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 173799   |\n",
            "|    policy_loss        | 0.165    |\n",
            "|    value_loss         | 21.2     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 428      |\n",
            "|    ep_rew_mean        | 80       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 173900   |\n",
            "|    time_elapsed       | 5825     |\n",
            "|    total_timesteps    | 6956000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.441   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 173899   |\n",
            "|    policy_loss        | 0.0436   |\n",
            "|    value_loss         | 2.49     |\n",
            "------------------------------------\n",
            "Num timesteps: 6960000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 82.77\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 408      |\n",
            "|    ep_rew_mean        | 82.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 174000   |\n",
            "|    time_elapsed       | 5829     |\n",
            "|    total_timesteps    | 6960000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.36    |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 173999   |\n",
            "|    policy_loss        | -0.00372 |\n",
            "|    value_loss         | 0.554    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 402      |\n",
            "|    ep_rew_mean        | 87.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 174100   |\n",
            "|    time_elapsed       | 5832     |\n",
            "|    total_timesteps    | 6964000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.377   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 174099   |\n",
            "|    policy_loss        | -0.0727  |\n",
            "|    value_loss         | 0.647    |\n",
            "------------------------------------\n",
            "Num timesteps: 6968000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 96.27\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 381      |\n",
            "|    ep_rew_mean        | 96.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 174200   |\n",
            "|    time_elapsed       | 5834     |\n",
            "|    total_timesteps    | 6968000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.373   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 174199   |\n",
            "|    policy_loss        | -0.0381  |\n",
            "|    value_loss         | 1.47     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 382      |\n",
            "|    ep_rew_mean        | 105      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 174300   |\n",
            "|    time_elapsed       | 5837     |\n",
            "|    total_timesteps    | 6972000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.459   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 174299   |\n",
            "|    policy_loss        | -0.676   |\n",
            "|    value_loss         | 4.52     |\n",
            "------------------------------------\n",
            "Num timesteps: 6976000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 113.17\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 389      |\n",
            "|    ep_rew_mean        | 113      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 174400   |\n",
            "|    time_elapsed       | 5840     |\n",
            "|    total_timesteps    | 6976000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.477   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 174399   |\n",
            "|    policy_loss        | 0.00667  |\n",
            "|    value_loss         | 0.883    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 391      |\n",
            "|    ep_rew_mean        | 118      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 174500   |\n",
            "|    time_elapsed       | 5843     |\n",
            "|    total_timesteps    | 6980000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.302   |\n",
            "|    explained_variance | 0.883    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 174499   |\n",
            "|    policy_loss        | -1.19    |\n",
            "|    value_loss         | 72.7     |\n",
            "------------------------------------\n",
            "Num timesteps: 6984000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 123.52\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 389      |\n",
            "|    ep_rew_mean        | 124      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 174600   |\n",
            "|    time_elapsed       | 5846     |\n",
            "|    total_timesteps    | 6984000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.37    |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 174599   |\n",
            "|    policy_loss        | 0.0891   |\n",
            "|    value_loss         | 1.1      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 384      |\n",
            "|    ep_rew_mean        | 128      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 174700   |\n",
            "|    time_elapsed       | 5850     |\n",
            "|    total_timesteps    | 6988000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.327   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 174699   |\n",
            "|    policy_loss        | -0.155   |\n",
            "|    value_loss         | 1.22     |\n",
            "------------------------------------\n",
            "Num timesteps: 6992000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 128.02\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 405      |\n",
            "|    ep_rew_mean        | 128      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 174800   |\n",
            "|    time_elapsed       | 5854     |\n",
            "|    total_timesteps    | 6992000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.451   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 174799   |\n",
            "|    policy_loss        | 0.0313   |\n",
            "|    value_loss         | 1.6      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 419      |\n",
            "|    ep_rew_mean        | 124      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 174900   |\n",
            "|    time_elapsed       | 5859     |\n",
            "|    total_timesteps    | 6996000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.534   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 174899   |\n",
            "|    policy_loss        | -0.0768  |\n",
            "|    value_loss         | 2.06     |\n",
            "------------------------------------\n",
            "Num timesteps: 7000000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 127.62\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 431      |\n",
            "|    ep_rew_mean        | 128      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 175000   |\n",
            "|    time_elapsed       | 5863     |\n",
            "|    total_timesteps    | 7000000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.38    |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 174999   |\n",
            "|    policy_loss        | 0.109    |\n",
            "|    value_loss         | 9.37     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 409      |\n",
            "|    ep_rew_mean        | 122      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 175100   |\n",
            "|    time_elapsed       | 5865     |\n",
            "|    total_timesteps    | 7004000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.331   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 175099   |\n",
            "|    policy_loss        | -0.121   |\n",
            "|    value_loss         | 5.73     |\n",
            "------------------------------------\n",
            "Num timesteps: 7008000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 115.99\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 405      |\n",
            "|    ep_rew_mean        | 116      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 175200   |\n",
            "|    time_elapsed       | 5868     |\n",
            "|    total_timesteps    | 7008000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.3     |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 175199   |\n",
            "|    policy_loss        | -0.102   |\n",
            "|    value_loss         | 1.4      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 406      |\n",
            "|    ep_rew_mean        | 116      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 175300   |\n",
            "|    time_elapsed       | 5871     |\n",
            "|    total_timesteps    | 7012000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.171   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 175299   |\n",
            "|    policy_loss        | 0.105    |\n",
            "|    value_loss         | 3.03     |\n",
            "------------------------------------\n",
            "Num timesteps: 7016000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 127.12\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 374      |\n",
            "|    ep_rew_mean        | 127      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 175400   |\n",
            "|    time_elapsed       | 5873     |\n",
            "|    total_timesteps    | 7016000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.38    |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 175399   |\n",
            "|    policy_loss        | -0.16    |\n",
            "|    value_loss         | 2.26     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 365      |\n",
            "|    ep_rew_mean        | 123      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 175500   |\n",
            "|    time_elapsed       | 5875     |\n",
            "|    total_timesteps    | 7020000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.324   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 175499   |\n",
            "|    policy_loss        | -0.0749  |\n",
            "|    value_loss         | 0.453    |\n",
            "------------------------------------\n",
            "Num timesteps: 7024000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 127.83\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 381      |\n",
            "|    ep_rew_mean        | 128      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 175600   |\n",
            "|    time_elapsed       | 5879     |\n",
            "|    total_timesteps    | 7024000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.397   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 175599   |\n",
            "|    policy_loss        | 0.518    |\n",
            "|    value_loss         | 3.03     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 371      |\n",
            "|    ep_rew_mean        | 120      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 175700   |\n",
            "|    time_elapsed       | 5883     |\n",
            "|    total_timesteps    | 7028000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.437   |\n",
            "|    explained_variance | 0.953    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 175699   |\n",
            "|    policy_loss        | -0.04    |\n",
            "|    value_loss         | 0.696    |\n",
            "------------------------------------\n",
            "Num timesteps: 7032000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 123.43\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 367      |\n",
            "|    ep_rew_mean        | 123      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 175800   |\n",
            "|    time_elapsed       | 5886     |\n",
            "|    total_timesteps    | 7032000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.314   |\n",
            "|    explained_variance | 0.877    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 175799   |\n",
            "|    policy_loss        | -0.162   |\n",
            "|    value_loss         | 150      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 354      |\n",
            "|    ep_rew_mean        | 132      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 175900   |\n",
            "|    time_elapsed       | 5889     |\n",
            "|    total_timesteps    | 7036000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.356   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 175899   |\n",
            "|    policy_loss        | 0.198    |\n",
            "|    value_loss         | 2.81     |\n",
            "------------------------------------\n",
            "Num timesteps: 7040000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 133.24\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 348      |\n",
            "|    ep_rew_mean        | 133      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 176000   |\n",
            "|    time_elapsed       | 5891     |\n",
            "|    total_timesteps    | 7040000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.33    |\n",
            "|    explained_variance | 0.507    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 175999   |\n",
            "|    policy_loss        | -4.39    |\n",
            "|    value_loss         | 1.14e+03 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 340      |\n",
            "|    ep_rew_mean        | 139      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 176100   |\n",
            "|    time_elapsed       | 5894     |\n",
            "|    total_timesteps    | 7044000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.278   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 176099   |\n",
            "|    policy_loss        | -0.0979  |\n",
            "|    value_loss         | 1.29     |\n",
            "------------------------------------\n",
            "Num timesteps: 7048000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 133.06\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 344      |\n",
            "|    ep_rew_mean        | 133      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 176200   |\n",
            "|    time_elapsed       | 5896     |\n",
            "|    total_timesteps    | 7048000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.457   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 176199   |\n",
            "|    policy_loss        | -0.133   |\n",
            "|    value_loss         | 0.678    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 344      |\n",
            "|    ep_rew_mean        | 132      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 176300   |\n",
            "|    time_elapsed       | 5899     |\n",
            "|    total_timesteps    | 7052000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.333   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 176299   |\n",
            "|    policy_loss        | 0.0289   |\n",
            "|    value_loss         | 1.64     |\n",
            "------------------------------------\n",
            "Num timesteps: 7056000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 139.47\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 309      |\n",
            "|    ep_rew_mean        | 139      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 176400   |\n",
            "|    time_elapsed       | 5901     |\n",
            "|    total_timesteps    | 7056000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.376   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 176399   |\n",
            "|    policy_loss        | -0.122   |\n",
            "|    value_loss         | 1.24     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 282      |\n",
            "|    ep_rew_mean        | 139      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 176500   |\n",
            "|    time_elapsed       | 5903     |\n",
            "|    total_timesteps    | 7060000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.296   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 176499   |\n",
            "|    policy_loss        | -0.228   |\n",
            "|    value_loss         | 1.61     |\n",
            "------------------------------------\n",
            "Num timesteps: 7064000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 128.53\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 269      |\n",
            "|    ep_rew_mean        | 129      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 176600   |\n",
            "|    time_elapsed       | 5905     |\n",
            "|    total_timesteps    | 7064000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.463   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 176599   |\n",
            "|    policy_loss        | 0.0965   |\n",
            "|    value_loss         | 0.792    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 265      |\n",
            "|    ep_rew_mean        | 126      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 176700   |\n",
            "|    time_elapsed       | 5907     |\n",
            "|    total_timesteps    | 7068000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.333   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 176699   |\n",
            "|    policy_loss        | -0.0491  |\n",
            "|    value_loss         | 0.883    |\n",
            "------------------------------------\n",
            "Num timesteps: 7072000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 133.42\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 275      |\n",
            "|    ep_rew_mean        | 133      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 176800   |\n",
            "|    time_elapsed       | 5909     |\n",
            "|    total_timesteps    | 7072000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.439   |\n",
            "|    explained_variance | 0.387    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 176799   |\n",
            "|    policy_loss        | -0.0905  |\n",
            "|    value_loss         | 844      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 273      |\n",
            "|    ep_rew_mean        | 127      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 176900   |\n",
            "|    time_elapsed       | 5911     |\n",
            "|    total_timesteps    | 7076000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.398   |\n",
            "|    explained_variance | 0.945    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 176899   |\n",
            "|    policy_loss        | -0.0686  |\n",
            "|    value_loss         | 12.7     |\n",
            "------------------------------------\n",
            "Num timesteps: 7080000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 121.67\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 262      |\n",
            "|    ep_rew_mean        | 122      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1197     |\n",
            "|    iterations         | 177000   |\n",
            "|    time_elapsed       | 5913     |\n",
            "|    total_timesteps    | 7080000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.423   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 176999   |\n",
            "|    policy_loss        | -0.0745  |\n",
            "|    value_loss         | 1.25     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 268      |\n",
            "|    ep_rew_mean        | 127      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1197     |\n",
            "|    iterations         | 177100   |\n",
            "|    time_elapsed       | 5916     |\n",
            "|    total_timesteps    | 7084000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.157   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 177099   |\n",
            "|    policy_loss        | 0.143    |\n",
            "|    value_loss         | 3.18     |\n",
            "------------------------------------\n",
            "Num timesteps: 7088000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 115.22\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 256      |\n",
            "|    ep_rew_mean        | 115      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1197     |\n",
            "|    iterations         | 177200   |\n",
            "|    time_elapsed       | 5917     |\n",
            "|    total_timesteps    | 7088000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.384   |\n",
            "|    explained_variance | 0.453    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 177199   |\n",
            "|    policy_loss        | -2.19    |\n",
            "|    value_loss         | 1.07e+03 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 250      |\n",
            "|    ep_rew_mean        | 117      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1197     |\n",
            "|    iterations         | 177300   |\n",
            "|    time_elapsed       | 5919     |\n",
            "|    total_timesteps    | 7092000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.344   |\n",
            "|    explained_variance | 0.918    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 177299   |\n",
            "|    policy_loss        | -1.48    |\n",
            "|    value_loss         | 116      |\n",
            "------------------------------------\n",
            "Num timesteps: 7096000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 112.59\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 243      |\n",
            "|    ep_rew_mean        | 113      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1198     |\n",
            "|    iterations         | 177400   |\n",
            "|    time_elapsed       | 5922     |\n",
            "|    total_timesteps    | 7096000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.335   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 177399   |\n",
            "|    policy_loss        | -0.111   |\n",
            "|    value_loss         | 2.07     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 255      |\n",
            "|    ep_rew_mean        | 130      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1198     |\n",
            "|    iterations         | 177500   |\n",
            "|    time_elapsed       | 5925     |\n",
            "|    total_timesteps    | 7100000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.362   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 177499   |\n",
            "|    policy_loss        | -0.0232  |\n",
            "|    value_loss         | 1.31     |\n",
            "------------------------------------\n",
            "Num timesteps: 7104000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 133.42\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 261      |\n",
            "|    ep_rew_mean        | 133      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1198     |\n",
            "|    iterations         | 177600   |\n",
            "|    time_elapsed       | 5927     |\n",
            "|    total_timesteps    | 7104000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.323   |\n",
            "|    explained_variance | 0.785    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 177599   |\n",
            "|    policy_loss        | 0.129    |\n",
            "|    value_loss         | 160      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 266      |\n",
            "|    ep_rew_mean        | 149      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1198     |\n",
            "|    iterations         | 177700   |\n",
            "|    time_elapsed       | 5929     |\n",
            "|    total_timesteps    | 7108000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.387   |\n",
            "|    explained_variance | -1.46    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 177699   |\n",
            "|    policy_loss        | 0.22     |\n",
            "|    value_loss         | 204      |\n",
            "------------------------------------\n",
            "Num timesteps: 7112000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 154.09\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 272      |\n",
            "|    ep_rew_mean        | 154      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1198     |\n",
            "|    iterations         | 177800   |\n",
            "|    time_elapsed       | 5932     |\n",
            "|    total_timesteps    | 7112000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.426   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 177799   |\n",
            "|    policy_loss        | -0.0481  |\n",
            "|    value_loss         | 1.07     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 287      |\n",
            "|    ep_rew_mean        | 153      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1199     |\n",
            "|    iterations         | 177900   |\n",
            "|    time_elapsed       | 5934     |\n",
            "|    total_timesteps    | 7116000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.377   |\n",
            "|    explained_variance | 0.349    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 177899   |\n",
            "|    policy_loss        | 1.27     |\n",
            "|    value_loss         | 144      |\n",
            "------------------------------------\n",
            "Num timesteps: 7120000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 156.70\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 303      |\n",
            "|    ep_rew_mean        | 157      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1199     |\n",
            "|    iterations         | 178000   |\n",
            "|    time_elapsed       | 5937     |\n",
            "|    total_timesteps    | 7120000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.356   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 177999   |\n",
            "|    policy_loss        | 0.198    |\n",
            "|    value_loss         | 1.02     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 319      |\n",
            "|    ep_rew_mean        | 168      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1199     |\n",
            "|    iterations         | 178100   |\n",
            "|    time_elapsed       | 5939     |\n",
            "|    total_timesteps    | 7124000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.338   |\n",
            "|    explained_variance | 0.251    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 178099   |\n",
            "|    policy_loss        | 0.829    |\n",
            "|    value_loss         | 807      |\n",
            "------------------------------------\n",
            "Num timesteps: 7128000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 175.25\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 328      |\n",
            "|    ep_rew_mean        | 175      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1199     |\n",
            "|    iterations         | 178200   |\n",
            "|    time_elapsed       | 5942     |\n",
            "|    total_timesteps    | 7128000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.29    |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 178199   |\n",
            "|    policy_loss        | 0.305    |\n",
            "|    value_loss         | 3.12     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 330      |\n",
            "|    ep_rew_mean        | 171      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1199     |\n",
            "|    iterations         | 178300   |\n",
            "|    time_elapsed       | 5945     |\n",
            "|    total_timesteps    | 7132000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.263   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 178299   |\n",
            "|    policy_loss        | 0.0589   |\n",
            "|    value_loss         | 2.23     |\n",
            "------------------------------------\n",
            "Num timesteps: 7136000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 168.36\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 329      |\n",
            "|    ep_rew_mean        | 168      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1199     |\n",
            "|    iterations         | 178400   |\n",
            "|    time_elapsed       | 5947     |\n",
            "|    total_timesteps    | 7136000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.209   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 178399   |\n",
            "|    policy_loss        | -0.0216  |\n",
            "|    value_loss         | 1.01     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 330      |\n",
            "|    ep_rew_mean        | 165      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1199     |\n",
            "|    iterations         | 178500   |\n",
            "|    time_elapsed       | 5950     |\n",
            "|    total_timesteps    | 7140000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.308   |\n",
            "|    explained_variance | 0.11     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 178499   |\n",
            "|    policy_loss        | 0.661    |\n",
            "|    value_loss         | 629      |\n",
            "------------------------------------\n",
            "Num timesteps: 7144000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 165.07\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 326      |\n",
            "|    ep_rew_mean        | 165      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1200     |\n",
            "|    iterations         | 178600   |\n",
            "|    time_elapsed       | 5952     |\n",
            "|    total_timesteps    | 7144000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.211   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 178599   |\n",
            "|    policy_loss        | -0.142   |\n",
            "|    value_loss         | 2.34     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 313      |\n",
            "|    ep_rew_mean        | 163      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1200     |\n",
            "|    iterations         | 178700   |\n",
            "|    time_elapsed       | 5954     |\n",
            "|    total_timesteps    | 7148000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.301   |\n",
            "|    explained_variance | 0.263    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 178699   |\n",
            "|    policy_loss        | 0.184    |\n",
            "|    value_loss         | 369      |\n",
            "------------------------------------\n",
            "Num timesteps: 7152000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 154.75\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 293      |\n",
            "|    ep_rew_mean        | 155      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1200     |\n",
            "|    iterations         | 178800   |\n",
            "|    time_elapsed       | 5956     |\n",
            "|    total_timesteps    | 7152000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.299   |\n",
            "|    explained_variance | 0.896    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 178799   |\n",
            "|    policy_loss        | 0.165    |\n",
            "|    value_loss         | 68       |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 262      |\n",
            "|    ep_rew_mean        | 142      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1201     |\n",
            "|    iterations         | 178900   |\n",
            "|    time_elapsed       | 5958     |\n",
            "|    total_timesteps    | 7156000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.319   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 178899   |\n",
            "|    policy_loss        | -0.081   |\n",
            "|    value_loss         | 1.3      |\n",
            "------------------------------------\n",
            "Num timesteps: 7160000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 143.17\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 249      |\n",
            "|    ep_rew_mean        | 143      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1201     |\n",
            "|    iterations         | 179000   |\n",
            "|    time_elapsed       | 5960     |\n",
            "|    total_timesteps    | 7160000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.272   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 178999   |\n",
            "|    policy_loss        | -0.0884  |\n",
            "|    value_loss         | 1.96     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 241      |\n",
            "|    ep_rew_mean        | 130      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1201     |\n",
            "|    iterations         | 179100   |\n",
            "|    time_elapsed       | 5962     |\n",
            "|    total_timesteps    | 7164000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.421   |\n",
            "|    explained_variance | 0.81     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 179099   |\n",
            "|    policy_loss        | 0.216    |\n",
            "|    value_loss         | 117      |\n",
            "------------------------------------\n",
            "Num timesteps: 7168000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 122.49\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 238      |\n",
            "|    ep_rew_mean        | 122      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1201     |\n",
            "|    iterations         | 179200   |\n",
            "|    time_elapsed       | 5964     |\n",
            "|    total_timesteps    | 7168000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.368   |\n",
            "|    explained_variance | 0.805    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 179199   |\n",
            "|    policy_loss        | 0.341    |\n",
            "|    value_loss         | 71.8     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 245      |\n",
            "|    ep_rew_mean        | 118      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1202     |\n",
            "|    iterations         | 179300   |\n",
            "|    time_elapsed       | 5966     |\n",
            "|    total_timesteps    | 7172000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.387   |\n",
            "|    explained_variance | 0.744    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 179299   |\n",
            "|    policy_loss        | -0.26    |\n",
            "|    value_loss         | 39.1     |\n",
            "------------------------------------\n",
            "Num timesteps: 7176000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 127.56\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 264      |\n",
            "|    ep_rew_mean        | 128      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1202     |\n",
            "|    iterations         | 179400   |\n",
            "|    time_elapsed       | 5969     |\n",
            "|    total_timesteps    | 7176000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.337   |\n",
            "|    explained_variance | 0.252    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 179399   |\n",
            "|    policy_loss        | -0.0568  |\n",
            "|    value_loss         | 645      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 276      |\n",
            "|    ep_rew_mean        | 126      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1202     |\n",
            "|    iterations         | 179500   |\n",
            "|    time_elapsed       | 5971     |\n",
            "|    total_timesteps    | 7180000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.294   |\n",
            "|    explained_variance | 0.821    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 179499   |\n",
            "|    policy_loss        | -0.0257  |\n",
            "|    value_loss         | 291      |\n",
            "------------------------------------\n",
            "Num timesteps: 7184000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 115.24\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 286      |\n",
            "|    ep_rew_mean        | 115      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1202     |\n",
            "|    iterations         | 179600   |\n",
            "|    time_elapsed       | 5974     |\n",
            "|    total_timesteps    | 7184000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.299   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 179599   |\n",
            "|    policy_loss        | 0.418    |\n",
            "|    value_loss         | 2.85     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 291      |\n",
            "|    ep_rew_mean        | 120      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1202     |\n",
            "|    iterations         | 179700   |\n",
            "|    time_elapsed       | 5976     |\n",
            "|    total_timesteps    | 7188000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.227   |\n",
            "|    explained_variance | 0.257    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 179699   |\n",
            "|    policy_loss        | -0.0718  |\n",
            "|    value_loss         | 695      |\n",
            "------------------------------------\n",
            "Num timesteps: 7192000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 114.74\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 313      |\n",
            "|    ep_rew_mean        | 115      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1202     |\n",
            "|    iterations         | 179800   |\n",
            "|    time_elapsed       | 5979     |\n",
            "|    total_timesteps    | 7192000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.262   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 179799   |\n",
            "|    policy_loss        | 0.428    |\n",
            "|    value_loss         | 1.48     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 328      |\n",
            "|    ep_rew_mean        | 122      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1202     |\n",
            "|    iterations         | 179900   |\n",
            "|    time_elapsed       | 5981     |\n",
            "|    total_timesteps    | 7196000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.281   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 179899   |\n",
            "|    policy_loss        | 0.0472   |\n",
            "|    value_loss         | 2.78     |\n",
            "------------------------------------\n",
            "Num timesteps: 7200000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 126.70\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 331      |\n",
            "|    ep_rew_mean        | 127      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1203     |\n",
            "|    iterations         | 180000   |\n",
            "|    time_elapsed       | 5984     |\n",
            "|    total_timesteps    | 7200000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.339   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 179999   |\n",
            "|    policy_loss        | -0.00732 |\n",
            "|    value_loss         | 0.89     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 334      |\n",
            "|    ep_rew_mean        | 122      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1203     |\n",
            "|    iterations         | 180100   |\n",
            "|    time_elapsed       | 5987     |\n",
            "|    total_timesteps    | 7204000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.412   |\n",
            "|    explained_variance | 0.98     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 180099   |\n",
            "|    policy_loss        | 0.743    |\n",
            "|    value_loss         | 22.3     |\n",
            "------------------------------------\n",
            "Num timesteps: 7208000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 119.09\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 323      |\n",
            "|    ep_rew_mean        | 119      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1203     |\n",
            "|    iterations         | 180200   |\n",
            "|    time_elapsed       | 5989     |\n",
            "|    total_timesteps    | 7208000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.305   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 180199   |\n",
            "|    policy_loss        | 0.00486  |\n",
            "|    value_loss         | 3.12     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 299      |\n",
            "|    ep_rew_mean        | 128      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1203     |\n",
            "|    iterations         | 180300   |\n",
            "|    time_elapsed       | 5991     |\n",
            "|    total_timesteps    | 7212000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.245   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 180299   |\n",
            "|    policy_loss        | -0.0484  |\n",
            "|    value_loss         | 0.596    |\n",
            "------------------------------------\n",
            "Num timesteps: 7216000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 121.73\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 275      |\n",
            "|    ep_rew_mean        | 122      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1204     |\n",
            "|    iterations         | 180400   |\n",
            "|    time_elapsed       | 5993     |\n",
            "|    total_timesteps    | 7216000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.266   |\n",
            "|    explained_variance | 0.341    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 180399   |\n",
            "|    policy_loss        | -5.74    |\n",
            "|    value_loss         | 1.02e+03 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 250      |\n",
            "|    ep_rew_mean        | 113      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1204     |\n",
            "|    iterations         | 180500   |\n",
            "|    time_elapsed       | 5995     |\n",
            "|    total_timesteps    | 7220000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.313   |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 180499   |\n",
            "|    policy_loss        | -0.147   |\n",
            "|    value_loss         | 1.56     |\n",
            "------------------------------------\n",
            "Num timesteps: 7224000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 100.73\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 239      |\n",
            "|    ep_rew_mean        | 101      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1204     |\n",
            "|    iterations         | 180600   |\n",
            "|    time_elapsed       | 5997     |\n",
            "|    total_timesteps    | 7224000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.299   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 180599   |\n",
            "|    policy_loss        | -0.11    |\n",
            "|    value_loss         | 0.56     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 240      |\n",
            "|    ep_rew_mean        | 105      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1204     |\n",
            "|    iterations         | 180700   |\n",
            "|    time_elapsed       | 5999     |\n",
            "|    total_timesteps    | 7228000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.185   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 180699   |\n",
            "|    policy_loss        | 0.00836  |\n",
            "|    value_loss         | 4.16     |\n",
            "------------------------------------\n",
            "Num timesteps: 7232000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 101.21\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 248      |\n",
            "|    ep_rew_mean        | 101      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1204     |\n",
            "|    iterations         | 180800   |\n",
            "|    time_elapsed       | 6002     |\n",
            "|    total_timesteps    | 7232000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.369   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 180799   |\n",
            "|    policy_loss        | 0.27     |\n",
            "|    value_loss         | 0.821    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 258      |\n",
            "|    ep_rew_mean        | 110      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1204     |\n",
            "|    iterations         | 180900   |\n",
            "|    time_elapsed       | 6005     |\n",
            "|    total_timesteps    | 7236000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.291   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 180899   |\n",
            "|    policy_loss        | 0.752    |\n",
            "|    value_loss         | 11       |\n",
            "------------------------------------\n",
            "Num timesteps: 7240000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 111.85\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 264      |\n",
            "|    ep_rew_mean        | 112      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1205     |\n",
            "|    iterations         | 181000   |\n",
            "|    time_elapsed       | 6007     |\n",
            "|    total_timesteps    | 7240000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.342   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 180999   |\n",
            "|    policy_loss        | -0.261   |\n",
            "|    value_loss         | 3.33     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 269      |\n",
            "|    ep_rew_mean        | 123      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1205     |\n",
            "|    iterations         | 181100   |\n",
            "|    time_elapsed       | 6009     |\n",
            "|    total_timesteps    | 7244000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.426   |\n",
            "|    explained_variance | 0.64     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 181099   |\n",
            "|    policy_loss        | 0.29     |\n",
            "|    value_loss         | 309      |\n",
            "------------------------------------\n",
            "Num timesteps: 7248000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 138.18\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 278      |\n",
            "|    ep_rew_mean        | 138      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1205     |\n",
            "|    iterations         | 181200   |\n",
            "|    time_elapsed       | 6012     |\n",
            "|    total_timesteps    | 7248000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.337   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 181199   |\n",
            "|    policy_loss        | 0.0146   |\n",
            "|    value_loss         | 1.03     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 292      |\n",
            "|    ep_rew_mean        | 151      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1205     |\n",
            "|    iterations         | 181300   |\n",
            "|    time_elapsed       | 6015     |\n",
            "|    total_timesteps    | 7252000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.269   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 181299   |\n",
            "|    policy_loss        | -0.00943 |\n",
            "|    value_loss         | 3.08     |\n",
            "------------------------------------\n",
            "Num timesteps: 7256000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 159.37\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 304      |\n",
            "|    ep_rew_mean        | 159      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1205     |\n",
            "|    iterations         | 181400   |\n",
            "|    time_elapsed       | 6017     |\n",
            "|    total_timesteps    | 7256000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.411   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 181399   |\n",
            "|    policy_loss        | 0.431    |\n",
            "|    value_loss         | 2.76     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 305      |\n",
            "|    ep_rew_mean        | 165      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1205     |\n",
            "|    iterations         | 181500   |\n",
            "|    time_elapsed       | 6020     |\n",
            "|    total_timesteps    | 7260000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.42    |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 181499   |\n",
            "|    policy_loss        | -0.666   |\n",
            "|    value_loss         | 1.87     |\n",
            "------------------------------------\n",
            "Num timesteps: 7264000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 161.13\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 305      |\n",
            "|    ep_rew_mean        | 161      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1206     |\n",
            "|    iterations         | 181600   |\n",
            "|    time_elapsed       | 6022     |\n",
            "|    total_timesteps    | 7264000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.252   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 181599   |\n",
            "|    policy_loss        | -0.0634  |\n",
            "|    value_loss         | 1.32     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 318      |\n",
            "|    ep_rew_mean        | 171      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1206     |\n",
            "|    iterations         | 181700   |\n",
            "|    time_elapsed       | 6025     |\n",
            "|    total_timesteps    | 7268000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.383   |\n",
            "|    explained_variance | 0.889    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 181699   |\n",
            "|    policy_loss        | 0.0197   |\n",
            "|    value_loss         | 53.7     |\n",
            "------------------------------------\n",
            "Num timesteps: 7272000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 175.44\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 329      |\n",
            "|    ep_rew_mean        | 175      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1206     |\n",
            "|    iterations         | 181800   |\n",
            "|    time_elapsed       | 6027     |\n",
            "|    total_timesteps    | 7272000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.273   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 181799   |\n",
            "|    policy_loss        | 0.0192   |\n",
            "|    value_loss         | 4.9      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 332      |\n",
            "|    ep_rew_mean        | 180      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1206     |\n",
            "|    iterations         | 181900   |\n",
            "|    time_elapsed       | 6029     |\n",
            "|    total_timesteps    | 7276000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.219   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 181899   |\n",
            "|    policy_loss        | 0.14     |\n",
            "|    value_loss         | 1.8      |\n",
            "------------------------------------\n",
            "Num timesteps: 7280000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 174.54\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 328      |\n",
            "|    ep_rew_mean        | 175      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1206     |\n",
            "|    iterations         | 182000   |\n",
            "|    time_elapsed       | 6032     |\n",
            "|    total_timesteps    | 7280000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.268   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 181999   |\n",
            "|    policy_loss        | -0.192   |\n",
            "|    value_loss         | 1.02     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 314      |\n",
            "|    ep_rew_mean        | 164      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1207     |\n",
            "|    iterations         | 182100   |\n",
            "|    time_elapsed       | 6034     |\n",
            "|    total_timesteps    | 7284000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.401   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 182099   |\n",
            "|    policy_loss        | -0.154   |\n",
            "|    value_loss         | 1.07     |\n",
            "------------------------------------\n",
            "Num timesteps: 7288000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 163.83\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 317      |\n",
            "|    ep_rew_mean        | 164      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1207     |\n",
            "|    iterations         | 182200   |\n",
            "|    time_elapsed       | 6036     |\n",
            "|    total_timesteps    | 7288000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.259   |\n",
            "|    explained_variance | 0.924    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 182199   |\n",
            "|    policy_loss        | 0.129    |\n",
            "|    value_loss         | 70.6     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 288      |\n",
            "|    ep_rew_mean        | 146      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1207     |\n",
            "|    iterations         | 182300   |\n",
            "|    time_elapsed       | 6038     |\n",
            "|    total_timesteps    | 7292000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.345   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 182299   |\n",
            "|    policy_loss        | -0.164   |\n",
            "|    value_loss         | 1.4      |\n",
            "------------------------------------\n",
            "Num timesteps: 7296000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 133.81\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 269      |\n",
            "|    ep_rew_mean        | 134      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1207     |\n",
            "|    iterations         | 182400   |\n",
            "|    time_elapsed       | 6040     |\n",
            "|    total_timesteps    | 7296000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.421   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 182399   |\n",
            "|    policy_loss        | -0.101   |\n",
            "|    value_loss         | 6.4      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 262      |\n",
            "|    ep_rew_mean        | 128      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1208     |\n",
            "|    iterations         | 182500   |\n",
            "|    time_elapsed       | 6042     |\n",
            "|    total_timesteps    | 7300000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.242   |\n",
            "|    explained_variance | 0.628    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 182499   |\n",
            "|    policy_loss        | -5.32    |\n",
            "|    value_loss         | 194      |\n",
            "------------------------------------\n",
            "Num timesteps: 7304000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 108.15\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 250      |\n",
            "|    ep_rew_mean        | 108      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1208     |\n",
            "|    iterations         | 182600   |\n",
            "|    time_elapsed       | 6045     |\n",
            "|    total_timesteps    | 7304000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.363   |\n",
            "|    explained_variance | 0.541    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 182599   |\n",
            "|    policy_loss        | 2.48     |\n",
            "|    value_loss         | 296      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 254      |\n",
            "|    ep_rew_mean        | 105      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1208     |\n",
            "|    iterations         | 182700   |\n",
            "|    time_elapsed       | 6047     |\n",
            "|    total_timesteps    | 7308000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.122   |\n",
            "|    explained_variance | 0.338    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 182699   |\n",
            "|    policy_loss        | 0.114    |\n",
            "|    value_loss         | 44.2     |\n",
            "------------------------------------\n",
            "Num timesteps: 7312000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 112.24\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 260      |\n",
            "|    ep_rew_mean        | 112      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1208     |\n",
            "|    iterations         | 182800   |\n",
            "|    time_elapsed       | 6049     |\n",
            "|    total_timesteps    | 7312000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.408   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 182799   |\n",
            "|    policy_loss        | -0.038   |\n",
            "|    value_loss         | 2.13     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 262      |\n",
            "|    ep_rew_mean        | 118      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1208     |\n",
            "|    iterations         | 182900   |\n",
            "|    time_elapsed       | 6051     |\n",
            "|    total_timesteps    | 7316000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.458   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 182899   |\n",
            "|    policy_loss        | -0.16    |\n",
            "|    value_loss         | 0.778    |\n",
            "------------------------------------\n",
            "Num timesteps: 7320000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 124.41\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 274      |\n",
            "|    ep_rew_mean        | 124      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1209     |\n",
            "|    iterations         | 183000   |\n",
            "|    time_elapsed       | 6054     |\n",
            "|    total_timesteps    | 7320000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.403   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 182999   |\n",
            "|    policy_loss        | 0.148    |\n",
            "|    value_loss         | 0.738    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 289      |\n",
            "|    ep_rew_mean        | 123      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1209     |\n",
            "|    iterations         | 183100   |\n",
            "|    time_elapsed       | 6057     |\n",
            "|    total_timesteps    | 7324000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.353   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 183099   |\n",
            "|    policy_loss        | 0.0282   |\n",
            "|    value_loss         | 1.76     |\n",
            "------------------------------------\n",
            "Num timesteps: 7328000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 130.14\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 297      |\n",
            "|    ep_rew_mean        | 130      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1209     |\n",
            "|    iterations         | 183200   |\n",
            "|    time_elapsed       | 6059     |\n",
            "|    total_timesteps    | 7328000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.375   |\n",
            "|    explained_variance | 0.251    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 183199   |\n",
            "|    policy_loss        | 0.061    |\n",
            "|    value_loss         | 226      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 304      |\n",
            "|    ep_rew_mean        | 139      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1209     |\n",
            "|    iterations         | 183300   |\n",
            "|    time_elapsed       | 6061     |\n",
            "|    total_timesteps    | 7332000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.311   |\n",
            "|    explained_variance | 0.973    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 183299   |\n",
            "|    policy_loss        | -0.00909 |\n",
            "|    value_loss         | 11.2     |\n",
            "------------------------------------\n",
            "Num timesteps: 7336000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 136.79\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 297      |\n",
            "|    ep_rew_mean        | 137      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1209     |\n",
            "|    iterations         | 183400   |\n",
            "|    time_elapsed       | 6063     |\n",
            "|    total_timesteps    | 7336000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.259   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 183399   |\n",
            "|    policy_loss        | 0.0383   |\n",
            "|    value_loss         | 1.7      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 300      |\n",
            "|    ep_rew_mean        | 130      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1210     |\n",
            "|    iterations         | 183500   |\n",
            "|    time_elapsed       | 6065     |\n",
            "|    total_timesteps    | 7340000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.378   |\n",
            "|    explained_variance | 0.546    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 183499   |\n",
            "|    policy_loss        | -2.54    |\n",
            "|    value_loss         | 825      |\n",
            "------------------------------------\n",
            "Num timesteps: 7344000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 121.02\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 298      |\n",
            "|    ep_rew_mean        | 121      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1210     |\n",
            "|    iterations         | 183600   |\n",
            "|    time_elapsed       | 6068     |\n",
            "|    total_timesteps    | 7344000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.408   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 183599   |\n",
            "|    policy_loss        | -0.101   |\n",
            "|    value_loss         | 0.563    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 310      |\n",
            "|    ep_rew_mean        | 120      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1210     |\n",
            "|    iterations         | 183700   |\n",
            "|    time_elapsed       | 6070     |\n",
            "|    total_timesteps    | 7348000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.335   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 183699   |\n",
            "|    policy_loss        | 0.0831   |\n",
            "|    value_loss         | 2.92     |\n",
            "------------------------------------\n",
            "Num timesteps: 7352000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 118.93\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 301      |\n",
            "|    ep_rew_mean        | 119      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1210     |\n",
            "|    iterations         | 183800   |\n",
            "|    time_elapsed       | 6073     |\n",
            "|    total_timesteps    | 7352000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.384   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 183799   |\n",
            "|    policy_loss        | 0.00578  |\n",
            "|    value_loss         | 2.17     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 309      |\n",
            "|    ep_rew_mean        | 128      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1210     |\n",
            "|    iterations         | 183900   |\n",
            "|    time_elapsed       | 6075     |\n",
            "|    total_timesteps    | 7356000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.342   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 183899   |\n",
            "|    policy_loss        | 0.222    |\n",
            "|    value_loss         | 1.85     |\n",
            "------------------------------------\n",
            "Num timesteps: 7360000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 124.66\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 310      |\n",
            "|    ep_rew_mean        | 125      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1210     |\n",
            "|    iterations         | 184000   |\n",
            "|    time_elapsed       | 6078     |\n",
            "|    total_timesteps    | 7360000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.374   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 183999   |\n",
            "|    policy_loss        | -0.097   |\n",
            "|    value_loss         | 5.55     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 324      |\n",
            "|    ep_rew_mean        | 122      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1211     |\n",
            "|    iterations         | 184100   |\n",
            "|    time_elapsed       | 6080     |\n",
            "|    total_timesteps    | 7364000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.36    |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 184099   |\n",
            "|    policy_loss        | -0.108   |\n",
            "|    value_loss         | 0.478    |\n",
            "------------------------------------\n",
            "Num timesteps: 7368000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 119.72\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 325      |\n",
            "|    ep_rew_mean        | 120      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1211     |\n",
            "|    iterations         | 184200   |\n",
            "|    time_elapsed       | 6083     |\n",
            "|    total_timesteps    | 7368000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.255   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 184199   |\n",
            "|    policy_loss        | 0.0376   |\n",
            "|    value_loss         | 0.499    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 341      |\n",
            "|    ep_rew_mean        | 123      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1211     |\n",
            "|    iterations         | 184300   |\n",
            "|    time_elapsed       | 6085     |\n",
            "|    total_timesteps    | 7372000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.295   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 184299   |\n",
            "|    policy_loss        | 0.22     |\n",
            "|    value_loss         | 0.761    |\n",
            "------------------------------------\n",
            "Num timesteps: 7376000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 129.39\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 347      |\n",
            "|    ep_rew_mean        | 129      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1211     |\n",
            "|    iterations         | 184400   |\n",
            "|    time_elapsed       | 6087     |\n",
            "|    total_timesteps    | 7376000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.354   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 184399   |\n",
            "|    policy_loss        | 0.0309   |\n",
            "|    value_loss         | 0.991    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 355      |\n",
            "|    ep_rew_mean        | 142      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1211     |\n",
            "|    iterations         | 184500   |\n",
            "|    time_elapsed       | 6090     |\n",
            "|    total_timesteps    | 7380000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.417   |\n",
            "|    explained_variance | 0.965    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 184499   |\n",
            "|    policy_loss        | -0.222   |\n",
            "|    value_loss         | 3.79     |\n",
            "------------------------------------\n",
            "Num timesteps: 7384000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 144.19\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 349      |\n",
            "|    ep_rew_mean        | 144      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1211     |\n",
            "|    iterations         | 184600   |\n",
            "|    time_elapsed       | 6093     |\n",
            "|    total_timesteps    | 7384000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.223   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 184599   |\n",
            "|    policy_loss        | -0.0486  |\n",
            "|    value_loss         | 1.52     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 350      |\n",
            "|    ep_rew_mean        | 139      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1212     |\n",
            "|    iterations         | 184700   |\n",
            "|    time_elapsed       | 6095     |\n",
            "|    total_timesteps    | 7388000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.406   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 184699   |\n",
            "|    policy_loss        | -0.00422 |\n",
            "|    value_loss         | 0.652    |\n",
            "------------------------------------\n",
            "Num timesteps: 7392000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 136.47\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 343      |\n",
            "|    ep_rew_mean        | 136      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1212     |\n",
            "|    iterations         | 184800   |\n",
            "|    time_elapsed       | 6098     |\n",
            "|    total_timesteps    | 7392000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.302   |\n",
            "|    explained_variance | 0.971    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 184799   |\n",
            "|    policy_loss        | -0.0445  |\n",
            "|    value_loss         | 43.1     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 334      |\n",
            "|    ep_rew_mean        | 148      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1212     |\n",
            "|    iterations         | 184900   |\n",
            "|    time_elapsed       | 6100     |\n",
            "|    total_timesteps    | 7396000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.282   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 184899   |\n",
            "|    policy_loss        | -0.00752 |\n",
            "|    value_loss         | 0.253    |\n",
            "------------------------------------\n",
            "Num timesteps: 7400000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 159.75\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 335      |\n",
            "|    ep_rew_mean        | 160      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1212     |\n",
            "|    iterations         | 185000   |\n",
            "|    time_elapsed       | 6102     |\n",
            "|    total_timesteps    | 7400000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.396   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 184999   |\n",
            "|    policy_loss        | -0.0625  |\n",
            "|    value_loss         | 0.353    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 321      |\n",
            "|    ep_rew_mean        | 159      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1212     |\n",
            "|    iterations         | 185100   |\n",
            "|    time_elapsed       | 6104     |\n",
            "|    total_timesteps    | 7404000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.346   |\n",
            "|    explained_variance | 0.375    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 185099   |\n",
            "|    policy_loss        | 0.00851  |\n",
            "|    value_loss         | 1.25e+03 |\n",
            "------------------------------------\n",
            "Num timesteps: 7408000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 162.53\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 319      |\n",
            "|    ep_rew_mean        | 163      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1212     |\n",
            "|    iterations         | 185200   |\n",
            "|    time_elapsed       | 6107     |\n",
            "|    total_timesteps    | 7408000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.282   |\n",
            "|    explained_variance | 0.791    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 185199   |\n",
            "|    policy_loss        | -0.0405  |\n",
            "|    value_loss         | 44.1     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 311      |\n",
            "|    ep_rew_mean        | 151      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1213     |\n",
            "|    iterations         | 185300   |\n",
            "|    time_elapsed       | 6109     |\n",
            "|    total_timesteps    | 7412000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.453   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 185299   |\n",
            "|    policy_loss        | 0.0243   |\n",
            "|    value_loss         | 0.479    |\n",
            "------------------------------------\n",
            "Num timesteps: 7416000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 141.44\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 303      |\n",
            "|    ep_rew_mean        | 141      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1213     |\n",
            "|    iterations         | 185400   |\n",
            "|    time_elapsed       | 6111     |\n",
            "|    total_timesteps    | 7416000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.33    |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 185399   |\n",
            "|    policy_loss        | -0.295   |\n",
            "|    value_loss         | 0.562    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 301      |\n",
            "|    ep_rew_mean        | 142      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1213     |\n",
            "|    iterations         | 185500   |\n",
            "|    time_elapsed       | 6114     |\n",
            "|    total_timesteps    | 7420000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.438   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 185499   |\n",
            "|    policy_loss        | -0.115   |\n",
            "|    value_loss         | 1.38     |\n",
            "------------------------------------\n",
            "Num timesteps: 7424000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 137.59\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 301      |\n",
            "|    ep_rew_mean        | 138      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1213     |\n",
            "|    iterations         | 185600   |\n",
            "|    time_elapsed       | 6116     |\n",
            "|    total_timesteps    | 7424000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.347   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 185599   |\n",
            "|    policy_loss        | 0.00763  |\n",
            "|    value_loss         | 0.597    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 310      |\n",
            "|    ep_rew_mean        | 130      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1213     |\n",
            "|    iterations         | 185700   |\n",
            "|    time_elapsed       | 6119     |\n",
            "|    total_timesteps    | 7428000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.369   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 185699   |\n",
            "|    policy_loss        | -0.0688  |\n",
            "|    value_loss         | 0.287    |\n",
            "------------------------------------\n",
            "Num timesteps: 7432000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 129.58\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 320      |\n",
            "|    ep_rew_mean        | 130      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1213     |\n",
            "|    iterations         | 185800   |\n",
            "|    time_elapsed       | 6122     |\n",
            "|    total_timesteps    | 7432000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.349   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 185799   |\n",
            "|    policy_loss        | 0.128    |\n",
            "|    value_loss         | 0.812    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 323      |\n",
            "|    ep_rew_mean        | 135      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1214     |\n",
            "|    iterations         | 185900   |\n",
            "|    time_elapsed       | 6124     |\n",
            "|    total_timesteps    | 7436000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.273   |\n",
            "|    explained_variance | 0.97     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 185899   |\n",
            "|    policy_loss        | 0.269    |\n",
            "|    value_loss         | 11       |\n",
            "------------------------------------\n",
            "Num timesteps: 7440000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 120.64\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 317      |\n",
            "|    ep_rew_mean        | 121      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1214     |\n",
            "|    iterations         | 186000   |\n",
            "|    time_elapsed       | 6126     |\n",
            "|    total_timesteps    | 7440000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.416   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 185999   |\n",
            "|    policy_loss        | -0.267   |\n",
            "|    value_loss         | 1.49     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 314      |\n",
            "|    ep_rew_mean        | 119      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1214     |\n",
            "|    iterations         | 186100   |\n",
            "|    time_elapsed       | 6129     |\n",
            "|    total_timesteps    | 7444000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.317   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 186099   |\n",
            "|    policy_loss        | 0.0495   |\n",
            "|    value_loss         | 2.15     |\n",
            "------------------------------------\n",
            "Num timesteps: 7448000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 126.61\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 318      |\n",
            "|    ep_rew_mean        | 127      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1214     |\n",
            "|    iterations         | 186200   |\n",
            "|    time_elapsed       | 6131     |\n",
            "|    total_timesteps    | 7448000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.254   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 186199   |\n",
            "|    policy_loss        | -0.0664  |\n",
            "|    value_loss         | 2.89     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 305      |\n",
            "|    ep_rew_mean        | 125      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1214     |\n",
            "|    iterations         | 186300   |\n",
            "|    time_elapsed       | 6133     |\n",
            "|    total_timesteps    | 7452000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.37    |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 186299   |\n",
            "|    policy_loss        | -0.275   |\n",
            "|    value_loss         | 1.07     |\n",
            "------------------------------------\n",
            "Num timesteps: 7456000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 127.10\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 292      |\n",
            "|    ep_rew_mean        | 127      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1215     |\n",
            "|    iterations         | 186400   |\n",
            "|    time_elapsed       | 6136     |\n",
            "|    total_timesteps    | 7456000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.379   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 186399   |\n",
            "|    policy_loss        | 0.054    |\n",
            "|    value_loss         | 0.69     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 275      |\n",
            "|    ep_rew_mean        | 110      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1215     |\n",
            "|    iterations         | 186500   |\n",
            "|    time_elapsed       | 6138     |\n",
            "|    total_timesteps    | 7460000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.289   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 186499   |\n",
            "|    policy_loss        | 0.0287   |\n",
            "|    value_loss         | 21.1     |\n",
            "------------------------------------\n",
            "Num timesteps: 7464000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 106.31\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 267      |\n",
            "|    ep_rew_mean        | 106      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1215     |\n",
            "|    iterations         | 186600   |\n",
            "|    time_elapsed       | 6140     |\n",
            "|    total_timesteps    | 7464000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.235   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 186599   |\n",
            "|    policy_loss        | 0.0161   |\n",
            "|    value_loss         | 0.673    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 260      |\n",
            "|    ep_rew_mean        | 108      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1215     |\n",
            "|    iterations         | 186700   |\n",
            "|    time_elapsed       | 6142     |\n",
            "|    total_timesteps    | 7468000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.326   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 186699   |\n",
            "|    policy_loss        | 0.063    |\n",
            "|    value_loss         | 1.38     |\n",
            "------------------------------------\n",
            "Num timesteps: 7472000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 104.25\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 250      |\n",
            "|    ep_rew_mean        | 104      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1216     |\n",
            "|    iterations         | 186800   |\n",
            "|    time_elapsed       | 6144     |\n",
            "|    total_timesteps    | 7472000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.407   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 186799   |\n",
            "|    policy_loss        | -0.0218  |\n",
            "|    value_loss         | 1.04     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 251      |\n",
            "|    ep_rew_mean        | 112      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1216     |\n",
            "|    iterations         | 186900   |\n",
            "|    time_elapsed       | 6146     |\n",
            "|    total_timesteps    | 7476000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.309   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 186899   |\n",
            "|    policy_loss        | -0.0953  |\n",
            "|    value_loss         | 1.35     |\n",
            "------------------------------------\n",
            "Num timesteps: 7480000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 122.18\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 256      |\n",
            "|    ep_rew_mean        | 122      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1216     |\n",
            "|    iterations         | 187000   |\n",
            "|    time_elapsed       | 6148     |\n",
            "|    total_timesteps    | 7480000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.385   |\n",
            "|    explained_variance | 0.898    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 186999   |\n",
            "|    policy_loss        | -1.9     |\n",
            "|    value_loss         | 98.4     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 256      |\n",
            "|    ep_rew_mean        | 127      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1216     |\n",
            "|    iterations         | 187100   |\n",
            "|    time_elapsed       | 6150     |\n",
            "|    total_timesteps    | 7484000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.445   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 187099   |\n",
            "|    policy_loss        | -0.0392  |\n",
            "|    value_loss         | 3.11     |\n",
            "------------------------------------\n",
            "Num timesteps: 7488000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 138.09\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 258      |\n",
            "|    ep_rew_mean        | 138      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1216     |\n",
            "|    iterations         | 187200   |\n",
            "|    time_elapsed       | 6153     |\n",
            "|    total_timesteps    | 7488000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.3     |\n",
            "|    explained_variance | 0.97     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 187199   |\n",
            "|    policy_loss        | 2.21     |\n",
            "|    value_loss         | 21.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 258      |\n",
            "|    ep_rew_mean        | 142      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1217     |\n",
            "|    iterations         | 187300   |\n",
            "|    time_elapsed       | 6155     |\n",
            "|    total_timesteps    | 7492000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.343   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 187299   |\n",
            "|    policy_loss        | -0.165   |\n",
            "|    value_loss         | 1.03     |\n",
            "------------------------------------\n",
            "Num timesteps: 7496000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 152.63\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 271      |\n",
            "|    ep_rew_mean        | 153      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1217     |\n",
            "|    iterations         | 187400   |\n",
            "|    time_elapsed       | 6157     |\n",
            "|    total_timesteps    | 7496000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.325   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 187399   |\n",
            "|    policy_loss        | -0.0666  |\n",
            "|    value_loss         | 0.751    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 281      |\n",
            "|    ep_rew_mean        | 159      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1217     |\n",
            "|    iterations         | 187500   |\n",
            "|    time_elapsed       | 6160     |\n",
            "|    total_timesteps    | 7500000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.396   |\n",
            "|    explained_variance | 0.962    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 187499   |\n",
            "|    policy_loss        | -0.131   |\n",
            "|    value_loss         | 53.6     |\n",
            "------------------------------------\n",
            "Num timesteps: 7504000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 156.14\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 293      |\n",
            "|    ep_rew_mean        | 156      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1217     |\n",
            "|    iterations         | 187600   |\n",
            "|    time_elapsed       | 6162     |\n",
            "|    total_timesteps    | 7504000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.323   |\n",
            "|    explained_variance | 0.725    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 187599   |\n",
            "|    policy_loss        | -6.2     |\n",
            "|    value_loss         | 603      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 294      |\n",
            "|    ep_rew_mean        | 155      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1217     |\n",
            "|    iterations         | 187700   |\n",
            "|    time_elapsed       | 6165     |\n",
            "|    total_timesteps    | 7508000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.374   |\n",
            "|    explained_variance | 0.741    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 187699   |\n",
            "|    policy_loss        | 0.11     |\n",
            "|    value_loss         | 487      |\n",
            "------------------------------------\n",
            "Num timesteps: 7512000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 153.36\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 300      |\n",
            "|    ep_rew_mean        | 153      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1217     |\n",
            "|    iterations         | 187800   |\n",
            "|    time_elapsed       | 6167     |\n",
            "|    total_timesteps    | 7512000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.396   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 187799   |\n",
            "|    policy_loss        | -0.0808  |\n",
            "|    value_loss         | 1.22     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 309      |\n",
            "|    ep_rew_mean        | 165      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1218     |\n",
            "|    iterations         | 187900   |\n",
            "|    time_elapsed       | 6170     |\n",
            "|    total_timesteps    | 7516000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.396   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 187899   |\n",
            "|    policy_loss        | -0.0547  |\n",
            "|    value_loss         | 0.515    |\n",
            "------------------------------------\n",
            "Num timesteps: 7520000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 177.36\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 314      |\n",
            "|    ep_rew_mean        | 177      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1218     |\n",
            "|    iterations         | 188000   |\n",
            "|    time_elapsed       | 6172     |\n",
            "|    total_timesteps    | 7520000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.273   |\n",
            "|    explained_variance | 0.119    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 187999   |\n",
            "|    policy_loss        | 1.02     |\n",
            "|    value_loss         | 152      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 321      |\n",
            "|    ep_rew_mean        | 184      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1218     |\n",
            "|    iterations         | 188100   |\n",
            "|    time_elapsed       | 6174     |\n",
            "|    total_timesteps    | 7524000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.341   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 188099   |\n",
            "|    policy_loss        | -0.182   |\n",
            "|    value_loss         | 1.36     |\n",
            "------------------------------------\n",
            "Num timesteps: 7528000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 178.97\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 305      |\n",
            "|    ep_rew_mean        | 179      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1218     |\n",
            "|    iterations         | 188200   |\n",
            "|    time_elapsed       | 6177     |\n",
            "|    total_timesteps    | 7528000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.315   |\n",
            "|    explained_variance | 0.98     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 188199   |\n",
            "|    policy_loss        | -0.0332  |\n",
            "|    value_loss         | 7.44     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 301      |\n",
            "|    ep_rew_mean        | 180      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1218     |\n",
            "|    iterations         | 188300   |\n",
            "|    time_elapsed       | 6179     |\n",
            "|    total_timesteps    | 7532000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.362   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 188299   |\n",
            "|    policy_loss        | 0.0353   |\n",
            "|    value_loss         | 2.29     |\n",
            "------------------------------------\n",
            "Num timesteps: 7536000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 187.97\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 303      |\n",
            "|    ep_rew_mean        | 188      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1219     |\n",
            "|    iterations         | 188400   |\n",
            "|    time_elapsed       | 6181     |\n",
            "|    total_timesteps    | 7536000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.379   |\n",
            "|    explained_variance | 0.979    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 188399   |\n",
            "|    policy_loss        | 0.0566   |\n",
            "|    value_loss         | 2.52     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 296      |\n",
            "|    ep_rew_mean        | 187      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1219     |\n",
            "|    iterations         | 188500   |\n",
            "|    time_elapsed       | 6184     |\n",
            "|    total_timesteps    | 7540000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.259   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 188499   |\n",
            "|    policy_loss        | 0.0238   |\n",
            "|    value_loss         | 0.536    |\n",
            "------------------------------------\n",
            "Num timesteps: 7544000\n",
            "Best mean reward: 193.19 - Last mean reward per episode: 193.30\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 290      |\n",
            "|    ep_rew_mean        | 193      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1219     |\n",
            "|    iterations         | 188600   |\n",
            "|    time_elapsed       | 6186     |\n",
            "|    total_timesteps    | 7544000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.368   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 188599   |\n",
            "|    policy_loss        | -0.157   |\n",
            "|    value_loss         | 1.14     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 274      |\n",
            "|    ep_rew_mean        | 180      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1219     |\n",
            "|    iterations         | 188700   |\n",
            "|    time_elapsed       | 6188     |\n",
            "|    total_timesteps    | 7548000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.375   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 188699   |\n",
            "|    policy_loss        | -0.18    |\n",
            "|    value_loss         | 2.88     |\n",
            "------------------------------------\n",
            "Num timesteps: 7552000\n",
            "Best mean reward: 193.30 - Last mean reward per episode: 176.17\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 268      |\n",
            "|    ep_rew_mean        | 176      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1219     |\n",
            "|    iterations         | 188800   |\n",
            "|    time_elapsed       | 6190     |\n",
            "|    total_timesteps    | 7552000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.335   |\n",
            "|    explained_variance | 0.464    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 188799   |\n",
            "|    policy_loss        | -0.238   |\n",
            "|    value_loss         | 561      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 280      |\n",
            "|    ep_rew_mean        | 182      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1220     |\n",
            "|    iterations         | 188900   |\n",
            "|    time_elapsed       | 6193     |\n",
            "|    total_timesteps    | 7556000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.232   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 188899   |\n",
            "|    policy_loss        | -0.0754  |\n",
            "|    value_loss         | 2.73     |\n",
            "------------------------------------\n",
            "Num timesteps: 7560000\n",
            "Best mean reward: 193.30 - Last mean reward per episode: 174.84\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 278      |\n",
            "|    ep_rew_mean        | 175      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1220     |\n",
            "|    iterations         | 189000   |\n",
            "|    time_elapsed       | 6195     |\n",
            "|    total_timesteps    | 7560000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.243   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 188999   |\n",
            "|    policy_loss        | 0.0072   |\n",
            "|    value_loss         | 0.408    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 266      |\n",
            "|    ep_rew_mean        | 169      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1220     |\n",
            "|    iterations         | 189100   |\n",
            "|    time_elapsed       | 6197     |\n",
            "|    total_timesteps    | 7564000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.323   |\n",
            "|    explained_variance | 0.681    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 189099   |\n",
            "|    policy_loss        | 0.218    |\n",
            "|    value_loss         | 156      |\n",
            "------------------------------------\n",
            "Num timesteps: 7568000\n",
            "Best mean reward: 193.30 - Last mean reward per episode: 177.12\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 262      |\n",
            "|    ep_rew_mean        | 177      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1220     |\n",
            "|    iterations         | 189200   |\n",
            "|    time_elapsed       | 6199     |\n",
            "|    total_timesteps    | 7568000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.228   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 189199   |\n",
            "|    policy_loss        | 0.255    |\n",
            "|    value_loss         | 3.07     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 258      |\n",
            "|    ep_rew_mean        | 180      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1221     |\n",
            "|    iterations         | 189300   |\n",
            "|    time_elapsed       | 6201     |\n",
            "|    total_timesteps    | 7572000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.312   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 189299   |\n",
            "|    policy_loss        | -0.0666  |\n",
            "|    value_loss         | 6.78     |\n",
            "------------------------------------\n",
            "Num timesteps: 7576000\n",
            "Best mean reward: 193.30 - Last mean reward per episode: 172.02\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 255      |\n",
            "|    ep_rew_mean        | 172      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1221     |\n",
            "|    iterations         | 189400   |\n",
            "|    time_elapsed       | 6203     |\n",
            "|    total_timesteps    | 7576000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.351   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 189399   |\n",
            "|    policy_loss        | -0.0477  |\n",
            "|    value_loss         | 4.57     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 269      |\n",
            "|    ep_rew_mean        | 168      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1221     |\n",
            "|    iterations         | 189500   |\n",
            "|    time_elapsed       | 6205     |\n",
            "|    total_timesteps    | 7580000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.267   |\n",
            "|    explained_variance | 0.74     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 189499   |\n",
            "|    policy_loss        | -3.37    |\n",
            "|    value_loss         | 465      |\n",
            "------------------------------------\n",
            "Num timesteps: 7584000\n",
            "Best mean reward: 193.30 - Last mean reward per episode: 182.24\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 256      |\n",
            "|    ep_rew_mean        | 182      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1221     |\n",
            "|    iterations         | 189600   |\n",
            "|    time_elapsed       | 6207     |\n",
            "|    total_timesteps    | 7584000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.468   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 189599   |\n",
            "|    policy_loss        | -0.0988  |\n",
            "|    value_loss         | 0.319    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 264      |\n",
            "|    ep_rew_mean        | 186      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1221     |\n",
            "|    iterations         | 189700   |\n",
            "|    time_elapsed       | 6209     |\n",
            "|    total_timesteps    | 7588000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.284   |\n",
            "|    explained_variance | 0.596    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 189699   |\n",
            "|    policy_loss        | 0.102    |\n",
            "|    value_loss         | 1.23e+03 |\n",
            "------------------------------------\n",
            "Num timesteps: 7592000\n",
            "Best mean reward: 193.30 - Last mean reward per episode: 189.15\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 264      |\n",
            "|    ep_rew_mean        | 189      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1222     |\n",
            "|    iterations         | 189800   |\n",
            "|    time_elapsed       | 6212     |\n",
            "|    total_timesteps    | 7592000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.368   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 189799   |\n",
            "|    policy_loss        | -0.0871  |\n",
            "|    value_loss         | 0.688    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 265      |\n",
            "|    ep_rew_mean        | 181      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1222     |\n",
            "|    iterations         | 189900   |\n",
            "|    time_elapsed       | 6214     |\n",
            "|    total_timesteps    | 7596000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.278   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 189899   |\n",
            "|    policy_loss        | -0.333   |\n",
            "|    value_loss         | 13.7     |\n",
            "------------------------------------\n",
            "Num timesteps: 7600000\n",
            "Best mean reward: 193.30 - Last mean reward per episode: 168.89\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 263      |\n",
            "|    ep_rew_mean        | 169      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1222     |\n",
            "|    iterations         | 190000   |\n",
            "|    time_elapsed       | 6216     |\n",
            "|    total_timesteps    | 7600000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.358   |\n",
            "|    explained_variance | 0.7      |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 189999   |\n",
            "|    policy_loss        | 0.0777   |\n",
            "|    value_loss         | 420      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 247      |\n",
            "|    ep_rew_mean        | 162      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1222     |\n",
            "|    iterations         | 190100   |\n",
            "|    time_elapsed       | 6218     |\n",
            "|    total_timesteps    | 7604000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.218   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 190099   |\n",
            "|    policy_loss        | 0.216    |\n",
            "|    value_loss         | 5.74     |\n",
            "------------------------------------\n",
            "Num timesteps: 7608000\n",
            "Best mean reward: 193.30 - Last mean reward per episode: 146.56\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 237      |\n",
            "|    ep_rew_mean        | 147      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1223     |\n",
            "|    iterations         | 190200   |\n",
            "|    time_elapsed       | 6220     |\n",
            "|    total_timesteps    | 7608000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.185   |\n",
            "|    explained_variance | 0.976    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 190199   |\n",
            "|    policy_loss        | 0.955    |\n",
            "|    value_loss         | 16.6     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 236      |\n",
            "|    ep_rew_mean        | 142      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1223     |\n",
            "|    iterations         | 190300   |\n",
            "|    time_elapsed       | 6222     |\n",
            "|    total_timesteps    | 7612000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.2     |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 190299   |\n",
            "|    policy_loss        | 0.0576   |\n",
            "|    value_loss         | 2.99     |\n",
            "------------------------------------\n",
            "Num timesteps: 7616000\n",
            "Best mean reward: 193.30 - Last mean reward per episode: 138.22\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 243      |\n",
            "|    ep_rew_mean        | 138      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1223     |\n",
            "|    iterations         | 190400   |\n",
            "|    time_elapsed       | 6224     |\n",
            "|    total_timesteps    | 7616000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.239   |\n",
            "|    explained_variance | 0.948    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 190399   |\n",
            "|    policy_loss        | 0.31     |\n",
            "|    value_loss         | 57.6     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 240      |\n",
            "|    ep_rew_mean        | 137      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1223     |\n",
            "|    iterations         | 190500   |\n",
            "|    time_elapsed       | 6226     |\n",
            "|    total_timesteps    | 7620000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.466   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 190499   |\n",
            "|    policy_loss        | 0.15     |\n",
            "|    value_loss         | 0.802    |\n",
            "------------------------------------\n",
            "Num timesteps: 7624000\n",
            "Best mean reward: 193.30 - Last mean reward per episode: 152.55\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 244      |\n",
            "|    ep_rew_mean        | 153      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1224     |\n",
            "|    iterations         | 190600   |\n",
            "|    time_elapsed       | 6228     |\n",
            "|    total_timesteps    | 7624000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.333   |\n",
            "|    explained_variance | 0.947    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 190599   |\n",
            "|    policy_loss        | 0.00652  |\n",
            "|    value_loss         | 32       |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 240      |\n",
            "|    ep_rew_mean        | 160      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1224     |\n",
            "|    iterations         | 190700   |\n",
            "|    time_elapsed       | 6230     |\n",
            "|    total_timesteps    | 7628000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.383   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 190699   |\n",
            "|    policy_loss        | -0.0471  |\n",
            "|    value_loss         | 1.28     |\n",
            "------------------------------------\n",
            "Num timesteps: 7632000\n",
            "Best mean reward: 193.30 - Last mean reward per episode: 170.62\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 246      |\n",
            "|    ep_rew_mean        | 171      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1224     |\n",
            "|    iterations         | 190800   |\n",
            "|    time_elapsed       | 6232     |\n",
            "|    total_timesteps    | 7632000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.218   |\n",
            "|    explained_variance | 0.468    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 190799   |\n",
            "|    policy_loss        | -0.0239  |\n",
            "|    value_loss         | 506      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 252      |\n",
            "|    ep_rew_mean        | 183      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1224     |\n",
            "|    iterations         | 190900   |\n",
            "|    time_elapsed       | 6234     |\n",
            "|    total_timesteps    | 7636000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.187   |\n",
            "|    explained_variance | 0.42     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 190899   |\n",
            "|    policy_loss        | 0.288    |\n",
            "|    value_loss         | 299      |\n",
            "------------------------------------\n",
            "Num timesteps: 7640000\n",
            "Best mean reward: 193.30 - Last mean reward per episode: 181.72\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 252      |\n",
            "|    ep_rew_mean        | 182      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1224     |\n",
            "|    iterations         | 191000   |\n",
            "|    time_elapsed       | 6237     |\n",
            "|    total_timesteps    | 7640000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.353   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 190999   |\n",
            "|    policy_loss        | -0.0118  |\n",
            "|    value_loss         | 0.684    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 257      |\n",
            "|    ep_rew_mean        | 189      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1225     |\n",
            "|    iterations         | 191100   |\n",
            "|    time_elapsed       | 6239     |\n",
            "|    total_timesteps    | 7644000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.306   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 191099   |\n",
            "|    policy_loss        | -0.249   |\n",
            "|    value_loss         | 2.26     |\n",
            "------------------------------------\n",
            "Num timesteps: 7648000\n",
            "Best mean reward: 193.30 - Last mean reward per episode: 190.45\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 258      |\n",
            "|    ep_rew_mean        | 190      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1225     |\n",
            "|    iterations         | 191200   |\n",
            "|    time_elapsed       | 6241     |\n",
            "|    total_timesteps    | 7648000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.242   |\n",
            "|    explained_variance | 0.889    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 191199   |\n",
            "|    policy_loss        | 0.0781   |\n",
            "|    value_loss         | 70.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 272      |\n",
            "|    ep_rew_mean        | 183      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1225     |\n",
            "|    iterations         | 191300   |\n",
            "|    time_elapsed       | 6244     |\n",
            "|    total_timesteps    | 7652000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.396   |\n",
            "|    explained_variance | 0.348    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 191299   |\n",
            "|    policy_loss        | 0.354    |\n",
            "|    value_loss         | 708      |\n",
            "------------------------------------\n",
            "Num timesteps: 7656000\n",
            "Best mean reward: 193.30 - Last mean reward per episode: 177.64\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 283      |\n",
            "|    ep_rew_mean        | 178      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1225     |\n",
            "|    iterations         | 191400   |\n",
            "|    time_elapsed       | 6247     |\n",
            "|    total_timesteps    | 7656000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.311   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 191399   |\n",
            "|    policy_loss        | -0.0501  |\n",
            "|    value_loss         | 0.881    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 288      |\n",
            "|    ep_rew_mean        | 178      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1225     |\n",
            "|    iterations         | 191500   |\n",
            "|    time_elapsed       | 6249     |\n",
            "|    total_timesteps    | 7660000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.402   |\n",
            "|    explained_variance | 0.949    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 191499   |\n",
            "|    policy_loss        | -0.265   |\n",
            "|    value_loss         | 23.7     |\n",
            "------------------------------------\n",
            "Num timesteps: 7664000\n",
            "Best mean reward: 193.30 - Last mean reward per episode: 174.64\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 285      |\n",
            "|    ep_rew_mean        | 175      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1225     |\n",
            "|    iterations         | 191600   |\n",
            "|    time_elapsed       | 6251     |\n",
            "|    total_timesteps    | 7664000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.336   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 191599   |\n",
            "|    policy_loss        | 0.0106   |\n",
            "|    value_loss         | 1.42     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 288      |\n",
            "|    ep_rew_mean        | 178      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1226     |\n",
            "|    iterations         | 191700   |\n",
            "|    time_elapsed       | 6253     |\n",
            "|    total_timesteps    | 7668000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.295   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 191699   |\n",
            "|    policy_loss        | 0.0338   |\n",
            "|    value_loss         | 1.79     |\n",
            "------------------------------------\n",
            "Num timesteps: 7672000\n",
            "Best mean reward: 193.30 - Last mean reward per episode: 174.78\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 288      |\n",
            "|    ep_rew_mean        | 175      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1226     |\n",
            "|    iterations         | 191800   |\n",
            "|    time_elapsed       | 6256     |\n",
            "|    total_timesteps    | 7672000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.339   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 191799   |\n",
            "|    policy_loss        | -0.361   |\n",
            "|    value_loss         | 3.06     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 295      |\n",
            "|    ep_rew_mean        | 169      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1226     |\n",
            "|    iterations         | 191900   |\n",
            "|    time_elapsed       | 6258     |\n",
            "|    total_timesteps    | 7676000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.288   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 191899   |\n",
            "|    policy_loss        | 0.00578  |\n",
            "|    value_loss         | 0.718    |\n",
            "------------------------------------\n",
            "Num timesteps: 7680000\n",
            "Best mean reward: 193.30 - Last mean reward per episode: 178.53\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 300      |\n",
            "|    ep_rew_mean        | 179      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1226     |\n",
            "|    iterations         | 192000   |\n",
            "|    time_elapsed       | 6260     |\n",
            "|    total_timesteps    | 7680000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.283   |\n",
            "|    explained_variance | 0.833    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 191999   |\n",
            "|    policy_loss        | 2.21     |\n",
            "|    value_loss         | 43.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 287      |\n",
            "|    ep_rew_mean        | 186      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1226     |\n",
            "|    iterations         | 192100   |\n",
            "|    time_elapsed       | 6262     |\n",
            "|    total_timesteps    | 7684000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.239   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 192099   |\n",
            "|    policy_loss        | -0.0579  |\n",
            "|    value_loss         | 0.469    |\n",
            "------------------------------------\n",
            "Num timesteps: 7688000\n",
            "Best mean reward: 193.30 - Last mean reward per episode: 181.51\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 284      |\n",
            "|    ep_rew_mean        | 182      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1227     |\n",
            "|    iterations         | 192200   |\n",
            "|    time_elapsed       | 6264     |\n",
            "|    total_timesteps    | 7688000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.141   |\n",
            "|    explained_variance | 0.243    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 192199   |\n",
            "|    policy_loss        | -0.0968  |\n",
            "|    value_loss         | 1e+03    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 278      |\n",
            "|    ep_rew_mean        | 177      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1227     |\n",
            "|    iterations         | 192300   |\n",
            "|    time_elapsed       | 6266     |\n",
            "|    total_timesteps    | 7692000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.36    |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 192299   |\n",
            "|    policy_loss        | 0.101    |\n",
            "|    value_loss         | 0.869    |\n",
            "------------------------------------\n",
            "Num timesteps: 7696000\n",
            "Best mean reward: 193.30 - Last mean reward per episode: 179.84\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 274      |\n",
            "|    ep_rew_mean        | 180      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1227     |\n",
            "|    iterations         | 192400   |\n",
            "|    time_elapsed       | 6269     |\n",
            "|    total_timesteps    | 7696000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.392   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 192399   |\n",
            "|    policy_loss        | -0.284   |\n",
            "|    value_loss         | 0.681    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 261      |\n",
            "|    ep_rew_mean        | 179      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1227     |\n",
            "|    iterations         | 192500   |\n",
            "|    time_elapsed       | 6271     |\n",
            "|    total_timesteps    | 7700000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.447   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 192499   |\n",
            "|    policy_loss        | -0.108   |\n",
            "|    value_loss         | 2.33     |\n",
            "------------------------------------\n",
            "Num timesteps: 7704000\n",
            "Best mean reward: 193.30 - Last mean reward per episode: 179.97\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 266      |\n",
            "|    ep_rew_mean        | 180      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1228     |\n",
            "|    iterations         | 192600   |\n",
            "|    time_elapsed       | 6273     |\n",
            "|    total_timesteps    | 7704000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.253   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 192599   |\n",
            "|    policy_loss        | 0.143    |\n",
            "|    value_loss         | 3.7      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 259      |\n",
            "|    ep_rew_mean        | 179      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1228     |\n",
            "|    iterations         | 192700   |\n",
            "|    time_elapsed       | 6275     |\n",
            "|    total_timesteps    | 7708000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.388   |\n",
            "|    explained_variance | 0.591    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 192699   |\n",
            "|    policy_loss        | 1        |\n",
            "|    value_loss         | 352      |\n",
            "------------------------------------\n",
            "Num timesteps: 7712000\n",
            "Best mean reward: 193.30 - Last mean reward per episode: 175.61\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 258      |\n",
            "|    ep_rew_mean        | 176      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1228     |\n",
            "|    iterations         | 192800   |\n",
            "|    time_elapsed       | 6277     |\n",
            "|    total_timesteps    | 7712000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.348   |\n",
            "|    explained_variance | 0.954    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 192799   |\n",
            "|    policy_loss        | 1.35     |\n",
            "|    value_loss         | 38.1     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 258      |\n",
            "|    ep_rew_mean        | 180      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1228     |\n",
            "|    iterations         | 192900   |\n",
            "|    time_elapsed       | 6279     |\n",
            "|    total_timesteps    | 7716000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.277   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 192899   |\n",
            "|    policy_loss        | -0.181   |\n",
            "|    value_loss         | 3.8      |\n",
            "------------------------------------\n",
            "Num timesteps: 7720000\n",
            "Best mean reward: 193.30 - Last mean reward per episode: 183.48\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 271      |\n",
            "|    ep_rew_mean        | 183      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1228     |\n",
            "|    iterations         | 193000   |\n",
            "|    time_elapsed       | 6282     |\n",
            "|    total_timesteps    | 7720000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.58    |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 192999   |\n",
            "|    policy_loss        | 0.0345   |\n",
            "|    value_loss         | 0.761    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 267      |\n",
            "|    ep_rew_mean        | 177      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1229     |\n",
            "|    iterations         | 193100   |\n",
            "|    time_elapsed       | 6284     |\n",
            "|    total_timesteps    | 7724000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.364   |\n",
            "|    explained_variance | 0.958    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 193099   |\n",
            "|    policy_loss        | 1.34     |\n",
            "|    value_loss         | 22.6     |\n",
            "------------------------------------\n",
            "Num timesteps: 7728000\n",
            "Best mean reward: 193.30 - Last mean reward per episode: 180.10\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 266      |\n",
            "|    ep_rew_mean        | 180      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1229     |\n",
            "|    iterations         | 193200   |\n",
            "|    time_elapsed       | 6286     |\n",
            "|    total_timesteps    | 7728000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.443   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 193199   |\n",
            "|    policy_loss        | 0.627    |\n",
            "|    value_loss         | 8.75     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 264      |\n",
            "|    ep_rew_mean        | 179      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1229     |\n",
            "|    iterations         | 193300   |\n",
            "|    time_elapsed       | 6288     |\n",
            "|    total_timesteps    | 7732000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.245   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 193299   |\n",
            "|    policy_loss        | 0.0802   |\n",
            "|    value_loss         | 2.18     |\n",
            "------------------------------------\n",
            "Num timesteps: 7736000\n",
            "Best mean reward: 193.30 - Last mean reward per episode: 169.55\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 262      |\n",
            "|    ep_rew_mean        | 170      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1229     |\n",
            "|    iterations         | 193400   |\n",
            "|    time_elapsed       | 6290     |\n",
            "|    total_timesteps    | 7736000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.297   |\n",
            "|    explained_variance | 0.884    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 193399   |\n",
            "|    policy_loss        | 0.112    |\n",
            "|    value_loss         | 107      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 258      |\n",
            "|    ep_rew_mean        | 165      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1229     |\n",
            "|    iterations         | 193500   |\n",
            "|    time_elapsed       | 6292     |\n",
            "|    total_timesteps    | 7740000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.53    |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 193499   |\n",
            "|    policy_loss        | 0.0319   |\n",
            "|    value_loss         | 0.68     |\n",
            "------------------------------------\n",
            "Num timesteps: 7744000\n",
            "Best mean reward: 193.30 - Last mean reward per episode: 160.50\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 276      |\n",
            "|    ep_rew_mean        | 160      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1230     |\n",
            "|    iterations         | 193600   |\n",
            "|    time_elapsed       | 6295     |\n",
            "|    total_timesteps    | 7744000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.328   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 193599   |\n",
            "|    policy_loss        | 0.237    |\n",
            "|    value_loss         | 4.81     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 269      |\n",
            "|    ep_rew_mean        | 162      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1230     |\n",
            "|    iterations         | 193700   |\n",
            "|    time_elapsed       | 6297     |\n",
            "|    total_timesteps    | 7748000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.335   |\n",
            "|    explained_variance | 0.235    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 193699   |\n",
            "|    policy_loss        | -0.0655  |\n",
            "|    value_loss         | 884      |\n",
            "------------------------------------\n",
            "Num timesteps: 7752000\n",
            "Best mean reward: 193.30 - Last mean reward per episode: 174.71\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 272      |\n",
            "|    ep_rew_mean        | 175      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1230     |\n",
            "|    iterations         | 193800   |\n",
            "|    time_elapsed       | 6299     |\n",
            "|    total_timesteps    | 7752000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.232   |\n",
            "|    explained_variance | 0.809    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 193799   |\n",
            "|    policy_loss        | 0.406    |\n",
            "|    value_loss         | 115      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 270      |\n",
            "|    ep_rew_mean        | 171      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1230     |\n",
            "|    iterations         | 193900   |\n",
            "|    time_elapsed       | 6301     |\n",
            "|    total_timesteps    | 7756000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.31    |\n",
            "|    explained_variance | 0.252    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 193899   |\n",
            "|    policy_loss        | 0.281    |\n",
            "|    value_loss         | 223      |\n",
            "------------------------------------\n",
            "Num timesteps: 7760000\n",
            "Best mean reward: 193.30 - Last mean reward per episode: 175.40\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 275      |\n",
            "|    ep_rew_mean        | 175      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1230     |\n",
            "|    iterations         | 194000   |\n",
            "|    time_elapsed       | 6304     |\n",
            "|    total_timesteps    | 7760000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.302   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 193999   |\n",
            "|    policy_loss        | -0.0373  |\n",
            "|    value_loss         | 0.736    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 290      |\n",
            "|    ep_rew_mean        | 181      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1231     |\n",
            "|    iterations         | 194100   |\n",
            "|    time_elapsed       | 6306     |\n",
            "|    total_timesteps    | 7764000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.176   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 194099   |\n",
            "|    policy_loss        | 0.187    |\n",
            "|    value_loss         | 1.98     |\n",
            "------------------------------------\n",
            "Num timesteps: 7768000\n",
            "Best mean reward: 193.30 - Last mean reward per episode: 190.76\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 300      |\n",
            "|    ep_rew_mean        | 191      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1231     |\n",
            "|    iterations         | 194200   |\n",
            "|    time_elapsed       | 6308     |\n",
            "|    total_timesteps    | 7768000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.356   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 194199   |\n",
            "|    policy_loss        | 0.201    |\n",
            "|    value_loss         | 1.83     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 293      |\n",
            "|    ep_rew_mean        | 195      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1231     |\n",
            "|    iterations         | 194300   |\n",
            "|    time_elapsed       | 6310     |\n",
            "|    total_timesteps    | 7772000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.375   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 194299   |\n",
            "|    policy_loss        | 0.0472   |\n",
            "|    value_loss         | 2.4      |\n",
            "------------------------------------\n",
            "Num timesteps: 7776000\n",
            "Best mean reward: 193.30 - Last mean reward per episode: 192.49\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 276      |\n",
            "|    ep_rew_mean        | 192      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1231     |\n",
            "|    iterations         | 194400   |\n",
            "|    time_elapsed       | 6313     |\n",
            "|    total_timesteps    | 7776000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.213   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 194399   |\n",
            "|    policy_loss        | -0.0117  |\n",
            "|    value_loss         | 0.713    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 278      |\n",
            "|    ep_rew_mean        | 185      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1231     |\n",
            "|    iterations         | 194500   |\n",
            "|    time_elapsed       | 6315     |\n",
            "|    total_timesteps    | 7780000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.316   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 194499   |\n",
            "|    policy_loss        | 0.411    |\n",
            "|    value_loss         | 4.34     |\n",
            "------------------------------------\n",
            "Num timesteps: 7784000\n",
            "Best mean reward: 193.30 - Last mean reward per episode: 195.85\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 276      |\n",
            "|    ep_rew_mean        | 196      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1232     |\n",
            "|    iterations         | 194600   |\n",
            "|    time_elapsed       | 6316     |\n",
            "|    total_timesteps    | 7784000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.413   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 194599   |\n",
            "|    policy_loss        | 0.231    |\n",
            "|    value_loss         | 2.1      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 273      |\n",
            "|    ep_rew_mean        | 195      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1232     |\n",
            "|    iterations         | 194700   |\n",
            "|    time_elapsed       | 6319     |\n",
            "|    total_timesteps    | 7788000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.372   |\n",
            "|    explained_variance | 0.977    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 194699   |\n",
            "|    policy_loss        | -0.545   |\n",
            "|    value_loss         | 26       |\n",
            "------------------------------------\n",
            "Num timesteps: 7792000\n",
            "Best mean reward: 195.85 - Last mean reward per episode: 197.70\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 261      |\n",
            "|    ep_rew_mean        | 198      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1232     |\n",
            "|    iterations         | 194800   |\n",
            "|    time_elapsed       | 6321     |\n",
            "|    total_timesteps    | 7792000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.388   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 194799   |\n",
            "|    policy_loss        | -0.027   |\n",
            "|    value_loss         | 2.23     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 266      |\n",
            "|    ep_rew_mean        | 196      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1232     |\n",
            "|    iterations         | 194900   |\n",
            "|    time_elapsed       | 6323     |\n",
            "|    total_timesteps    | 7796000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.324   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 194899   |\n",
            "|    policy_loss        | 0.0232   |\n",
            "|    value_loss         | 4.87     |\n",
            "------------------------------------\n",
            "Num timesteps: 7800000\n",
            "Best mean reward: 197.70 - Last mean reward per episode: 206.44\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 276      |\n",
            "|    ep_rew_mean        | 206      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1233     |\n",
            "|    iterations         | 195000   |\n",
            "|    time_elapsed       | 6325     |\n",
            "|    total_timesteps    | 7800000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.282   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 194999   |\n",
            "|    policy_loss        | -0.0849  |\n",
            "|    value_loss         | 1.29     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 281      |\n",
            "|    ep_rew_mean        | 206      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1233     |\n",
            "|    iterations         | 195100   |\n",
            "|    time_elapsed       | 6328     |\n",
            "|    total_timesteps    | 7804000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.244   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 195099   |\n",
            "|    policy_loss        | -0.00807 |\n",
            "|    value_loss         | 4.15     |\n",
            "------------------------------------\n",
            "Num timesteps: 7808000\n",
            "Best mean reward: 206.44 - Last mean reward per episode: 212.13\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 293      |\n",
            "|    ep_rew_mean        | 212      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1233     |\n",
            "|    iterations         | 195200   |\n",
            "|    time_elapsed       | 6330     |\n",
            "|    total_timesteps    | 7808000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.465   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 195199   |\n",
            "|    policy_loss        | -0.0978  |\n",
            "|    value_loss         | 1.95     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 301      |\n",
            "|    ep_rew_mean        | 214      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1233     |\n",
            "|    iterations         | 195300   |\n",
            "|    time_elapsed       | 6333     |\n",
            "|    total_timesteps    | 7812000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.362   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 195299   |\n",
            "|    policy_loss        | -0.043   |\n",
            "|    value_loss         | 1.91     |\n",
            "------------------------------------\n",
            "Num timesteps: 7816000\n",
            "Best mean reward: 212.13 - Last mean reward per episode: 203.35\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 324      |\n",
            "|    ep_rew_mean        | 203      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1233     |\n",
            "|    iterations         | 195400   |\n",
            "|    time_elapsed       | 6336     |\n",
            "|    total_timesteps    | 7816000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.367   |\n",
            "|    explained_variance | 0.217    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 195399   |\n",
            "|    policy_loss        | -0.358   |\n",
            "|    value_loss         | 1.67e+03 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 329      |\n",
            "|    ep_rew_mean        | 201      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1233     |\n",
            "|    iterations         | 195500   |\n",
            "|    time_elapsed       | 6339     |\n",
            "|    total_timesteps    | 7820000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.379   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 195499   |\n",
            "|    policy_loss        | -0.195   |\n",
            "|    value_loss         | 1.53     |\n",
            "------------------------------------\n",
            "Num timesteps: 7824000\n",
            "Best mean reward: 212.13 - Last mean reward per episode: 203.59\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 320      |\n",
            "|    ep_rew_mean        | 204      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1233     |\n",
            "|    iterations         | 195600   |\n",
            "|    time_elapsed       | 6341     |\n",
            "|    total_timesteps    | 7824000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.173   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 195599   |\n",
            "|    policy_loss        | 0.0692   |\n",
            "|    value_loss         | 1.76     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 319      |\n",
            "|    ep_rew_mean        | 208      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1234     |\n",
            "|    iterations         | 195700   |\n",
            "|    time_elapsed       | 6343     |\n",
            "|    total_timesteps    | 7828000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.234   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 195699   |\n",
            "|    policy_loss        | -0.0122  |\n",
            "|    value_loss         | 1.62     |\n",
            "------------------------------------\n",
            "Num timesteps: 7832000\n",
            "Best mean reward: 212.13 - Last mean reward per episode: 205.43\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 316      |\n",
            "|    ep_rew_mean        | 205      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1234     |\n",
            "|    iterations         | 195800   |\n",
            "|    time_elapsed       | 6345     |\n",
            "|    total_timesteps    | 7832000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.203   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 195799   |\n",
            "|    policy_loss        | -0.0432  |\n",
            "|    value_loss         | 1.14     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 309      |\n",
            "|    ep_rew_mean        | 210      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1234     |\n",
            "|    iterations         | 195900   |\n",
            "|    time_elapsed       | 6347     |\n",
            "|    total_timesteps    | 7836000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.189   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 195899   |\n",
            "|    policy_loss        | -0.0468  |\n",
            "|    value_loss         | 2.44     |\n",
            "------------------------------------\n",
            "Num timesteps: 7840000\n",
            "Best mean reward: 212.13 - Last mean reward per episode: 211.42\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 300      |\n",
            "|    ep_rew_mean        | 211      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1234     |\n",
            "|    iterations         | 196000   |\n",
            "|    time_elapsed       | 6349     |\n",
            "|    total_timesteps    | 7840000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.434   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 195999   |\n",
            "|    policy_loss        | -0.0452  |\n",
            "|    value_loss         | 0.738    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 280      |\n",
            "|    ep_rew_mean        | 215      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1234     |\n",
            "|    iterations         | 196100   |\n",
            "|    time_elapsed       | 6351     |\n",
            "|    total_timesteps    | 7844000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.245   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 196099   |\n",
            "|    policy_loss        | -0.0505  |\n",
            "|    value_loss         | 1.16     |\n",
            "------------------------------------\n",
            "Num timesteps: 7848000\n",
            "Best mean reward: 212.13 - Last mean reward per episode: 217.06\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 272      |\n",
            "|    ep_rew_mean        | 217      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1235     |\n",
            "|    iterations         | 196200   |\n",
            "|    time_elapsed       | 6354     |\n",
            "|    total_timesteps    | 7848000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.236   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 196199   |\n",
            "|    policy_loss        | -0.143   |\n",
            "|    value_loss         | 1.54     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 277      |\n",
            "|    ep_rew_mean        | 220      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1235     |\n",
            "|    iterations         | 196300   |\n",
            "|    time_elapsed       | 6356     |\n",
            "|    total_timesteps    | 7852000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.444   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 196299   |\n",
            "|    policy_loss        | 0.725    |\n",
            "|    value_loss         | 3.5      |\n",
            "------------------------------------\n",
            "Num timesteps: 7856000\n",
            "Best mean reward: 217.06 - Last mean reward per episode: 224.07\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 293      |\n",
            "|    ep_rew_mean        | 224      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1235     |\n",
            "|    iterations         | 196400   |\n",
            "|    time_elapsed       | 6358     |\n",
            "|    total_timesteps    | 7856000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.337   |\n",
            "|    explained_variance | 0.952    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 196399   |\n",
            "|    policy_loss        | 0.123    |\n",
            "|    value_loss         | 31.2     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 288      |\n",
            "|    ep_rew_mean        | 221      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1235     |\n",
            "|    iterations         | 196500   |\n",
            "|    time_elapsed       | 6361     |\n",
            "|    total_timesteps    | 7860000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.249   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 196499   |\n",
            "|    policy_loss        | 0.015    |\n",
            "|    value_loss         | 1.05     |\n",
            "------------------------------------\n",
            "Num timesteps: 7864000\n",
            "Best mean reward: 224.07 - Last mean reward per episode: 205.64\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 284      |\n",
            "|    ep_rew_mean        | 206      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1235     |\n",
            "|    iterations         | 196600   |\n",
            "|    time_elapsed       | 6363     |\n",
            "|    total_timesteps    | 7864000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.317   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 196599   |\n",
            "|    policy_loss        | -0.0947  |\n",
            "|    value_loss         | 1.54     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 280      |\n",
            "|    ep_rew_mean        | 197      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1236     |\n",
            "|    iterations         | 196700   |\n",
            "|    time_elapsed       | 6365     |\n",
            "|    total_timesteps    | 7868000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.246   |\n",
            "|    explained_variance | 0.317    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 196699   |\n",
            "|    policy_loss        | 0.39     |\n",
            "|    value_loss         | 492      |\n",
            "------------------------------------\n",
            "Num timesteps: 7872000\n",
            "Best mean reward: 224.07 - Last mean reward per episode: 201.85\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 273      |\n",
            "|    ep_rew_mean        | 202      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1236     |\n",
            "|    iterations         | 196800   |\n",
            "|    time_elapsed       | 6367     |\n",
            "|    total_timesteps    | 7872000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.352   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 196799   |\n",
            "|    policy_loss        | -0.0808  |\n",
            "|    value_loss         | 1.72     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 280      |\n",
            "|    ep_rew_mean        | 214      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1236     |\n",
            "|    iterations         | 196900   |\n",
            "|    time_elapsed       | 6369     |\n",
            "|    total_timesteps    | 7876000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.259   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 196899   |\n",
            "|    policy_loss        | -0.244   |\n",
            "|    value_loss         | 2.26     |\n",
            "------------------------------------\n",
            "Num timesteps: 7880000\n",
            "Best mean reward: 224.07 - Last mean reward per episode: 212.67\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 266      |\n",
            "|    ep_rew_mean        | 213      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1236     |\n",
            "|    iterations         | 197000   |\n",
            "|    time_elapsed       | 6371     |\n",
            "|    total_timesteps    | 7880000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.138   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 196999   |\n",
            "|    policy_loss        | -0.0605  |\n",
            "|    value_loss         | 5.1      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 252      |\n",
            "|    ep_rew_mean        | 203      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1237     |\n",
            "|    iterations         | 197100   |\n",
            "|    time_elapsed       | 6373     |\n",
            "|    total_timesteps    | 7884000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.222   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 197099   |\n",
            "|    policy_loss        | -0.103   |\n",
            "|    value_loss         | 2.59     |\n",
            "------------------------------------\n",
            "Num timesteps: 7888000\n",
            "Best mean reward: 224.07 - Last mean reward per episode: 211.58\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 250      |\n",
            "|    ep_rew_mean        | 212      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1237     |\n",
            "|    iterations         | 197200   |\n",
            "|    time_elapsed       | 6375     |\n",
            "|    total_timesteps    | 7888000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.278   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 197199   |\n",
            "|    policy_loss        | -0.0177  |\n",
            "|    value_loss         | 1.72     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 261      |\n",
            "|    ep_rew_mean        | 217      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1237     |\n",
            "|    iterations         | 197300   |\n",
            "|    time_elapsed       | 6377     |\n",
            "|    total_timesteps    | 7892000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.535   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 197299   |\n",
            "|    policy_loss        | -0.0666  |\n",
            "|    value_loss         | 1.6      |\n",
            "------------------------------------\n",
            "Num timesteps: 7896000\n",
            "Best mean reward: 224.07 - Last mean reward per episode: 211.30\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 270      |\n",
            "|    ep_rew_mean        | 211      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1237     |\n",
            "|    iterations         | 197400   |\n",
            "|    time_elapsed       | 6380     |\n",
            "|    total_timesteps    | 7896000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.268   |\n",
            "|    explained_variance | 0.693    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 197399   |\n",
            "|    policy_loss        | 0.111    |\n",
            "|    value_loss         | 373      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 266      |\n",
            "|    ep_rew_mean        | 212      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1237     |\n",
            "|    iterations         | 197500   |\n",
            "|    time_elapsed       | 6382     |\n",
            "|    total_timesteps    | 7900000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.189   |\n",
            "|    explained_variance | 0.831    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 197499   |\n",
            "|    policy_loss        | 0.338    |\n",
            "|    value_loss         | 119      |\n",
            "------------------------------------\n",
            "Num timesteps: 7904000\n",
            "Best mean reward: 224.07 - Last mean reward per episode: 200.37\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 266      |\n",
            "|    ep_rew_mean        | 200      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1238     |\n",
            "|    iterations         | 197600   |\n",
            "|    time_elapsed       | 6384     |\n",
            "|    total_timesteps    | 7904000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.357   |\n",
            "|    explained_variance | 0.936    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 197599   |\n",
            "|    policy_loss        | 0.037    |\n",
            "|    value_loss         | 28       |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 263      |\n",
            "|    ep_rew_mean        | 208      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1238     |\n",
            "|    iterations         | 197700   |\n",
            "|    time_elapsed       | 6386     |\n",
            "|    total_timesteps    | 7908000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.41    |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 197699   |\n",
            "|    policy_loss        | -0.299   |\n",
            "|    value_loss         | 3.38     |\n",
            "------------------------------------\n",
            "Num timesteps: 7912000\n",
            "Best mean reward: 224.07 - Last mean reward per episode: 219.01\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 274      |\n",
            "|    ep_rew_mean        | 219      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1238     |\n",
            "|    iterations         | 197800   |\n",
            "|    time_elapsed       | 6388     |\n",
            "|    total_timesteps    | 7912000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.228   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 197799   |\n",
            "|    policy_loss        | 0.0107   |\n",
            "|    value_loss         | 0.656    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 272      |\n",
            "|    ep_rew_mean        | 215      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1238     |\n",
            "|    iterations         | 197900   |\n",
            "|    time_elapsed       | 6390     |\n",
            "|    total_timesteps    | 7916000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.318   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 197899   |\n",
            "|    policy_loss        | 0.23     |\n",
            "|    value_loss         | 0.649    |\n",
            "------------------------------------\n",
            "Num timesteps: 7920000\n",
            "Best mean reward: 224.07 - Last mean reward per episode: 211.77\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 264      |\n",
            "|    ep_rew_mean        | 212      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1238     |\n",
            "|    iterations         | 198000   |\n",
            "|    time_elapsed       | 6392     |\n",
            "|    total_timesteps    | 7920000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.384   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 197999   |\n",
            "|    policy_loss        | -0.121   |\n",
            "|    value_loss         | 1.62     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 261      |\n",
            "|    ep_rew_mean        | 214      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1239     |\n",
            "|    iterations         | 198100   |\n",
            "|    time_elapsed       | 6395     |\n",
            "|    total_timesteps    | 7924000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.413   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 198099   |\n",
            "|    policy_loss        | -0.801   |\n",
            "|    value_loss         | 8.84     |\n",
            "------------------------------------\n",
            "Num timesteps: 7928000\n",
            "Best mean reward: 224.07 - Last mean reward per episode: 212.96\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 266      |\n",
            "|    ep_rew_mean        | 213      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1239     |\n",
            "|    iterations         | 198200   |\n",
            "|    time_elapsed       | 6397     |\n",
            "|    total_timesteps    | 7928000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.295   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 198199   |\n",
            "|    policy_loss        | -0.0274  |\n",
            "|    value_loss         | 1.25     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 273      |\n",
            "|    ep_rew_mean        | 217      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1239     |\n",
            "|    iterations         | 198300   |\n",
            "|    time_elapsed       | 6399     |\n",
            "|    total_timesteps    | 7932000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.42    |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 198299   |\n",
            "|    policy_loss        | -0.0929  |\n",
            "|    value_loss         | 0.581    |\n",
            "------------------------------------\n",
            "Num timesteps: 7936000\n",
            "Best mean reward: 224.07 - Last mean reward per episode: 215.44\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 282      |\n",
            "|    ep_rew_mean        | 215      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1239     |\n",
            "|    iterations         | 198400   |\n",
            "|    time_elapsed       | 6402     |\n",
            "|    total_timesteps    | 7936000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.368   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 198399   |\n",
            "|    policy_loss        | -0.119   |\n",
            "|    value_loss         | 1.08     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 290      |\n",
            "|    ep_rew_mean        | 207      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1239     |\n",
            "|    iterations         | 198500   |\n",
            "|    time_elapsed       | 6404     |\n",
            "|    total_timesteps    | 7940000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.349   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 198499   |\n",
            "|    policy_loss        | -0.122   |\n",
            "|    value_loss         | 6.55     |\n",
            "------------------------------------\n",
            "Num timesteps: 7944000\n",
            "Best mean reward: 224.07 - Last mean reward per episode: 204.61\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 294      |\n",
            "|    ep_rew_mean        | 205      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1239     |\n",
            "|    iterations         | 198600   |\n",
            "|    time_elapsed       | 6406     |\n",
            "|    total_timesteps    | 7944000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.277   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 198599   |\n",
            "|    policy_loss        | 0.00809  |\n",
            "|    value_loss         | 2.93     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 290      |\n",
            "|    ep_rew_mean        | 201      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1240     |\n",
            "|    iterations         | 198700   |\n",
            "|    time_elapsed       | 6408     |\n",
            "|    total_timesteps    | 7948000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.252   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 198699   |\n",
            "|    policy_loss        | -0.0733  |\n",
            "|    value_loss         | 6.66     |\n",
            "------------------------------------\n",
            "Num timesteps: 7952000\n",
            "Best mean reward: 224.07 - Last mean reward per episode: 199.40\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 290      |\n",
            "|    ep_rew_mean        | 199      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1240     |\n",
            "|    iterations         | 198800   |\n",
            "|    time_elapsed       | 6410     |\n",
            "|    total_timesteps    | 7952000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.213   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 198799   |\n",
            "|    policy_loss        | -0.0567  |\n",
            "|    value_loss         | 0.952    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 281      |\n",
            "|    ep_rew_mean        | 191      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1240     |\n",
            "|    iterations         | 198900   |\n",
            "|    time_elapsed       | 6413     |\n",
            "|    total_timesteps    | 7956000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.343   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 198899   |\n",
            "|    policy_loss        | 0.00897  |\n",
            "|    value_loss         | 0.908    |\n",
            "------------------------------------\n",
            "Num timesteps: 7960000\n",
            "Best mean reward: 224.07 - Last mean reward per episode: 193.54\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 288      |\n",
            "|    ep_rew_mean        | 194      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1240     |\n",
            "|    iterations         | 199000   |\n",
            "|    time_elapsed       | 6415     |\n",
            "|    total_timesteps    | 7960000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.287   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 198999   |\n",
            "|    policy_loss        | 0.126    |\n",
            "|    value_loss         | 0.649    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 299      |\n",
            "|    ep_rew_mean        | 188      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1240     |\n",
            "|    iterations         | 199100   |\n",
            "|    time_elapsed       | 6418     |\n",
            "|    total_timesteps    | 7964000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.25    |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 199099   |\n",
            "|    policy_loss        | -0.00778 |\n",
            "|    value_loss         | 0.891    |\n",
            "------------------------------------\n",
            "Num timesteps: 7968000\n",
            "Best mean reward: 224.07 - Last mean reward per episode: 187.38\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 289      |\n",
            "|    ep_rew_mean        | 187      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1241     |\n",
            "|    iterations         | 199200   |\n",
            "|    time_elapsed       | 6420     |\n",
            "|    total_timesteps    | 7968000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.127   |\n",
            "|    explained_variance | 0.216    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 199199   |\n",
            "|    policy_loss        | -0.122   |\n",
            "|    value_loss         | 899      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 286      |\n",
            "|    ep_rew_mean        | 176      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1241     |\n",
            "|    iterations         | 199300   |\n",
            "|    time_elapsed       | 6422     |\n",
            "|    total_timesteps    | 7972000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.324   |\n",
            "|    explained_variance | 0.638    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 199299   |\n",
            "|    policy_loss        | 0.145    |\n",
            "|    value_loss         | 353      |\n",
            "------------------------------------\n",
            "Num timesteps: 7976000\n",
            "Best mean reward: 224.07 - Last mean reward per episode: 175.00\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 281      |\n",
            "|    ep_rew_mean        | 175      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1241     |\n",
            "|    iterations         | 199400   |\n",
            "|    time_elapsed       | 6424     |\n",
            "|    total_timesteps    | 7976000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.249   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 199399   |\n",
            "|    policy_loss        | -0.053   |\n",
            "|    value_loss         | 2.11     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 277      |\n",
            "|    ep_rew_mean        | 171      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1241     |\n",
            "|    iterations         | 199500   |\n",
            "|    time_elapsed       | 6426     |\n",
            "|    total_timesteps    | 7980000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.154   |\n",
            "|    explained_variance | 0.954    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 199499   |\n",
            "|    policy_loss        | -0.351   |\n",
            "|    value_loss         | 5.32     |\n",
            "------------------------------------\n",
            "Num timesteps: 7984000\n",
            "Best mean reward: 224.07 - Last mean reward per episode: 165.78\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 270      |\n",
            "|    ep_rew_mean        | 166      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1242     |\n",
            "|    iterations         | 199600   |\n",
            "|    time_elapsed       | 6428     |\n",
            "|    total_timesteps    | 7984000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.187   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 199599   |\n",
            "|    policy_loss        | -0.074   |\n",
            "|    value_loss         | 1.45     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 263      |\n",
            "|    ep_rew_mean        | 161      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1242     |\n",
            "|    iterations         | 199700   |\n",
            "|    time_elapsed       | 6430     |\n",
            "|    total_timesteps    | 7988000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.433   |\n",
            "|    explained_variance | 0.941    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 199699   |\n",
            "|    policy_loss        | -0.238   |\n",
            "|    value_loss         | 147      |\n",
            "------------------------------------\n",
            "Num timesteps: 7992000\n",
            "Best mean reward: 224.07 - Last mean reward per episode: 161.72\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 265      |\n",
            "|    ep_rew_mean        | 162      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1242     |\n",
            "|    iterations         | 199800   |\n",
            "|    time_elapsed       | 6433     |\n",
            "|    total_timesteps    | 7992000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.263   |\n",
            "|    explained_variance | 0.956    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 199799   |\n",
            "|    policy_loss        | 0.894    |\n",
            "|    value_loss         | 19       |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 269      |\n",
            "|    ep_rew_mean        | 168      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1242     |\n",
            "|    iterations         | 199900   |\n",
            "|    time_elapsed       | 6435     |\n",
            "|    total_timesteps    | 7996000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.249   |\n",
            "|    explained_variance | 0.319    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 199899   |\n",
            "|    policy_loss        | -7.48    |\n",
            "|    value_loss         | 1.6e+03  |\n",
            "------------------------------------\n",
            "Num timesteps: 8000000\n",
            "Best mean reward: 224.07 - Last mean reward per episode: 165.06\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 265      |\n",
            "|    ep_rew_mean        | 165      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1242     |\n",
            "|    iterations         | 200000   |\n",
            "|    time_elapsed       | 6437     |\n",
            "|    total_timesteps    | 8000000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.305   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 199999   |\n",
            "|    policy_loss        | 0.0988   |\n",
            "|    value_loss         | 1.28     |\n",
            "------------------------------------\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<stable_baselines3.a2c.a2c.A2C at 0x7f69c5b02730>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_from_file = False\n",
        "# Hyperparameters are from RL_Zoo\n",
        "# https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/a2c.yml\n",
        "policy = \"MlpPolicy\"\n",
        "n_steps = 5\n",
        "learning_rate = 0.00083\n",
        "# batch_size = 256\n",
        "# n_epochs = 10\n",
        "n_envs = 8\n",
        "n_timesteps = 8e6\n",
        "gamma = 0.995\n",
        "ent_coef = 0.00001\n",
        "\n",
        "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=\"log_dir_A2C/\")\n",
        "\n",
        "# env\n",
        "env = make_vec_env(\"LunarLander-v2\", n_envs=n_envs, monitor_dir=\"log_dir_A2C/\")\n",
        "\n",
        "# instantiate the agent\n",
        "if train_from_file:\n",
        "  model = A2C.load(path=\"log_dir_A2C/best_model.zip\", env=env)\n",
        "else:\n",
        "  model = A2C(policy, env, learning_rate = learning_rate, n_steps = n_steps, ent_coef= ent_coef, tensorboard_log=\"./TensorBoardLog/\", verbose=1)\n",
        "\n",
        "# train the agent\n",
        "model.learn(total_timesteps=n_timesteps, callback=callback)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b1dc8fa",
      "metadata": {
        "id": "5b1dc8fa"
      },
      "source": [
        "# Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "366b80a8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "366b80a8",
        "outputId": "417c672c-1a65-4a71-f550-d10a5c1626ae"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def moving_average(values, window):\n",
        "    \"\"\"\n",
        "    Smooth values by doing a moving average\n",
        "    :param values: (numpy array)\n",
        "    :param window: (int)\n",
        "    :return: (numpy array)\n",
        "    \"\"\"\n",
        "    weights = np.repeat(1.0, window) / window\n",
        "    return np.convolve(values, weights, 'valid')\n",
        "\n",
        "def plot_results(log_folder, title='Learning Curve'):\n",
        "    \"\"\"\n",
        "    plot the results\n",
        "\n",
        "    :param log_folder: (str) the save location of the results to plot\n",
        "    :param title: (str) the title of the task to plot\n",
        "    \"\"\"\n",
        "\n",
        "    x, y = ts2xy(load_results(log_folder), 'timesteps')\n",
        "    y = moving_average(y, window=100)\n",
        "    # Truncate x\n",
        "    x = x[len(x) - len(y):]\n",
        "    fig = plt.figure(title, figsize=(12,5))\n",
        "    plt.plot(x, y)\n",
        "    plt.xlabel('Number of Timesteps')\n",
        "    plt.ylabel('Rewards')\n",
        "    # plt.title(title + \" Smoothed A2C after 8000,000 Timesteps\")\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "plot_results(\"log_dir_A2C\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b00f2a81",
      "metadata": {
        "id": "b00f2a81"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "815393a0",
      "metadata": {
        "id": "815393a0"
      },
      "outputs": [],
      "source": [
        "env = make_vec_env(\"LunarLander-v2\", n_envs=1,monitor_dir=\"log_dir_A2C\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "63611e6e",
      "metadata": {
        "id": "63611e6e"
      },
      "outputs": [],
      "source": [
        "model = A2C.load(path=\"log_dir_A2C/best_model.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b06e1a3",
      "metadata": {
        "id": "3b06e1a3"
      },
      "source": [
        "#### Stable Baseline 3 Evaluation Function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "9d4fd326",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d4fd326",
        "outputId": "e75bd790-909d-4ea8-d242-83632d84147e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean & Std Reward after 10 max run is 215.35980949999998 & 87.08629193000739\n"
          ]
        }
      ],
      "source": [
        "mean_reward, std_reward = evaluate_policy(model, env,n_eval_episodes=10, render=True, deterministic=True)\n",
        "print(\"Mean & Std Reward after {} max run is {} & {}\".format(10,mean_reward, std_reward))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e49c5168",
      "metadata": {
        "id": "e49c5168"
      },
      "source": [
        "# GIF of a Train Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "60cc63dc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60cc63dc",
        "outputId": "9a42d8c7-7a7c-49af-a020-b0d647d5bbfe"
      },
      "outputs": [],
      "source": [
        "env = make_vec_env(\"LunarLander-v2\", n_envs=1)\n",
        "model = A2C.load(path=\"log_dir_A2C/best_model.zip\")\n",
        "\n",
        "images = []\n",
        "obs = env.reset()\n",
        "img = env.render(mode=\"rgb_array\")\n",
        "for i in range(1000):\n",
        "    images.append(img)\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, _, _ ,_ = env.step(action)\n",
        "    img = env.render(mode=\"rgb_array\")\n",
        "\n",
        "imageio.mimsave(\"lunar lander_A2C.gif\", [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3afc060b",
      "metadata": {
        "id": "3afc060b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "857970f990130bbcaee778cf1846f7875676d945310dca1379fe4b5ef3d258a5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
