{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "57601915",
      "metadata": {
        "id": "57601915"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# !pip install shap\n",
        "# !pip install opencv-python\n",
        "# !pip install swig\n",
        "# !pip install Box2D\n",
        "\n",
        "\n",
        "# # !pip install box2d pygame\n",
        "\n",
        "\n",
        "# !pip install gym\n",
        "# !pip install pyglet==1.5.27\n",
        "# !pip install stable-baseline3\n",
        "# !pip install \"gymnasium[all]\"\n",
        "\n",
        "# !pip install stable_baselines3\n",
        "\n",
        "## FOR LOCAL Jupyter notebook\n",
        "# !pip install tensorflow\n",
        "# !pip install torch\n",
        "# !pip install pygame\n",
        "# !pip install tensorboard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "b00a128f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b00a128f",
        "outputId": "f0a196be-2f8b-4fb1-a62f-bfdd10254bae"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import imageio\n",
        "import os\n",
        "from stable_baselines3 import PPO, A2C\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack, SubprocVecEnv, VecNormalize\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common import results_plotter\n",
        "import gymnasium  as gym\n",
        "import matplotlib.pyplot as plt\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "import scipy.stats as stats\n",
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "import tensorflow as tf\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "YtZN-eC7NwuS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtZN-eC7NwuS",
        "outputId": "2fc6167f-8579-442f-f41c-6cae6b8abde7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n",
            "GPU not found. Please ensure that GPU is enabled in Colab.\n",
            "Num GPUs Available:  0\n"
          ]
        }
      ],
      "source": [
        "# seeds\n",
        "# Set seed for numpy\n",
        "np.random.seed(100)\n",
        "\n",
        "# Set seed for Python random module\n",
        "import random\n",
        "random.seed(100)\n",
        "\n",
        "# Set seed for TensorFlow\n",
        "tf.random.set_seed(100)\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "if tf.test.gpu_device_name():\n",
        "    print('Default GPU Device:', tf.test.gpu_device_name())\n",
        "else:\n",
        "    print(\"GPU not found. Please ensure that GPU is enabled in Colab.\")\n",
        "    \n",
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "780afb92",
      "metadata": {
        "id": "780afb92"
      },
      "source": [
        "<h1> Important Libraries To Install </h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2826cd85",
      "metadata": {
        "id": "2826cd85"
      },
      "source": [
        "<h1> Parameter & Environment Information </h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87ef75ca",
      "metadata": {
        "id": "87ef75ca"
      },
      "source": [
        "<p>\n",
        "    This environment is part of the Box2D environments.\n",
        "</p>\n",
        "\n",
        "<ul>\n",
        "    <li> Action Space Discrete(4) </li>\n",
        "    <li> Observation Shape (8,) </li>\n",
        "    <li> Observation High [1.5 1.5 5. 5. 3.14 5. 1. 1. ] </li>\n",
        "    <li> Observation Low [-1.5 -1.5 -5. -5. -3.14 -5. -0. -0. ] </li>\n",
        "    <li> Import gymnasium.make(\"LunarLander-v2\") </li>\n",
        "</ul>\n",
        "\n",
        "<h3> Description </h3>\n",
        "<p>This environment is a classic rocket trajectory optimization problem. According to Pontryagin’s maximum principle, it is optimal to fire the engine at full throttle or turn it off. This is the reason why this environment has discrete actions: engine on or off.\n",
        "\n",
        "There are two environment versions: discrete or continuous. The landing pad is always at coordinates (0,0). The coordinates are the first two numbers in the state vector. Landing outside of the landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt.</p>\n",
        "\n",
        "<h3> Action Space </h3>\n",
        "<p>\n",
        "There are four discrete actions available:\n",
        "\n",
        "* 0: do nothing\n",
        "* 1: fire left orientation engine\n",
        "* 2: fire main engine\n",
        "* 3: fire right orientation engine\n",
        "\n",
        "</p>\n",
        "\n",
        "<h3> Observation Space </h3>\n",
        "<p>\n",
        "The state is an 8-dimensional vector: the coordinates of the lander in x & y, its linear velocities in x & y, its angle, its angular velocity, and two booleans that represent whether each leg is in contact with the ground or not.\n",
        "</p>\n",
        "\n",
        "<h3> Reward </h3>\n",
        "<p>\n",
        "After every step a reward is granted. The total reward of an episode is the sum of the rewards for all the steps within that episode.\n",
        "\n",
        "For each step, the reward:\n",
        "\n",
        "* is increased/decreased the closer/further the lander is to the landing pad.\n",
        "* is increased/decreased the slower/faster the lander is moving.\n",
        "* is decreased the more the lander is tilted (angle not horizontal).\n",
        "* is increased by 10 points for each leg that is in contact with the ground.\n",
        "* is decreased by 0.03 points each frame a side engine is firing.\n",
        "* is decreased by 0.3 points each frame the main engine is firing.\n",
        "\n",
        "The episode receive an additional reward of -100 or +100 points for crashing or landing safely respectively.\n",
        "\n",
        "An episode is considered a solution if it scores at least 200 points.\n",
        "</p>\n",
        "\n",
        "<h3> Starting State </h3>\n",
        "\n",
        "<p>The lander starts at the top center of the viewport with a random initial force applied to its center of mass.</p>\n",
        "\n",
        "<h3> Episode Termination </h3>\n",
        "<p> The episode finishes if:<br>\n",
        "    \n",
        "1. the lander crashes (the lander body gets in contact with the moon);<br>\n",
        "2. the lander gets outside of the viewport (x coordinate is greater than 1);<br>\n",
        "3. the lander is not awake. From the Box2D docs, a body which is not awake is a body which doesn’t move and doesn’t collide with any other body:<br>\n",
        "\n",
        "When Box2D determines that a body (or group of bodies) has come to rest, the body enters a sleep state which has very little CPU overhead. If a body is awake and collides with a sleeping body, then the sleeping body wakes up. Bodies will also wake up if a joint or contact attached to them is destroyed.\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "3d0543a5",
      "metadata": {
        "id": "3d0543a5",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "b3fb45b2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3fb45b2",
        "outputId": "2416218d-5bf4-48aa-fd24-8899b2af453e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Action inter is descrete 4\n",
            "Shape of Observation is (8,)\n"
          ]
        }
      ],
      "source": [
        "print(\"The Action inter is descrete {}\".format(env.action_space.n))\n",
        "print(\"Shape of Observation is {}\".format(env.observation_space.sample().shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b9ff9c9",
      "metadata": {
        "id": "8b9ff9c9"
      },
      "source": [
        "<h1> Baseline Model. </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "d35101af",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d35101af",
        "outputId": "4873fc86-1916-4745-d40f-732c7dca2d00",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Reward after 10 max run is -1.3516385756908398\n"
          ]
        }
      ],
      "source": [
        "rewards = []\n",
        "obs = env.reset()\n",
        "done = False\n",
        "MAX_RUN = 10\n",
        "\n",
        "for i in range(MAX_RUN):\n",
        "    while not done:\n",
        "        env.render()\n",
        "        action_sample = env.action_space.sample()\n",
        "        # let's take a step in the environment\n",
        "        obs, rwd, done, info ,_  = env.step(action_sample)\n",
        "        rewards.append(rwd)\n",
        "env.close()\n",
        "print(\"Mean Reward after {} max run is {}\".format(MAX_RUN, np.mean(np.array(rewards))))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac737551",
      "metadata": {
        "id": "ac737551"
      },
      "source": [
        "<h1> Reinforcement Learning For Training The Model </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "bdaa0e55",
      "metadata": {
        "id": "bdaa0e55"
      },
      "outputs": [],
      "source": [
        "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
        "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
        "\n",
        "    :param check_freq: (int)\n",
        "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
        "      It must contains the file created by the ``Monitor`` wrapper.\n",
        "    :param verbose: (int)\n",
        "    \"\"\"\n",
        "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
        "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
        "        self.check_freq = check_freq\n",
        "        self.log_dir = log_dir\n",
        "        self.save_path = os.path.join(log_dir, 'best_model')\n",
        "        self.best_mean_reward = -np.inf\n",
        "\n",
        "    def _init_callback(self) -> None:\n",
        "        # Create folder if needed\n",
        "        if self.save_path is not None:\n",
        "            os.makedirs(self.save_path, exist_ok=True)\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.n_calls % self.check_freq == 0:\n",
        "\n",
        "          # Retrieve training reward\n",
        "          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
        "          if len(x) > 0:\n",
        "              # Mean training reward over the last 100 episodes\n",
        "              mean_reward = np.mean(y[-100:])\n",
        "              if self.verbose > 0:\n",
        "                print(f\"Num timesteps: {self.num_timesteps}\")\n",
        "                print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n",
        "\n",
        "              # New best model, you could save the agent here\n",
        "              if mean_reward > self.best_mean_reward:\n",
        "                  self.best_mean_reward = mean_reward\n",
        "                  # Example for saving best model\n",
        "                  if self.verbose > 0:\n",
        "                    print(f\"Saving new best model to {self.save_path}.zip\")\n",
        "                  self.model.save(self.save_path)\n",
        "\n",
        "        return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "2d311d6a",
      "metadata": {
        "id": "2d311d6a",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "Logging to ./TensorBoardLog/A2C_9\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 110      |\n",
            "|    ep_rew_mean        | -171     |\n",
            "| time/                 |          |\n",
            "|    fps                | 2677     |\n",
            "|    iterations         | 100      |\n",
            "|    time_elapsed       | 1        |\n",
            "|    total_timesteps    | 4000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.15    |\n",
            "|    explained_variance | 0.256    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 99       |\n",
            "|    policy_loss        | 3.66     |\n",
            "|    value_loss         | 100      |\n",
            "------------------------------------\n",
            "Num timesteps: 8000\n",
            "Best mean reward: -inf - Last mean reward per episode: -197.74\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 143      |\n",
            "|    ep_rew_mean        | -198     |\n",
            "| time/                 |          |\n",
            "|    fps                | 2272     |\n",
            "|    iterations         | 200      |\n",
            "|    time_elapsed       | 3        |\n",
            "|    total_timesteps    | 8000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.12    |\n",
            "|    explained_variance | -0.0475  |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 199      |\n",
            "|    policy_loss        | -7.34    |\n",
            "|    value_loss         | 99.5     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        | 172       |\n",
            "|    ep_rew_mean        | -184      |\n",
            "| time/                 |           |\n",
            "|    fps                | 2157      |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 12000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.927    |\n",
            "|    explained_variance | -0.000948 |\n",
            "|    learning_rate      | 0.00083   |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | -1.3      |\n",
            "|    value_loss         | 36.5      |\n",
            "-------------------------------------\n",
            "Num timesteps: 16000\n",
            "Best mean reward: -197.74 - Last mean reward per episode: -171.27\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 187      |\n",
            "|    ep_rew_mean        | -171     |\n",
            "| time/                 |          |\n",
            "|    fps                | 2104     |\n",
            "|    iterations         | 400      |\n",
            "|    time_elapsed       | 7        |\n",
            "|    total_timesteps    | 16000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.835   |\n",
            "|    explained_variance | 0.594    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 399      |\n",
            "|    policy_loss        | -0.368   |\n",
            "|    value_loss         | 13.6     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 191      |\n",
            "|    ep_rew_mean        | -168     |\n",
            "| time/                 |          |\n",
            "|    fps                | 2032     |\n",
            "|    iterations         | 500      |\n",
            "|    time_elapsed       | 9        |\n",
            "|    total_timesteps    | 20000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.891   |\n",
            "|    explained_variance | 0.184    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 499      |\n",
            "|    policy_loss        | 2.95     |\n",
            "|    value_loss         | 104      |\n",
            "------------------------------------\n",
            "Num timesteps: 24000\n",
            "Best mean reward: -171.27 - Last mean reward per episode: -157.19\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 217      |\n",
            "|    ep_rew_mean        | -157     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1943     |\n",
            "|    iterations         | 600      |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 24000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.709   |\n",
            "|    explained_variance | 0.00173  |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 599      |\n",
            "|    policy_loss        | 0.836    |\n",
            "|    value_loss         | 1.11e+03 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 239      |\n",
            "|    ep_rew_mean        | -153     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1911     |\n",
            "|    iterations         | 700      |\n",
            "|    time_elapsed       | 14       |\n",
            "|    total_timesteps    | 28000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.575   |\n",
            "|    explained_variance | 0.761    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 699      |\n",
            "|    policy_loss        | 2.34     |\n",
            "|    value_loss         | 36.9     |\n",
            "------------------------------------\n",
            "Num timesteps: 32000\n",
            "Best mean reward: -157.19 - Last mean reward per episode: -133.91\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 249      |\n",
            "|    ep_rew_mean        | -134     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1852     |\n",
            "|    iterations         | 800      |\n",
            "|    time_elapsed       | 17       |\n",
            "|    total_timesteps    | 32000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.816   |\n",
            "|    explained_variance | 0.903    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 799      |\n",
            "|    policy_loss        | 0.101    |\n",
            "|    value_loss         | 43.7     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 255      |\n",
            "|    ep_rew_mean        | -122     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1868     |\n",
            "|    iterations         | 900      |\n",
            "|    time_elapsed       | 19       |\n",
            "|    total_timesteps    | 36000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.629   |\n",
            "|    explained_variance | 0.358    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 899      |\n",
            "|    policy_loss        | -2.51    |\n",
            "|    value_loss         | 643      |\n",
            "------------------------------------\n",
            "Num timesteps: 40000\n",
            "Best mean reward: -133.91 - Last mean reward per episode: -114.64\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 248      |\n",
            "|    ep_rew_mean        | -115     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1891     |\n",
            "|    iterations         | 1000     |\n",
            "|    time_elapsed       | 21       |\n",
            "|    total_timesteps    | 40000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.733   |\n",
            "|    explained_variance | 0.915    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 999      |\n",
            "|    policy_loss        | 2.19     |\n",
            "|    value_loss         | 44.1     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 254      |\n",
            "|    ep_rew_mean        | -112     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1832     |\n",
            "|    iterations         | 1100     |\n",
            "|    time_elapsed       | 24       |\n",
            "|    total_timesteps    | 44000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.624   |\n",
            "|    explained_variance | 0.648    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 1099     |\n",
            "|    policy_loss        | -2.79    |\n",
            "|    value_loss         | 42.4     |\n",
            "------------------------------------\n",
            "Num timesteps: 48000\n",
            "Best mean reward: -114.64 - Last mean reward per episode: -99.89\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 267      |\n",
            "|    ep_rew_mean        | -99.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1817     |\n",
            "|    iterations         | 1200     |\n",
            "|    time_elapsed       | 26       |\n",
            "|    total_timesteps    | 48000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.727   |\n",
            "|    explained_variance | 0.907    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 1199     |\n",
            "|    policy_loss        | -2.57    |\n",
            "|    value_loss         | 51.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 265      |\n",
            "|    ep_rew_mean        | -107     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1813     |\n",
            "|    iterations         | 1300     |\n",
            "|    time_elapsed       | 28       |\n",
            "|    total_timesteps    | 52000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.456   |\n",
            "|    explained_variance | 0.701    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 1299     |\n",
            "|    policy_loss        | -1.96    |\n",
            "|    value_loss         | 150      |\n",
            "------------------------------------\n",
            "Num timesteps: 56000\n",
            "Best mean reward: -99.89 - Last mean reward per episode: -103.23\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 265      |\n",
            "|    ep_rew_mean        | -103     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1803     |\n",
            "|    iterations         | 1400     |\n",
            "|    time_elapsed       | 31       |\n",
            "|    total_timesteps    | 56000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.556   |\n",
            "|    explained_variance | 0.858    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 1399     |\n",
            "|    policy_loss        | 0.542    |\n",
            "|    value_loss         | 39.7     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 266      |\n",
            "|    ep_rew_mean        | -101     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1799     |\n",
            "|    iterations         | 1500     |\n",
            "|    time_elapsed       | 33       |\n",
            "|    total_timesteps    | 60000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.468   |\n",
            "|    explained_variance | 0.809    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 1499     |\n",
            "|    policy_loss        | -1.19    |\n",
            "|    value_loss         | 115      |\n",
            "------------------------------------\n",
            "Num timesteps: 64000\n",
            "Best mean reward: -99.89 - Last mean reward per episode: -102.07\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 264      |\n",
            "|    ep_rew_mean        | -102     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1795     |\n",
            "|    iterations         | 1600     |\n",
            "|    time_elapsed       | 35       |\n",
            "|    total_timesteps    | 64000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.7     |\n",
            "|    explained_variance | 0.884    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 1599     |\n",
            "|    policy_loss        | -1.58    |\n",
            "|    value_loss         | 30.9     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 284      |\n",
            "|    ep_rew_mean        | -105     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1799     |\n",
            "|    iterations         | 1700     |\n",
            "|    time_elapsed       | 37       |\n",
            "|    total_timesteps    | 68000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.61    |\n",
            "|    explained_variance | 0.794    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 1699     |\n",
            "|    policy_loss        | -0.515   |\n",
            "|    value_loss         | 54.1     |\n",
            "------------------------------------\n",
            "Num timesteps: 72000\n",
            "Best mean reward: -99.89 - Last mean reward per episode: -111.15\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 272      |\n",
            "|    ep_rew_mean        | -111     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1801     |\n",
            "|    iterations         | 1800     |\n",
            "|    time_elapsed       | 39       |\n",
            "|    total_timesteps    | 72000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.511   |\n",
            "|    explained_variance | 0.804    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 1799     |\n",
            "|    policy_loss        | 1.98     |\n",
            "|    value_loss         | 128      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 264      |\n",
            "|    ep_rew_mean        | -113     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1767     |\n",
            "|    iterations         | 1900     |\n",
            "|    time_elapsed       | 42       |\n",
            "|    total_timesteps    | 76000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.683   |\n",
            "|    explained_variance | 0.9      |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 1899     |\n",
            "|    policy_loss        | 0.133    |\n",
            "|    value_loss         | 36.2     |\n",
            "------------------------------------\n",
            "Num timesteps: 80000\n",
            "Best mean reward: -99.89 - Last mean reward per episode: -111.05\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 268      |\n",
            "|    ep_rew_mean        | -111     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1747     |\n",
            "|    iterations         | 2000     |\n",
            "|    time_elapsed       | 45       |\n",
            "|    total_timesteps    | 80000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.653   |\n",
            "|    explained_variance | 0.816    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 1999     |\n",
            "|    policy_loss        | -0.188   |\n",
            "|    value_loss         | 44.1     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 272      |\n",
            "|    ep_rew_mean        | -103     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1721     |\n",
            "|    iterations         | 2100     |\n",
            "|    time_elapsed       | 48       |\n",
            "|    total_timesteps    | 84000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.656   |\n",
            "|    explained_variance | 0.911    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 2099     |\n",
            "|    policy_loss        | 0.29     |\n",
            "|    value_loss         | 21.9     |\n",
            "------------------------------------\n",
            "Num timesteps: 88000\n",
            "Best mean reward: -99.89 - Last mean reward per episode: -88.05\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 283      |\n",
            "|    ep_rew_mean        | -88.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1718     |\n",
            "|    iterations         | 2200     |\n",
            "|    time_elapsed       | 51       |\n",
            "|    total_timesteps    | 88000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.626   |\n",
            "|    explained_variance | 0.941    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 2199     |\n",
            "|    policy_loss        | -0.96    |\n",
            "|    value_loss         | 48.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 295      |\n",
            "|    ep_rew_mean        | -69.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1710     |\n",
            "|    iterations         | 2300     |\n",
            "|    time_elapsed       | 53       |\n",
            "|    total_timesteps    | 92000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.513   |\n",
            "|    explained_variance | 0.916    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 2299     |\n",
            "|    policy_loss        | 1.49     |\n",
            "|    value_loss         | 18.6     |\n",
            "------------------------------------\n",
            "Num timesteps: 96000\n",
            "Best mean reward: -88.05 - Last mean reward per episode: -59.92\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 290      |\n",
            "|    ep_rew_mean        | -59.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1712     |\n",
            "|    iterations         | 2400     |\n",
            "|    time_elapsed       | 56       |\n",
            "|    total_timesteps    | 96000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.311   |\n",
            "|    explained_variance | 0.552    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 2399     |\n",
            "|    policy_loss        | -1.28    |\n",
            "|    value_loss         | 682      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 292      |\n",
            "|    ep_rew_mean        | -44.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1707     |\n",
            "|    iterations         | 2500     |\n",
            "|    time_elapsed       | 58       |\n",
            "|    total_timesteps    | 100000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.579   |\n",
            "|    explained_variance | 0.9      |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 2499     |\n",
            "|    policy_loss        | -0.111   |\n",
            "|    value_loss         | 42.1     |\n",
            "------------------------------------\n",
            "Num timesteps: 104000\n",
            "Best mean reward: -59.92 - Last mean reward per episode: -45.48\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 304      |\n",
            "|    ep_rew_mean        | -45.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1688     |\n",
            "|    iterations         | 2600     |\n",
            "|    time_elapsed       | 61       |\n",
            "|    total_timesteps    | 104000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.587   |\n",
            "|    explained_variance | 0.794    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 2599     |\n",
            "|    policy_loss        | -0.552   |\n",
            "|    value_loss         | 11.2     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 314      |\n",
            "|    ep_rew_mean        | -37.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1676     |\n",
            "|    iterations         | 2700     |\n",
            "|    time_elapsed       | 64       |\n",
            "|    total_timesteps    | 108000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.623   |\n",
            "|    explained_variance | 0.544    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 2699     |\n",
            "|    policy_loss        | -0.921   |\n",
            "|    value_loss         | 19       |\n",
            "------------------------------------\n",
            "Num timesteps: 112000\n",
            "Best mean reward: -45.48 - Last mean reward per episode: -35.50\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 323      |\n",
            "|    ep_rew_mean        | -35.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1663     |\n",
            "|    iterations         | 2800     |\n",
            "|    time_elapsed       | 67       |\n",
            "|    total_timesteps    | 112000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.563   |\n",
            "|    explained_variance | 0.934    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 2799     |\n",
            "|    policy_loss        | -0.543   |\n",
            "|    value_loss         | 6.87     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 330      |\n",
            "|    ep_rew_mean        | -27.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1629     |\n",
            "|    iterations         | 2900     |\n",
            "|    time_elapsed       | 71       |\n",
            "|    total_timesteps    | 116000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.529   |\n",
            "|    explained_variance | 0.974    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 2899     |\n",
            "|    policy_loss        | -1.12    |\n",
            "|    value_loss         | 9.58     |\n",
            "------------------------------------\n",
            "Num timesteps: 120000\n",
            "Best mean reward: -35.50 - Last mean reward per episode: -22.19\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 369      |\n",
            "|    ep_rew_mean        | -22.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1621     |\n",
            "|    iterations         | 3000     |\n",
            "|    time_elapsed       | 74       |\n",
            "|    total_timesteps    | 120000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.613   |\n",
            "|    explained_variance | 0.918    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 2999     |\n",
            "|    policy_loss        | 1.08     |\n",
            "|    value_loss         | 50.9     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 359      |\n",
            "|    ep_rew_mean        | -18.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1621     |\n",
            "|    iterations         | 3100     |\n",
            "|    time_elapsed       | 76       |\n",
            "|    total_timesteps    | 124000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.591   |\n",
            "|    explained_variance | 0.136    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 3099     |\n",
            "|    policy_loss        | -0.491   |\n",
            "|    value_loss         | 108      |\n",
            "------------------------------------\n",
            "Num timesteps: 128000\n",
            "Best mean reward: -22.19 - Last mean reward per episode: -18.59\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 362      |\n",
            "|    ep_rew_mean        | -18.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1611     |\n",
            "|    iterations         | 3200     |\n",
            "|    time_elapsed       | 79       |\n",
            "|    total_timesteps    | 128000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.45    |\n",
            "|    explained_variance | 0.939    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 3199     |\n",
            "|    policy_loss        | 0.851    |\n",
            "|    value_loss         | 26.7     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 378      |\n",
            "|    ep_rew_mean        | -9.58    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1602     |\n",
            "|    iterations         | 3300     |\n",
            "|    time_elapsed       | 82       |\n",
            "|    total_timesteps    | 132000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.583   |\n",
            "|    explained_variance | 0.856    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 3299     |\n",
            "|    policy_loss        | -0.948   |\n",
            "|    value_loss         | 29.9     |\n",
            "------------------------------------\n",
            "Num timesteps: 136000\n",
            "Best mean reward: -18.59 - Last mean reward per episode: -9.74\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 384      |\n",
            "|    ep_rew_mean        | -9.74    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1601     |\n",
            "|    iterations         | 3400     |\n",
            "|    time_elapsed       | 84       |\n",
            "|    total_timesteps    | 136000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.4     |\n",
            "|    explained_variance | 0.935    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 3399     |\n",
            "|    policy_loss        | 1.04     |\n",
            "|    value_loss         | 37.9     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 392      |\n",
            "|    ep_rew_mean        | 0.57     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1599     |\n",
            "|    iterations         | 3500     |\n",
            "|    time_elapsed       | 87       |\n",
            "|    total_timesteps    | 140000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.555   |\n",
            "|    explained_variance | 0.952    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 3499     |\n",
            "|    policy_loss        | 0.143    |\n",
            "|    value_loss         | 27.5     |\n",
            "------------------------------------\n",
            "Num timesteps: 144000\n",
            "Best mean reward: -9.74 - Last mean reward per episode: 5.94\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 392      |\n",
            "|    ep_rew_mean        | 5.94     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1593     |\n",
            "|    iterations         | 3600     |\n",
            "|    time_elapsed       | 90       |\n",
            "|    total_timesteps    | 144000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.412   |\n",
            "|    explained_variance | 0.925    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 3599     |\n",
            "|    policy_loss        | 0.404    |\n",
            "|    value_loss         | 8.15     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 395      |\n",
            "|    ep_rew_mean        | 17       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1583     |\n",
            "|    iterations         | 3700     |\n",
            "|    time_elapsed       | 93       |\n",
            "|    total_timesteps    | 148000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.512   |\n",
            "|    explained_variance | 0.851    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 3699     |\n",
            "|    policy_loss        | 1.12     |\n",
            "|    value_loss         | 60.1     |\n",
            "------------------------------------\n",
            "Num timesteps: 152000\n",
            "Best mean reward: 5.94 - Last mean reward per episode: 12.16\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 383      |\n",
            "|    ep_rew_mean        | 12.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1578     |\n",
            "|    iterations         | 3800     |\n",
            "|    time_elapsed       | 96       |\n",
            "|    total_timesteps    | 152000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.481   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 3799     |\n",
            "|    policy_loss        | 0.568    |\n",
            "|    value_loss         | 10.9     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 366      |\n",
            "|    ep_rew_mean        | 9.93     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1577     |\n",
            "|    iterations         | 3900     |\n",
            "|    time_elapsed       | 98       |\n",
            "|    total_timesteps    | 156000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.578   |\n",
            "|    explained_variance | 0.95     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 3899     |\n",
            "|    policy_loss        | 2.85     |\n",
            "|    value_loss         | 37.8     |\n",
            "------------------------------------\n",
            "Num timesteps: 160000\n",
            "Best mean reward: 12.16 - Last mean reward per episode: 18.47\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 369      |\n",
            "|    ep_rew_mean        | 18.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1572     |\n",
            "|    iterations         | 4000     |\n",
            "|    time_elapsed       | 101      |\n",
            "|    total_timesteps    | 160000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.489   |\n",
            "|    explained_variance | 0.899    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 3999     |\n",
            "|    policy_loss        | 0.65     |\n",
            "|    value_loss         | 10.8     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 381      |\n",
            "|    ep_rew_mean        | 20       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1560     |\n",
            "|    iterations         | 4100     |\n",
            "|    time_elapsed       | 105      |\n",
            "|    total_timesteps    | 164000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.373   |\n",
            "|    explained_variance | -0.276   |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 4099     |\n",
            "|    policy_loss        | 1.4      |\n",
            "|    value_loss         | 134      |\n",
            "------------------------------------\n",
            "Num timesteps: 168000\n",
            "Best mean reward: 18.47 - Last mean reward per episode: 36.71\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 361      |\n",
            "|    ep_rew_mean        | 36.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1563     |\n",
            "|    iterations         | 4200     |\n",
            "|    time_elapsed       | 107      |\n",
            "|    total_timesteps    | 168000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.518   |\n",
            "|    explained_variance | 0.331    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 4199     |\n",
            "|    policy_loss        | 1.15     |\n",
            "|    value_loss         | 44.7     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 352      |\n",
            "|    ep_rew_mean        | 41.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1561     |\n",
            "|    iterations         | 4300     |\n",
            "|    time_elapsed       | 110      |\n",
            "|    total_timesteps    | 172000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.419   |\n",
            "|    explained_variance | 0.54     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 4299     |\n",
            "|    policy_loss        | -0.783   |\n",
            "|    value_loss         | 30.3     |\n",
            "------------------------------------\n",
            "Num timesteps: 176000\n",
            "Best mean reward: 36.71 - Last mean reward per episode: 43.33\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 352      |\n",
            "|    ep_rew_mean        | 43.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1559     |\n",
            "|    iterations         | 4400     |\n",
            "|    time_elapsed       | 112      |\n",
            "|    total_timesteps    | 176000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.449   |\n",
            "|    explained_variance | 0.455    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 4399     |\n",
            "|    policy_loss        | 1.85     |\n",
            "|    value_loss         | 407      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 338      |\n",
            "|    ep_rew_mean        | 40.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1564     |\n",
            "|    iterations         | 4500     |\n",
            "|    time_elapsed       | 115      |\n",
            "|    total_timesteps    | 180000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.535   |\n",
            "|    explained_variance | 0.954    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 4499     |\n",
            "|    policy_loss        | 1.55     |\n",
            "|    value_loss         | 37       |\n",
            "------------------------------------\n",
            "Num timesteps: 184000\n",
            "Best mean reward: 43.33 - Last mean reward per episode: 49.16\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 318      |\n",
            "|    ep_rew_mean        | 49.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1570     |\n",
            "|    iterations         | 4600     |\n",
            "|    time_elapsed       | 117      |\n",
            "|    total_timesteps    | 184000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.44    |\n",
            "|    explained_variance | 0.704    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 4599     |\n",
            "|    policy_loss        | 0.373    |\n",
            "|    value_loss         | 68.4     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 309      |\n",
            "|    ep_rew_mean        | 53.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1572     |\n",
            "|    iterations         | 4700     |\n",
            "|    time_elapsed       | 119      |\n",
            "|    total_timesteps    | 188000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.552   |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 4699     |\n",
            "|    policy_loss        | 0.177    |\n",
            "|    value_loss         | 5.84     |\n",
            "------------------------------------\n",
            "Num timesteps: 192000\n",
            "Best mean reward: 49.16 - Last mean reward per episode: 50.85\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 315      |\n",
            "|    ep_rew_mean        | 50.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1569     |\n",
            "|    iterations         | 4800     |\n",
            "|    time_elapsed       | 122      |\n",
            "|    total_timesteps    | 192000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.659   |\n",
            "|    explained_variance | 0.867    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 4799     |\n",
            "|    policy_loss        | 0.871    |\n",
            "|    value_loss         | 70.1     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 301      |\n",
            "|    ep_rew_mean        | 45       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1568     |\n",
            "|    iterations         | 4900     |\n",
            "|    time_elapsed       | 124      |\n",
            "|    total_timesteps    | 196000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.594   |\n",
            "|    explained_variance | 0.885    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 4899     |\n",
            "|    policy_loss        | -0.18    |\n",
            "|    value_loss         | 37.3     |\n",
            "------------------------------------\n",
            "Num timesteps: 200000\n",
            "Best mean reward: 50.85 - Last mean reward per episode: 32.42\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 303      |\n",
            "|    ep_rew_mean        | 32.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1567     |\n",
            "|    iterations         | 5000     |\n",
            "|    time_elapsed       | 127      |\n",
            "|    total_timesteps    | 200000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.662   |\n",
            "|    explained_variance | 0.878    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 4999     |\n",
            "|    policy_loss        | 2.32     |\n",
            "|    value_loss         | 16.7     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 314      |\n",
            "|    ep_rew_mean        | 39.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1557     |\n",
            "|    iterations         | 5100     |\n",
            "|    time_elapsed       | 130      |\n",
            "|    total_timesteps    | 204000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.413   |\n",
            "|    explained_variance | 0.643    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 5099     |\n",
            "|    policy_loss        | -0.674   |\n",
            "|    value_loss         | 6.61     |\n",
            "------------------------------------\n",
            "Num timesteps: 208000\n",
            "Best mean reward: 50.85 - Last mean reward per episode: 52.80\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 333      |\n",
            "|    ep_rew_mean        | 52.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1543     |\n",
            "|    iterations         | 5200     |\n",
            "|    time_elapsed       | 134      |\n",
            "|    total_timesteps    | 208000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.484   |\n",
            "|    explained_variance | 0.000832 |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 5199     |\n",
            "|    policy_loss        | -1.65    |\n",
            "|    value_loss         | 773      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 326      |\n",
            "|    ep_rew_mean        | 55.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1544     |\n",
            "|    iterations         | 5300     |\n",
            "|    time_elapsed       | 137      |\n",
            "|    total_timesteps    | 212000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.585   |\n",
            "|    explained_variance | 0.95     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 5299     |\n",
            "|    policy_loss        | 0.218    |\n",
            "|    value_loss         | 11       |\n",
            "------------------------------------\n",
            "Num timesteps: 216000\n",
            "Best mean reward: 52.80 - Last mean reward per episode: 55.97\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 333      |\n",
            "|    ep_rew_mean        | 56       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1538     |\n",
            "|    iterations         | 5400     |\n",
            "|    time_elapsed       | 140      |\n",
            "|    total_timesteps    | 216000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.568   |\n",
            "|    explained_variance | 0.491    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 5399     |\n",
            "|    policy_loss        | 0.161    |\n",
            "|    value_loss         | 9.43     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 353      |\n",
            "|    ep_rew_mean        | 56.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1531     |\n",
            "|    iterations         | 5500     |\n",
            "|    time_elapsed       | 143      |\n",
            "|    total_timesteps    | 220000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.627   |\n",
            "|    explained_variance | 0.792    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 5499     |\n",
            "|    policy_loss        | -0.0173  |\n",
            "|    value_loss         | 10.9     |\n",
            "------------------------------------\n",
            "Num timesteps: 224000\n",
            "Best mean reward: 55.97 - Last mean reward per episode: 62.30\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 371      |\n",
            "|    ep_rew_mean        | 62.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1522     |\n",
            "|    iterations         | 5600     |\n",
            "|    time_elapsed       | 147      |\n",
            "|    total_timesteps    | 224000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.691   |\n",
            "|    explained_variance | 0.0908   |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 5599     |\n",
            "|    policy_loss        | -2.28    |\n",
            "|    value_loss         | 620      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 374      |\n",
            "|    ep_rew_mean        | 57.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1507     |\n",
            "|    iterations         | 5700     |\n",
            "|    time_elapsed       | 151      |\n",
            "|    total_timesteps    | 228000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.433   |\n",
            "|    explained_variance | -0.25    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 5699     |\n",
            "|    policy_loss        | -3.51    |\n",
            "|    value_loss         | 1.3e+03  |\n",
            "------------------------------------\n",
            "Num timesteps: 232000\n",
            "Best mean reward: 62.30 - Last mean reward per episode: 56.78\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 374      |\n",
            "|    ep_rew_mean        | 56.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1503     |\n",
            "|    iterations         | 5800     |\n",
            "|    time_elapsed       | 154      |\n",
            "|    total_timesteps    | 232000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.623   |\n",
            "|    explained_variance | 0.871    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 5799     |\n",
            "|    policy_loss        | 0.0862   |\n",
            "|    value_loss         | 2.53     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 396      |\n",
            "|    ep_rew_mean        | 55.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1482     |\n",
            "|    iterations         | 5900     |\n",
            "|    time_elapsed       | 159      |\n",
            "|    total_timesteps    | 236000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.603   |\n",
            "|    explained_variance | 0.495    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 5899     |\n",
            "|    policy_loss        | -0.397   |\n",
            "|    value_loss         | 114      |\n",
            "------------------------------------\n",
            "Num timesteps: 240000\n",
            "Best mean reward: 62.30 - Last mean reward per episode: 55.48\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 405      |\n",
            "|    ep_rew_mean        | 55.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1474     |\n",
            "|    iterations         | 6000     |\n",
            "|    time_elapsed       | 162      |\n",
            "|    total_timesteps    | 240000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.755   |\n",
            "|    explained_variance | 0.739    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 5999     |\n",
            "|    policy_loss        | -0.0823  |\n",
            "|    value_loss         | 5.99     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 435      |\n",
            "|    ep_rew_mean        | 63       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1453     |\n",
            "|    iterations         | 6100     |\n",
            "|    time_elapsed       | 167      |\n",
            "|    total_timesteps    | 244000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.609   |\n",
            "|    explained_variance | 0.971    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 6099     |\n",
            "|    policy_loss        | 0.665    |\n",
            "|    value_loss         | 4.73     |\n",
            "------------------------------------\n",
            "Num timesteps: 248000\n",
            "Best mean reward: 62.30 - Last mean reward per episode: 54.82\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 430      |\n",
            "|    ep_rew_mean        | 54.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1448     |\n",
            "|    iterations         | 6200     |\n",
            "|    time_elapsed       | 171      |\n",
            "|    total_timesteps    | 248000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.595   |\n",
            "|    explained_variance | 0.973    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 6199     |\n",
            "|    policy_loss        | -0.104   |\n",
            "|    value_loss         | 8.42     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 436      |\n",
            "|    ep_rew_mean        | 54.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1440     |\n",
            "|    iterations         | 6300     |\n",
            "|    time_elapsed       | 174      |\n",
            "|    total_timesteps    | 252000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.678   |\n",
            "|    explained_variance | 0.821    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 6299     |\n",
            "|    policy_loss        | -0.189   |\n",
            "|    value_loss         | 9.37     |\n",
            "------------------------------------\n",
            "Num timesteps: 256000\n",
            "Best mean reward: 62.30 - Last mean reward per episode: 47.48\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 454      |\n",
            "|    ep_rew_mean        | 47.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1420     |\n",
            "|    iterations         | 6400     |\n",
            "|    time_elapsed       | 180      |\n",
            "|    total_timesteps    | 256000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.374   |\n",
            "|    explained_variance | 0.896    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 6399     |\n",
            "|    policy_loss        | 0.658    |\n",
            "|    value_loss         | 14.8     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 468      |\n",
            "|    ep_rew_mean        | 49       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1415     |\n",
            "|    iterations         | 6500     |\n",
            "|    time_elapsed       | 183      |\n",
            "|    total_timesteps    | 260000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.447   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 6499     |\n",
            "|    policy_loss        | 0.69     |\n",
            "|    value_loss         | 10.2     |\n",
            "------------------------------------\n",
            "Num timesteps: 264000\n",
            "Best mean reward: 62.30 - Last mean reward per episode: 49.63\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 477      |\n",
            "|    ep_rew_mean        | 49.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1413     |\n",
            "|    iterations         | 6600     |\n",
            "|    time_elapsed       | 186      |\n",
            "|    total_timesteps    | 264000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.826   |\n",
            "|    explained_variance | 0.93     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 6599     |\n",
            "|    policy_loss        | -0.107   |\n",
            "|    value_loss         | 8.62     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 493      |\n",
            "|    ep_rew_mean        | 57.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1404     |\n",
            "|    iterations         | 6700     |\n",
            "|    time_elapsed       | 190      |\n",
            "|    total_timesteps    | 268000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.687   |\n",
            "|    explained_variance | 0.96     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 6699     |\n",
            "|    policy_loss        | -0.011   |\n",
            "|    value_loss         | 9.82     |\n",
            "------------------------------------\n",
            "Num timesteps: 272000\n",
            "Best mean reward: 62.30 - Last mean reward per episode: 58.17\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 505      |\n",
            "|    ep_rew_mean        | 58.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1387     |\n",
            "|    iterations         | 6800     |\n",
            "|    time_elapsed       | 196      |\n",
            "|    total_timesteps    | 272000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.849   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 6799     |\n",
            "|    policy_loss        | 0.188    |\n",
            "|    value_loss         | 1.51     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 508      |\n",
            "|    ep_rew_mean        | 55.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1374     |\n",
            "|    iterations         | 6900     |\n",
            "|    time_elapsed       | 200      |\n",
            "|    total_timesteps    | 276000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.627   |\n",
            "|    explained_variance | -0.0552  |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 6899     |\n",
            "|    policy_loss        | -0.0848  |\n",
            "|    value_loss         | 951      |\n",
            "------------------------------------\n",
            "Num timesteps: 280000\n",
            "Best mean reward: 62.30 - Last mean reward per episode: 62.68\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 523      |\n",
            "|    ep_rew_mean        | 62.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1365     |\n",
            "|    iterations         | 7000     |\n",
            "|    time_elapsed       | 205      |\n",
            "|    total_timesteps    | 280000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.501   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 6999     |\n",
            "|    policy_loss        | -0.234   |\n",
            "|    value_loss         | 6.92     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 479      |\n",
            "|    ep_rew_mean        | 52.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1370     |\n",
            "|    iterations         | 7100     |\n",
            "|    time_elapsed       | 207      |\n",
            "|    total_timesteps    | 284000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.628   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 7099     |\n",
            "|    policy_loss        | -0.0411  |\n",
            "|    value_loss         | 2.33     |\n",
            "------------------------------------\n",
            "Num timesteps: 288000\n",
            "Best mean reward: 62.68 - Last mean reward per episode: 39.00\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 423      |\n",
            "|    ep_rew_mean        | 39       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1375     |\n",
            "|    iterations         | 7200     |\n",
            "|    time_elapsed       | 209      |\n",
            "|    total_timesteps    | 288000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.675   |\n",
            "|    explained_variance | 0.974    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 7199     |\n",
            "|    policy_loss        | -0.56    |\n",
            "|    value_loss         | 11.9     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 353      |\n",
            "|    ep_rew_mean        | 28.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1379     |\n",
            "|    iterations         | 7300     |\n",
            "|    time_elapsed       | 211      |\n",
            "|    total_timesteps    | 292000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.552   |\n",
            "|    explained_variance | 0.962    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 7299     |\n",
            "|    policy_loss        | 0.518    |\n",
            "|    value_loss         | 38.2     |\n",
            "------------------------------------\n",
            "Num timesteps: 296000\n",
            "Best mean reward: 62.68 - Last mean reward per episode: 3.26\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 284      |\n",
            "|    ep_rew_mean        | 3.26     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1385     |\n",
            "|    iterations         | 7400     |\n",
            "|    time_elapsed       | 213      |\n",
            "|    total_timesteps    | 296000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.671   |\n",
            "|    explained_variance | 0.952    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 7399     |\n",
            "|    policy_loss        | -1.61    |\n",
            "|    value_loss         | 21.3     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 177      |\n",
            "|    ep_rew_mean        | -36.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1393     |\n",
            "|    iterations         | 7500     |\n",
            "|    time_elapsed       | 215      |\n",
            "|    total_timesteps    | 300000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.55    |\n",
            "|    explained_variance | 0.949    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 7499     |\n",
            "|    policy_loss        | 0.841    |\n",
            "|    value_loss         | 45.8     |\n",
            "------------------------------------\n",
            "Num timesteps: 304000\n",
            "Best mean reward: 62.68 - Last mean reward per episode: -47.69\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 141      |\n",
            "|    ep_rew_mean        | -47.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1403     |\n",
            "|    iterations         | 7600     |\n",
            "|    time_elapsed       | 216      |\n",
            "|    total_timesteps    | 304000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.51    |\n",
            "|    explained_variance | 0.92     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 7599     |\n",
            "|    policy_loss        | 0.437    |\n",
            "|    value_loss         | 40.2     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 110      |\n",
            "|    ep_rew_mean        | -45.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1412     |\n",
            "|    iterations         | 7700     |\n",
            "|    time_elapsed       | 218      |\n",
            "|    total_timesteps    | 308000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.589   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 7699     |\n",
            "|    policy_loss        | -0.915   |\n",
            "|    value_loss         | 11.4     |\n",
            "------------------------------------\n",
            "Num timesteps: 312000\n",
            "Best mean reward: 62.68 - Last mean reward per episode: -36.31\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 114      |\n",
            "|    ep_rew_mean        | -36.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1418     |\n",
            "|    iterations         | 7800     |\n",
            "|    time_elapsed       | 219      |\n",
            "|    total_timesteps    | 312000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.601   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 7799     |\n",
            "|    policy_loss        | 0.151    |\n",
            "|    value_loss         | 2.89     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 131      |\n",
            "|    ep_rew_mean        | -27.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1417     |\n",
            "|    iterations         | 7900     |\n",
            "|    time_elapsed       | 222      |\n",
            "|    total_timesteps    | 316000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.607   |\n",
            "|    explained_variance | 0.606    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 7899     |\n",
            "|    policy_loss        | 0.182    |\n",
            "|    value_loss         | 3.82     |\n",
            "------------------------------------\n",
            "Num timesteps: 320000\n",
            "Best mean reward: 62.68 - Last mean reward per episode: -13.86\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 172      |\n",
            "|    ep_rew_mean        | -13.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1415     |\n",
            "|    iterations         | 8000     |\n",
            "|    time_elapsed       | 226      |\n",
            "|    total_timesteps    | 320000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.59    |\n",
            "|    explained_variance | 0.937    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 7999     |\n",
            "|    policy_loss        | -0.201   |\n",
            "|    value_loss         | 2.7      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 179      |\n",
            "|    ep_rew_mean        | -11.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1405     |\n",
            "|    iterations         | 8100     |\n",
            "|    time_elapsed       | 230      |\n",
            "|    total_timesteps    | 324000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.786   |\n",
            "|    explained_variance | -0.321   |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 8099     |\n",
            "|    policy_loss        | -0.298   |\n",
            "|    value_loss         | 6.03     |\n",
            "------------------------------------\n",
            "Num timesteps: 328000\n",
            "Best mean reward: 62.68 - Last mean reward per episode: -11.72\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 242      |\n",
            "|    ep_rew_mean        | -11.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1395     |\n",
            "|    iterations         | 8200     |\n",
            "|    time_elapsed       | 234      |\n",
            "|    total_timesteps    | 328000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.852   |\n",
            "|    explained_variance | 0.82     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 8199     |\n",
            "|    policy_loss        | -1.57    |\n",
            "|    value_loss         | 9.33     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 251      |\n",
            "|    ep_rew_mean        | -12.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1380     |\n",
            "|    iterations         | 8300     |\n",
            "|    time_elapsed       | 240      |\n",
            "|    total_timesteps    | 332000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.993   |\n",
            "|    explained_variance | 0.339    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 8299     |\n",
            "|    policy_loss        | -1.29    |\n",
            "|    value_loss         | 3.56     |\n",
            "------------------------------------\n",
            "Num timesteps: 336000\n",
            "Best mean reward: 62.68 - Last mean reward per episode: -17.08\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 313      |\n",
            "|    ep_rew_mean        | -17.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1356     |\n",
            "|    iterations         | 8400     |\n",
            "|    time_elapsed       | 247      |\n",
            "|    total_timesteps    | 336000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.765   |\n",
            "|    explained_variance | 0.932    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 8399     |\n",
            "|    policy_loss        | -0.724   |\n",
            "|    value_loss         | 5.01     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 322      |\n",
            "|    ep_rew_mean        | -17.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1345     |\n",
            "|    iterations         | 8500     |\n",
            "|    time_elapsed       | 252      |\n",
            "|    total_timesteps    | 340000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.739   |\n",
            "|    explained_variance | 0.537    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 8499     |\n",
            "|    policy_loss        | -0.138   |\n",
            "|    value_loss         | 5.46     |\n",
            "------------------------------------\n",
            "Num timesteps: 344000\n",
            "Best mean reward: 62.68 - Last mean reward per episode: -14.71\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 390      |\n",
            "|    ep_rew_mean        | -14.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1336     |\n",
            "|    iterations         | 8600     |\n",
            "|    time_elapsed       | 257      |\n",
            "|    total_timesteps    | 344000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.443   |\n",
            "|    explained_variance | 0.839    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 8599     |\n",
            "|    policy_loss        | 0.839    |\n",
            "|    value_loss         | 23.7     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 405      |\n",
            "|    ep_rew_mean        | -5.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1329     |\n",
            "|    iterations         | 8700     |\n",
            "|    time_elapsed       | 261      |\n",
            "|    total_timesteps    | 348000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.718   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 8699     |\n",
            "|    policy_loss        | -0.0836  |\n",
            "|    value_loss         | 3.12     |\n",
            "------------------------------------\n",
            "Num timesteps: 352000\n",
            "Best mean reward: 62.68 - Last mean reward per episode: 1.24\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 434      |\n",
            "|    ep_rew_mean        | 1.24     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1320     |\n",
            "|    iterations         | 8800     |\n",
            "|    time_elapsed       | 266      |\n",
            "|    total_timesteps    | 352000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.783   |\n",
            "|    explained_variance | 0.215    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 8799     |\n",
            "|    policy_loss        | -1.25    |\n",
            "|    value_loss         | 10.8     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 471      |\n",
            "|    ep_rew_mean        | 0.117    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1308     |\n",
            "|    iterations         | 8900     |\n",
            "|    time_elapsed       | 272      |\n",
            "|    total_timesteps    | 356000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.708   |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 8899     |\n",
            "|    policy_loss        | 0.205    |\n",
            "|    value_loss         | 4.99     |\n",
            "------------------------------------\n",
            "Num timesteps: 360000\n",
            "Best mean reward: 62.68 - Last mean reward per episode: -0.53\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 506      |\n",
            "|    ep_rew_mean        | -0.526   |\n",
            "| time/                 |          |\n",
            "|    fps                | 1300     |\n",
            "|    iterations         | 9000     |\n",
            "|    time_elapsed       | 276      |\n",
            "|    total_timesteps    | 360000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.828   |\n",
            "|    explained_variance | -0.172   |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 8999     |\n",
            "|    policy_loss        | -0.887   |\n",
            "|    value_loss         | 6.93     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 543      |\n",
            "|    ep_rew_mean        | -0.341   |\n",
            "| time/                 |          |\n",
            "|    fps                | 1287     |\n",
            "|    iterations         | 9100     |\n",
            "|    time_elapsed       | 282      |\n",
            "|    total_timesteps    | 364000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.744   |\n",
            "|    explained_variance | 0.975    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 9099     |\n",
            "|    policy_loss        | 1.05     |\n",
            "|    value_loss         | 5.24     |\n",
            "------------------------------------\n",
            "Num timesteps: 368000\n",
            "Best mean reward: 62.68 - Last mean reward per episode: 4.66\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 581      |\n",
            "|    ep_rew_mean        | 4.66     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1283     |\n",
            "|    iterations         | 9200     |\n",
            "|    time_elapsed       | 286      |\n",
            "|    total_timesteps    | 368000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.586   |\n",
            "|    explained_variance | 0.976    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 9199     |\n",
            "|    policy_loss        | -0.0178  |\n",
            "|    value_loss         | 3.72     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 608      |\n",
            "|    ep_rew_mean        | 5.38     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1273     |\n",
            "|    iterations         | 9300     |\n",
            "|    time_elapsed       | 292      |\n",
            "|    total_timesteps    | 372000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.731   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 9299     |\n",
            "|    policy_loss        | 0.819    |\n",
            "|    value_loss         | 3.25     |\n",
            "------------------------------------\n",
            "Num timesteps: 376000\n",
            "Best mean reward: 62.68 - Last mean reward per episode: 6.03\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 651      |\n",
            "|    ep_rew_mean        | 6.03     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1265     |\n",
            "|    iterations         | 9400     |\n",
            "|    time_elapsed       | 297      |\n",
            "|    total_timesteps    | 376000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.761   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 9399     |\n",
            "|    policy_loss        | -0.38    |\n",
            "|    value_loss         | 1.86     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 678      |\n",
            "|    ep_rew_mean        | 5.1      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1255     |\n",
            "|    iterations         | 9500     |\n",
            "|    time_elapsed       | 302      |\n",
            "|    total_timesteps    | 380000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.893   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 9499     |\n",
            "|    policy_loss        | 0.191    |\n",
            "|    value_loss         | 0.922    |\n",
            "------------------------------------\n",
            "Num timesteps: 384000\n",
            "Best mean reward: 62.68 - Last mean reward per episode: 12.71\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 725      |\n",
            "|    ep_rew_mean        | 12.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1250     |\n",
            "|    iterations         | 9600     |\n",
            "|    time_elapsed       | 307      |\n",
            "|    total_timesteps    | 384000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.541   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 9599     |\n",
            "|    policy_loss        | -0.0305  |\n",
            "|    value_loss         | 1.46     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 749      |\n",
            "|    ep_rew_mean        | 13.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1245     |\n",
            "|    iterations         | 9700     |\n",
            "|    time_elapsed       | 311      |\n",
            "|    total_timesteps    | 388000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.668   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 9699     |\n",
            "|    policy_loss        | 0.274    |\n",
            "|    value_loss         | 2.81     |\n",
            "------------------------------------\n",
            "Num timesteps: 392000\n",
            "Best mean reward: 62.68 - Last mean reward per episode: 9.64\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 792      |\n",
            "|    ep_rew_mean        | 9.64     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1239     |\n",
            "|    iterations         | 9800     |\n",
            "|    time_elapsed       | 316      |\n",
            "|    total_timesteps    | 392000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.717   |\n",
            "|    explained_variance | 0.968    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 9799     |\n",
            "|    policy_loss        | 0.691    |\n",
            "|    value_loss         | 9.05     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 832      |\n",
            "|    ep_rew_mean        | 7.3      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1235     |\n",
            "|    iterations         | 9900     |\n",
            "|    time_elapsed       | 320      |\n",
            "|    total_timesteps    | 396000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.507   |\n",
            "|    explained_variance | 0.963    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 9899     |\n",
            "|    policy_loss        | 0.207    |\n",
            "|    value_loss         | 41       |\n",
            "------------------------------------\n",
            "Num timesteps: 400000\n",
            "Best mean reward: 62.68 - Last mean reward per episode: 2.04\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 828      |\n",
            "|    ep_rew_mean        | 2.04     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1236     |\n",
            "|    iterations         | 10000    |\n",
            "|    time_elapsed       | 323      |\n",
            "|    total_timesteps    | 400000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.531   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 9999     |\n",
            "|    policy_loss        | 0.18     |\n",
            "|    value_loss         | 6.34     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 812      |\n",
            "|    ep_rew_mean        | 1.38     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1233     |\n",
            "|    iterations         | 10100    |\n",
            "|    time_elapsed       | 327      |\n",
            "|    total_timesteps    | 404000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.632   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 10099    |\n",
            "|    policy_loss        | 0.204    |\n",
            "|    value_loss         | 2.6      |\n",
            "------------------------------------\n",
            "Num timesteps: 408000\n",
            "Best mean reward: 62.68 - Last mean reward per episode: 8.81\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 747      |\n",
            "|    ep_rew_mean        | 8.81     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1230     |\n",
            "|    iterations         | 10200    |\n",
            "|    time_elapsed       | 331      |\n",
            "|    total_timesteps    | 408000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.511   |\n",
            "|    explained_variance | 0.916    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 10199    |\n",
            "|    policy_loss        | -0.615   |\n",
            "|    value_loss         | 30.1     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 670      |\n",
            "|    ep_rew_mean        | 2.64     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1231     |\n",
            "|    iterations         | 10300    |\n",
            "|    time_elapsed       | 334      |\n",
            "|    total_timesteps    | 412000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.502   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 10299    |\n",
            "|    policy_loss        | -0.136   |\n",
            "|    value_loss         | 6.99     |\n",
            "------------------------------------\n",
            "Num timesteps: 416000\n",
            "Best mean reward: 62.68 - Last mean reward per episode: 2.96\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 614      |\n",
            "|    ep_rew_mean        | 2.96     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1231     |\n",
            "|    iterations         | 10400    |\n",
            "|    time_elapsed       | 337      |\n",
            "|    total_timesteps    | 416000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.514   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 10399    |\n",
            "|    policy_loss        | -0.0674  |\n",
            "|    value_loss         | 14.8     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 460      |\n",
            "|    ep_rew_mean        | 4.77     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1232     |\n",
            "|    iterations         | 10500    |\n",
            "|    time_elapsed       | 340      |\n",
            "|    total_timesteps    | 420000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.452   |\n",
            "|    explained_variance | 0.844    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 10499    |\n",
            "|    policy_loss        | 0.21     |\n",
            "|    value_loss         | 111      |\n",
            "------------------------------------\n",
            "Num timesteps: 424000\n",
            "Best mean reward: 62.68 - Last mean reward per episode: -6.55\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 250      |\n",
            "|    ep_rew_mean        | -6.55    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1238     |\n",
            "|    iterations         | 10600    |\n",
            "|    time_elapsed       | 342      |\n",
            "|    total_timesteps    | 424000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.482   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 10599    |\n",
            "|    policy_loss        | 0.59     |\n",
            "|    value_loss         | 6.06     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 143      |\n",
            "|    ep_rew_mean        | -12.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1245     |\n",
            "|    iterations         | 10700    |\n",
            "|    time_elapsed       | 343      |\n",
            "|    total_timesteps    | 428000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.539   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 10699    |\n",
            "|    policy_loss        | 0.384    |\n",
            "|    value_loss         | 8.22     |\n",
            "------------------------------------\n",
            "Num timesteps: 432000\n",
            "Best mean reward: 62.68 - Last mean reward per episode: -12.63\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 97.3     |\n",
            "|    ep_rew_mean        | -12.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1251     |\n",
            "|    iterations         | 10800    |\n",
            "|    time_elapsed       | 345      |\n",
            "|    total_timesteps    | 432000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.25    |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 10799    |\n",
            "|    policy_loss        | 0.0706   |\n",
            "|    value_loss         | 2.86     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 94.4     |\n",
            "|    ep_rew_mean        | -16      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1258     |\n",
            "|    iterations         | 10900    |\n",
            "|    time_elapsed       | 346      |\n",
            "|    total_timesteps    | 436000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.254   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 10899    |\n",
            "|    policy_loss        | -0.139   |\n",
            "|    value_loss         | 6.42     |\n",
            "------------------------------------\n",
            "Num timesteps: 440000\n",
            "Best mean reward: 62.68 - Last mean reward per episode: -18.19\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 93.7     |\n",
            "|    ep_rew_mean        | -18.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1265     |\n",
            "|    iterations         | 11000    |\n",
            "|    time_elapsed       | 347      |\n",
            "|    total_timesteps    | 440000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.29    |\n",
            "|    explained_variance | 0.874    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 10999    |\n",
            "|    policy_loss        | -3.22    |\n",
            "|    value_loss         | 299      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 94       |\n",
            "|    ep_rew_mean        | -13.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1272     |\n",
            "|    iterations         | 11100    |\n",
            "|    time_elapsed       | 349      |\n",
            "|    total_timesteps    | 444000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.324   |\n",
            "|    explained_variance | 0.943    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 11099    |\n",
            "|    policy_loss        | 0.554    |\n",
            "|    value_loss         | 26.3     |\n",
            "------------------------------------\n",
            "Num timesteps: 448000\n",
            "Best mean reward: 62.68 - Last mean reward per episode: -8.28\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 98.3     |\n",
            "|    ep_rew_mean        | -8.28    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1278     |\n",
            "|    iterations         | 11200    |\n",
            "|    time_elapsed       | 350      |\n",
            "|    total_timesteps    | 448000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.392   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 11199    |\n",
            "|    policy_loss        | 0.288    |\n",
            "|    value_loss         | 16.9     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 113      |\n",
            "|    ep_rew_mean        | 19       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1284     |\n",
            "|    iterations         | 11300    |\n",
            "|    time_elapsed       | 352      |\n",
            "|    total_timesteps    | 452000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.268   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 11299    |\n",
            "|    policy_loss        | 0.196    |\n",
            "|    value_loss         | 4.85     |\n",
            "------------------------------------\n",
            "Num timesteps: 456000\n",
            "Best mean reward: 62.68 - Last mean reward per episode: 35.25\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 124      |\n",
            "|    ep_rew_mean        | 35.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1289     |\n",
            "|    iterations         | 11400    |\n",
            "|    time_elapsed       | 353      |\n",
            "|    total_timesteps    | 456000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.373   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 11399    |\n",
            "|    policy_loss        | 0.478    |\n",
            "|    value_loss         | 11.4     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 144      |\n",
            "|    ep_rew_mean        | 58.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1294     |\n",
            "|    iterations         | 11500    |\n",
            "|    time_elapsed       | 355      |\n",
            "|    total_timesteps    | 460000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.471   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 11499    |\n",
            "|    policy_loss        | -0.672   |\n",
            "|    value_loss         | 13.5     |\n",
            "------------------------------------\n",
            "Num timesteps: 464000\n",
            "Best mean reward: 62.68 - Last mean reward per episode: 77.48\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 158      |\n",
            "|    ep_rew_mean        | 77.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1297     |\n",
            "|    iterations         | 11600    |\n",
            "|    time_elapsed       | 357      |\n",
            "|    total_timesteps    | 464000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.415   |\n",
            "|    explained_variance | 0.881    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 11599    |\n",
            "|    policy_loss        | -0.23    |\n",
            "|    value_loss         | 256      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 171      |\n",
            "|    ep_rew_mean        | 79.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1299     |\n",
            "|    iterations         | 11700    |\n",
            "|    time_elapsed       | 360      |\n",
            "|    total_timesteps    | 468000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.682   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 11699    |\n",
            "|    policy_loss        | 0.244    |\n",
            "|    value_loss         | 2.69     |\n",
            "------------------------------------\n",
            "Num timesteps: 472000\n",
            "Best mean reward: 77.48 - Last mean reward per episode: 79.92\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 197      |\n",
            "|    ep_rew_mean        | 79.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1298     |\n",
            "|    iterations         | 11800    |\n",
            "|    time_elapsed       | 363      |\n",
            "|    total_timesteps    | 472000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.536   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 11799    |\n",
            "|    policy_loss        | 0.188    |\n",
            "|    value_loss         | 11.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 221      |\n",
            "|    ep_rew_mean        | 78.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1295     |\n",
            "|    iterations         | 11900    |\n",
            "|    time_elapsed       | 367      |\n",
            "|    total_timesteps    | 476000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.688   |\n",
            "|    explained_variance | 0.703    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 11899    |\n",
            "|    policy_loss        | 0.661    |\n",
            "|    value_loss         | 4.86     |\n",
            "------------------------------------\n",
            "Num timesteps: 480000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: 73.81\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 254      |\n",
            "|    ep_rew_mean        | 73.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1290     |\n",
            "|    iterations         | 12000    |\n",
            "|    time_elapsed       | 371      |\n",
            "|    total_timesteps    | 480000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.693   |\n",
            "|    explained_variance | 0.958    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 11999    |\n",
            "|    policy_loss        | -0.247   |\n",
            "|    value_loss         | 3.5      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 286      |\n",
            "|    ep_rew_mean        | 67.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1283     |\n",
            "|    iterations         | 12100    |\n",
            "|    time_elapsed       | 377      |\n",
            "|    total_timesteps    | 484000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.79    |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 12099    |\n",
            "|    policy_loss        | -0.445   |\n",
            "|    value_loss         | 4.28     |\n",
            "------------------------------------\n",
            "Num timesteps: 488000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: 66.17\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 319      |\n",
            "|    ep_rew_mean        | 66.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1275     |\n",
            "|    iterations         | 12200    |\n",
            "|    time_elapsed       | 382      |\n",
            "|    total_timesteps    | 488000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.734   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 12199    |\n",
            "|    policy_loss        | -0.564   |\n",
            "|    value_loss         | 2.43     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 352      |\n",
            "|    ep_rew_mean        | 59.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1267     |\n",
            "|    iterations         | 12300    |\n",
            "|    time_elapsed       | 388      |\n",
            "|    total_timesteps    | 492000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.725   |\n",
            "|    explained_variance | 0.897    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 12299    |\n",
            "|    policy_loss        | -0.986   |\n",
            "|    value_loss         | 8.2      |\n",
            "------------------------------------\n",
            "Num timesteps: 496000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: 54.06\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 387      |\n",
            "|    ep_rew_mean        | 54.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1259     |\n",
            "|    iterations         | 12400    |\n",
            "|    time_elapsed       | 393      |\n",
            "|    total_timesteps    | 496000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.689   |\n",
            "|    explained_variance | 0.917    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 12399    |\n",
            "|    policy_loss        | 0.52     |\n",
            "|    value_loss         | 4.19     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 422      |\n",
            "|    ep_rew_mean        | 52.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1251     |\n",
            "|    iterations         | 12500    |\n",
            "|    time_elapsed       | 399      |\n",
            "|    total_timesteps    | 500000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.638   |\n",
            "|    explained_variance | 0.81     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 12499    |\n",
            "|    policy_loss        | -0.297   |\n",
            "|    value_loss         | 1.93     |\n",
            "------------------------------------\n",
            "Num timesteps: 504000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: 48.33\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 458      |\n",
            "|    ep_rew_mean        | 48.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1242     |\n",
            "|    iterations         | 12600    |\n",
            "|    time_elapsed       | 405      |\n",
            "|    total_timesteps    | 504000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.625   |\n",
            "|    explained_variance | 0.866    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 12599    |\n",
            "|    policy_loss        | -0.0383  |\n",
            "|    value_loss         | 3.54     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 510      |\n",
            "|    ep_rew_mean        | 35       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1236     |\n",
            "|    iterations         | 12700    |\n",
            "|    time_elapsed       | 410      |\n",
            "|    total_timesteps    | 508000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.636   |\n",
            "|    explained_variance | 0.93     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 12699    |\n",
            "|    policy_loss        | 0.0489   |\n",
            "|    value_loss         | 1.37     |\n",
            "------------------------------------\n",
            "Num timesteps: 512000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: 9.60\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 528      |\n",
            "|    ep_rew_mean        | 9.6      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1238     |\n",
            "|    iterations         | 12800    |\n",
            "|    time_elapsed       | 413      |\n",
            "|    total_timesteps    | 512000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.531   |\n",
            "|    explained_variance | 0.329    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 12799    |\n",
            "|    policy_loss        | -2.27    |\n",
            "|    value_loss         | 605      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 544      |\n",
            "|    ep_rew_mean        | -13.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1240     |\n",
            "|    iterations         | 12900    |\n",
            "|    time_elapsed       | 415      |\n",
            "|    total_timesteps    | 516000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.607   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 12899    |\n",
            "|    policy_loss        | -0.108   |\n",
            "|    value_loss         | 1.73     |\n",
            "------------------------------------\n",
            "Num timesteps: 520000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -45.44\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 541      |\n",
            "|    ep_rew_mean        | -45.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1243     |\n",
            "|    iterations         | 13000    |\n",
            "|    time_elapsed       | 418      |\n",
            "|    total_timesteps    | 520000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.515   |\n",
            "|    explained_variance | 0.959    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 12999    |\n",
            "|    policy_loss        | 0.138    |\n",
            "|    value_loss         | 30.6     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 489      |\n",
            "|    ep_rew_mean        | -80.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1247     |\n",
            "|    iterations         | 13100    |\n",
            "|    time_elapsed       | 420      |\n",
            "|    total_timesteps    | 524000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.58    |\n",
            "|    explained_variance | 0.964    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 13099    |\n",
            "|    policy_loss        | -1.03    |\n",
            "|    value_loss         | 16.1     |\n",
            "------------------------------------\n",
            "Num timesteps: 528000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -97.63\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 375      |\n",
            "|    ep_rew_mean        | -97.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1250     |\n",
            "|    iterations         | 13200    |\n",
            "|    time_elapsed       | 422      |\n",
            "|    total_timesteps    | 528000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.686   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 13199    |\n",
            "|    policy_loss        | -0.0652  |\n",
            "|    value_loss         | 4.25     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 267      |\n",
            "|    ep_rew_mean        | -101     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1252     |\n",
            "|    iterations         | 13300    |\n",
            "|    time_elapsed       | 424      |\n",
            "|    total_timesteps    | 532000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.561   |\n",
            "|    explained_variance | 0.89     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 13299    |\n",
            "|    policy_loss        | -0.547   |\n",
            "|    value_loss         | 97.9     |\n",
            "------------------------------------\n",
            "Num timesteps: 536000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -103.71\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 246      |\n",
            "|    ep_rew_mean        | -104     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1254     |\n",
            "|    iterations         | 13400    |\n",
            "|    time_elapsed       | 427      |\n",
            "|    total_timesteps    | 536000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.521   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 13399    |\n",
            "|    policy_loss        | 0.382    |\n",
            "|    value_loss         | 1.64     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 259      |\n",
            "|    ep_rew_mean        | -103     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1250     |\n",
            "|    iterations         | 13500    |\n",
            "|    time_elapsed       | 431      |\n",
            "|    total_timesteps    | 540000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.515   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 13499    |\n",
            "|    policy_loss        | -0.851   |\n",
            "|    value_loss         | 3.26     |\n",
            "------------------------------------\n",
            "Num timesteps: 544000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -103.42\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 271      |\n",
            "|    ep_rew_mean        | -103     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1246     |\n",
            "|    iterations         | 13600    |\n",
            "|    time_elapsed       | 436      |\n",
            "|    total_timesteps    | 544000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.587   |\n",
            "|    explained_variance | 0.721    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 13599    |\n",
            "|    policy_loss        | -0.838   |\n",
            "|    value_loss         | 6.03     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 310      |\n",
            "|    ep_rew_mean        | -104     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1238     |\n",
            "|    iterations         | 13700    |\n",
            "|    time_elapsed       | 442      |\n",
            "|    total_timesteps    | 548000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.621   |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 13699    |\n",
            "|    policy_loss        | 0.238    |\n",
            "|    value_loss         | 4.21     |\n",
            "------------------------------------\n",
            "Num timesteps: 552000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -106.37\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 335      |\n",
            "|    ep_rew_mean        | -106     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1236     |\n",
            "|    iterations         | 13800    |\n",
            "|    time_elapsed       | 446      |\n",
            "|    total_timesteps    | 552000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.511   |\n",
            "|    explained_variance | 0.939    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 13799    |\n",
            "|    policy_loss        | -0.296   |\n",
            "|    value_loss         | 2.07     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 365      |\n",
            "|    ep_rew_mean        | -107     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1232     |\n",
            "|    iterations         | 13900    |\n",
            "|    time_elapsed       | 451      |\n",
            "|    total_timesteps    | 556000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.518   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 13899    |\n",
            "|    policy_loss        | -0.0608  |\n",
            "|    value_loss         | 2.09     |\n",
            "------------------------------------\n",
            "Num timesteps: 560000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -107.06\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 397      |\n",
            "|    ep_rew_mean        | -107     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1228     |\n",
            "|    iterations         | 14000    |\n",
            "|    time_elapsed       | 455      |\n",
            "|    total_timesteps    | 560000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.509   |\n",
            "|    explained_variance | 0.85     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 13999    |\n",
            "|    policy_loss        | -0.598   |\n",
            "|    value_loss         | 3.32     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 431      |\n",
            "|    ep_rew_mean        | -105     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1224     |\n",
            "|    iterations         | 14100    |\n",
            "|    time_elapsed       | 460      |\n",
            "|    total_timesteps    | 564000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.498   |\n",
            "|    explained_variance | 0.956    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 14099    |\n",
            "|    policy_loss        | -0.221   |\n",
            "|    value_loss         | 4.41     |\n",
            "------------------------------------\n",
            "Num timesteps: 568000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -105.80\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 445      |\n",
            "|    ep_rew_mean        | -106     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1220     |\n",
            "|    iterations         | 14200    |\n",
            "|    time_elapsed       | 465      |\n",
            "|    total_timesteps    | 568000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.499   |\n",
            "|    explained_variance | 0.35     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 14199    |\n",
            "|    policy_loss        | -0.0769  |\n",
            "|    value_loss         | 3        |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 492      |\n",
            "|    ep_rew_mean        | -104     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1213     |\n",
            "|    iterations         | 14300    |\n",
            "|    time_elapsed       | 471      |\n",
            "|    total_timesteps    | 572000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.434   |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 14299    |\n",
            "|    policy_loss        | -0.215   |\n",
            "|    value_loss         | 2.12     |\n",
            "------------------------------------\n",
            "Num timesteps: 576000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -101.89\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 509      |\n",
            "|    ep_rew_mean        | -102     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1212     |\n",
            "|    iterations         | 14400    |\n",
            "|    time_elapsed       | 475      |\n",
            "|    total_timesteps    | 576000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.538   |\n",
            "|    explained_variance | 0.924    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 14399    |\n",
            "|    policy_loss        | -0.0814  |\n",
            "|    value_loss         | 1.49     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 548      |\n",
            "|    ep_rew_mean        | -99.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1204     |\n",
            "|    iterations         | 14500    |\n",
            "|    time_elapsed       | 481      |\n",
            "|    total_timesteps    | 580000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.443   |\n",
            "|    explained_variance | 0.978    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 14499    |\n",
            "|    policy_loss        | -0.142   |\n",
            "|    value_loss         | 1.38     |\n",
            "------------------------------------\n",
            "Num timesteps: 584000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -96.62\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 572      |\n",
            "|    ep_rew_mean        | -96.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1201     |\n",
            "|    iterations         | 14600    |\n",
            "|    time_elapsed       | 486      |\n",
            "|    total_timesteps    | 584000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.526   |\n",
            "|    explained_variance | 0.962    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 14599    |\n",
            "|    policy_loss        | 0.0438   |\n",
            "|    value_loss         | 1.79     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 611      |\n",
            "|    ep_rew_mean        | -93.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 14700    |\n",
            "|    time_elapsed       | 492      |\n",
            "|    total_timesteps    | 588000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.538   |\n",
            "|    explained_variance | 0.942    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 14699    |\n",
            "|    policy_loss        | -0.14    |\n",
            "|    value_loss         | 3.69     |\n",
            "------------------------------------\n",
            "Num timesteps: 592000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -91.03\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 633      |\n",
            "|    ep_rew_mean        | -91      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 14800    |\n",
            "|    time_elapsed       | 496      |\n",
            "|    total_timesteps    | 592000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.469   |\n",
            "|    explained_variance | 0.541    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 14799    |\n",
            "|    policy_loss        | -0.477   |\n",
            "|    value_loss         | 1.66     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 673      |\n",
            "|    ep_rew_mean        | -87.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1184     |\n",
            "|    iterations         | 14900    |\n",
            "|    time_elapsed       | 503      |\n",
            "|    total_timesteps    | 596000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.435   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 14899    |\n",
            "|    policy_loss        | 0.106    |\n",
            "|    value_loss         | 1.25     |\n",
            "------------------------------------\n",
            "Num timesteps: 600000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -86.49\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 703      |\n",
            "|    ep_rew_mean        | -86.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1183     |\n",
            "|    iterations         | 15000    |\n",
            "|    time_elapsed       | 506      |\n",
            "|    total_timesteps    | 600000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.41    |\n",
            "|    explained_variance | 0.947    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 14999    |\n",
            "|    policy_loss        | 0.0344   |\n",
            "|    value_loss         | 2.58     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 734      |\n",
            "|    ep_rew_mean        | -85.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1176     |\n",
            "|    iterations         | 15100    |\n",
            "|    time_elapsed       | 513      |\n",
            "|    total_timesteps    | 604000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.509   |\n",
            "|    explained_variance | 0.948    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 15099    |\n",
            "|    policy_loss        | -0.163   |\n",
            "|    value_loss         | 45.3     |\n",
            "------------------------------------\n",
            "Num timesteps: 608000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -84.55\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 763      |\n",
            "|    ep_rew_mean        | -84.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1174     |\n",
            "|    iterations         | 15200    |\n",
            "|    time_elapsed       | 517      |\n",
            "|    total_timesteps    | 608000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.508   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 15199    |\n",
            "|    policy_loss        | -0.116   |\n",
            "|    value_loss         | 1.84     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 786      |\n",
            "|    ep_rew_mean        | -82.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1167     |\n",
            "|    iterations         | 15300    |\n",
            "|    time_elapsed       | 524      |\n",
            "|    total_timesteps    | 612000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.495   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 15299    |\n",
            "|    policy_loss        | 0.238    |\n",
            "|    value_loss         | 2.66     |\n",
            "------------------------------------\n",
            "Num timesteps: 616000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -79.04\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 825      |\n",
            "|    ep_rew_mean        | -79      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1165     |\n",
            "|    iterations         | 15400    |\n",
            "|    time_elapsed       | 528      |\n",
            "|    total_timesteps    | 616000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.492   |\n",
            "|    explained_variance | 0.898    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 15399    |\n",
            "|    policy_loss        | 0.0262   |\n",
            "|    value_loss         | 3.22     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 847      |\n",
            "|    ep_rew_mean        | -77.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1156     |\n",
            "|    iterations         | 15500    |\n",
            "|    time_elapsed       | 536      |\n",
            "|    total_timesteps    | 620000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.488   |\n",
            "|    explained_variance | 0.945    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 15499    |\n",
            "|    policy_loss        | 0.339    |\n",
            "|    value_loss         | 5.1      |\n",
            "------------------------------------\n",
            "Num timesteps: 624000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -75.06\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 886      |\n",
            "|    ep_rew_mean        | -75.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1151     |\n",
            "|    iterations         | 15600    |\n",
            "|    time_elapsed       | 541      |\n",
            "|    total_timesteps    | 624000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.435   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 15599    |\n",
            "|    policy_loss        | -0.344   |\n",
            "|    value_loss         | 1.99     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 907      |\n",
            "|    ep_rew_mean        | -73.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1146     |\n",
            "|    iterations         | 15700    |\n",
            "|    time_elapsed       | 547      |\n",
            "|    total_timesteps    | 628000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.582   |\n",
            "|    explained_variance | 0.98     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 15699    |\n",
            "|    policy_loss        | 0.0451   |\n",
            "|    value_loss         | 3.37     |\n",
            "------------------------------------\n",
            "Num timesteps: 632000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -73.02\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 939      |\n",
            "|    ep_rew_mean        | -73      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1143     |\n",
            "|    iterations         | 15800    |\n",
            "|    time_elapsed       | 552      |\n",
            "|    total_timesteps    | 632000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.559   |\n",
            "|    explained_variance | 0.967    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 15799    |\n",
            "|    policy_loss        | -0.521   |\n",
            "|    value_loss         | 2.96     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 955      |\n",
            "|    ep_rew_mean        | -71.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1139     |\n",
            "|    iterations         | 15900    |\n",
            "|    time_elapsed       | 558      |\n",
            "|    total_timesteps    | 636000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.551   |\n",
            "|    explained_variance | 0.967    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 15899    |\n",
            "|    policy_loss        | -0.421   |\n",
            "|    value_loss         | 5.86     |\n",
            "------------------------------------\n",
            "Num timesteps: 640000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -73.34\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 972      |\n",
            "|    ep_rew_mean        | -73.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1136     |\n",
            "|    iterations         | 16000    |\n",
            "|    time_elapsed       | 563      |\n",
            "|    total_timesteps    | 640000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.41    |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 15999    |\n",
            "|    policy_loss        | -0.516   |\n",
            "|    value_loss         | 1.47     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 972      |\n",
            "|    ep_rew_mean        | -71.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1129     |\n",
            "|    iterations         | 16100    |\n",
            "|    time_elapsed       | 570      |\n",
            "|    total_timesteps    | 644000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.516   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 16099    |\n",
            "|    policy_loss        | -0.0975  |\n",
            "|    value_loss         | 2.62     |\n",
            "------------------------------------\n",
            "Num timesteps: 648000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -69.08\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 976      |\n",
            "|    ep_rew_mean        | -69.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1126     |\n",
            "|    iterations         | 16200    |\n",
            "|    time_elapsed       | 575      |\n",
            "|    total_timesteps    | 648000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.476   |\n",
            "|    explained_variance | 0.971    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 16199    |\n",
            "|    policy_loss        | -0.191   |\n",
            "|    value_loss         | 2.07     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 976      |\n",
            "|    ep_rew_mean        | -66.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1119     |\n",
            "|    iterations         | 16300    |\n",
            "|    time_elapsed       | 582      |\n",
            "|    total_timesteps    | 652000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.357   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 16299    |\n",
            "|    policy_loss        | -0.154   |\n",
            "|    value_loss         | 1.07     |\n",
            "------------------------------------\n",
            "Num timesteps: 656000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -63.16\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 974      |\n",
            "|    ep_rew_mean        | -63.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 16400    |\n",
            "|    time_elapsed       | 587      |\n",
            "|    total_timesteps    | 656000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.457   |\n",
            "|    explained_variance | 0.881    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 16399    |\n",
            "|    policy_loss        | -0.0401  |\n",
            "|    value_loss         | 2.75     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 981      |\n",
            "|    ep_rew_mean        | -61.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 16500    |\n",
            "|    time_elapsed       | 594      |\n",
            "|    total_timesteps    | 660000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.405   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 16499    |\n",
            "|    policy_loss        | 0.0507   |\n",
            "|    value_loss         | 1.56     |\n",
            "------------------------------------\n",
            "Num timesteps: 664000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -56.20\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 978      |\n",
            "|    ep_rew_mean        | -56.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1107     |\n",
            "|    iterations         | 16600    |\n",
            "|    time_elapsed       | 599      |\n",
            "|    total_timesteps    | 664000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.329   |\n",
            "|    explained_variance | 0.8      |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 16599    |\n",
            "|    policy_loss        | 0.258    |\n",
            "|    value_loss         | 3.22     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 978      |\n",
            "|    ep_rew_mean        | -56.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 16700    |\n",
            "|    time_elapsed       | 607      |\n",
            "|    total_timesteps    | 668000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.387   |\n",
            "|    explained_variance | 0.827    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 16699    |\n",
            "|    policy_loss        | 0.0938   |\n",
            "|    value_loss         | 3.2      |\n",
            "------------------------------------\n",
            "Num timesteps: 672000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -55.48\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 983      |\n",
            "|    ep_rew_mean        | -55.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 16800    |\n",
            "|    time_elapsed       | 611      |\n",
            "|    total_timesteps    | 672000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.44    |\n",
            "|    explained_variance | 0.878    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 16799    |\n",
            "|    policy_loss        | -0.723   |\n",
            "|    value_loss         | 2.74     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 983      |\n",
            "|    ep_rew_mean        | -56.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1093     |\n",
            "|    iterations         | 16900    |\n",
            "|    time_elapsed       | 618      |\n",
            "|    total_timesteps    | 676000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.42    |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 16899    |\n",
            "|    policy_loss        | -0.168   |\n",
            "|    value_loss         | 1.66     |\n",
            "------------------------------------\n",
            "Num timesteps: 680000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -55.93\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 983      |\n",
            "|    ep_rew_mean        | -55.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1092     |\n",
            "|    iterations         | 17000    |\n",
            "|    time_elapsed       | 622      |\n",
            "|    total_timesteps    | 680000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.342   |\n",
            "|    explained_variance | 0.93     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 16999    |\n",
            "|    policy_loss        | -0.66    |\n",
            "|    value_loss         | 1.99     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 983      |\n",
            "|    ep_rew_mean        | -55.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1086     |\n",
            "|    iterations         | 17100    |\n",
            "|    time_elapsed       | 629      |\n",
            "|    total_timesteps    | 684000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.475   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 17099    |\n",
            "|    policy_loss        | -0.11    |\n",
            "|    value_loss         | 1.05     |\n",
            "------------------------------------\n",
            "Num timesteps: 688000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -53.91\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 983      |\n",
            "|    ep_rew_mean        | -53.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1084     |\n",
            "|    iterations         | 17200    |\n",
            "|    time_elapsed       | 634      |\n",
            "|    total_timesteps    | 688000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.418   |\n",
            "|    explained_variance | 0.917    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 17199    |\n",
            "|    policy_loss        | -0.222   |\n",
            "|    value_loss         | 0.854    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 983      |\n",
            "|    ep_rew_mean        | -53.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1079     |\n",
            "|    iterations         | 17300    |\n",
            "|    time_elapsed       | 640      |\n",
            "|    total_timesteps    | 692000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.478   |\n",
            "|    explained_variance | 0.653    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 17299    |\n",
            "|    policy_loss        | -0.934   |\n",
            "|    value_loss         | 4.34     |\n",
            "------------------------------------\n",
            "Num timesteps: 696000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -54.96\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 983      |\n",
            "|    ep_rew_mean        | -55      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1078     |\n",
            "|    iterations         | 17400    |\n",
            "|    time_elapsed       | 645      |\n",
            "|    total_timesteps    | 696000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.496   |\n",
            "|    explained_variance | 0.759    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 17399    |\n",
            "|    policy_loss        | 0.016    |\n",
            "|    value_loss         | 1.75     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 986      |\n",
            "|    ep_rew_mean        | -54.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1074     |\n",
            "|    iterations         | 17500    |\n",
            "|    time_elapsed       | 651      |\n",
            "|    total_timesteps    | 700000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.416   |\n",
            "|    explained_variance | 0.908    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 17499    |\n",
            "|    policy_loss        | -0.137   |\n",
            "|    value_loss         | 0.663    |\n",
            "------------------------------------\n",
            "Num timesteps: 704000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -52.67\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 986      |\n",
            "|    ep_rew_mean        | -52.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1073     |\n",
            "|    iterations         | 17600    |\n",
            "|    time_elapsed       | 655      |\n",
            "|    total_timesteps    | 704000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.429   |\n",
            "|    explained_variance | 0.777    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 17599    |\n",
            "|    policy_loss        | 0.195    |\n",
            "|    value_loss         | 1.65     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 986      |\n",
            "|    ep_rew_mean        | -53.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1069     |\n",
            "|    iterations         | 17700    |\n",
            "|    time_elapsed       | 661      |\n",
            "|    total_timesteps    | 708000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.489   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 17699    |\n",
            "|    policy_loss        | -0.237   |\n",
            "|    value_loss         | 0.909    |\n",
            "------------------------------------\n",
            "Num timesteps: 712000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -54.18\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 990      |\n",
            "|    ep_rew_mean        | -54.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1068     |\n",
            "|    iterations         | 17800    |\n",
            "|    time_elapsed       | 666      |\n",
            "|    total_timesteps    | 712000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.407   |\n",
            "|    explained_variance | 0.929    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 17799    |\n",
            "|    policy_loss        | 0.237    |\n",
            "|    value_loss         | 1.51     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 990      |\n",
            "|    ep_rew_mean        | -53.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1063     |\n",
            "|    iterations         | 17900    |\n",
            "|    time_elapsed       | 673      |\n",
            "|    total_timesteps    | 716000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.57    |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 17899    |\n",
            "|    policy_loss        | -0.109   |\n",
            "|    value_loss         | 0.486    |\n",
            "------------------------------------\n",
            "Num timesteps: 720000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -52.78\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 990      |\n",
            "|    ep_rew_mean        | -52.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1062     |\n",
            "|    iterations         | 18000    |\n",
            "|    time_elapsed       | 677      |\n",
            "|    total_timesteps    | 720000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.527   |\n",
            "|    explained_variance | 0.976    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 17999    |\n",
            "|    policy_loss        | -0.206   |\n",
            "|    value_loss         | 0.578    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 990      |\n",
            "|    ep_rew_mean        | -53      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1057     |\n",
            "|    iterations         | 18100    |\n",
            "|    time_elapsed       | 684      |\n",
            "|    total_timesteps    | 724000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.488   |\n",
            "|    explained_variance | 0.947    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 18099    |\n",
            "|    policy_loss        | 0.0439   |\n",
            "|    value_loss         | 1.91     |\n",
            "------------------------------------\n",
            "Num timesteps: 728000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -51.96\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 990      |\n",
            "|    ep_rew_mean        | -52      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1055     |\n",
            "|    iterations         | 18200    |\n",
            "|    time_elapsed       | 689      |\n",
            "|    total_timesteps    | 728000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.532   |\n",
            "|    explained_variance | 0.97     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 18199    |\n",
            "|    policy_loss        | -0.0374  |\n",
            "|    value_loss         | 0.771    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 990      |\n",
            "|    ep_rew_mean        | -53.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1051     |\n",
            "|    iterations         | 18300    |\n",
            "|    time_elapsed       | 695      |\n",
            "|    total_timesteps    | 732000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.428   |\n",
            "|    explained_variance | 0.71     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 18299    |\n",
            "|    policy_loss        | 0.212    |\n",
            "|    value_loss         | 2.37     |\n",
            "------------------------------------\n",
            "Num timesteps: 736000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -55.98\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 990      |\n",
            "|    ep_rew_mean        | -56      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1051     |\n",
            "|    iterations         | 18400    |\n",
            "|    time_elapsed       | 700      |\n",
            "|    total_timesteps    | 736000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.494   |\n",
            "|    explained_variance | 0.875    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 18399    |\n",
            "|    policy_loss        | -0.227   |\n",
            "|    value_loss         | 3.22     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 990      |\n",
            "|    ep_rew_mean        | -56.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1046     |\n",
            "|    iterations         | 18500    |\n",
            "|    time_elapsed       | 706      |\n",
            "|    total_timesteps    | 740000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.437   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 18499    |\n",
            "|    policy_loss        | -0.0954  |\n",
            "|    value_loss         | 1.46     |\n",
            "------------------------------------\n",
            "Num timesteps: 744000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -56.68\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 990      |\n",
            "|    ep_rew_mean        | -56.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1045     |\n",
            "|    iterations         | 18600    |\n",
            "|    time_elapsed       | 711      |\n",
            "|    total_timesteps    | 744000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.433   |\n",
            "|    explained_variance | 0.963    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 18599    |\n",
            "|    policy_loss        | 0.0116   |\n",
            "|    value_loss         | 0.978    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 990      |\n",
            "|    ep_rew_mean        | -55.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1041     |\n",
            "|    iterations         | 18700    |\n",
            "|    time_elapsed       | 717      |\n",
            "|    total_timesteps    | 748000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.555   |\n",
            "|    explained_variance | 0.95     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 18699    |\n",
            "|    policy_loss        | -0.401   |\n",
            "|    value_loss         | 1.81     |\n",
            "------------------------------------\n",
            "Num timesteps: 752000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -56.01\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 990      |\n",
            "|    ep_rew_mean        | -56      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1040     |\n",
            "|    iterations         | 18800    |\n",
            "|    time_elapsed       | 722      |\n",
            "|    total_timesteps    | 752000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.541   |\n",
            "|    explained_variance | 0.685    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 18799    |\n",
            "|    policy_loss        | -0.41    |\n",
            "|    value_loss         | 2.72     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 992      |\n",
            "|    ep_rew_mean        | -58.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1035     |\n",
            "|    iterations         | 18900    |\n",
            "|    time_elapsed       | 730      |\n",
            "|    total_timesteps    | 756000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.421   |\n",
            "|    explained_variance | 0.934    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 18899    |\n",
            "|    policy_loss        | -0.215   |\n",
            "|    value_loss         | 1.56     |\n",
            "------------------------------------\n",
            "Num timesteps: 760000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -60.24\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 992      |\n",
            "|    ep_rew_mean        | -60.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1033     |\n",
            "|    iterations         | 19000    |\n",
            "|    time_elapsed       | 735      |\n",
            "|    total_timesteps    | 760000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.471   |\n",
            "|    explained_variance | 0.975    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 18999    |\n",
            "|    policy_loss        | -0.182   |\n",
            "|    value_loss         | 0.8      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 992      |\n",
            "|    ep_rew_mean        | -60      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1028     |\n",
            "|    iterations         | 19100    |\n",
            "|    time_elapsed       | 742      |\n",
            "|    total_timesteps    | 764000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.478   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 19099    |\n",
            "|    policy_loss        | -0.0197  |\n",
            "|    value_loss         | 0.847    |\n",
            "------------------------------------\n",
            "Num timesteps: 768000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -60.06\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 1e+03    |\n",
            "|    ep_rew_mean        | -60.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1028     |\n",
            "|    iterations         | 19200    |\n",
            "|    time_elapsed       | 746      |\n",
            "|    total_timesteps    | 768000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.49    |\n",
            "|    explained_variance | 0.968    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 19199    |\n",
            "|    policy_loss        | -0.126   |\n",
            "|    value_loss         | 0.976    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 1e+03    |\n",
            "|    ep_rew_mean        | -59.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1026     |\n",
            "|    iterations         | 19300    |\n",
            "|    time_elapsed       | 752      |\n",
            "|    total_timesteps    | 772000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.406   |\n",
            "|    explained_variance | 0.957    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 19299    |\n",
            "|    policy_loss        | -0.23    |\n",
            "|    value_loss         | 1.3      |\n",
            "------------------------------------\n",
            "Num timesteps: 776000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -58.82\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 998      |\n",
            "|    ep_rew_mean        | -58.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1025     |\n",
            "|    iterations         | 19400    |\n",
            "|    time_elapsed       | 756      |\n",
            "|    total_timesteps    | 776000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.486   |\n",
            "|    explained_variance | 0.912    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 19399    |\n",
            "|    policy_loss        | 0.454    |\n",
            "|    value_loss         | 2.54     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 998      |\n",
            "|    ep_rew_mean        | -59.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1022     |\n",
            "|    iterations         | 19500    |\n",
            "|    time_elapsed       | 763      |\n",
            "|    total_timesteps    | 780000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.438   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 19499    |\n",
            "|    policy_loss        | -0.109   |\n",
            "|    value_loss         | 2.29     |\n",
            "------------------------------------\n",
            "Num timesteps: 784000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -60.31\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 998      |\n",
            "|    ep_rew_mean        | -60.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1021     |\n",
            "|    iterations         | 19600    |\n",
            "|    time_elapsed       | 767      |\n",
            "|    total_timesteps    | 784000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.364   |\n",
            "|    explained_variance | 0.936    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 19599    |\n",
            "|    policy_loss        | 0.0318   |\n",
            "|    value_loss         | 1.97     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 998      |\n",
            "|    ep_rew_mean        | -61      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1018     |\n",
            "|    iterations         | 19700    |\n",
            "|    time_elapsed       | 773      |\n",
            "|    total_timesteps    | 788000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.418   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 19699    |\n",
            "|    policy_loss        | -0.5     |\n",
            "|    value_loss         | 1.89     |\n",
            "------------------------------------\n",
            "Num timesteps: 792000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -62.40\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 998      |\n",
            "|    ep_rew_mean        | -62.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1017     |\n",
            "|    iterations         | 19800    |\n",
            "|    time_elapsed       | 778      |\n",
            "|    total_timesteps    | 792000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.444   |\n",
            "|    explained_variance | 0.953    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 19799    |\n",
            "|    policy_loss        | 0.193    |\n",
            "|    value_loss         | 1.48     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 998      |\n",
            "|    ep_rew_mean        | -61.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1015     |\n",
            "|    iterations         | 19900    |\n",
            "|    time_elapsed       | 783      |\n",
            "|    total_timesteps    | 796000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.53    |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 19899    |\n",
            "|    policy_loss        | 0.0569   |\n",
            "|    value_loss         | 0.766    |\n",
            "------------------------------------\n",
            "Num timesteps: 800000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -59.86\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 998      |\n",
            "|    ep_rew_mean        | -59.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1015     |\n",
            "|    iterations         | 20000    |\n",
            "|    time_elapsed       | 787      |\n",
            "|    total_timesteps    | 800000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.465   |\n",
            "|    explained_variance | 0.964    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 19999    |\n",
            "|    policy_loss        | -0.203   |\n",
            "|    value_loss         | 0.78     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 998      |\n",
            "|    ep_rew_mean        | -59.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1011     |\n",
            "|    iterations         | 20100    |\n",
            "|    time_elapsed       | 795      |\n",
            "|    total_timesteps    | 804000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.391   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 20099    |\n",
            "|    policy_loss        | -0.154   |\n",
            "|    value_loss         | 1.27     |\n",
            "------------------------------------\n",
            "Num timesteps: 808000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -58.02\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 998      |\n",
            "|    ep_rew_mean        | -58      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1010     |\n",
            "|    iterations         | 20200    |\n",
            "|    time_elapsed       | 799      |\n",
            "|    total_timesteps    | 808000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.391   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 20199    |\n",
            "|    policy_loss        | 0.0524   |\n",
            "|    value_loss         | 2.25     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 992      |\n",
            "|    ep_rew_mean        | -54.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1007     |\n",
            "|    iterations         | 20300    |\n",
            "|    time_elapsed       | 806      |\n",
            "|    total_timesteps    | 812000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.458   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 20299    |\n",
            "|    policy_loss        | -0.0761  |\n",
            "|    value_loss         | 2.08     |\n",
            "------------------------------------\n",
            "Num timesteps: 816000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -52.59\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 978      |\n",
            "|    ep_rew_mean        | -52.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1006     |\n",
            "|    iterations         | 20400    |\n",
            "|    time_elapsed       | 810      |\n",
            "|    total_timesteps    | 816000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.517   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 20399    |\n",
            "|    policy_loss        | 0.0737   |\n",
            "|    value_loss         | 0.958    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 978      |\n",
            "|    ep_rew_mean        | -52.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1004     |\n",
            "|    iterations         | 20500    |\n",
            "|    time_elapsed       | 816      |\n",
            "|    total_timesteps    | 820000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.527   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 20499    |\n",
            "|    policy_loss        | 0.336    |\n",
            "|    value_loss         | 2.62     |\n",
            "------------------------------------\n",
            "Num timesteps: 824000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -52.89\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 967      |\n",
            "|    ep_rew_mean        | -52.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1004     |\n",
            "|    iterations         | 20600    |\n",
            "|    time_elapsed       | 820      |\n",
            "|    total_timesteps    | 824000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.417   |\n",
            "|    explained_variance | 0.973    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 20599    |\n",
            "|    policy_loss        | -0.247   |\n",
            "|    value_loss         | 1.78     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 959      |\n",
            "|    ep_rew_mean        | -50.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1003     |\n",
            "|    iterations         | 20700    |\n",
            "|    time_elapsed       | 824      |\n",
            "|    total_timesteps    | 828000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.477   |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 20699    |\n",
            "|    policy_loss        | 0.0984   |\n",
            "|    value_loss         | 0.715    |\n",
            "------------------------------------\n",
            "Num timesteps: 832000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -50.07\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 958      |\n",
            "|    ep_rew_mean        | -50.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1000     |\n",
            "|    iterations         | 20800    |\n",
            "|    time_elapsed       | 831      |\n",
            "|    total_timesteps    | 832000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.417   |\n",
            "|    explained_variance | 0.932    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 20799    |\n",
            "|    policy_loss        | -0.16    |\n",
            "|    value_loss         | 3.42     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 955      |\n",
            "|    ep_rew_mean        | -51.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 998      |\n",
            "|    iterations         | 20900    |\n",
            "|    time_elapsed       | 837      |\n",
            "|    total_timesteps    | 836000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.435   |\n",
            "|    explained_variance | 0.929    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 20899    |\n",
            "|    policy_loss        | -0.19    |\n",
            "|    value_loss         | 3.13     |\n",
            "------------------------------------\n",
            "Num timesteps: 840000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -53.76\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 948      |\n",
            "|    ep_rew_mean        | -53.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 996      |\n",
            "|    iterations         | 21000    |\n",
            "|    time_elapsed       | 842      |\n",
            "|    total_timesteps    | 840000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.404   |\n",
            "|    explained_variance | 0.933    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 20999    |\n",
            "|    policy_loss        | 0.0302   |\n",
            "|    value_loss         | 1.79     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 945      |\n",
            "|    ep_rew_mean        | -56.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 995      |\n",
            "|    iterations         | 21100    |\n",
            "|    time_elapsed       | 847      |\n",
            "|    total_timesteps    | 844000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.392   |\n",
            "|    explained_variance | 0.964    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 21099    |\n",
            "|    policy_loss        | -0.198   |\n",
            "|    value_loss         | 1.28     |\n",
            "------------------------------------\n",
            "Num timesteps: 848000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -55.98\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 945      |\n",
            "|    ep_rew_mean        | -56      |\n",
            "| time/                 |          |\n",
            "|    fps                | 993      |\n",
            "|    iterations         | 21200    |\n",
            "|    time_elapsed       | 853      |\n",
            "|    total_timesteps    | 848000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.428   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 21199    |\n",
            "|    policy_loss        | -0.107   |\n",
            "|    value_loss         | 1.7      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 945      |\n",
            "|    ep_rew_mean        | -56.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 991      |\n",
            "|    iterations         | 21300    |\n",
            "|    time_elapsed       | 859      |\n",
            "|    total_timesteps    | 852000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.508   |\n",
            "|    explained_variance | 0.979    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 21299    |\n",
            "|    policy_loss        | 0.0629   |\n",
            "|    value_loss         | 2.15     |\n",
            "------------------------------------\n",
            "Num timesteps: 856000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -58.40\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 942      |\n",
            "|    ep_rew_mean        | -58.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 990      |\n",
            "|    iterations         | 21400    |\n",
            "|    time_elapsed       | 864      |\n",
            "|    total_timesteps    | 856000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.457   |\n",
            "|    explained_variance | 0.971    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 21399    |\n",
            "|    policy_loss        | 0.452    |\n",
            "|    value_loss         | 2.23     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 932      |\n",
            "|    ep_rew_mean        | -58.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 988      |\n",
            "|    iterations         | 21500    |\n",
            "|    time_elapsed       | 870      |\n",
            "|    total_timesteps    | 860000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.483   |\n",
            "|    explained_variance | 0.896    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 21499    |\n",
            "|    policy_loss        | -0.166   |\n",
            "|    value_loss         | 2.74     |\n",
            "------------------------------------\n",
            "Num timesteps: 864000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -60.93\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 916      |\n",
            "|    ep_rew_mean        | -60.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 987      |\n",
            "|    iterations         | 21600    |\n",
            "|    time_elapsed       | 874      |\n",
            "|    total_timesteps    | 864000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.465   |\n",
            "|    explained_variance | 0.966    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 21599    |\n",
            "|    policy_loss        | 0.00326  |\n",
            "|    value_loss         | 1.71     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 913      |\n",
            "|    ep_rew_mean        | -61.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 986      |\n",
            "|    iterations         | 21700    |\n",
            "|    time_elapsed       | 880      |\n",
            "|    total_timesteps    | 868000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.579   |\n",
            "|    explained_variance | 0.97     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 21699    |\n",
            "|    policy_loss        | 0.4      |\n",
            "|    value_loss         | 1.28     |\n",
            "------------------------------------\n",
            "Num timesteps: 872000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -61.22\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 913      |\n",
            "|    ep_rew_mean        | -61.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 986      |\n",
            "|    iterations         | 21800    |\n",
            "|    time_elapsed       | 884      |\n",
            "|    total_timesteps    | 872000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.542   |\n",
            "|    explained_variance | 0.813    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 21799    |\n",
            "|    policy_loss        | -0.307   |\n",
            "|    value_loss         | 4.72     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 895      |\n",
            "|    ep_rew_mean        | -62.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 985      |\n",
            "|    iterations         | 21900    |\n",
            "|    time_elapsed       | 888      |\n",
            "|    total_timesteps    | 876000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.418   |\n",
            "|    explained_variance | 0.929    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 21899    |\n",
            "|    policy_loss        | -0.301   |\n",
            "|    value_loss         | 3.83     |\n",
            "------------------------------------\n",
            "Num timesteps: 880000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -67.57\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 864      |\n",
            "|    ep_rew_mean        | -67.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 985      |\n",
            "|    iterations         | 22000    |\n",
            "|    time_elapsed       | 893      |\n",
            "|    total_timesteps    | 880000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.47    |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 21999    |\n",
            "|    policy_loss        | 0.0321   |\n",
            "|    value_loss         | 1.27     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 852      |\n",
            "|    ep_rew_mean        | -66.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 984      |\n",
            "|    iterations         | 22100    |\n",
            "|    time_elapsed       | 897      |\n",
            "|    total_timesteps    | 884000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.332   |\n",
            "|    explained_variance | 0.866    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 22099    |\n",
            "|    policy_loss        | 0.0196   |\n",
            "|    value_loss         | 33.4     |\n",
            "------------------------------------\n",
            "Num timesteps: 888000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -65.38\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 831      |\n",
            "|    ep_rew_mean        | -65.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 984      |\n",
            "|    iterations         | 22200    |\n",
            "|    time_elapsed       | 902      |\n",
            "|    total_timesteps    | 888000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.487   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 22199    |\n",
            "|    policy_loss        | -0.297   |\n",
            "|    value_loss         | 3        |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 808      |\n",
            "|    ep_rew_mean        | -65.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 983      |\n",
            "|    iterations         | 22300    |\n",
            "|    time_elapsed       | 907      |\n",
            "|    total_timesteps    | 892000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.408   |\n",
            "|    explained_variance | 0.98     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 22299    |\n",
            "|    policy_loss        | -0.145   |\n",
            "|    value_loss         | 5.92     |\n",
            "------------------------------------\n",
            "Num timesteps: 896000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -66.02\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 791      |\n",
            "|    ep_rew_mean        | -66      |\n",
            "| time/                 |          |\n",
            "|    fps                | 982      |\n",
            "|    iterations         | 22400    |\n",
            "|    time_elapsed       | 911      |\n",
            "|    total_timesteps    | 896000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.537   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 22399    |\n",
            "|    policy_loss        | 0.15     |\n",
            "|    value_loss         | 1.4      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 755      |\n",
            "|    ep_rew_mean        | -67.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 983      |\n",
            "|    iterations         | 22500    |\n",
            "|    time_elapsed       | 915      |\n",
            "|    total_timesteps    | 900000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.45    |\n",
            "|    explained_variance | 0.977    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 22499    |\n",
            "|    policy_loss        | -0.22    |\n",
            "|    value_loss         | 6.13     |\n",
            "------------------------------------\n",
            "Num timesteps: 904000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -71.27\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 741      |\n",
            "|    ep_rew_mean        | -71.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 982      |\n",
            "|    iterations         | 22600    |\n",
            "|    time_elapsed       | 919      |\n",
            "|    total_timesteps    | 904000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.514   |\n",
            "|    explained_variance | 0.945    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 22599    |\n",
            "|    policy_loss        | -0.195   |\n",
            "|    value_loss         | 2.25     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 725      |\n",
            "|    ep_rew_mean        | -71.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 981      |\n",
            "|    iterations         | 22700    |\n",
            "|    time_elapsed       | 925      |\n",
            "|    total_timesteps    | 908000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.427   |\n",
            "|    explained_variance | 0.98     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 22699    |\n",
            "|    policy_loss        | 0.549    |\n",
            "|    value_loss         | 4.39     |\n",
            "------------------------------------\n",
            "Num timesteps: 912000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -71.34\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 694      |\n",
            "|    ep_rew_mean        | -71.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 980      |\n",
            "|    iterations         | 22800    |\n",
            "|    time_elapsed       | 929      |\n",
            "|    total_timesteps    | 912000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.43    |\n",
            "|    explained_variance | 0.98     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 22799    |\n",
            "|    policy_loss        | 0.199    |\n",
            "|    value_loss         | 2.18     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 673      |\n",
            "|    ep_rew_mean        | -70.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 980      |\n",
            "|    iterations         | 22900    |\n",
            "|    time_elapsed       | 933      |\n",
            "|    total_timesteps    | 916000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.307   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 22899    |\n",
            "|    policy_loss        | 0.0121   |\n",
            "|    value_loss         | 0.743    |\n",
            "------------------------------------\n",
            "Num timesteps: 920000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -70.26\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 644      |\n",
            "|    ep_rew_mean        | -70.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 980      |\n",
            "|    iterations         | 23000    |\n",
            "|    time_elapsed       | 938      |\n",
            "|    total_timesteps    | 920000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.442   |\n",
            "|    explained_variance | 0.335    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 22999    |\n",
            "|    policy_loss        | -0.103   |\n",
            "|    value_loss         | 171      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 655      |\n",
            "|    ep_rew_mean        | -66.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 979      |\n",
            "|    iterations         | 23100    |\n",
            "|    time_elapsed       | 943      |\n",
            "|    total_timesteps    | 924000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.516   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 23099    |\n",
            "|    policy_loss        | 0.306    |\n",
            "|    value_loss         | 1.35     |\n",
            "------------------------------------\n",
            "Num timesteps: 928000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -68.61\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 657      |\n",
            "|    ep_rew_mean        | -68.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 978      |\n",
            "|    iterations         | 23200    |\n",
            "|    time_elapsed       | 947      |\n",
            "|    total_timesteps    | 928000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.387   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 23199    |\n",
            "|    policy_loss        | 0.292    |\n",
            "|    value_loss         | 2.54     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 665      |\n",
            "|    ep_rew_mean        | -66.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 976      |\n",
            "|    iterations         | 23300    |\n",
            "|    time_elapsed       | 953      |\n",
            "|    total_timesteps    | 932000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.356   |\n",
            "|    explained_variance | 0.96     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 23299    |\n",
            "|    policy_loss        | 0.347    |\n",
            "|    value_loss         | 4.97     |\n",
            "------------------------------------\n",
            "Num timesteps: 936000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -65.77\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 656      |\n",
            "|    ep_rew_mean        | -65.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 975      |\n",
            "|    iterations         | 23400    |\n",
            "|    time_elapsed       | 959      |\n",
            "|    total_timesteps    | 936000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.393   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 23399    |\n",
            "|    policy_loss        | -0.378   |\n",
            "|    value_loss         | 3.2      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 663      |\n",
            "|    ep_rew_mean        | -65.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 974      |\n",
            "|    iterations         | 23500    |\n",
            "|    time_elapsed       | 964      |\n",
            "|    total_timesteps    | 940000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.551   |\n",
            "|    explained_variance | 0.969    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 23499    |\n",
            "|    policy_loss        | 0.12     |\n",
            "|    value_loss         | 2.04     |\n",
            "------------------------------------\n",
            "Num timesteps: 944000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -64.58\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 676      |\n",
            "|    ep_rew_mean        | -64.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 973      |\n",
            "|    iterations         | 23600    |\n",
            "|    time_elapsed       | 970      |\n",
            "|    total_timesteps    | 944000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.334   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 23599    |\n",
            "|    policy_loss        | -0.0475  |\n",
            "|    value_loss         | 1.72     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 688      |\n",
            "|    ep_rew_mean        | -62.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 971      |\n",
            "|    iterations         | 23700    |\n",
            "|    time_elapsed       | 976      |\n",
            "|    total_timesteps    | 948000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.504   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 23699    |\n",
            "|    policy_loss        | 0.183    |\n",
            "|    value_loss         | 1.3      |\n",
            "------------------------------------\n",
            "Num timesteps: 952000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -63.64\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 693      |\n",
            "|    ep_rew_mean        | -63.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 969      |\n",
            "|    iterations         | 23800    |\n",
            "|    time_elapsed       | 981      |\n",
            "|    total_timesteps    | 952000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.385   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 23799    |\n",
            "|    policy_loss        | 0.234    |\n",
            "|    value_loss         | 1.54     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 700      |\n",
            "|    ep_rew_mean        | -69.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 967      |\n",
            "|    iterations         | 23900    |\n",
            "|    time_elapsed       | 987      |\n",
            "|    total_timesteps    | 956000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.415   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 23899    |\n",
            "|    policy_loss        | -0.0788  |\n",
            "|    value_loss         | 1.7      |\n",
            "------------------------------------\n",
            "Num timesteps: 960000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -69.46\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 698      |\n",
            "|    ep_rew_mean        | -69.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 967      |\n",
            "|    iterations         | 24000    |\n",
            "|    time_elapsed       | 992      |\n",
            "|    total_timesteps    | 960000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.441   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 23999    |\n",
            "|    policy_loss        | 0.108    |\n",
            "|    value_loss         | 3.25     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 719      |\n",
            "|    ep_rew_mean        | -75.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 966      |\n",
            "|    iterations         | 24100    |\n",
            "|    time_elapsed       | 997      |\n",
            "|    total_timesteps    | 964000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.431   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 24099    |\n",
            "|    policy_loss        | -0.332   |\n",
            "|    value_loss         | 3.11     |\n",
            "------------------------------------\n",
            "Num timesteps: 968000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -79.30\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 722      |\n",
            "|    ep_rew_mean        | -79.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 965      |\n",
            "|    iterations         | 24200    |\n",
            "|    time_elapsed       | 1002     |\n",
            "|    total_timesteps    | 968000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.396   |\n",
            "|    explained_variance | 0.976    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 24199    |\n",
            "|    policy_loss        | 0.0571   |\n",
            "|    value_loss         | 3.34     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 744      |\n",
            "|    ep_rew_mean        | -73.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 965      |\n",
            "|    iterations         | 24300    |\n",
            "|    time_elapsed       | 1007     |\n",
            "|    total_timesteps    | 972000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.406   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 24299    |\n",
            "|    policy_loss        | 0.213    |\n",
            "|    value_loss         | 1.95     |\n",
            "------------------------------------\n",
            "Num timesteps: 976000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -74.29\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 748      |\n",
            "|    ep_rew_mean        | -74.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 964      |\n",
            "|    iterations         | 24400    |\n",
            "|    time_elapsed       | 1011     |\n",
            "|    total_timesteps    | 976000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.421   |\n",
            "|    explained_variance | 0.945    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 24399    |\n",
            "|    policy_loss        | 0.157    |\n",
            "|    value_loss         | 2.33     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 759      |\n",
            "|    ep_rew_mean        | -72.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 964      |\n",
            "|    iterations         | 24500    |\n",
            "|    time_elapsed       | 1016     |\n",
            "|    total_timesteps    | 980000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.415   |\n",
            "|    explained_variance | 0.857    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 24499    |\n",
            "|    policy_loss        | -0.114   |\n",
            "|    value_loss         | 3.01     |\n",
            "------------------------------------\n",
            "Num timesteps: 984000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -70.12\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 775      |\n",
            "|    ep_rew_mean        | -70.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 963      |\n",
            "|    iterations         | 24600    |\n",
            "|    time_elapsed       | 1021     |\n",
            "|    total_timesteps    | 984000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.342   |\n",
            "|    explained_variance | 0.408    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 24599    |\n",
            "|    policy_loss        | -0.0882  |\n",
            "|    value_loss         | 178      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 774      |\n",
            "|    ep_rew_mean        | -72.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 962      |\n",
            "|    iterations         | 24700    |\n",
            "|    time_elapsed       | 1026     |\n",
            "|    total_timesteps    | 988000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.381   |\n",
            "|    explained_variance | 0.8      |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 24699    |\n",
            "|    policy_loss        | 0.077    |\n",
            "|    value_loss         | 1.37     |\n",
            "------------------------------------\n",
            "Num timesteps: 992000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -71.84\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 810      |\n",
            "|    ep_rew_mean        | -71.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 961      |\n",
            "|    iterations         | 24800    |\n",
            "|    time_elapsed       | 1031     |\n",
            "|    total_timesteps    | 992000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.354   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 24799    |\n",
            "|    policy_loss        | -0.0943  |\n",
            "|    value_loss         | 2.01     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 818      |\n",
            "|    ep_rew_mean        | -71.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 960      |\n",
            "|    iterations         | 24900    |\n",
            "|    time_elapsed       | 1036     |\n",
            "|    total_timesteps    | 996000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.455   |\n",
            "|    explained_variance | 0.957    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 24899    |\n",
            "|    policy_loss        | -0.172   |\n",
            "|    value_loss         | 1.52     |\n",
            "------------------------------------\n",
            "Num timesteps: 1000000\n",
            "Best mean reward: 79.92 - Last mean reward per episode: -71.16\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 832      |\n",
            "|    ep_rew_mean        | -71.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 959      |\n",
            "|    iterations         | 25000    |\n",
            "|    time_elapsed       | 1042     |\n",
            "|    total_timesteps    | 1000000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.356   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 24999    |\n",
            "|    policy_loss        | -0.202   |\n",
            "|    value_loss         | 2.34     |\n",
            "------------------------------------\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<stable_baselines3.a2c.a2c.A2C at 0x7f03f2244550>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_from_file = False\n",
        "# Hyperparameters are from RL_Zoo\n",
        "# https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/a2c.yml\n",
        "policy = \"MlpPolicy\"\n",
        "n_steps = 5\n",
        "learning_rate = 0.00083\n",
        "# batch_size = 256\n",
        "# n_epochs = 10\n",
        "n_envs = 8\n",
        "n_timesteps = 1e6\n",
        "gamma = 0.995\n",
        "ent_coef = 0.00001\n",
        "\n",
        "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=\"log_dir_A2C/\")\n",
        "\n",
        "# env\n",
        "env = make_vec_env(\"LunarLander-v2\", n_envs=n_envs, monitor_dir=\"log_dir_A2C/\")\n",
        "\n",
        "# instantiate the agent\n",
        "if train_from_file:\n",
        "  model = A2C.load(path=\"log_dir_A2C/best_model.zip\", env=env)\n",
        "else:\n",
        "  model = A2C(policy, env, learning_rate = learning_rate, n_steps = n_steps, ent_coef= ent_coef, tensorboard_log=\"./TensorBoardLog/\", verbose=1)\n",
        "\n",
        "# train the agent\n",
        "model.learn(total_timesteps=n_timesteps, callback=callback)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b1dc8fa",
      "metadata": {
        "id": "5b1dc8fa"
      },
      "source": [
        "# Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "366b80a8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "366b80a8",
        "outputId": "417c672c-1a65-4a71-f550-d10a5c1626ae"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def moving_average(values, window):\n",
        "    \"\"\"\n",
        "    Smooth values by doing a moving average\n",
        "    :param values: (numpy array)\n",
        "    :param window: (int)\n",
        "    :return: (numpy array)\n",
        "    \"\"\"\n",
        "    weights = np.repeat(1.0, window) / window\n",
        "    return np.convolve(values, weights, 'valid')\n",
        "\n",
        "def plot_results(log_folder, title='Learning Curve'):\n",
        "    \"\"\"\n",
        "    plot the results\n",
        "\n",
        "    :param log_folder: (str) the save location of the results to plot\n",
        "    :param title: (str) the title of the task to plot\n",
        "    \"\"\"\n",
        "\n",
        "    x, y = ts2xy(load_results(log_folder), 'timesteps')\n",
        "    y = moving_average(y, window=100)\n",
        "    # Truncate x\n",
        "    x = x[len(x) - len(y):]\n",
        "    fig = plt.figure(title, figsize=(12,5))\n",
        "    plt.plot(x, y)\n",
        "    plt.xlabel('Number of Timesteps')\n",
        "    plt.ylabel('Rewards')\n",
        "    # plt.title(title + \" Smoothed A2C after 8000,000 Timesteps\")\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "plot_results(\"log_dir_A2C\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b00f2a81",
      "metadata": {
        "id": "b00f2a81"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "815393a0",
      "metadata": {
        "id": "815393a0"
      },
      "outputs": [],
      "source": [
        "env = make_vec_env(\"LunarLander-v2\", n_envs=1,monitor_dir=\"evaluate_log_dir_A2C\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "63611e6e",
      "metadata": {
        "id": "63611e6e"
      },
      "outputs": [],
      "source": [
        "model = A2C.load(path=\"log_dir_A2C/best_model.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b06e1a3",
      "metadata": {
        "id": "3b06e1a3"
      },
      "source": [
        "#### Stable Baseline 3 Evaluation Function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "9d4fd326",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d4fd326",
        "outputId": "e75bd790-909d-4ea8-d242-83632d84147e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean & Std Reward after 10 max run is 153.7792738 & 106.17661167799479\n"
          ]
        }
      ],
      "source": [
        "mean_reward, std_reward = evaluate_policy(model, env,n_eval_episodes=10, render=True, deterministic=True)\n",
        "print(\"Mean & Std Reward after {} max run is {} & {}\".format(10,mean_reward, std_reward))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e49c5168",
      "metadata": {
        "id": "e49c5168"
      },
      "source": [
        "# GIF of a Train Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "60cc63dc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60cc63dc",
        "outputId": "9a42d8c7-7a7c-49af-a020-b0d647d5bbfe"
      },
      "outputs": [],
      "source": [
        "env = make_vec_env(\"LunarLander-v2\", n_envs=1)\n",
        "model = A2C.load(path=\"log_dir_A2C/best_model.zip\")\n",
        "\n",
        "images = []\n",
        "obs = env.reset()\n",
        "img = env.render(mode=\"rgb_array\")\n",
        "for i in range(1000):\n",
        "    images.append(img)\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, _, _ ,_ = env.step(action)\n",
        "    img = env.render(mode=\"rgb_array\")\n",
        "\n",
        "imageio.mimsave(\"lunar lander_A2C.gif\", [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3afc060b",
      "metadata": {
        "id": "3afc060b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "857970f990130bbcaee778cf1846f7875676d945310dca1379fe4b5ef3d258a5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
