{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "57601915",
      "metadata": {
        "id": "57601915"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# !pip install shap\n",
        "# !pip install opencv-python\n",
        "# !pip install swig\n",
        "# !pip install Box2D\n",
        "\n",
        "\n",
        "# # !pip install box2d pygame\n",
        "\n",
        "\n",
        "# !pip install gym\n",
        "# !pip install pyglet==1.5.27\n",
        "# !pip install stable-baseline3\n",
        "# !pip install \"gymnasium[all]\"\n",
        "\n",
        "# !pip install stable_baselines3\n",
        "\n",
        "## FOR LOCAL Jupyter notebook\n",
        "# !pip install tensorflow\n",
        "# !pip install torch\n",
        "# !pip install pygame\n",
        "# !pip install tensorboard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b00a128f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b00a128f",
        "outputId": "f0a196be-2f8b-4fb1-a62f-bfdd10254bae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-24 13:52:11.920222: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-03-24 13:52:12.583004: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import imageio\n",
        "import os\n",
        "from stable_baselines3 import PPO, A2C\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack, SubprocVecEnv, VecNormalize\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common import results_plotter\n",
        "import gymnasium  as gym\n",
        "import matplotlib.pyplot as plt\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "import scipy.stats as stats\n",
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "import tensorflow as tf\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "YtZN-eC7NwuS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtZN-eC7NwuS",
        "outputId": "2fc6167f-8579-442f-f41c-6cae6b8abde7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n",
            "GPU not found. Please ensure that GPU is enabled in Colab.\n",
            "Num GPUs Available:  0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-24 13:52:14.680558: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
          ]
        }
      ],
      "source": [
        "# seeds\n",
        "# Set seed for numpy\n",
        "np.random.seed(100)\n",
        "\n",
        "# Set seed for Python random module\n",
        "import random\n",
        "random.seed(100)\n",
        "\n",
        "# Set seed for TensorFlow\n",
        "tf.random.set_seed(100)\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "if tf.test.gpu_device_name():\n",
        "    print('Default GPU Device:', tf.test.gpu_device_name())\n",
        "else:\n",
        "    print(\"GPU not found. Please ensure that GPU is enabled in Colab.\")\n",
        "    \n",
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "780afb92",
      "metadata": {
        "id": "780afb92"
      },
      "source": [
        "<h1> Important Libraries To Install </h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2826cd85",
      "metadata": {
        "id": "2826cd85"
      },
      "source": [
        "<h1> Parameter & Environment Information </h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87ef75ca",
      "metadata": {
        "id": "87ef75ca"
      },
      "source": [
        "<p>\n",
        "    This environment is part of the Box2D environments.\n",
        "</p>\n",
        "\n",
        "<ul>\n",
        "    <li> Action Space Discrete(4) </li>\n",
        "    <li> Observation Shape (8,) </li>\n",
        "    <li> Observation High [1.5 1.5 5. 5. 3.14 5. 1. 1. ] </li>\n",
        "    <li> Observation Low [-1.5 -1.5 -5. -5. -3.14 -5. -0. -0. ] </li>\n",
        "    <li> Import gymnasium.make(\"LunarLander-v2\") </li>\n",
        "</ul>\n",
        "\n",
        "<h3> Description </h3>\n",
        "<p>This environment is a classic rocket trajectory optimization problem. According to Pontryagin’s maximum principle, it is optimal to fire the engine at full throttle or turn it off. This is the reason why this environment has discrete actions: engine on or off.\n",
        "\n",
        "There are two environment versions: discrete or continuous. The landing pad is always at coordinates (0,0). The coordinates are the first two numbers in the state vector. Landing outside of the landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt.</p>\n",
        "\n",
        "<h3> Action Space </h3>\n",
        "<p>\n",
        "There are four discrete actions available:\n",
        "\n",
        "* 0: do nothing\n",
        "* 1: fire left orientation engine\n",
        "* 2: fire main engine\n",
        "* 3: fire right orientation engine\n",
        "\n",
        "</p>\n",
        "\n",
        "<h3> Observation Space </h3>\n",
        "<p>\n",
        "The state is an 8-dimensional vector: the coordinates of the lander in x & y, its linear velocities in x & y, its angle, its angular velocity, and two booleans that represent whether each leg is in contact with the ground or not.\n",
        "</p>\n",
        "\n",
        "<h3> Reward </h3>\n",
        "<p>\n",
        "After every step a reward is granted. The total reward of an episode is the sum of the rewards for all the steps within that episode.\n",
        "\n",
        "For each step, the reward:\n",
        "\n",
        "* is increased/decreased the closer/further the lander is to the landing pad.\n",
        "* is increased/decreased the slower/faster the lander is moving.\n",
        "* is decreased the more the lander is tilted (angle not horizontal).\n",
        "* is increased by 10 points for each leg that is in contact with the ground.\n",
        "* is decreased by 0.03 points each frame a side engine is firing.\n",
        "* is decreased by 0.3 points each frame the main engine is firing.\n",
        "\n",
        "The episode receive an additional reward of -100 or +100 points for crashing or landing safely respectively.\n",
        "\n",
        "An episode is considered a solution if it scores at least 200 points.\n",
        "</p>\n",
        "\n",
        "<h3> Starting State </h3>\n",
        "\n",
        "<p>The lander starts at the top center of the viewport with a random initial force applied to its center of mass.</p>\n",
        "\n",
        "<h3> Episode Termination </h3>\n",
        "<p> The episode finishes if:<br>\n",
        "    \n",
        "1. the lander crashes (the lander body gets in contact with the moon);<br>\n",
        "2. the lander gets outside of the viewport (x coordinate is greater than 1);<br>\n",
        "3. the lander is not awake. From the Box2D docs, a body which is not awake is a body which doesn’t move and doesn’t collide with any other body:<br>\n",
        "\n",
        "When Box2D determines that a body (or group of bodies) has come to rest, the body enters a sleep state which has very little CPU overhead. If a body is awake and collides with a sleeping body, then the sleeping body wakes up. Bodies will also wake up if a joint or contact attached to them is destroyed.\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3d0543a5",
      "metadata": {
        "id": "3d0543a5",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b3fb45b2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3fb45b2",
        "outputId": "2416218d-5bf4-48aa-fd24-8899b2af453e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Action inter is descrete 4\n",
            "Shape of Observation is (8,)\n"
          ]
        }
      ],
      "source": [
        "print(\"The Action inter is descrete {}\".format(env.action_space.n))\n",
        "print(\"Shape of Observation is {}\".format(env.observation_space.sample().shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b9ff9c9",
      "metadata": {
        "id": "8b9ff9c9"
      },
      "source": [
        "<h1> Baseline Model. </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d35101af",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d35101af",
        "outputId": "4873fc86-1916-4745-d40f-732c7dca2d00",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Reward after 10 max run is -1.3004164000428067\n"
          ]
        }
      ],
      "source": [
        "rewards = []\n",
        "obs = env.reset()\n",
        "done = False\n",
        "MAX_RUN = 10\n",
        "\n",
        "for i in range(MAX_RUN):\n",
        "    while not done:\n",
        "        env.render()\n",
        "        action_sample = env.action_space.sample()\n",
        "        # let's take a step in the environment\n",
        "        obs, rwd, done, info ,_  = env.step(action_sample)\n",
        "        rewards.append(rwd)\n",
        "env.close()\n",
        "print(\"Mean Reward after {} max run is {}\".format(MAX_RUN, np.mean(np.array(rewards))))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac737551",
      "metadata": {
        "id": "ac737551"
      },
      "source": [
        "<h1> Reinforcement Learning For Training The Model </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "bdaa0e55",
      "metadata": {
        "id": "bdaa0e55"
      },
      "outputs": [],
      "source": [
        "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
        "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
        "\n",
        "    :param check_freq: (int)\n",
        "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
        "      It must contains the file created by the ``Monitor`` wrapper.\n",
        "    :param verbose: (int)\n",
        "    \"\"\"\n",
        "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
        "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
        "        self.check_freq = check_freq\n",
        "        self.log_dir = log_dir\n",
        "        self.save_path = os.path.join(log_dir, 'best_model')\n",
        "        self.best_mean_reward = -np.inf\n",
        "\n",
        "    def _init_callback(self) -> None:\n",
        "        # Create folder if needed\n",
        "        if self.save_path is not None:\n",
        "            os.makedirs(self.save_path, exist_ok=True)\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.n_calls % self.check_freq == 0:\n",
        "\n",
        "          # Retrieve training reward\n",
        "          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
        "          if len(x) > 0:\n",
        "              # Mean training reward over the last 100 episodes\n",
        "              mean_reward = np.mean(y[-100:])\n",
        "              if self.verbose > 0:\n",
        "                print(f\"Num timesteps: {self.num_timesteps}\")\n",
        "                print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n",
        "\n",
        "              # New best model, you could save the agent here\n",
        "              if mean_reward > self.best_mean_reward:\n",
        "                  self.best_mean_reward = mean_reward\n",
        "                  # Example for saving best model\n",
        "                  if self.verbose > 0:\n",
        "                    print(f\"Saving new best model to {self.save_path}.zip\")\n",
        "                  self.model.save(self.save_path)\n",
        "\n",
        "        return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2d311d6a",
      "metadata": {
        "id": "2d311d6a",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "Logging to ./TensorBoardLog/A2C_7\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 103      |\n",
            "|    ep_rew_mean        | -325     |\n",
            "| time/                 |          |\n",
            "|    fps                | 2942     |\n",
            "|    iterations         | 100      |\n",
            "|    time_elapsed       | 1        |\n",
            "|    total_timesteps    | 4000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.3     |\n",
            "|    explained_variance | 0.0488   |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 99       |\n",
            "|    policy_loss        | -8.36    |\n",
            "|    value_loss         | 91.2     |\n",
            "------------------------------------\n",
            "Num timesteps: 8000\n",
            "Best mean reward: -inf - Last mean reward per episode: -260.29\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 112      |\n",
            "|    ep_rew_mean        | -260     |\n",
            "| time/                 |          |\n",
            "|    fps                | 2819     |\n",
            "|    iterations         | 200      |\n",
            "|    time_elapsed       | 2        |\n",
            "|    total_timesteps    | 8000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.07    |\n",
            "|    explained_variance | -0.0502  |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 199      |\n",
            "|    policy_loss        | -10.7    |\n",
            "|    value_loss         | 791      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 124      |\n",
            "|    ep_rew_mean        | -240     |\n",
            "| time/                 |          |\n",
            "|    fps                | 2460     |\n",
            "|    iterations         | 300      |\n",
            "|    time_elapsed       | 4        |\n",
            "|    total_timesteps    | 12000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.12    |\n",
            "|    explained_variance | 0.425    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 299      |\n",
            "|    policy_loss        | 0.00538  |\n",
            "|    value_loss         | 56.2     |\n",
            "------------------------------------\n",
            "Num timesteps: 16000\n",
            "Best mean reward: -260.29 - Last mean reward per episode: -221.71\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 145      |\n",
            "|    ep_rew_mean        | -222     |\n",
            "| time/                 |          |\n",
            "|    fps                | 2034     |\n",
            "|    iterations         | 400      |\n",
            "|    time_elapsed       | 7        |\n",
            "|    total_timesteps    | 16000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.846   |\n",
            "|    explained_variance | -0.0632  |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 399      |\n",
            "|    policy_loss        | 0.921    |\n",
            "|    value_loss         | 30       |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 170      |\n",
            "|    ep_rew_mean        | -194     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1986     |\n",
            "|    iterations         | 500      |\n",
            "|    time_elapsed       | 10       |\n",
            "|    total_timesteps    | 20000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.872   |\n",
            "|    explained_variance | -0.0383  |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 499      |\n",
            "|    policy_loss        | -0.72    |\n",
            "|    value_loss         | 25.5     |\n",
            "------------------------------------\n",
            "Num timesteps: 24000\n",
            "Best mean reward: -221.71 - Last mean reward per episode: -154.59\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 199      |\n",
            "|    ep_rew_mean        | -155     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1991     |\n",
            "|    iterations         | 600      |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 24000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.924   |\n",
            "|    explained_variance | 0.367    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 599      |\n",
            "|    policy_loss        | 0.305    |\n",
            "|    value_loss         | 82       |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 219      |\n",
            "|    ep_rew_mean        | -141     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1978     |\n",
            "|    iterations         | 700      |\n",
            "|    time_elapsed       | 14       |\n",
            "|    total_timesteps    | 28000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.768   |\n",
            "|    explained_variance | 0.326    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 699      |\n",
            "|    policy_loss        | 0.294    |\n",
            "|    value_loss         | 58.9     |\n",
            "------------------------------------\n",
            "Num timesteps: 32000\n",
            "Best mean reward: -154.59 - Last mean reward per episode: -125.20\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 237      |\n",
            "|    ep_rew_mean        | -125     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1938     |\n",
            "|    iterations         | 800      |\n",
            "|    time_elapsed       | 16       |\n",
            "|    total_timesteps    | 32000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.711   |\n",
            "|    explained_variance | 0.84     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 799      |\n",
            "|    policy_loss        | -0.232   |\n",
            "|    value_loss         | 31       |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 256      |\n",
            "|    ep_rew_mean        | -116     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1906     |\n",
            "|    iterations         | 900      |\n",
            "|    time_elapsed       | 18       |\n",
            "|    total_timesteps    | 36000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.809   |\n",
            "|    explained_variance | 0.869    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 899      |\n",
            "|    policy_loss        | 0.316    |\n",
            "|    value_loss         | 28       |\n",
            "------------------------------------\n",
            "Num timesteps: 40000\n",
            "Best mean reward: -125.20 - Last mean reward per episode: -111.39\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 265      |\n",
            "|    ep_rew_mean        | -111     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1857     |\n",
            "|    iterations         | 1000     |\n",
            "|    time_elapsed       | 21       |\n",
            "|    total_timesteps    | 40000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.669   |\n",
            "|    explained_variance | 0.468    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 999      |\n",
            "|    policy_loss        | 1.03     |\n",
            "|    value_loss         | 35.7     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 260      |\n",
            "|    ep_rew_mean        | -116     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1845     |\n",
            "|    iterations         | 1100     |\n",
            "|    time_elapsed       | 23       |\n",
            "|    total_timesteps    | 44000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.802   |\n",
            "|    explained_variance | 0.8      |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 1099     |\n",
            "|    policy_loss        | -1.28    |\n",
            "|    value_loss         | 23.2     |\n",
            "------------------------------------\n",
            "Num timesteps: 48000\n",
            "Best mean reward: -111.39 - Last mean reward per episode: -106.18\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 282      |\n",
            "|    ep_rew_mean        | -106     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1783     |\n",
            "|    iterations         | 1200     |\n",
            "|    time_elapsed       | 26       |\n",
            "|    total_timesteps    | 48000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.738   |\n",
            "|    explained_variance | 0.379    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 1199     |\n",
            "|    policy_loss        | 1.9      |\n",
            "|    value_loss         | 23.2     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 292      |\n",
            "|    ep_rew_mean        | -104     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1743     |\n",
            "|    iterations         | 1300     |\n",
            "|    time_elapsed       | 29       |\n",
            "|    total_timesteps    | 52000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.764   |\n",
            "|    explained_variance | 0.8      |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 1299     |\n",
            "|    policy_loss        | -2.13    |\n",
            "|    value_loss         | 43.2     |\n",
            "------------------------------------\n",
            "Num timesteps: 56000\n",
            "Best mean reward: -106.18 - Last mean reward per episode: -94.26\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 320      |\n",
            "|    ep_rew_mean        | -94.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1709     |\n",
            "|    iterations         | 1400     |\n",
            "|    time_elapsed       | 32       |\n",
            "|    total_timesteps    | 56000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.567   |\n",
            "|    explained_variance | 0.636    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 1399     |\n",
            "|    policy_loss        | 1.84     |\n",
            "|    value_loss         | 87.2     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 325      |\n",
            "|    ep_rew_mean        | -80.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1698     |\n",
            "|    iterations         | 1500     |\n",
            "|    time_elapsed       | 35       |\n",
            "|    total_timesteps    | 60000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.525   |\n",
            "|    explained_variance | 0.0787   |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 1499     |\n",
            "|    policy_loss        | 4.94     |\n",
            "|    value_loss         | 344      |\n",
            "------------------------------------\n",
            "Num timesteps: 64000\n",
            "Best mean reward: -94.26 - Last mean reward per episode: -87.09\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 323      |\n",
            "|    ep_rew_mean        | -87.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1703     |\n",
            "|    iterations         | 1600     |\n",
            "|    time_elapsed       | 37       |\n",
            "|    total_timesteps    | 64000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.561   |\n",
            "|    explained_variance | 0.808    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 1599     |\n",
            "|    policy_loss        | 0.249    |\n",
            "|    value_loss         | 35.9     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 323      |\n",
            "|    ep_rew_mean        | -87.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1686     |\n",
            "|    iterations         | 1700     |\n",
            "|    time_elapsed       | 40       |\n",
            "|    total_timesteps    | 68000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.593   |\n",
            "|    explained_variance | 0.142    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 1699     |\n",
            "|    policy_loss        | -9.1     |\n",
            "|    value_loss         | 594      |\n",
            "------------------------------------\n",
            "Num timesteps: 72000\n",
            "Best mean reward: -87.09 - Last mean reward per episode: -77.56\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 322      |\n",
            "|    ep_rew_mean        | -77.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1691     |\n",
            "|    iterations         | 1800     |\n",
            "|    time_elapsed       | 42       |\n",
            "|    total_timesteps    | 72000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.486   |\n",
            "|    explained_variance | 0.909    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 1799     |\n",
            "|    policy_loss        | 3.21     |\n",
            "|    value_loss         | 50.1     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 312      |\n",
            "|    ep_rew_mean        | -78.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1691     |\n",
            "|    iterations         | 1900     |\n",
            "|    time_elapsed       | 44       |\n",
            "|    total_timesteps    | 76000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.498   |\n",
            "|    explained_variance | 0.59     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 1899     |\n",
            "|    policy_loss        | 0.687    |\n",
            "|    value_loss         | 120      |\n",
            "------------------------------------\n",
            "Num timesteps: 80000\n",
            "Best mean reward: -77.56 - Last mean reward per episode: -80.33\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 285      |\n",
            "|    ep_rew_mean        | -80.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1675     |\n",
            "|    iterations         | 2000     |\n",
            "|    time_elapsed       | 47       |\n",
            "|    total_timesteps    | 80000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.546   |\n",
            "|    explained_variance | 0.889    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 1999     |\n",
            "|    policy_loss        | -0.41    |\n",
            "|    value_loss         | 12.1     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 278      |\n",
            "|    ep_rew_mean        | -80.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1670     |\n",
            "|    iterations         | 2100     |\n",
            "|    time_elapsed       | 50       |\n",
            "|    total_timesteps    | 84000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.568   |\n",
            "|    explained_variance | 0.351    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 2099     |\n",
            "|    policy_loss        | 2.73     |\n",
            "|    value_loss         | 130      |\n",
            "------------------------------------\n",
            "Num timesteps: 88000\n",
            "Best mean reward: -77.56 - Last mean reward per episode: -81.08\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 277      |\n",
            "|    ep_rew_mean        | -81.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1670     |\n",
            "|    iterations         | 2200     |\n",
            "|    time_elapsed       | 52       |\n",
            "|    total_timesteps    | 88000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.4     |\n",
            "|    explained_variance | -0.104   |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 2199     |\n",
            "|    policy_loss        | 2.42     |\n",
            "|    value_loss         | 1.83e+03 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 276      |\n",
            "|    ep_rew_mean        | -78.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1671     |\n",
            "|    iterations         | 2300     |\n",
            "|    time_elapsed       | 55       |\n",
            "|    total_timesteps    | 92000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.599   |\n",
            "|    explained_variance | 0.677    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 2299     |\n",
            "|    policy_loss        | 1.32     |\n",
            "|    value_loss         | 40.9     |\n",
            "------------------------------------\n",
            "Num timesteps: 96000\n",
            "Best mean reward: -77.56 - Last mean reward per episode: -78.83\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 277      |\n",
            "|    ep_rew_mean        | -78.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1664     |\n",
            "|    iterations         | 2400     |\n",
            "|    time_elapsed       | 57       |\n",
            "|    total_timesteps    | 96000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.533   |\n",
            "|    explained_variance | 0.831    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 2399     |\n",
            "|    policy_loss        | 2.16     |\n",
            "|    value_loss         | 20.3     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 288      |\n",
            "|    ep_rew_mean        | -60.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1655     |\n",
            "|    iterations         | 2500     |\n",
            "|    time_elapsed       | 60       |\n",
            "|    total_timesteps    | 100000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.515   |\n",
            "|    explained_variance | 0.252    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 2499     |\n",
            "|    policy_loss        | 1.34     |\n",
            "|    value_loss         | 311      |\n",
            "------------------------------------\n",
            "Num timesteps: 104000\n",
            "Best mean reward: -77.56 - Last mean reward per episode: -39.11\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 305      |\n",
            "|    ep_rew_mean        | -39.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1654     |\n",
            "|    iterations         | 2600     |\n",
            "|    time_elapsed       | 62       |\n",
            "|    total_timesteps    | 104000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.392   |\n",
            "|    explained_variance | 0.793    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 2599     |\n",
            "|    policy_loss        | -0.042   |\n",
            "|    value_loss         | 48.6     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 295      |\n",
            "|    ep_rew_mean        | -31.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1665     |\n",
            "|    iterations         | 2700     |\n",
            "|    time_elapsed       | 64       |\n",
            "|    total_timesteps    | 108000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.429   |\n",
            "|    explained_variance | 0.791    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 2699     |\n",
            "|    policy_loss        | -1.18    |\n",
            "|    value_loss         | 99.8     |\n",
            "------------------------------------\n",
            "Num timesteps: 112000\n",
            "Best mean reward: -39.11 - Last mean reward per episode: -27.51\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 284      |\n",
            "|    ep_rew_mean        | -27.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1666     |\n",
            "|    iterations         | 2800     |\n",
            "|    time_elapsed       | 67       |\n",
            "|    total_timesteps    | 112000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.551   |\n",
            "|    explained_variance | 0.645    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 2799     |\n",
            "|    policy_loss        | 0.995    |\n",
            "|    value_loss         | 43.7     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 281      |\n",
            "|    ep_rew_mean        | -27.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1673     |\n",
            "|    iterations         | 2900     |\n",
            "|    time_elapsed       | 69       |\n",
            "|    total_timesteps    | 116000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.465   |\n",
            "|    explained_variance | 0.911    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 2899     |\n",
            "|    policy_loss        | -0.286   |\n",
            "|    value_loss         | 22.5     |\n",
            "------------------------------------\n",
            "Num timesteps: 120000\n",
            "Best mean reward: -27.51 - Last mean reward per episode: -16.92\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 289      |\n",
            "|    ep_rew_mean        | -16.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1665     |\n",
            "|    iterations         | 3000     |\n",
            "|    time_elapsed       | 72       |\n",
            "|    total_timesteps    | 120000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.577   |\n",
            "|    explained_variance | 0.861    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 2999     |\n",
            "|    policy_loss        | 2.58     |\n",
            "|    value_loss         | 63       |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 257      |\n",
            "|    ep_rew_mean        | -32.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1660     |\n",
            "|    iterations         | 3100     |\n",
            "|    time_elapsed       | 74       |\n",
            "|    total_timesteps    | 124000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.534   |\n",
            "|    explained_variance | 0.0822   |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 3099     |\n",
            "|    policy_loss        | 0.989    |\n",
            "|    value_loss         | 1.18e+03 |\n",
            "------------------------------------\n",
            "Num timesteps: 128000\n",
            "Best mean reward: -16.92 - Last mean reward per episode: -39.14\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 268      |\n",
            "|    ep_rew_mean        | -39.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1646     |\n",
            "|    iterations         | 3200     |\n",
            "|    time_elapsed       | 77       |\n",
            "|    total_timesteps    | 128000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.451   |\n",
            "|    explained_variance | 0.385    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 3199     |\n",
            "|    policy_loss        | -0.436   |\n",
            "|    value_loss         | 124      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 280      |\n",
            "|    ep_rew_mean        | -51.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1638     |\n",
            "|    iterations         | 3300     |\n",
            "|    time_elapsed       | 80       |\n",
            "|    total_timesteps    | 132000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.742   |\n",
            "|    explained_variance | 0.213    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 3299     |\n",
            "|    policy_loss        | 0.855    |\n",
            "|    value_loss         | 504      |\n",
            "------------------------------------\n",
            "Num timesteps: 136000\n",
            "Best mean reward: -16.92 - Last mean reward per episode: -45.83\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 304      |\n",
            "|    ep_rew_mean        | -45.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1626     |\n",
            "|    iterations         | 3400     |\n",
            "|    time_elapsed       | 83       |\n",
            "|    total_timesteps    | 136000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.381   |\n",
            "|    explained_variance | 0.907    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 3399     |\n",
            "|    policy_loss        | -0.593   |\n",
            "|    value_loss         | 20.7     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 319      |\n",
            "|    ep_rew_mean        | -38.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1619     |\n",
            "|    iterations         | 3500     |\n",
            "|    time_elapsed       | 86       |\n",
            "|    total_timesteps    | 140000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.516   |\n",
            "|    explained_variance | 0.947    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 3499     |\n",
            "|    policy_loss        | 1.93     |\n",
            "|    value_loss         | 20.4     |\n",
            "------------------------------------\n",
            "Num timesteps: 144000\n",
            "Best mean reward: -16.92 - Last mean reward per episode: -36.60\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 320      |\n",
            "|    ep_rew_mean        | -36.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1620     |\n",
            "|    iterations         | 3600     |\n",
            "|    time_elapsed       | 88       |\n",
            "|    total_timesteps    | 144000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.478   |\n",
            "|    explained_variance | 0.936    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 3599     |\n",
            "|    policy_loss        | 0.0747   |\n",
            "|    value_loss         | 17.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 324      |\n",
            "|    ep_rew_mean        | -27.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1605     |\n",
            "|    iterations         | 3700     |\n",
            "|    time_elapsed       | 92       |\n",
            "|    total_timesteps    | 148000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.608   |\n",
            "|    explained_variance | 0.639    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 3699     |\n",
            "|    policy_loss        | 0.174    |\n",
            "|    value_loss         | 85.6     |\n",
            "------------------------------------\n",
            "Num timesteps: 152000\n",
            "Best mean reward: -16.92 - Last mean reward per episode: -18.01\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 352      |\n",
            "|    ep_rew_mean        | -18      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1594     |\n",
            "|    iterations         | 3800     |\n",
            "|    time_elapsed       | 95       |\n",
            "|    total_timesteps    | 152000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.431   |\n",
            "|    explained_variance | 0.819    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 3799     |\n",
            "|    policy_loss        | 0.0785   |\n",
            "|    value_loss         | 46.2     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 359      |\n",
            "|    ep_rew_mean        | -5.42    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1588     |\n",
            "|    iterations         | 3900     |\n",
            "|    time_elapsed       | 98       |\n",
            "|    total_timesteps    | 156000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.345   |\n",
            "|    explained_variance | 0.237    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 3899     |\n",
            "|    policy_loss        | -0.389   |\n",
            "|    value_loss         | 1.05e+03 |\n",
            "------------------------------------\n",
            "Num timesteps: 160000\n",
            "Best mean reward: -16.92 - Last mean reward per episode: 15.72\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 368      |\n",
            "|    ep_rew_mean        | 15.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1587     |\n",
            "|    iterations         | 4000     |\n",
            "|    time_elapsed       | 100      |\n",
            "|    total_timesteps    | 160000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.308   |\n",
            "|    explained_variance | 0.922    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 3999     |\n",
            "|    policy_loss        | -0.188   |\n",
            "|    value_loss         | 13.6     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 368      |\n",
            "|    ep_rew_mean        | 32.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1586     |\n",
            "|    iterations         | 4100     |\n",
            "|    time_elapsed       | 103      |\n",
            "|    total_timesteps    | 164000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.501   |\n",
            "|    explained_variance | 0.37     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 4099     |\n",
            "|    policy_loss        | 2.37     |\n",
            "|    value_loss         | 475      |\n",
            "------------------------------------\n",
            "Num timesteps: 168000\n",
            "Best mean reward: 15.72 - Last mean reward per episode: 40.04\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 351      |\n",
            "|    ep_rew_mean        | 40       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1577     |\n",
            "|    iterations         | 4200     |\n",
            "|    time_elapsed       | 106      |\n",
            "|    total_timesteps    | 168000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.523   |\n",
            "|    explained_variance | 0.945    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 4199     |\n",
            "|    policy_loss        | -1.02    |\n",
            "|    value_loss         | 21       |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 353      |\n",
            "|    ep_rew_mean        | 42.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1564     |\n",
            "|    iterations         | 4300     |\n",
            "|    time_elapsed       | 109      |\n",
            "|    total_timesteps    | 172000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.603   |\n",
            "|    explained_variance | 0.185    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 4299     |\n",
            "|    policy_loss        | -1.31    |\n",
            "|    value_loss         | 740      |\n",
            "------------------------------------\n",
            "Num timesteps: 176000\n",
            "Best mean reward: 40.04 - Last mean reward per episode: 26.88\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 362      |\n",
            "|    ep_rew_mean        | 26.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1558     |\n",
            "|    iterations         | 4400     |\n",
            "|    time_elapsed       | 112      |\n",
            "|    total_timesteps    | 176000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.532   |\n",
            "|    explained_variance | 0.837    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 4399     |\n",
            "|    policy_loss        | -0.411   |\n",
            "|    value_loss         | 39.1     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 365      |\n",
            "|    ep_rew_mean        | 31.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1556     |\n",
            "|    iterations         | 4500     |\n",
            "|    time_elapsed       | 115      |\n",
            "|    total_timesteps    | 180000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.373   |\n",
            "|    explained_variance | 0.933    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 4499     |\n",
            "|    policy_loss        | 0.0414   |\n",
            "|    value_loss         | 27.7     |\n",
            "------------------------------------\n",
            "Num timesteps: 184000\n",
            "Best mean reward: 40.04 - Last mean reward per episode: 34.82\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 379      |\n",
            "|    ep_rew_mean        | 34.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1550     |\n",
            "|    iterations         | 4600     |\n",
            "|    time_elapsed       | 118      |\n",
            "|    total_timesteps    | 184000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.648   |\n",
            "|    explained_variance | 0.963    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 4599     |\n",
            "|    policy_loss        | 0.549    |\n",
            "|    value_loss         | 15.6     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 364      |\n",
            "|    ep_rew_mean        | 26.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1548     |\n",
            "|    iterations         | 4700     |\n",
            "|    time_elapsed       | 121      |\n",
            "|    total_timesteps    | 188000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.6     |\n",
            "|    explained_variance | 0.885    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 4699     |\n",
            "|    policy_loss        | -0.459   |\n",
            "|    value_loss         | 52.7     |\n",
            "------------------------------------\n",
            "Num timesteps: 192000\n",
            "Best mean reward: 40.04 - Last mean reward per episode: 16.94\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 361      |\n",
            "|    ep_rew_mean        | 16.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1548     |\n",
            "|    iterations         | 4800     |\n",
            "|    time_elapsed       | 124      |\n",
            "|    total_timesteps    | 192000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.401   |\n",
            "|    explained_variance | 0.91     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 4799     |\n",
            "|    policy_loss        | -0.225   |\n",
            "|    value_loss         | 16.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 366      |\n",
            "|    ep_rew_mean        | 19.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1546     |\n",
            "|    iterations         | 4900     |\n",
            "|    time_elapsed       | 126      |\n",
            "|    total_timesteps    | 196000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.427   |\n",
            "|    explained_variance | 0.889    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 4899     |\n",
            "|    policy_loss        | -1.29    |\n",
            "|    value_loss         | 32.6     |\n",
            "------------------------------------\n",
            "Num timesteps: 200000\n",
            "Best mean reward: 40.04 - Last mean reward per episode: 11.93\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 366      |\n",
            "|    ep_rew_mean        | 11.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1540     |\n",
            "|    iterations         | 5000     |\n",
            "|    time_elapsed       | 129      |\n",
            "|    total_timesteps    | 200000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.334   |\n",
            "|    explained_variance | 0.84     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 4999     |\n",
            "|    policy_loss        | 0.882    |\n",
            "|    value_loss         | 8.64     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 379      |\n",
            "|    ep_rew_mean        | 9.23     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1532     |\n",
            "|    iterations         | 5100     |\n",
            "|    time_elapsed       | 133      |\n",
            "|    total_timesteps    | 204000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.514   |\n",
            "|    explained_variance | 0.538    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 5099     |\n",
            "|    policy_loss        | 1.27     |\n",
            "|    value_loss         | 136      |\n",
            "------------------------------------\n",
            "Num timesteps: 208000\n",
            "Best mean reward: 40.04 - Last mean reward per episode: 8.78\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 382      |\n",
            "|    ep_rew_mean        | 8.78     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1522     |\n",
            "|    iterations         | 5200     |\n",
            "|    time_elapsed       | 136      |\n",
            "|    total_timesteps    | 208000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.396   |\n",
            "|    explained_variance | 0.628    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 5199     |\n",
            "|    policy_loss        | 0.0303   |\n",
            "|    value_loss         | 222      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 394      |\n",
            "|    ep_rew_mean        | 9.8      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1511     |\n",
            "|    iterations         | 5300     |\n",
            "|    time_elapsed       | 140      |\n",
            "|    total_timesteps    | 212000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.601   |\n",
            "|    explained_variance | 0.931    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 5299     |\n",
            "|    policy_loss        | -0.243   |\n",
            "|    value_loss         | 16.7     |\n",
            "------------------------------------\n",
            "Num timesteps: 216000\n",
            "Best mean reward: 40.04 - Last mean reward per episode: 14.82\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 387      |\n",
            "|    ep_rew_mean        | 14.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1500     |\n",
            "|    iterations         | 5400     |\n",
            "|    time_elapsed       | 143      |\n",
            "|    total_timesteps    | 216000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.487   |\n",
            "|    explained_variance | -0.962   |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 5399     |\n",
            "|    policy_loss        | 1.05     |\n",
            "|    value_loss         | 126      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 422      |\n",
            "|    ep_rew_mean        | 33.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1486     |\n",
            "|    iterations         | 5500     |\n",
            "|    time_elapsed       | 147      |\n",
            "|    total_timesteps    | 220000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.59    |\n",
            "|    explained_variance | 0.928    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 5499     |\n",
            "|    policy_loss        | -0.962   |\n",
            "|    value_loss         | 20.1     |\n",
            "------------------------------------\n",
            "Num timesteps: 224000\n",
            "Best mean reward: 40.04 - Last mean reward per episode: 35.84\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 429      |\n",
            "|    ep_rew_mean        | 35.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1485     |\n",
            "|    iterations         | 5600     |\n",
            "|    time_elapsed       | 150      |\n",
            "|    total_timesteps    | 224000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.596   |\n",
            "|    explained_variance | 0.91     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 5599     |\n",
            "|    policy_loss        | 0.0905   |\n",
            "|    value_loss         | 29.3     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 417      |\n",
            "|    ep_rew_mean        | 39.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1487     |\n",
            "|    iterations         | 5700     |\n",
            "|    time_elapsed       | 153      |\n",
            "|    total_timesteps    | 228000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.576   |\n",
            "|    explained_variance | 0.907    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 5699     |\n",
            "|    policy_loss        | 0.924    |\n",
            "|    value_loss         | 10.7     |\n",
            "------------------------------------\n",
            "Num timesteps: 232000\n",
            "Best mean reward: 40.04 - Last mean reward per episode: 43.41\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 421      |\n",
            "|    ep_rew_mean        | 43.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1479     |\n",
            "|    iterations         | 5800     |\n",
            "|    time_elapsed       | 156      |\n",
            "|    total_timesteps    | 232000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.486   |\n",
            "|    explained_variance | 0.671    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 5799     |\n",
            "|    policy_loss        | 1.45     |\n",
            "|    value_loss         | 118      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 431      |\n",
            "|    ep_rew_mean        | 45.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1474     |\n",
            "|    iterations         | 5900     |\n",
            "|    time_elapsed       | 160      |\n",
            "|    total_timesteps    | 236000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.437   |\n",
            "|    explained_variance | 0.209    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 5899     |\n",
            "|    policy_loss        | -0.287   |\n",
            "|    value_loss         | 718      |\n",
            "------------------------------------\n",
            "Num timesteps: 240000\n",
            "Best mean reward: 43.41 - Last mean reward per episode: 44.42\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 427      |\n",
            "|    ep_rew_mean        | 44.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1475     |\n",
            "|    iterations         | 6000     |\n",
            "|    time_elapsed       | 162      |\n",
            "|    total_timesteps    | 240000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.51    |\n",
            "|    explained_variance | 0.946    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 5999     |\n",
            "|    policy_loss        | 2.08     |\n",
            "|    value_loss         | 21.9     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 431      |\n",
            "|    ep_rew_mean        | 50.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1472     |\n",
            "|    iterations         | 6100     |\n",
            "|    time_elapsed       | 165      |\n",
            "|    total_timesteps    | 244000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.501   |\n",
            "|    explained_variance | 0.966    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 6099     |\n",
            "|    policy_loss        | -0.697   |\n",
            "|    value_loss         | 9.37     |\n",
            "------------------------------------\n",
            "Num timesteps: 248000\n",
            "Best mean reward: 44.42 - Last mean reward per episode: 49.42\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 431      |\n",
            "|    ep_rew_mean        | 49.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1472     |\n",
            "|    iterations         | 6200     |\n",
            "|    time_elapsed       | 168      |\n",
            "|    total_timesteps    | 248000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.294   |\n",
            "|    explained_variance | 0.092    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 6199     |\n",
            "|    policy_loss        | -0.319   |\n",
            "|    value_loss         | 708      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 407      |\n",
            "|    ep_rew_mean        | 43.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1473     |\n",
            "|    iterations         | 6300     |\n",
            "|    time_elapsed       | 171      |\n",
            "|    total_timesteps    | 252000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.562   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 6299     |\n",
            "|    policy_loss        | 0.235    |\n",
            "|    value_loss         | 1.64     |\n",
            "------------------------------------\n",
            "Num timesteps: 256000\n",
            "Best mean reward: 49.42 - Last mean reward per episode: 52.04\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 422      |\n",
            "|    ep_rew_mean        | 52       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1463     |\n",
            "|    iterations         | 6400     |\n",
            "|    time_elapsed       | 174      |\n",
            "|    total_timesteps    | 256000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.615   |\n",
            "|    explained_variance | 0.959    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 6399     |\n",
            "|    policy_loss        | 0.252    |\n",
            "|    value_loss         | 6.06     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 416      |\n",
            "|    ep_rew_mean        | 48.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1446     |\n",
            "|    iterations         | 6500     |\n",
            "|    time_elapsed       | 179      |\n",
            "|    total_timesteps    | 260000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.547   |\n",
            "|    explained_variance | 0.979    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 6499     |\n",
            "|    policy_loss        | -0.192   |\n",
            "|    value_loss         | 4.73     |\n",
            "------------------------------------\n",
            "Num timesteps: 264000\n",
            "Best mean reward: 52.04 - Last mean reward per episode: 37.66\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 421      |\n",
            "|    ep_rew_mean        | 37.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1434     |\n",
            "|    iterations         | 6600     |\n",
            "|    time_elapsed       | 183      |\n",
            "|    total_timesteps    | 264000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.142   |\n",
            "|    explained_variance | 0.734    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 6599     |\n",
            "|    policy_loss        | -1.16    |\n",
            "|    value_loss         | 60.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 422      |\n",
            "|    ep_rew_mean        | 44.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1436     |\n",
            "|    iterations         | 6700     |\n",
            "|    time_elapsed       | 186      |\n",
            "|    total_timesteps    | 268000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.437   |\n",
            "|    explained_variance | 0.909    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 6699     |\n",
            "|    policy_loss        | 1.01     |\n",
            "|    value_loss         | 38.3     |\n",
            "------------------------------------\n",
            "Num timesteps: 272000\n",
            "Best mean reward: 52.04 - Last mean reward per episode: 46.21\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 424      |\n",
            "|    ep_rew_mean        | 46.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1436     |\n",
            "|    iterations         | 6800     |\n",
            "|    time_elapsed       | 189      |\n",
            "|    total_timesteps    | 272000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.481   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 6799     |\n",
            "|    policy_loss        | -0.169   |\n",
            "|    value_loss         | 3.36     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 418      |\n",
            "|    ep_rew_mean        | 52.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1428     |\n",
            "|    iterations         | 6900     |\n",
            "|    time_elapsed       | 193      |\n",
            "|    total_timesteps    | 276000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.699   |\n",
            "|    explained_variance | 0.886    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 6899     |\n",
            "|    policy_loss        | -0.0534  |\n",
            "|    value_loss         | 17.1     |\n",
            "------------------------------------\n",
            "Num timesteps: 280000\n",
            "Best mean reward: 52.04 - Last mean reward per episode: 45.41\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 430      |\n",
            "|    ep_rew_mean        | 45.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1422     |\n",
            "|    iterations         | 7000     |\n",
            "|    time_elapsed       | 196      |\n",
            "|    total_timesteps    | 280000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.554   |\n",
            "|    explained_variance | 0.896    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 6999     |\n",
            "|    policy_loss        | 0.103    |\n",
            "|    value_loss         | 15.6     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 448      |\n",
            "|    ep_rew_mean        | 49.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1405     |\n",
            "|    iterations         | 7100     |\n",
            "|    time_elapsed       | 202      |\n",
            "|    total_timesteps    | 284000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.6     |\n",
            "|    explained_variance | 0.445    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 7099     |\n",
            "|    policy_loss        | -0.23    |\n",
            "|    value_loss         | 199      |\n",
            "------------------------------------\n",
            "Num timesteps: 288000\n",
            "Best mean reward: 52.04 - Last mean reward per episode: 52.71\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 469      |\n",
            "|    ep_rew_mean        | 52.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1383     |\n",
            "|    iterations         | 7200     |\n",
            "|    time_elapsed       | 208      |\n",
            "|    total_timesteps    | 288000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.604   |\n",
            "|    explained_variance | 0.935    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 7199     |\n",
            "|    policy_loss        | 0.0674   |\n",
            "|    value_loss         | 2.67     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 486      |\n",
            "|    ep_rew_mean        | 47.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1374     |\n",
            "|    iterations         | 7300     |\n",
            "|    time_elapsed       | 212      |\n",
            "|    total_timesteps    | 292000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.657   |\n",
            "|    explained_variance | 0.866    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 7299     |\n",
            "|    policy_loss        | -0.562   |\n",
            "|    value_loss         | 34.3     |\n",
            "------------------------------------\n",
            "Num timesteps: 296000\n",
            "Best mean reward: 52.71 - Last mean reward per episode: 51.70\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 481      |\n",
            "|    ep_rew_mean        | 51.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1363     |\n",
            "|    iterations         | 7400     |\n",
            "|    time_elapsed       | 217      |\n",
            "|    total_timesteps    | 296000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.475   |\n",
            "|    explained_variance | 0.935    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 7399     |\n",
            "|    policy_loss        | 0.449    |\n",
            "|    value_loss         | 22.9     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 501      |\n",
            "|    ep_rew_mean        | 57.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1356     |\n",
            "|    iterations         | 7500     |\n",
            "|    time_elapsed       | 221      |\n",
            "|    total_timesteps    | 300000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.426   |\n",
            "|    explained_variance | 0.979    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 7499     |\n",
            "|    policy_loss        | -0.178   |\n",
            "|    value_loss         | 6.78     |\n",
            "------------------------------------\n",
            "Num timesteps: 304000\n",
            "Best mean reward: 52.71 - Last mean reward per episode: 51.54\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 475      |\n",
            "|    ep_rew_mean        | 51.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1356     |\n",
            "|    iterations         | 7600     |\n",
            "|    time_elapsed       | 224      |\n",
            "|    total_timesteps    | 304000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.372   |\n",
            "|    explained_variance | 0.876    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 7599     |\n",
            "|    policy_loss        | 0.716    |\n",
            "|    value_loss         | 53.2     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 459      |\n",
            "|    ep_rew_mean        | 55.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1350     |\n",
            "|    iterations         | 7700     |\n",
            "|    time_elapsed       | 228      |\n",
            "|    total_timesteps    | 308000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.45    |\n",
            "|    explained_variance | 0.961    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 7699     |\n",
            "|    policy_loss        | 0.759    |\n",
            "|    value_loss         | 8.25     |\n",
            "------------------------------------\n",
            "Num timesteps: 312000\n",
            "Best mean reward: 52.71 - Last mean reward per episode: 55.51\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 457      |\n",
            "|    ep_rew_mean        | 55.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1343     |\n",
            "|    iterations         | 7800     |\n",
            "|    time_elapsed       | 232      |\n",
            "|    total_timesteps    | 312000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.579   |\n",
            "|    explained_variance | 0.922    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 7799     |\n",
            "|    policy_loss        | -1.18    |\n",
            "|    value_loss         | 16       |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 478      |\n",
            "|    ep_rew_mean        | 59.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1337     |\n",
            "|    iterations         | 7900     |\n",
            "|    time_elapsed       | 236      |\n",
            "|    total_timesteps    | 316000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.678   |\n",
            "|    explained_variance | 0.961    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 7899     |\n",
            "|    policy_loss        | 0.644    |\n",
            "|    value_loss         | 8.9      |\n",
            "------------------------------------\n",
            "Num timesteps: 320000\n",
            "Best mean reward: 55.51 - Last mean reward per episode: 63.88\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 488      |\n",
            "|    ep_rew_mean        | 63.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1331     |\n",
            "|    iterations         | 8000     |\n",
            "|    time_elapsed       | 240      |\n",
            "|    total_timesteps    | 320000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.453   |\n",
            "|    explained_variance | -0.0398  |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 7999     |\n",
            "|    policy_loss        | -4.53    |\n",
            "|    value_loss         | 869      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 503      |\n",
            "|    ep_rew_mean        | 57.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1326     |\n",
            "|    iterations         | 8100     |\n",
            "|    time_elapsed       | 244      |\n",
            "|    total_timesteps    | 324000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.413   |\n",
            "|    explained_variance | 0.955    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 8099     |\n",
            "|    policy_loss        | 1.9      |\n",
            "|    value_loss         | 21.2     |\n",
            "------------------------------------\n",
            "Num timesteps: 328000\n",
            "Best mean reward: 63.88 - Last mean reward per episode: 54.22\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 501      |\n",
            "|    ep_rew_mean        | 54.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1319     |\n",
            "|    iterations         | 8200     |\n",
            "|    time_elapsed       | 248      |\n",
            "|    total_timesteps    | 328000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.534   |\n",
            "|    explained_variance | 0.97     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 8199     |\n",
            "|    policy_loss        | 0.0224   |\n",
            "|    value_loss         | 8.04     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 511      |\n",
            "|    ep_rew_mean        | 53       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1312     |\n",
            "|    iterations         | 8300     |\n",
            "|    time_elapsed       | 252      |\n",
            "|    total_timesteps    | 332000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.623   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 8299     |\n",
            "|    policy_loss        | -0.708   |\n",
            "|    value_loss         | 1.2      |\n",
            "------------------------------------\n",
            "Num timesteps: 336000\n",
            "Best mean reward: 63.88 - Last mean reward per episode: 52.95\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 516      |\n",
            "|    ep_rew_mean        | 53       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1305     |\n",
            "|    iterations         | 8400     |\n",
            "|    time_elapsed       | 257      |\n",
            "|    total_timesteps    | 336000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.628   |\n",
            "|    explained_variance | 0.817    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 8399     |\n",
            "|    policy_loss        | 0.908    |\n",
            "|    value_loss         | 49.8     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 517      |\n",
            "|    ep_rew_mean        | 57.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1294     |\n",
            "|    iterations         | 8500     |\n",
            "|    time_elapsed       | 262      |\n",
            "|    total_timesteps    | 340000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.64    |\n",
            "|    explained_variance | 0.931    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 8499     |\n",
            "|    policy_loss        | -0.321   |\n",
            "|    value_loss         | 9.86     |\n",
            "------------------------------------\n",
            "Num timesteps: 344000\n",
            "Best mean reward: 63.88 - Last mean reward per episode: 56.57\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 517      |\n",
            "|    ep_rew_mean        | 56.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1285     |\n",
            "|    iterations         | 8600     |\n",
            "|    time_elapsed       | 267      |\n",
            "|    total_timesteps    | 344000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.69    |\n",
            "|    explained_variance | 0.931    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 8599     |\n",
            "|    policy_loss        | -0.206   |\n",
            "|    value_loss         | 4.72     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 540      |\n",
            "|    ep_rew_mean        | 53.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1275     |\n",
            "|    iterations         | 8700     |\n",
            "|    time_elapsed       | 272      |\n",
            "|    total_timesteps    | 348000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.628   |\n",
            "|    explained_variance | 0.925    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 8699     |\n",
            "|    policy_loss        | 0.277    |\n",
            "|    value_loss         | 22.7     |\n",
            "------------------------------------\n",
            "Num timesteps: 352000\n",
            "Best mean reward: 63.88 - Last mean reward per episode: 49.29\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 559      |\n",
            "|    ep_rew_mean        | 49.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1267     |\n",
            "|    iterations         | 8800     |\n",
            "|    time_elapsed       | 277      |\n",
            "|    total_timesteps    | 352000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.462   |\n",
            "|    explained_variance | 0.918    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 8799     |\n",
            "|    policy_loss        | -0.722   |\n",
            "|    value_loss         | 26.7     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 556      |\n",
            "|    ep_rew_mean        | 55.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1263     |\n",
            "|    iterations         | 8900     |\n",
            "|    time_elapsed       | 281      |\n",
            "|    total_timesteps    | 356000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.609   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 8899     |\n",
            "|    policy_loss        | -0.385   |\n",
            "|    value_loss         | 2.36     |\n",
            "------------------------------------\n",
            "Num timesteps: 360000\n",
            "Best mean reward: 63.88 - Last mean reward per episode: 50.60\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 580      |\n",
            "|    ep_rew_mean        | 50.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1253     |\n",
            "|    iterations         | 9000     |\n",
            "|    time_elapsed       | 287      |\n",
            "|    total_timesteps    | 360000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.528   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 8999     |\n",
            "|    policy_loss        | -0.167   |\n",
            "|    value_loss         | 2.22     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 604      |\n",
            "|    ep_rew_mean        | 51.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1246     |\n",
            "|    iterations         | 9100     |\n",
            "|    time_elapsed       | 291      |\n",
            "|    total_timesteps    | 364000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.6     |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 9099     |\n",
            "|    policy_loss        | -0.261   |\n",
            "|    value_loss         | 4.08     |\n",
            "------------------------------------\n",
            "Num timesteps: 368000\n",
            "Best mean reward: 63.88 - Last mean reward per episode: 53.25\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 615      |\n",
            "|    ep_rew_mean        | 53.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1242     |\n",
            "|    iterations         | 9200     |\n",
            "|    time_elapsed       | 296      |\n",
            "|    total_timesteps    | 368000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.639   |\n",
            "|    explained_variance | 0.97     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 9199     |\n",
            "|    policy_loss        | -0.217   |\n",
            "|    value_loss         | 6.66     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 637      |\n",
            "|    ep_rew_mean        | 57.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1235     |\n",
            "|    iterations         | 9300     |\n",
            "|    time_elapsed       | 301      |\n",
            "|    total_timesteps    | 372000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.513   |\n",
            "|    explained_variance | 0.952    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 9299     |\n",
            "|    policy_loss        | 1.08     |\n",
            "|    value_loss         | 7.51     |\n",
            "------------------------------------\n",
            "Num timesteps: 376000\n",
            "Best mean reward: 63.88 - Last mean reward per episode: 56.26\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 650      |\n",
            "|    ep_rew_mean        | 56.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1230     |\n",
            "|    iterations         | 9400     |\n",
            "|    time_elapsed       | 305      |\n",
            "|    total_timesteps    | 376000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.694   |\n",
            "|    explained_variance | 0.967    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 9399     |\n",
            "|    policy_loss        | -0.0168  |\n",
            "|    value_loss         | 2.92     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 664      |\n",
            "|    ep_rew_mean        | 56       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1219     |\n",
            "|    iterations         | 9500     |\n",
            "|    time_elapsed       | 311      |\n",
            "|    total_timesteps    | 380000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.644   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 9499     |\n",
            "|    policy_loss        | -0.2     |\n",
            "|    value_loss         | 2.7      |\n",
            "------------------------------------\n",
            "Num timesteps: 384000\n",
            "Best mean reward: 63.88 - Last mean reward per episode: 54.48\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 679      |\n",
            "|    ep_rew_mean        | 54.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1215     |\n",
            "|    iterations         | 9600     |\n",
            "|    time_elapsed       | 315      |\n",
            "|    total_timesteps    | 384000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.608   |\n",
            "|    explained_variance | 0.951    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 9599     |\n",
            "|    policy_loss        | -0.65    |\n",
            "|    value_loss         | 6.57     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 683      |\n",
            "|    ep_rew_mean        | 54.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1209     |\n",
            "|    iterations         | 9700     |\n",
            "|    time_elapsed       | 320      |\n",
            "|    total_timesteps    | 388000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.547   |\n",
            "|    explained_variance | 0.963    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 9699     |\n",
            "|    policy_loss        | 0.111    |\n",
            "|    value_loss         | 1.82     |\n",
            "------------------------------------\n",
            "Num timesteps: 392000\n",
            "Best mean reward: 63.88 - Last mean reward per episode: 51.25\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 712      |\n",
            "|    ep_rew_mean        | 51.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1202     |\n",
            "|    iterations         | 9800     |\n",
            "|    time_elapsed       | 326      |\n",
            "|    total_timesteps    | 392000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.559   |\n",
            "|    explained_variance | 0.678    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 9799     |\n",
            "|    policy_loss        | 2.04     |\n",
            "|    value_loss         | 33.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 715      |\n",
            "|    ep_rew_mean        | 50.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 9900     |\n",
            "|    time_elapsed       | 331      |\n",
            "|    total_timesteps    | 396000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.622   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 9899     |\n",
            "|    policy_loss        | -0.118   |\n",
            "|    value_loss         | 1.14     |\n",
            "------------------------------------\n",
            "Num timesteps: 400000\n",
            "Best mean reward: 63.88 - Last mean reward per episode: 49.49\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 739      |\n",
            "|    ep_rew_mean        | 49.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1184     |\n",
            "|    iterations         | 10000    |\n",
            "|    time_elapsed       | 337      |\n",
            "|    total_timesteps    | 400000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.782   |\n",
            "|    explained_variance | 0.951    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 9999     |\n",
            "|    policy_loss        | 0.482    |\n",
            "|    value_loss         | 3.83     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 739      |\n",
            "|    ep_rew_mean        | 41.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1177     |\n",
            "|    iterations         | 10100    |\n",
            "|    time_elapsed       | 343      |\n",
            "|    total_timesteps    | 404000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.4     |\n",
            "|    explained_variance | 0.942    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 10099    |\n",
            "|    policy_loss        | 0.336    |\n",
            "|    value_loss         | 17.5     |\n",
            "------------------------------------\n",
            "Num timesteps: 408000\n",
            "Best mean reward: 63.88 - Last mean reward per episode: 42.60\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 735      |\n",
            "|    ep_rew_mean        | 42.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1174     |\n",
            "|    iterations         | 10200    |\n",
            "|    time_elapsed       | 347      |\n",
            "|    total_timesteps    | 408000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.567   |\n",
            "|    explained_variance | 0.883    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 10199    |\n",
            "|    policy_loss        | -0.537   |\n",
            "|    value_loss         | 13.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 724      |\n",
            "|    ep_rew_mean        | 47.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1168     |\n",
            "|    iterations         | 10300    |\n",
            "|    time_elapsed       | 352      |\n",
            "|    total_timesteps    | 412000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.541   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 10299    |\n",
            "|    policy_loss        | -0.136   |\n",
            "|    value_loss         | 1.68     |\n",
            "------------------------------------\n",
            "Num timesteps: 416000\n",
            "Best mean reward: 63.88 - Last mean reward per episode: 41.80\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 736      |\n",
            "|    ep_rew_mean        | 41.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1163     |\n",
            "|    iterations         | 10400    |\n",
            "|    time_elapsed       | 357      |\n",
            "|    total_timesteps    | 416000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.631   |\n",
            "|    explained_variance | 0.929    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 10399    |\n",
            "|    policy_loss        | -0.648   |\n",
            "|    value_loss         | 5.14     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 734      |\n",
            "|    ep_rew_mean        | 46       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1154     |\n",
            "|    iterations         | 10500    |\n",
            "|    time_elapsed       | 363      |\n",
            "|    total_timesteps    | 420000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.501   |\n",
            "|    explained_variance | 0.979    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 10499    |\n",
            "|    policy_loss        | 0.0427   |\n",
            "|    value_loss         | 3.85     |\n",
            "------------------------------------\n",
            "Num timesteps: 424000\n",
            "Best mean reward: 63.88 - Last mean reward per episode: 47.57\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 737      |\n",
            "|    ep_rew_mean        | 47.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1151     |\n",
            "|    iterations         | 10600    |\n",
            "|    time_elapsed       | 368      |\n",
            "|    total_timesteps    | 424000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.617   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 10599    |\n",
            "|    policy_loss        | -0.17    |\n",
            "|    value_loss         | 3.01     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 738      |\n",
            "|    ep_rew_mean        | 48.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1146     |\n",
            "|    iterations         | 10700    |\n",
            "|    time_elapsed       | 373      |\n",
            "|    total_timesteps    | 428000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.441   |\n",
            "|    explained_variance | 0.977    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 10699    |\n",
            "|    policy_loss        | -0.261   |\n",
            "|    value_loss         | 4.57     |\n",
            "------------------------------------\n",
            "Num timesteps: 432000\n",
            "Best mean reward: 63.88 - Last mean reward per episode: 46.74\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 746      |\n",
            "|    ep_rew_mean        | 46.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1144     |\n",
            "|    iterations         | 10800    |\n",
            "|    time_elapsed       | 377      |\n",
            "|    total_timesteps    | 432000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.544   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 10799    |\n",
            "|    policy_loss        | -0.214   |\n",
            "|    value_loss         | 0.877    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 741      |\n",
            "|    ep_rew_mean        | 50.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1137     |\n",
            "|    iterations         | 10900    |\n",
            "|    time_elapsed       | 383      |\n",
            "|    total_timesteps    | 436000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.464   |\n",
            "|    explained_variance | 0.978    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 10899    |\n",
            "|    policy_loss        | 0.318    |\n",
            "|    value_loss         | 3.62     |\n",
            "------------------------------------\n",
            "Num timesteps: 440000\n",
            "Best mean reward: 63.88 - Last mean reward per episode: 50.47\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 756      |\n",
            "|    ep_rew_mean        | 50.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1134     |\n",
            "|    iterations         | 11000    |\n",
            "|    time_elapsed       | 387      |\n",
            "|    total_timesteps    | 440000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.597   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 10999    |\n",
            "|    policy_loss        | 0.134    |\n",
            "|    value_loss         | 1.12     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 758      |\n",
            "|    ep_rew_mean        | 58       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1131     |\n",
            "|    iterations         | 11100    |\n",
            "|    time_elapsed       | 392      |\n",
            "|    total_timesteps    | 444000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.527   |\n",
            "|    explained_variance | 0.978    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 11099    |\n",
            "|    policy_loss        | -0.0856  |\n",
            "|    value_loss         | 3.35     |\n",
            "------------------------------------\n",
            "Num timesteps: 448000\n",
            "Best mean reward: 63.88 - Last mean reward per episode: 55.06\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 740      |\n",
            "|    ep_rew_mean        | 55.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1128     |\n",
            "|    iterations         | 11200    |\n",
            "|    time_elapsed       | 396      |\n",
            "|    total_timesteps    | 448000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.599   |\n",
            "|    explained_variance | 0.012    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 11199    |\n",
            "|    policy_loss        | 5.23     |\n",
            "|    value_loss         | 194      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 717      |\n",
            "|    ep_rew_mean        | 67.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1129     |\n",
            "|    iterations         | 11300    |\n",
            "|    time_elapsed       | 400      |\n",
            "|    total_timesteps    | 452000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.583   |\n",
            "|    explained_variance | 0.976    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 11299    |\n",
            "|    policy_loss        | -0.368   |\n",
            "|    value_loss         | 2.16     |\n",
            "------------------------------------\n",
            "Num timesteps: 456000\n",
            "Best mean reward: 63.88 - Last mean reward per episode: 70.23\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 714      |\n",
            "|    ep_rew_mean        | 70.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1123     |\n",
            "|    iterations         | 11400    |\n",
            "|    time_elapsed       | 405      |\n",
            "|    total_timesteps    | 456000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.677   |\n",
            "|    explained_variance | 0.973    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 11399    |\n",
            "|    policy_loss        | 0.268    |\n",
            "|    value_loss         | 4.3      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 694      |\n",
            "|    ep_rew_mean        | 62.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1121     |\n",
            "|    iterations         | 11500    |\n",
            "|    time_elapsed       | 410      |\n",
            "|    total_timesteps    | 460000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.466   |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 11499    |\n",
            "|    policy_loss        | -0.199   |\n",
            "|    value_loss         | 1.84     |\n",
            "------------------------------------\n",
            "Num timesteps: 464000\n",
            "Best mean reward: 70.23 - Last mean reward per episode: 62.97\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 697      |\n",
            "|    ep_rew_mean        | 63       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1118     |\n",
            "|    iterations         | 11600    |\n",
            "|    time_elapsed       | 414      |\n",
            "|    total_timesteps    | 464000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.576   |\n",
            "|    explained_variance | 0.463    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 11599    |\n",
            "|    policy_loss        | 1.4      |\n",
            "|    value_loss         | 114      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 691      |\n",
            "|    ep_rew_mean        | 70.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1115     |\n",
            "|    iterations         | 11700    |\n",
            "|    time_elapsed       | 419      |\n",
            "|    total_timesteps    | 468000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.522   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 11699    |\n",
            "|    policy_loss        | -0.173   |\n",
            "|    value_loss         | 1.97     |\n",
            "------------------------------------\n",
            "Num timesteps: 472000\n",
            "Best mean reward: 70.23 - Last mean reward per episode: 75.28\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 686      |\n",
            "|    ep_rew_mean        | 75.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1113     |\n",
            "|    iterations         | 11800    |\n",
            "|    time_elapsed       | 423      |\n",
            "|    total_timesteps    | 472000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.532   |\n",
            "|    explained_variance | 0.924    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 11799    |\n",
            "|    policy_loss        | -0.243   |\n",
            "|    value_loss         | 0.991    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 693      |\n",
            "|    ep_rew_mean        | 73.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 11900    |\n",
            "|    time_elapsed       | 429      |\n",
            "|    total_timesteps    | 476000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.45    |\n",
            "|    explained_variance | 0.874    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 11899    |\n",
            "|    policy_loss        | 0.801    |\n",
            "|    value_loss         | 35.8     |\n",
            "------------------------------------\n",
            "Num timesteps: 480000\n",
            "Best mean reward: 75.28 - Last mean reward per episode: 72.86\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 690      |\n",
            "|    ep_rew_mean        | 72.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1107     |\n",
            "|    iterations         | 12000    |\n",
            "|    time_elapsed       | 433      |\n",
            "|    total_timesteps    | 480000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.654   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 11999    |\n",
            "|    policy_loss        | 0.373    |\n",
            "|    value_loss         | 2.08     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 697      |\n",
            "|    ep_rew_mean        | 75.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 12100    |\n",
            "|    time_elapsed       | 439      |\n",
            "|    total_timesteps    | 484000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.461   |\n",
            "|    explained_variance | 0.972    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 12099    |\n",
            "|    policy_loss        | -0.193   |\n",
            "|    value_loss         | 3.62     |\n",
            "------------------------------------\n",
            "Num timesteps: 488000\n",
            "Best mean reward: 75.28 - Last mean reward per episode: 81.98\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 686      |\n",
            "|    ep_rew_mean        | 82       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 12200    |\n",
            "|    time_elapsed       | 443      |\n",
            "|    total_timesteps    | 488000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.393   |\n",
            "|    explained_variance | 0.278    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 12199    |\n",
            "|    policy_loss        | 1.48     |\n",
            "|    value_loss         | 886      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 685      |\n",
            "|    ep_rew_mean        | 90.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 12300    |\n",
            "|    time_elapsed       | 448      |\n",
            "|    total_timesteps    | 492000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.429   |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 12299    |\n",
            "|    policy_loss        | -0.201   |\n",
            "|    value_loss         | 1.2      |\n",
            "------------------------------------\n",
            "Num timesteps: 496000\n",
            "Best mean reward: 81.98 - Last mean reward per episode: 92.85\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 674      |\n",
            "|    ep_rew_mean        | 92.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 12400    |\n",
            "|    time_elapsed       | 452      |\n",
            "|    total_timesteps    | 496000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.53    |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 12399    |\n",
            "|    policy_loss        | -0.133   |\n",
            "|    value_loss         | 1.45     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 659      |\n",
            "|    ep_rew_mean        | 102      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 12500    |\n",
            "|    time_elapsed       | 456      |\n",
            "|    total_timesteps    | 500000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.486   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 12499    |\n",
            "|    policy_loss        | 0.119    |\n",
            "|    value_loss         | 0.832    |\n",
            "------------------------------------\n",
            "Num timesteps: 504000\n",
            "Best mean reward: 92.85 - Last mean reward per episode: 111.44\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 639      |\n",
            "|    ep_rew_mean        | 111      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 12600    |\n",
            "|    time_elapsed       | 459      |\n",
            "|    total_timesteps    | 504000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.871   |\n",
            "|    explained_variance | 0.0825   |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 12599    |\n",
            "|    policy_loss        | 0.821    |\n",
            "|    value_loss         | 80.8     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 624      |\n",
            "|    ep_rew_mean        | 116      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 12700    |\n",
            "|    time_elapsed       | 463      |\n",
            "|    total_timesteps    | 508000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.494   |\n",
            "|    explained_variance | 0.979    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 12699    |\n",
            "|    policy_loss        | -0.034   |\n",
            "|    value_loss         | 0.925    |\n",
            "------------------------------------\n",
            "Num timesteps: 512000\n",
            "Best mean reward: 111.44 - Last mean reward per episode: 127.36\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 629      |\n",
            "|    ep_rew_mean        | 127      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 12800    |\n",
            "|    time_elapsed       | 467      |\n",
            "|    total_timesteps    | 512000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.523   |\n",
            "|    explained_variance | 0.925    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 12799    |\n",
            "|    policy_loss        | 0.0909   |\n",
            "|    value_loss         | 3.43     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 587      |\n",
            "|    ep_rew_mean        | 144      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 12900    |\n",
            "|    time_elapsed       | 469      |\n",
            "|    total_timesteps    | 516000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.469   |\n",
            "|    explained_variance | 0.152    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 12899    |\n",
            "|    policy_loss        | -0.459   |\n",
            "|    value_loss         | 501      |\n",
            "------------------------------------\n",
            "Num timesteps: 520000\n",
            "Best mean reward: 127.36 - Last mean reward per episode: 155.15\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 572      |\n",
            "|    ep_rew_mean        | 155      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 13000    |\n",
            "|    time_elapsed       | 472      |\n",
            "|    total_timesteps    | 520000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.408   |\n",
            "|    explained_variance | 0.455    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 12999    |\n",
            "|    policy_loss        | -0.169   |\n",
            "|    value_loss         | 495      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 571      |\n",
            "|    ep_rew_mean        | 158      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 13100    |\n",
            "|    time_elapsed       | 476      |\n",
            "|    total_timesteps    | 524000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.56    |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 13099    |\n",
            "|    policy_loss        | -0.424   |\n",
            "|    value_loss         | 1.93     |\n",
            "------------------------------------\n",
            "Num timesteps: 528000\n",
            "Best mean reward: 155.15 - Last mean reward per episode: 164.96\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 573      |\n",
            "|    ep_rew_mean        | 165      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1093     |\n",
            "|    iterations         | 13200    |\n",
            "|    time_elapsed       | 482      |\n",
            "|    total_timesteps    | 528000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.542   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 13199    |\n",
            "|    policy_loss        | -0.322   |\n",
            "|    value_loss         | 2.88     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 574      |\n",
            "|    ep_rew_mean        | 169      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1090     |\n",
            "|    iterations         | 13300    |\n",
            "|    time_elapsed       | 487      |\n",
            "|    total_timesteps    | 532000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.652   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 13299    |\n",
            "|    policy_loss        | 0.0937   |\n",
            "|    value_loss         | 3.76     |\n",
            "------------------------------------\n",
            "Num timesteps: 536000\n",
            "Best mean reward: 164.96 - Last mean reward per episode: 166.17\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 588      |\n",
            "|    ep_rew_mean        | 166      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1085     |\n",
            "|    iterations         | 13400    |\n",
            "|    time_elapsed       | 493      |\n",
            "|    total_timesteps    | 536000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.4     |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 13399    |\n",
            "|    policy_loss        | -0.0103  |\n",
            "|    value_loss         | 0.435    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 583      |\n",
            "|    ep_rew_mean        | 169      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1085     |\n",
            "|    iterations         | 13500    |\n",
            "|    time_elapsed       | 497      |\n",
            "|    total_timesteps    | 540000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.548   |\n",
            "|    explained_variance | 0.37     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 13499    |\n",
            "|    policy_loss        | 0.604    |\n",
            "|    value_loss         | 570      |\n",
            "------------------------------------\n",
            "Num timesteps: 544000\n",
            "Best mean reward: 166.17 - Last mean reward per episode: 169.94\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 572      |\n",
            "|    ep_rew_mean        | 170      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1086     |\n",
            "|    iterations         | 13600    |\n",
            "|    time_elapsed       | 500      |\n",
            "|    total_timesteps    | 544000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.636   |\n",
            "|    explained_variance | 0.949    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 13599    |\n",
            "|    policy_loss        | -0.489   |\n",
            "|    value_loss         | 3.57     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 570      |\n",
            "|    ep_rew_mean        | 170      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1086     |\n",
            "|    iterations         | 13700    |\n",
            "|    time_elapsed       | 504      |\n",
            "|    total_timesteps    | 548000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.612   |\n",
            "|    explained_variance | 0.817    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 13699    |\n",
            "|    policy_loss        | 0.0846   |\n",
            "|    value_loss         | 1.04     |\n",
            "------------------------------------\n",
            "Num timesteps: 552000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 166.10\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 569      |\n",
            "|    ep_rew_mean        | 166      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1085     |\n",
            "|    iterations         | 13800    |\n",
            "|    time_elapsed       | 508      |\n",
            "|    total_timesteps    | 552000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.589   |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 13799    |\n",
            "|    policy_loss        | -0.0341  |\n",
            "|    value_loss         | 1.91     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 564      |\n",
            "|    ep_rew_mean        | 161      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1085     |\n",
            "|    iterations         | 13900    |\n",
            "|    time_elapsed       | 512      |\n",
            "|    total_timesteps    | 556000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.523   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 13899    |\n",
            "|    policy_loss        | -0.319   |\n",
            "|    value_loss         | 1.16     |\n",
            "------------------------------------\n",
            "Num timesteps: 560000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 153.01\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 562      |\n",
            "|    ep_rew_mean        | 153      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1085     |\n",
            "|    iterations         | 14000    |\n",
            "|    time_elapsed       | 515      |\n",
            "|    total_timesteps    | 560000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.507   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 13999    |\n",
            "|    policy_loss        | -0.422   |\n",
            "|    value_loss         | 0.894    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 562      |\n",
            "|    ep_rew_mean        | 147      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1085     |\n",
            "|    iterations         | 14100    |\n",
            "|    time_elapsed       | 519      |\n",
            "|    total_timesteps    | 564000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.58    |\n",
            "|    explained_variance | 0.518    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 14099    |\n",
            "|    policy_loss        | -0.288   |\n",
            "|    value_loss         | 2.15     |\n",
            "------------------------------------\n",
            "Num timesteps: 568000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 143.14\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 575      |\n",
            "|    ep_rew_mean        | 143      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1082     |\n",
            "|    iterations         | 14200    |\n",
            "|    time_elapsed       | 524      |\n",
            "|    total_timesteps    | 568000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.492   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 14199    |\n",
            "|    policy_loss        | 0.0342   |\n",
            "|    value_loss         | 0.356    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 594      |\n",
            "|    ep_rew_mean        | 132      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1080     |\n",
            "|    iterations         | 14300    |\n",
            "|    time_elapsed       | 529      |\n",
            "|    total_timesteps    | 572000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.713   |\n",
            "|    explained_variance | 0.947    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 14299    |\n",
            "|    policy_loss        | -0.0911  |\n",
            "|    value_loss         | 2.23     |\n",
            "------------------------------------\n",
            "Num timesteps: 576000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 117.74\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 620      |\n",
            "|    ep_rew_mean        | 118      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1077     |\n",
            "|    iterations         | 14400    |\n",
            "|    time_elapsed       | 534      |\n",
            "|    total_timesteps    | 576000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.627   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 14399    |\n",
            "|    policy_loss        | 0.036    |\n",
            "|    value_loss         | 0.332    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 640      |\n",
            "|    ep_rew_mean        | 111      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1077     |\n",
            "|    iterations         | 14500    |\n",
            "|    time_elapsed       | 538      |\n",
            "|    total_timesteps    | 580000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.622   |\n",
            "|    explained_variance | -0.247   |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 14499    |\n",
            "|    policy_loss        | -0.685   |\n",
            "|    value_loss         | 71.5     |\n",
            "------------------------------------\n",
            "Num timesteps: 584000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 104.48\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 658      |\n",
            "|    ep_rew_mean        | 104      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1077     |\n",
            "|    iterations         | 14600    |\n",
            "|    time_elapsed       | 541      |\n",
            "|    total_timesteps    | 584000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.619   |\n",
            "|    explained_variance | 0.979    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 14599    |\n",
            "|    policy_loss        | 0.257    |\n",
            "|    value_loss         | 0.805    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 678      |\n",
            "|    ep_rew_mean        | 101      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1077     |\n",
            "|    iterations         | 14700    |\n",
            "|    time_elapsed       | 545      |\n",
            "|    total_timesteps    | 588000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.551   |\n",
            "|    explained_variance | 0.969    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 14699    |\n",
            "|    policy_loss        | 0.16     |\n",
            "|    value_loss         | 3.69     |\n",
            "------------------------------------\n",
            "Num timesteps: 592000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 96.33\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 675      |\n",
            "|    ep_rew_mean        | 96.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1077     |\n",
            "|    iterations         | 14800    |\n",
            "|    time_elapsed       | 549      |\n",
            "|    total_timesteps    | 592000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.603   |\n",
            "|    explained_variance | 0.978    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 14799    |\n",
            "|    policy_loss        | -0.164   |\n",
            "|    value_loss         | 1.9      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 665      |\n",
            "|    ep_rew_mean        | 98.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1076     |\n",
            "|    iterations         | 14900    |\n",
            "|    time_elapsed       | 553      |\n",
            "|    total_timesteps    | 596000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.467   |\n",
            "|    explained_variance | 0.0575   |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 14899    |\n",
            "|    policy_loss        | 0.103    |\n",
            "|    value_loss         | 843      |\n",
            "------------------------------------\n",
            "Num timesteps: 600000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 103.86\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 648      |\n",
            "|    ep_rew_mean        | 104      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1077     |\n",
            "|    iterations         | 15000    |\n",
            "|    time_elapsed       | 557      |\n",
            "|    total_timesteps    | 600000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.555   |\n",
            "|    explained_variance | 0.528    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 14999    |\n",
            "|    policy_loss        | 0.0882   |\n",
            "|    value_loss         | 9.67     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 645      |\n",
            "|    ep_rew_mean        | 105      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1077     |\n",
            "|    iterations         | 15100    |\n",
            "|    time_elapsed       | 560      |\n",
            "|    total_timesteps    | 604000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.413   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 15099    |\n",
            "|    policy_loss        | 0.0122   |\n",
            "|    value_loss         | 0.348    |\n",
            "------------------------------------\n",
            "Num timesteps: 608000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 105.12\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 639      |\n",
            "|    ep_rew_mean        | 105      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1078     |\n",
            "|    iterations         | 15200    |\n",
            "|    time_elapsed       | 563      |\n",
            "|    total_timesteps    | 608000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.617   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 15199    |\n",
            "|    policy_loss        | -0.0609  |\n",
            "|    value_loss         | 0.239    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 633      |\n",
            "|    ep_rew_mean        | 99.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1078     |\n",
            "|    iterations         | 15300    |\n",
            "|    time_elapsed       | 567      |\n",
            "|    total_timesteps    | 612000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.599   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 15299    |\n",
            "|    policy_loss        | 0.313    |\n",
            "|    value_loss         | 0.734    |\n",
            "------------------------------------\n",
            "Num timesteps: 616000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 103.27\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 644      |\n",
            "|    ep_rew_mean        | 103      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1077     |\n",
            "|    iterations         | 15400    |\n",
            "|    time_elapsed       | 571      |\n",
            "|    total_timesteps    | 616000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.664   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 15399    |\n",
            "|    policy_loss        | 0.0918   |\n",
            "|    value_loss         | 0.973    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 650      |\n",
            "|    ep_rew_mean        | 100      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1074     |\n",
            "|    iterations         | 15500    |\n",
            "|    time_elapsed       | 576      |\n",
            "|    total_timesteps    | 620000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.749   |\n",
            "|    explained_variance | 0.98     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 15499    |\n",
            "|    policy_loss        | -0.501   |\n",
            "|    value_loss         | 1.18     |\n",
            "------------------------------------\n",
            "Num timesteps: 624000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 91.04\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 678      |\n",
            "|    ep_rew_mean        | 91       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1071     |\n",
            "|    iterations         | 15600    |\n",
            "|    time_elapsed       | 582      |\n",
            "|    total_timesteps    | 624000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.639   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 15599    |\n",
            "|    policy_loss        | -0.0606  |\n",
            "|    value_loss         | 0.899    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 686      |\n",
            "|    ep_rew_mean        | 86       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1069     |\n",
            "|    iterations         | 15700    |\n",
            "|    time_elapsed       | 587      |\n",
            "|    total_timesteps    | 628000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.612   |\n",
            "|    explained_variance | 0.959    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 15699    |\n",
            "|    policy_loss        | 0.229    |\n",
            "|    value_loss         | 1.31     |\n",
            "------------------------------------\n",
            "Num timesteps: 632000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 72.42\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 716      |\n",
            "|    ep_rew_mean        | 72.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1068     |\n",
            "|    iterations         | 15800    |\n",
            "|    time_elapsed       | 591      |\n",
            "|    total_timesteps    | 632000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.72    |\n",
            "|    explained_variance | 0.945    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 15799    |\n",
            "|    policy_loss        | -0.158   |\n",
            "|    value_loss         | 0.676    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 724      |\n",
            "|    ep_rew_mean        | 69.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1067     |\n",
            "|    iterations         | 15900    |\n",
            "|    time_elapsed       | 595      |\n",
            "|    total_timesteps    | 636000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.518   |\n",
            "|    explained_variance | 0.934    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 15899    |\n",
            "|    policy_loss        | 0.454    |\n",
            "|    value_loss         | 12.9     |\n",
            "------------------------------------\n",
            "Num timesteps: 640000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 62.17\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 739      |\n",
            "|    ep_rew_mean        | 62.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 16000    |\n",
            "|    time_elapsed       | 600      |\n",
            "|    total_timesteps    | 640000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.603   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 15999    |\n",
            "|    policy_loss        | 0.111    |\n",
            "|    value_loss         | 0.41     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 713      |\n",
            "|    ep_rew_mean        | 77       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1066     |\n",
            "|    iterations         | 16100    |\n",
            "|    time_elapsed       | 603      |\n",
            "|    total_timesteps    | 644000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.611   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 16099    |\n",
            "|    policy_loss        | -0.368   |\n",
            "|    value_loss         | 2.01     |\n",
            "------------------------------------\n",
            "Num timesteps: 648000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 79.43\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 681      |\n",
            "|    ep_rew_mean        | 79.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1067     |\n",
            "|    iterations         | 16200    |\n",
            "|    time_elapsed       | 606      |\n",
            "|    total_timesteps    | 648000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.549   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 16199    |\n",
            "|    policy_loss        | -0.314   |\n",
            "|    value_loss         | 0.888    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 666      |\n",
            "|    ep_rew_mean        | 69.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1069     |\n",
            "|    iterations         | 16300    |\n",
            "|    time_elapsed       | 609      |\n",
            "|    total_timesteps    | 652000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.681   |\n",
            "|    explained_variance | 0.934    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 16299    |\n",
            "|    policy_loss        | -0.51    |\n",
            "|    value_loss         | 10.1     |\n",
            "------------------------------------\n",
            "Num timesteps: 656000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 62.65\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 653      |\n",
            "|    ep_rew_mean        | 62.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1070     |\n",
            "|    iterations         | 16400    |\n",
            "|    time_elapsed       | 612      |\n",
            "|    total_timesteps    | 656000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.598   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 16399    |\n",
            "|    policy_loss        | 0.254    |\n",
            "|    value_loss         | 1.86     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 632      |\n",
            "|    ep_rew_mean        | 47.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1071     |\n",
            "|    iterations         | 16500    |\n",
            "|    time_elapsed       | 615      |\n",
            "|    total_timesteps    | 660000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.529   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 16499    |\n",
            "|    policy_loss        | 0.175    |\n",
            "|    value_loss         | 2.14     |\n",
            "------------------------------------\n",
            "Num timesteps: 664000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 37.97\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 612      |\n",
            "|    ep_rew_mean        | 38       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1073     |\n",
            "|    iterations         | 16600    |\n",
            "|    time_elapsed       | 618      |\n",
            "|    total_timesteps    | 664000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.584   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 16599    |\n",
            "|    policy_loss        | 0.0265   |\n",
            "|    value_loss         | 2.46     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 598      |\n",
            "|    ep_rew_mean        | 24.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1074     |\n",
            "|    iterations         | 16700    |\n",
            "|    time_elapsed       | 621      |\n",
            "|    total_timesteps    | 668000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.771   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 16699    |\n",
            "|    policy_loss        | -0.0142  |\n",
            "|    value_loss         | 2.06     |\n",
            "------------------------------------\n",
            "Num timesteps: 672000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 9.88\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 569      |\n",
            "|    ep_rew_mean        | 9.88     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1076     |\n",
            "|    iterations         | 16800    |\n",
            "|    time_elapsed       | 624      |\n",
            "|    total_timesteps    | 672000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.703   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 16799    |\n",
            "|    policy_loss        | 0.0289   |\n",
            "|    value_loss         | 0.973    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 486      |\n",
            "|    ep_rew_mean        | 9.22     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1078     |\n",
            "|    iterations         | 16900    |\n",
            "|    time_elapsed       | 626      |\n",
            "|    total_timesteps    | 676000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.487   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 16899    |\n",
            "|    policy_loss        | -0.0909  |\n",
            "|    value_loss         | 1.52     |\n",
            "------------------------------------\n",
            "Num timesteps: 680000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 13.70\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 428      |\n",
            "|    ep_rew_mean        | 13.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1080     |\n",
            "|    iterations         | 17000    |\n",
            "|    time_elapsed       | 629      |\n",
            "|    total_timesteps    | 680000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.622   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 16999    |\n",
            "|    policy_loss        | 0.0293   |\n",
            "|    value_loss         | 2.8      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 395      |\n",
            "|    ep_rew_mean        | 2.77     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1082     |\n",
            "|    iterations         | 17100    |\n",
            "|    time_elapsed       | 631      |\n",
            "|    total_timesteps    | 684000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.665   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 17099    |\n",
            "|    policy_loss        | -0.208   |\n",
            "|    value_loss         | 2.28     |\n",
            "------------------------------------\n",
            "Num timesteps: 688000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -12.29\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 384      |\n",
            "|    ep_rew_mean        | -12.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1084     |\n",
            "|    iterations         | 17200    |\n",
            "|    time_elapsed       | 634      |\n",
            "|    total_timesteps    | 688000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.685   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 17199    |\n",
            "|    policy_loss        | 0.453    |\n",
            "|    value_loss         | 3.17     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 363      |\n",
            "|    ep_rew_mean        | -17.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1086     |\n",
            "|    iterations         | 17300    |\n",
            "|    time_elapsed       | 636      |\n",
            "|    total_timesteps    | 692000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.559   |\n",
            "|    explained_variance | 0.86     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 17299    |\n",
            "|    policy_loss        | 1.65     |\n",
            "|    value_loss         | 114      |\n",
            "------------------------------------\n",
            "Num timesteps: 696000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -24.58\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 341      |\n",
            "|    ep_rew_mean        | -24.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1089     |\n",
            "|    iterations         | 17400    |\n",
            "|    time_elapsed       | 638      |\n",
            "|    total_timesteps    | 696000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.513   |\n",
            "|    explained_variance | 0.953    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 17399    |\n",
            "|    policy_loss        | -0.642   |\n",
            "|    value_loss         | 7.56     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 315      |\n",
            "|    ep_rew_mean        | -32.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1091     |\n",
            "|    iterations         | 17500    |\n",
            "|    time_elapsed       | 641      |\n",
            "|    total_timesteps    | 700000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.699   |\n",
            "|    explained_variance | 0.376    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 17499    |\n",
            "|    policy_loss        | 5.58     |\n",
            "|    value_loss         | 223      |\n",
            "------------------------------------\n",
            "Num timesteps: 704000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -26.31\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 305      |\n",
            "|    ep_rew_mean        | -26.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1093     |\n",
            "|    iterations         | 17600    |\n",
            "|    time_elapsed       | 643      |\n",
            "|    total_timesteps    | 704000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.661   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 17599    |\n",
            "|    policy_loss        | -0.903   |\n",
            "|    value_loss         | 6.63     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 297      |\n",
            "|    ep_rew_mean        | -20.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 17700    |\n",
            "|    time_elapsed       | 645      |\n",
            "|    total_timesteps    | 708000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.638   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 17699    |\n",
            "|    policy_loss        | -0.426   |\n",
            "|    value_loss         | 2.53     |\n",
            "------------------------------------\n",
            "Num timesteps: 712000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -26.51\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 275      |\n",
            "|    ep_rew_mean        | -26.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 17800    |\n",
            "|    time_elapsed       | 648      |\n",
            "|    total_timesteps    | 712000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.68    |\n",
            "|    explained_variance | 0.971    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 17799    |\n",
            "|    policy_loss        | 0.235    |\n",
            "|    value_loss         | 5.46     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 276      |\n",
            "|    ep_rew_mean        | -32.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 17900    |\n",
            "|    time_elapsed       | 652      |\n",
            "|    total_timesteps    | 716000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.767   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 17899    |\n",
            "|    policy_loss        | 0.0471   |\n",
            "|    value_loss         | 2.92     |\n",
            "------------------------------------\n",
            "Num timesteps: 720000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -44.25\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 290      |\n",
            "|    ep_rew_mean        | -44.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 18000    |\n",
            "|    time_elapsed       | 655      |\n",
            "|    total_timesteps    | 720000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.686   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 17999    |\n",
            "|    policy_loss        | 0.0398   |\n",
            "|    value_loss         | 3.79     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 295      |\n",
            "|    ep_rew_mean        | -53.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 18100    |\n",
            "|    time_elapsed       | 657      |\n",
            "|    total_timesteps    | 724000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.617   |\n",
            "|    explained_variance | 0.978    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 18099    |\n",
            "|    policy_loss        | -0.675   |\n",
            "|    value_loss         | 4.56     |\n",
            "------------------------------------\n",
            "Num timesteps: 728000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -70.70\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 299      |\n",
            "|    ep_rew_mean        | -70.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 18200    |\n",
            "|    time_elapsed       | 660      |\n",
            "|    total_timesteps    | 728000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.627   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 18199    |\n",
            "|    policy_loss        | -0.656   |\n",
            "|    value_loss         | 3.59     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 323      |\n",
            "|    ep_rew_mean        | -76.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 18300    |\n",
            "|    time_elapsed       | 663      |\n",
            "|    total_timesteps    | 732000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.662   |\n",
            "|    explained_variance | 0.963    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 18299    |\n",
            "|    policy_loss        | -0.0592  |\n",
            "|    value_loss         | 3.42     |\n",
            "------------------------------------\n",
            "Num timesteps: 736000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -85.59\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 328      |\n",
            "|    ep_rew_mean        | -85.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 18400    |\n",
            "|    time_elapsed       | 667      |\n",
            "|    total_timesteps    | 736000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.434   |\n",
            "|    explained_variance | 0.954    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 18399    |\n",
            "|    policy_loss        | 0.469    |\n",
            "|    value_loss         | 4.63     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 351      |\n",
            "|    ep_rew_mean        | -98.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 18500    |\n",
            "|    time_elapsed       | 671      |\n",
            "|    total_timesteps    | 740000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.835   |\n",
            "|    explained_variance | 0.389    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 18499    |\n",
            "|    policy_loss        | -6.99    |\n",
            "|    value_loss         | 568      |\n",
            "------------------------------------\n",
            "Num timesteps: 744000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -109.52\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 355      |\n",
            "|    ep_rew_mean        | -110     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 18600    |\n",
            "|    time_elapsed       | 675      |\n",
            "|    total_timesteps    | 744000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.69    |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 18599    |\n",
            "|    policy_loss        | 0.0921   |\n",
            "|    value_loss         | 2.86     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 378      |\n",
            "|    ep_rew_mean        | -114     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 18700    |\n",
            "|    time_elapsed       | 681      |\n",
            "|    total_timesteps    | 748000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.704   |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 18699    |\n",
            "|    policy_loss        | 0.132    |\n",
            "|    value_loss         | 1.35     |\n",
            "------------------------------------\n",
            "Num timesteps: 752000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -123.86\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 405      |\n",
            "|    ep_rew_mean        | -124     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 18800    |\n",
            "|    time_elapsed       | 684      |\n",
            "|    total_timesteps    | 752000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.554   |\n",
            "|    explained_variance | 0.844    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 18799    |\n",
            "|    policy_loss        | 0.293    |\n",
            "|    value_loss         | 6.5      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 418      |\n",
            "|    ep_rew_mean        | -126     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 18900    |\n",
            "|    time_elapsed       | 688      |\n",
            "|    total_timesteps    | 756000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.692   |\n",
            "|    explained_variance | 0.956    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 18899    |\n",
            "|    policy_loss        | 0.0232   |\n",
            "|    value_loss         | 4.63     |\n",
            "------------------------------------\n",
            "Num timesteps: 760000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -126.56\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 414      |\n",
            "|    ep_rew_mean        | -127     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 19000    |\n",
            "|    time_elapsed       | 692      |\n",
            "|    total_timesteps    | 760000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.717   |\n",
            "|    explained_variance | 0.969    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 18999    |\n",
            "|    policy_loss        | -0.227   |\n",
            "|    value_loss         | 2.27     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 428      |\n",
            "|    ep_rew_mean        | -127     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 19100    |\n",
            "|    time_elapsed       | 697      |\n",
            "|    total_timesteps    | 764000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.783   |\n",
            "|    explained_variance | 0.932    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 19099    |\n",
            "|    policy_loss        | -0.117   |\n",
            "|    value_loss         | 5.23     |\n",
            "------------------------------------\n",
            "Num timesteps: 768000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -127.98\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 445      |\n",
            "|    ep_rew_mean        | -128     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 19200    |\n",
            "|    time_elapsed       | 699      |\n",
            "|    total_timesteps    | 768000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.634   |\n",
            "|    explained_variance | 0.969    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 19199    |\n",
            "|    policy_loss        | 0.279    |\n",
            "|    value_loss         | 5.54     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 437      |\n",
            "|    ep_rew_mean        | -129     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 19300    |\n",
            "|    time_elapsed       | 703      |\n",
            "|    total_timesteps    | 772000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.659   |\n",
            "|    explained_variance | 0.939    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 19299    |\n",
            "|    policy_loss        | -0.722   |\n",
            "|    value_loss         | 5.27     |\n",
            "------------------------------------\n",
            "Num timesteps: 776000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -127.31\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 446      |\n",
            "|    ep_rew_mean        | -127     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 19400    |\n",
            "|    time_elapsed       | 706      |\n",
            "|    total_timesteps    | 776000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.673   |\n",
            "|    explained_variance | 0.952    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 19399    |\n",
            "|    policy_loss        | 0.503    |\n",
            "|    value_loss         | 4.07     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 458      |\n",
            "|    ep_rew_mean        | -127     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 19500    |\n",
            "|    time_elapsed       | 710      |\n",
            "|    total_timesteps    | 780000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.578   |\n",
            "|    explained_variance | 0.968    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 19499    |\n",
            "|    policy_loss        | -0.213   |\n",
            "|    value_loss         | 8.6      |\n",
            "------------------------------------\n",
            "Num timesteps: 784000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -124.88\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 449      |\n",
            "|    ep_rew_mean        | -125     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 19600    |\n",
            "|    time_elapsed       | 712      |\n",
            "|    total_timesteps    | 784000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.575   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 19599    |\n",
            "|    policy_loss        | -0.104   |\n",
            "|    value_loss         | 1.53     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 416      |\n",
            "|    ep_rew_mean        | -122     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 19700    |\n",
            "|    time_elapsed       | 715      |\n",
            "|    total_timesteps    | 788000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.647   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 19699    |\n",
            "|    policy_loss        | -0.269   |\n",
            "|    value_loss         | 3.32     |\n",
            "------------------------------------\n",
            "Num timesteps: 792000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -120.02\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 407      |\n",
            "|    ep_rew_mean        | -120     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 19800    |\n",
            "|    time_elapsed       | 718      |\n",
            "|    total_timesteps    | 792000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.604   |\n",
            "|    explained_variance | 0.958    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 19799    |\n",
            "|    policy_loss        | 0.396    |\n",
            "|    value_loss         | 3.82     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 406      |\n",
            "|    ep_rew_mean        | -122     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1103     |\n",
            "|    iterations         | 19900    |\n",
            "|    time_elapsed       | 721      |\n",
            "|    total_timesteps    | 796000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.688   |\n",
            "|    explained_variance | 0.949    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 19899    |\n",
            "|    policy_loss        | -0.174   |\n",
            "|    value_loss         | 2.99     |\n",
            "------------------------------------\n",
            "Num timesteps: 800000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -122.11\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 402      |\n",
            "|    ep_rew_mean        | -122     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1103     |\n",
            "|    iterations         | 20000    |\n",
            "|    time_elapsed       | 724      |\n",
            "|    total_timesteps    | 800000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.564   |\n",
            "|    explained_variance | 0.942    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 19999    |\n",
            "|    policy_loss        | 0.342    |\n",
            "|    value_loss         | 6.32     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 383      |\n",
            "|    ep_rew_mean        | -124     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1103     |\n",
            "|    iterations         | 20100    |\n",
            "|    time_elapsed       | 728      |\n",
            "|    total_timesteps    | 804000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.797   |\n",
            "|    explained_variance | 0.976    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 20099    |\n",
            "|    policy_loss        | -0.177   |\n",
            "|    value_loss         | 3.31     |\n",
            "------------------------------------\n",
            "Num timesteps: 808000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -123.65\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 378      |\n",
            "|    ep_rew_mean        | -124     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 20200    |\n",
            "|    time_elapsed       | 732      |\n",
            "|    total_timesteps    | 808000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.675   |\n",
            "|    explained_variance | 0.959    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 20199    |\n",
            "|    policy_loss        | -0.248   |\n",
            "|    value_loss         | 1.5      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 401      |\n",
            "|    ep_rew_mean        | -124     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 20300    |\n",
            "|    time_elapsed       | 737      |\n",
            "|    total_timesteps    | 812000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.618   |\n",
            "|    explained_variance | 0.967    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 20299    |\n",
            "|    policy_loss        | 0.117    |\n",
            "|    value_loss         | 1.72     |\n",
            "------------------------------------\n",
            "Num timesteps: 816000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -124.58\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 399      |\n",
            "|    ep_rew_mean        | -125     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 20400    |\n",
            "|    time_elapsed       | 740      |\n",
            "|    total_timesteps    | 816000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.57    |\n",
            "|    explained_variance | 0.865    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 20399    |\n",
            "|    policy_loss        | 0.0659   |\n",
            "|    value_loss         | 6.04     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 386      |\n",
            "|    ep_rew_mean        | -128     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1103     |\n",
            "|    iterations         | 20500    |\n",
            "|    time_elapsed       | 743      |\n",
            "|    total_timesteps    | 820000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.605   |\n",
            "|    explained_variance | 0.941    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 20499    |\n",
            "|    policy_loss        | 0.0921   |\n",
            "|    value_loss         | 2.78     |\n",
            "------------------------------------\n",
            "Num timesteps: 824000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -129.98\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 391      |\n",
            "|    ep_rew_mean        | -130     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 20600    |\n",
            "|    time_elapsed       | 747      |\n",
            "|    total_timesteps    | 824000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.611   |\n",
            "|    explained_variance | 0.958    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 20599    |\n",
            "|    policy_loss        | -0.0538  |\n",
            "|    value_loss         | 3.17     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 421      |\n",
            "|    ep_rew_mean        | -132     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 20700    |\n",
            "|    time_elapsed       | 751      |\n",
            "|    total_timesteps    | 828000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.58    |\n",
            "|    explained_variance | 0.945    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 20699    |\n",
            "|    policy_loss        | -0.549   |\n",
            "|    value_loss         | 8.11     |\n",
            "------------------------------------\n",
            "Num timesteps: 832000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -135.12\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 434      |\n",
            "|    ep_rew_mean        | -135     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 20800    |\n",
            "|    time_elapsed       | 755      |\n",
            "|    total_timesteps    | 832000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.66    |\n",
            "|    explained_variance | 0.91     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 20799    |\n",
            "|    policy_loss        | 0.226    |\n",
            "|    value_loss         | 3.96     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 438      |\n",
            "|    ep_rew_mean        | -135     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 20900    |\n",
            "|    time_elapsed       | 760      |\n",
            "|    total_timesteps    | 836000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.796   |\n",
            "|    explained_variance | 0.977    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 20899    |\n",
            "|    policy_loss        | 0.269    |\n",
            "|    value_loss         | 3.65     |\n",
            "------------------------------------\n",
            "Num timesteps: 840000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -135.26\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 442      |\n",
            "|    ep_rew_mean        | -135     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 21000    |\n",
            "|    time_elapsed       | 763      |\n",
            "|    total_timesteps    | 840000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.624   |\n",
            "|    explained_variance | 0.494    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 20999    |\n",
            "|    policy_loss        | -3.49    |\n",
            "|    value_loss         | 388      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 436      |\n",
            "|    ep_rew_mean        | -135     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 21100    |\n",
            "|    time_elapsed       | 766      |\n",
            "|    total_timesteps    | 844000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.68    |\n",
            "|    explained_variance | 0.965    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 21099    |\n",
            "|    policy_loss        | 0.335    |\n",
            "|    value_loss         | 4.54     |\n",
            "------------------------------------\n",
            "Num timesteps: 848000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -135.12\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 428      |\n",
            "|    ep_rew_mean        | -135     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 21200    |\n",
            "|    time_elapsed       | 769      |\n",
            "|    total_timesteps    | 848000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.654   |\n",
            "|    explained_variance | 0.971    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 21199    |\n",
            "|    policy_loss        | 0.178    |\n",
            "|    value_loss         | 4        |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 410      |\n",
            "|    ep_rew_mean        | -134     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 21300    |\n",
            "|    time_elapsed       | 773      |\n",
            "|    total_timesteps    | 852000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.752   |\n",
            "|    explained_variance | 0.969    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 21299    |\n",
            "|    policy_loss        | -0.413   |\n",
            "|    value_loss         | 3.41     |\n",
            "------------------------------------\n",
            "Num timesteps: 856000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -133.96\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 402      |\n",
            "|    ep_rew_mean        | -134     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 21400    |\n",
            "|    time_elapsed       | 777      |\n",
            "|    total_timesteps    | 856000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.675   |\n",
            "|    explained_variance | 0.878    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 21399    |\n",
            "|    policy_loss        | 0.038    |\n",
            "|    value_loss         | 3.8      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 417      |\n",
            "|    ep_rew_mean        | -132     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 21500    |\n",
            "|    time_elapsed       | 781      |\n",
            "|    total_timesteps    | 860000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.704   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 21499    |\n",
            "|    policy_loss        | 0.0135   |\n",
            "|    value_loss         | 1.89     |\n",
            "------------------------------------\n",
            "Num timesteps: 864000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -124.84\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 419      |\n",
            "|    ep_rew_mean        | -125     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 21600    |\n",
            "|    time_elapsed       | 785      |\n",
            "|    total_timesteps    | 864000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.582   |\n",
            "|    explained_variance | 0.974    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 21599    |\n",
            "|    policy_loss        | -0.209   |\n",
            "|    value_loss         | 8.64     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 382      |\n",
            "|    ep_rew_mean        | -119     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 21700    |\n",
            "|    time_elapsed       | 789      |\n",
            "|    total_timesteps    | 868000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.458   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 21699    |\n",
            "|    policy_loss        | -0.623   |\n",
            "|    value_loss         | 2.04     |\n",
            "------------------------------------\n",
            "Num timesteps: 872000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -115.17\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 366      |\n",
            "|    ep_rew_mean        | -115     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 21800    |\n",
            "|    time_elapsed       | 792      |\n",
            "|    total_timesteps    | 872000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.645   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 21799    |\n",
            "|    policy_loss        | -0.124   |\n",
            "|    value_loss         | 2.93     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 352      |\n",
            "|    ep_rew_mean        | -115     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 21900    |\n",
            "|    time_elapsed       | 796      |\n",
            "|    total_timesteps    | 876000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.835   |\n",
            "|    explained_variance | 0.977    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 21899    |\n",
            "|    policy_loss        | -0.612   |\n",
            "|    value_loss         | 6.12     |\n",
            "------------------------------------\n",
            "Num timesteps: 880000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -112.01\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 355      |\n",
            "|    ep_rew_mean        | -112     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 22000    |\n",
            "|    time_elapsed       | 799      |\n",
            "|    total_timesteps    | 880000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.607   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 21999    |\n",
            "|    policy_loss        | -0.486   |\n",
            "|    value_loss         | 3.18     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 378      |\n",
            "|    ep_rew_mean        | -112     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 22100    |\n",
            "|    time_elapsed       | 803      |\n",
            "|    total_timesteps    | 884000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.65    |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 22099    |\n",
            "|    policy_loss        | -0.117   |\n",
            "|    value_loss         | 1.54     |\n",
            "------------------------------------\n",
            "Num timesteps: 888000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -112.33\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 389      |\n",
            "|    ep_rew_mean        | -112     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 22200    |\n",
            "|    time_elapsed       | 807      |\n",
            "|    total_timesteps    | 888000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.716   |\n",
            "|    explained_variance | 0.954    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 22199    |\n",
            "|    policy_loss        | 0.615    |\n",
            "|    value_loss         | 3.02     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 391      |\n",
            "|    ep_rew_mean        | -109     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 22300    |\n",
            "|    time_elapsed       | 810      |\n",
            "|    total_timesteps    | 892000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.7     |\n",
            "|    explained_variance | 0.977    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 22299    |\n",
            "|    policy_loss        | 0.139    |\n",
            "|    value_loss         | 2.14     |\n",
            "------------------------------------\n",
            "Num timesteps: 896000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -108.19\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 376      |\n",
            "|    ep_rew_mean        | -108     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 22400    |\n",
            "|    time_elapsed       | 814      |\n",
            "|    total_timesteps    | 896000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.627   |\n",
            "|    explained_variance | 0.979    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 22399    |\n",
            "|    policy_loss        | 0.3      |\n",
            "|    value_loss         | 3.41     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 384      |\n",
            "|    ep_rew_mean        | -111     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 22500    |\n",
            "|    time_elapsed       | 818      |\n",
            "|    total_timesteps    | 900000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.646   |\n",
            "|    explained_variance | 0.91     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 22499    |\n",
            "|    policy_loss        | -0.849   |\n",
            "|    value_loss         | 5.45     |\n",
            "------------------------------------\n",
            "Num timesteps: 904000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -109.89\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 395      |\n",
            "|    ep_rew_mean        | -110     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 22600    |\n",
            "|    time_elapsed       | 822      |\n",
            "|    total_timesteps    | 904000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.642   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 22599    |\n",
            "|    policy_loss        | 0.00577  |\n",
            "|    value_loss         | 2.34     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 399      |\n",
            "|    ep_rew_mean        | -111     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 22700    |\n",
            "|    time_elapsed       | 826      |\n",
            "|    total_timesteps    | 908000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.742   |\n",
            "|    explained_variance | 0.965    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 22699    |\n",
            "|    policy_loss        | -0.83    |\n",
            "|    value_loss         | 6.43     |\n",
            "------------------------------------\n",
            "Num timesteps: 912000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -113.58\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 423      |\n",
            "|    ep_rew_mean        | -114     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 22800    |\n",
            "|    time_elapsed       | 830      |\n",
            "|    total_timesteps    | 912000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.623   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 22799    |\n",
            "|    policy_loss        | -0.16    |\n",
            "|    value_loss         | 1.53     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 439      |\n",
            "|    ep_rew_mean        | -114     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 22900    |\n",
            "|    time_elapsed       | 834      |\n",
            "|    total_timesteps    | 916000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.551   |\n",
            "|    explained_variance | 0.957    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 22899    |\n",
            "|    policy_loss        | 0.207    |\n",
            "|    value_loss         | 4.78     |\n",
            "------------------------------------\n",
            "Num timesteps: 920000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -115.75\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 456      |\n",
            "|    ep_rew_mean        | -116     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 23000    |\n",
            "|    time_elapsed       | 838      |\n",
            "|    total_timesteps    | 920000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.804   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 22999    |\n",
            "|    policy_loss        | -0.266   |\n",
            "|    value_loss         | 1.48     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 478      |\n",
            "|    ep_rew_mean        | -116     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 23100    |\n",
            "|    time_elapsed       | 842      |\n",
            "|    total_timesteps    | 924000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.666   |\n",
            "|    explained_variance | 0.957    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 23099    |\n",
            "|    policy_loss        | 0.333    |\n",
            "|    value_loss         | 5.22     |\n",
            "------------------------------------\n",
            "Num timesteps: 928000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -119.06\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 480      |\n",
            "|    ep_rew_mean        | -119     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 23200    |\n",
            "|    time_elapsed       | 845      |\n",
            "|    total_timesteps    | 928000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.582   |\n",
            "|    explained_variance | 0.607    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 23199    |\n",
            "|    policy_loss        | -0.148   |\n",
            "|    value_loss         | 92.8     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 471      |\n",
            "|    ep_rew_mean        | -118     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 23300    |\n",
            "|    time_elapsed       | 849      |\n",
            "|    total_timesteps    | 932000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.624   |\n",
            "|    explained_variance | 0.779    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 23299    |\n",
            "|    policy_loss        | 0.0254   |\n",
            "|    value_loss         | 4.88     |\n",
            "------------------------------------\n",
            "Num timesteps: 936000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -117.09\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 484      |\n",
            "|    ep_rew_mean        | -117     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 23400    |\n",
            "|    time_elapsed       | 854      |\n",
            "|    total_timesteps    | 936000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.586   |\n",
            "|    explained_variance | 0.975    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 23399    |\n",
            "|    policy_loss        | -0.173   |\n",
            "|    value_loss         | 4.62     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 476      |\n",
            "|    ep_rew_mean        | -119     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 23500    |\n",
            "|    time_elapsed       | 857      |\n",
            "|    total_timesteps    | 940000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.628   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 23499    |\n",
            "|    policy_loss        | 0.17     |\n",
            "|    value_loss         | 3.74     |\n",
            "------------------------------------\n",
            "Num timesteps: 944000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -120.95\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 486      |\n",
            "|    ep_rew_mean        | -121     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 23600    |\n",
            "|    time_elapsed       | 861      |\n",
            "|    total_timesteps    | 944000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.709   |\n",
            "|    explained_variance | 0.963    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 23599    |\n",
            "|    policy_loss        | -0.161   |\n",
            "|    value_loss         | 2.06     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 495      |\n",
            "|    ep_rew_mean        | -117     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 23700    |\n",
            "|    time_elapsed       | 866      |\n",
            "|    total_timesteps    | 948000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.508   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 23699    |\n",
            "|    policy_loss        | -0.427   |\n",
            "|    value_loss         | 3.46     |\n",
            "------------------------------------\n",
            "Num timesteps: 952000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -119.33\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 490      |\n",
            "|    ep_rew_mean        | -119     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1093     |\n",
            "|    iterations         | 23800    |\n",
            "|    time_elapsed       | 870      |\n",
            "|    total_timesteps    | 952000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.545   |\n",
            "|    explained_variance | 0.532    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 23799    |\n",
            "|    policy_loss        | -2.46    |\n",
            "|    value_loss         | 277      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 472      |\n",
            "|    ep_rew_mean        | -120     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1093     |\n",
            "|    iterations         | 23900    |\n",
            "|    time_elapsed       | 874      |\n",
            "|    total_timesteps    | 956000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.687   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 23899    |\n",
            "|    policy_loss        | -0.669   |\n",
            "|    value_loss         | 1.73     |\n",
            "------------------------------------\n",
            "Num timesteps: 960000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -118.49\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 458      |\n",
            "|    ep_rew_mean        | -118     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1093     |\n",
            "|    iterations         | 24000    |\n",
            "|    time_elapsed       | 878      |\n",
            "|    total_timesteps    | 960000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.651   |\n",
            "|    explained_variance | 0.973    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 23999    |\n",
            "|    policy_loss        | -0.284   |\n",
            "|    value_loss         | 2.76     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 457      |\n",
            "|    ep_rew_mean        | -119     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1093     |\n",
            "|    iterations         | 24100    |\n",
            "|    time_elapsed       | 881      |\n",
            "|    total_timesteps    | 964000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.453   |\n",
            "|    explained_variance | 0.882    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 24099    |\n",
            "|    policy_loss        | 0.367    |\n",
            "|    value_loss         | 4.24     |\n",
            "------------------------------------\n",
            "Num timesteps: 968000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -122.32\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 459      |\n",
            "|    ep_rew_mean        | -122     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1093     |\n",
            "|    iterations         | 24200    |\n",
            "|    time_elapsed       | 885      |\n",
            "|    total_timesteps    | 968000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.605   |\n",
            "|    explained_variance | 0.959    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 24199    |\n",
            "|    policy_loss        | 0.194    |\n",
            "|    value_loss         | 5.81     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 460      |\n",
            "|    ep_rew_mean        | -123     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1093     |\n",
            "|    iterations         | 24300    |\n",
            "|    time_elapsed       | 888      |\n",
            "|    total_timesteps    | 972000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.698   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 24299    |\n",
            "|    policy_loss        | 0.51     |\n",
            "|    value_loss         | 4.02     |\n",
            "------------------------------------\n",
            "Num timesteps: 976000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -122.29\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 456      |\n",
            "|    ep_rew_mean        | -122     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1093     |\n",
            "|    iterations         | 24400    |\n",
            "|    time_elapsed       | 892      |\n",
            "|    total_timesteps    | 976000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.536   |\n",
            "|    explained_variance | 0.958    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 24399    |\n",
            "|    policy_loss        | -0.359   |\n",
            "|    value_loss         | 4.87     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 451      |\n",
            "|    ep_rew_mean        | -124     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1093     |\n",
            "|    iterations         | 24500    |\n",
            "|    time_elapsed       | 896      |\n",
            "|    total_timesteps    | 980000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.675   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 24499    |\n",
            "|    policy_loss        | -0.5     |\n",
            "|    value_loss         | 1.34     |\n",
            "------------------------------------\n",
            "Num timesteps: 984000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -124.94\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 451      |\n",
            "|    ep_rew_mean        | -125     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 24600    |\n",
            "|    time_elapsed       | 899      |\n",
            "|    total_timesteps    | 984000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.677   |\n",
            "|    explained_variance | 0.979    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 24599    |\n",
            "|    policy_loss        | -0.671   |\n",
            "|    value_loss         | 4.65     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 452      |\n",
            "|    ep_rew_mean        | -125     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 24700    |\n",
            "|    time_elapsed       | 902      |\n",
            "|    total_timesteps    | 988000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.631   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 24699    |\n",
            "|    policy_loss        | -0.954   |\n",
            "|    value_loss         | 4.06     |\n",
            "------------------------------------\n",
            "Num timesteps: 992000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -127.36\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 433      |\n",
            "|    ep_rew_mean        | -127     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 24800    |\n",
            "|    time_elapsed       | 905      |\n",
            "|    total_timesteps    | 992000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.598   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 24799    |\n",
            "|    policy_loss        | -0.0992  |\n",
            "|    value_loss         | 3.28     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 426      |\n",
            "|    ep_rew_mean        | -125     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 24900    |\n",
            "|    time_elapsed       | 909      |\n",
            "|    total_timesteps    | 996000   |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.552   |\n",
            "|    explained_variance | 0.979    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 24899    |\n",
            "|    policy_loss        | -0.304   |\n",
            "|    value_loss         | 5.51     |\n",
            "------------------------------------\n",
            "Num timesteps: 1000000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -124.81\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 410      |\n",
            "|    ep_rew_mean        | -125     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 25000    |\n",
            "|    time_elapsed       | 911      |\n",
            "|    total_timesteps    | 1000000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.653   |\n",
            "|    explained_variance | 0.752    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 24999    |\n",
            "|    policy_loss        | -3.01    |\n",
            "|    value_loss         | 168      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 390      |\n",
            "|    ep_rew_mean        | -121     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 25100    |\n",
            "|    time_elapsed       | 914      |\n",
            "|    total_timesteps    | 1004000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.689   |\n",
            "|    explained_variance | 0.782    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 25099    |\n",
            "|    policy_loss        | 0.155    |\n",
            "|    value_loss         | 4.61     |\n",
            "------------------------------------\n",
            "Num timesteps: 1008000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -119.03\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 386      |\n",
            "|    ep_rew_mean        | -119     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 25200    |\n",
            "|    time_elapsed       | 919      |\n",
            "|    total_timesteps    | 1008000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.538   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 25199    |\n",
            "|    policy_loss        | -0.303   |\n",
            "|    value_loss         | 2.62     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 396      |\n",
            "|    ep_rew_mean        | -116     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 25300    |\n",
            "|    time_elapsed       | 922      |\n",
            "|    total_timesteps    | 1012000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.623   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 25299    |\n",
            "|    policy_loss        | 0.0403   |\n",
            "|    value_loss         | 2.66     |\n",
            "------------------------------------\n",
            "Num timesteps: 1016000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -116.18\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 398      |\n",
            "|    ep_rew_mean        | -116     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 25400    |\n",
            "|    time_elapsed       | 925      |\n",
            "|    total_timesteps    | 1016000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.452   |\n",
            "|    explained_variance | 0.945    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 25399    |\n",
            "|    policy_loss        | 0.292    |\n",
            "|    value_loss         | 5.64     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 384      |\n",
            "|    ep_rew_mean        | -116     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 25500    |\n",
            "|    time_elapsed       | 929      |\n",
            "|    total_timesteps    | 1020000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.61    |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 25499    |\n",
            "|    policy_loss        | -0.285   |\n",
            "|    value_loss         | 3.48     |\n",
            "------------------------------------\n",
            "Num timesteps: 1024000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -112.92\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 404      |\n",
            "|    ep_rew_mean        | -113     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 25600    |\n",
            "|    time_elapsed       | 933      |\n",
            "|    total_timesteps    | 1024000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.437   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 25599    |\n",
            "|    policy_loss        | -0.663   |\n",
            "|    value_loss         | 4.83     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 407      |\n",
            "|    ep_rew_mean        | -114     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 25700    |\n",
            "|    time_elapsed       | 937      |\n",
            "|    total_timesteps    | 1028000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.699   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 25699    |\n",
            "|    policy_loss        | -0.0809  |\n",
            "|    value_loss         | 2.06     |\n",
            "------------------------------------\n",
            "Num timesteps: 1032000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -114.23\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 420      |\n",
            "|    ep_rew_mean        | -114     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 25800    |\n",
            "|    time_elapsed       | 941      |\n",
            "|    total_timesteps    | 1032000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.593   |\n",
            "|    explained_variance | 0.98     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 25799    |\n",
            "|    policy_loss        | 0.374    |\n",
            "|    value_loss         | 5.83     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 430      |\n",
            "|    ep_rew_mean        | -113     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 25900    |\n",
            "|    time_elapsed       | 945      |\n",
            "|    total_timesteps    | 1036000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.594   |\n",
            "|    explained_variance | 0.955    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 25899    |\n",
            "|    policy_loss        | 0.151    |\n",
            "|    value_loss         | 2.46     |\n",
            "------------------------------------\n",
            "Num timesteps: 1040000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -117.08\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 446      |\n",
            "|    ep_rew_mean        | -117     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 26000    |\n",
            "|    time_elapsed       | 950      |\n",
            "|    total_timesteps    | 1040000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.69    |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 25999    |\n",
            "|    policy_loss        | -0.489   |\n",
            "|    value_loss         | 2.85     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 453      |\n",
            "|    ep_rew_mean        | -116     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 26100    |\n",
            "|    time_elapsed       | 953      |\n",
            "|    total_timesteps    | 1044000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.565   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 26099    |\n",
            "|    policy_loss        | -0.459   |\n",
            "|    value_loss         | 2.4      |\n",
            "------------------------------------\n",
            "Num timesteps: 1048000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -116.83\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 457      |\n",
            "|    ep_rew_mean        | -117     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 26200    |\n",
            "|    time_elapsed       | 955      |\n",
            "|    total_timesteps    | 1048000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.592   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 26199    |\n",
            "|    policy_loss        | 0.0971   |\n",
            "|    value_loss         | 3.37     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 423      |\n",
            "|    ep_rew_mean        | -115     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 26300    |\n",
            "|    time_elapsed       | 959      |\n",
            "|    total_timesteps    | 1052000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.646   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 26299    |\n",
            "|    policy_loss        | -0.141   |\n",
            "|    value_loss         | 2.34     |\n",
            "------------------------------------\n",
            "Num timesteps: 1056000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -116.34\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 438      |\n",
            "|    ep_rew_mean        | -116     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 26400    |\n",
            "|    time_elapsed       | 963      |\n",
            "|    total_timesteps    | 1056000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.63    |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 26399    |\n",
            "|    policy_loss        | -0.968   |\n",
            "|    value_loss         | 6.66     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 434      |\n",
            "|    ep_rew_mean        | -115     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 26500    |\n",
            "|    time_elapsed       | 967      |\n",
            "|    total_timesteps    | 1060000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.456   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 26499    |\n",
            "|    policy_loss        | -0.37    |\n",
            "|    value_loss         | 4.21     |\n",
            "------------------------------------\n",
            "Num timesteps: 1064000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -114.39\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 418      |\n",
            "|    ep_rew_mean        | -114     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 26600    |\n",
            "|    time_elapsed       | 969      |\n",
            "|    total_timesteps    | 1064000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.605   |\n",
            "|    explained_variance | 0.973    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 26599    |\n",
            "|    policy_loss        | -0.63    |\n",
            "|    value_loss         | 4.25     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 405      |\n",
            "|    ep_rew_mean        | -115     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 26700    |\n",
            "|    time_elapsed       | 973      |\n",
            "|    total_timesteps    | 1068000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.577   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 26699    |\n",
            "|    policy_loss        | -0.131   |\n",
            "|    value_loss         | 5.21     |\n",
            "------------------------------------\n",
            "Num timesteps: 1072000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -110.60\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 396      |\n",
            "|    ep_rew_mean        | -111     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 26800    |\n",
            "|    time_elapsed       | 977      |\n",
            "|    total_timesteps    | 1072000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.765   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 26799    |\n",
            "|    policy_loss        | 0.229    |\n",
            "|    value_loss         | 2.94     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 377      |\n",
            "|    ep_rew_mean        | -108     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 26900    |\n",
            "|    time_elapsed       | 981      |\n",
            "|    total_timesteps    | 1076000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.599   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 26899    |\n",
            "|    policy_loss        | -0.201   |\n",
            "|    value_loss         | 2.9      |\n",
            "------------------------------------\n",
            "Num timesteps: 1080000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -105.77\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 378      |\n",
            "|    ep_rew_mean        | -106     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 27000    |\n",
            "|    time_elapsed       | 984      |\n",
            "|    total_timesteps    | 1080000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.664   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 26999    |\n",
            "|    policy_loss        | -0.499   |\n",
            "|    value_loss         | 6.36     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 368      |\n",
            "|    ep_rew_mean        | -105     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 27100    |\n",
            "|    time_elapsed       | 987      |\n",
            "|    total_timesteps    | 1084000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.655   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 27099    |\n",
            "|    policy_loss        | 0.656    |\n",
            "|    value_loss         | 5.29     |\n",
            "------------------------------------\n",
            "Num timesteps: 1088000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -99.27\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 383      |\n",
            "|    ep_rew_mean        | -99.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 27200    |\n",
            "|    time_elapsed       | 990      |\n",
            "|    total_timesteps    | 1088000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.719   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 27199    |\n",
            "|    policy_loss        | -0.378   |\n",
            "|    value_loss         | 2.91     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 380      |\n",
            "|    ep_rew_mean        | -99.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 27300    |\n",
            "|    time_elapsed       | 994      |\n",
            "|    total_timesteps    | 1092000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.612   |\n",
            "|    explained_variance | 0.886    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 27299    |\n",
            "|    policy_loss        | 0.494    |\n",
            "|    value_loss         | 7.87     |\n",
            "------------------------------------\n",
            "Num timesteps: 1096000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -99.29\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 386      |\n",
            "|    ep_rew_mean        | -99.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 27400    |\n",
            "|    time_elapsed       | 998      |\n",
            "|    total_timesteps    | 1096000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.626   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 27399    |\n",
            "|    policy_loss        | 0.174    |\n",
            "|    value_loss         | 3.79     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 377      |\n",
            "|    ep_rew_mean        | -102     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 27500    |\n",
            "|    time_elapsed       | 1001     |\n",
            "|    total_timesteps    | 1100000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.495   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 27499    |\n",
            "|    policy_loss        | -0.529   |\n",
            "|    value_loss         | 3.43     |\n",
            "------------------------------------\n",
            "Num timesteps: 1104000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -99.44\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 364      |\n",
            "|    ep_rew_mean        | -99.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 27600    |\n",
            "|    time_elapsed       | 1004     |\n",
            "|    total_timesteps    | 1104000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.662   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 27599    |\n",
            "|    policy_loss        | 0.115    |\n",
            "|    value_loss         | 3.99     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 368      |\n",
            "|    ep_rew_mean        | -103     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 27700    |\n",
            "|    time_elapsed       | 1007     |\n",
            "|    total_timesteps    | 1108000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.613   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 27699    |\n",
            "|    policy_loss        | 0.0618   |\n",
            "|    value_loss         | 5.08     |\n",
            "------------------------------------\n",
            "Num timesteps: 1112000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -102.84\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 357      |\n",
            "|    ep_rew_mean        | -103     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 27800    |\n",
            "|    time_elapsed       | 1010     |\n",
            "|    total_timesteps    | 1112000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.614   |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 27799    |\n",
            "|    policy_loss        | 0.18     |\n",
            "|    value_loss         | 2.27     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 358      |\n",
            "|    ep_rew_mean        | -103     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 27900    |\n",
            "|    time_elapsed       | 1014     |\n",
            "|    total_timesteps    | 1116000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.574   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 27899    |\n",
            "|    policy_loss        | -0.377   |\n",
            "|    value_loss         | 2.44     |\n",
            "------------------------------------\n",
            "Num timesteps: 1120000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -99.52\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 370      |\n",
            "|    ep_rew_mean        | -99.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 28000    |\n",
            "|    time_elapsed       | 1019     |\n",
            "|    total_timesteps    | 1120000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.527   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 27999    |\n",
            "|    policy_loss        | -1.27    |\n",
            "|    value_loss         | 4.29     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 397      |\n",
            "|    ep_rew_mean        | -102     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 28100    |\n",
            "|    time_elapsed       | 1023     |\n",
            "|    total_timesteps    | 1124000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.662   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 28099    |\n",
            "|    policy_loss        | 0.128    |\n",
            "|    value_loss         | 2.29     |\n",
            "------------------------------------\n",
            "Num timesteps: 1128000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -98.83\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 415      |\n",
            "|    ep_rew_mean        | -98.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 28200    |\n",
            "|    time_elapsed       | 1028     |\n",
            "|    total_timesteps    | 1128000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.48    |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 28199    |\n",
            "|    policy_loss        | -0.193   |\n",
            "|    value_loss         | 1.73     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 428      |\n",
            "|    ep_rew_mean        | -101     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 28300    |\n",
            "|    time_elapsed       | 1032     |\n",
            "|    total_timesteps    | 1132000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.576   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 28299    |\n",
            "|    policy_loss        | 0.0501   |\n",
            "|    value_loss         | 4.49     |\n",
            "------------------------------------\n",
            "Num timesteps: 1136000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -100.13\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 435      |\n",
            "|    ep_rew_mean        | -100     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 28400    |\n",
            "|    time_elapsed       | 1036     |\n",
            "|    total_timesteps    | 1136000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.686   |\n",
            "|    explained_variance | 0.606    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 28399    |\n",
            "|    policy_loss        | -5.19    |\n",
            "|    value_loss         | 286      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 429      |\n",
            "|    ep_rew_mean        | -98.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 28500    |\n",
            "|    time_elapsed       | 1040     |\n",
            "|    total_timesteps    | 1140000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.567   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 28499    |\n",
            "|    policy_loss        | -0.484   |\n",
            "|    value_loss         | 1.9      |\n",
            "------------------------------------\n",
            "Num timesteps: 1144000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -99.25\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 431      |\n",
            "|    ep_rew_mean        | -99.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 28600    |\n",
            "|    time_elapsed       | 1043     |\n",
            "|    total_timesteps    | 1144000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.557   |\n",
            "|    explained_variance | 0.975    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 28599    |\n",
            "|    policy_loss        | 0.348    |\n",
            "|    value_loss         | 9.27     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 452      |\n",
            "|    ep_rew_mean        | -100     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 28700    |\n",
            "|    time_elapsed       | 1047     |\n",
            "|    total_timesteps    | 1148000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.555   |\n",
            "|    explained_variance | 0.934    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 28699    |\n",
            "|    policy_loss        | -1.13    |\n",
            "|    value_loss         | 60.4     |\n",
            "------------------------------------\n",
            "Num timesteps: 1152000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -98.37\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 454      |\n",
            "|    ep_rew_mean        | -98.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 28800    |\n",
            "|    time_elapsed       | 1050     |\n",
            "|    total_timesteps    | 1152000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.588   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 28799    |\n",
            "|    policy_loss        | -0.513   |\n",
            "|    value_loss         | 4.03     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 470      |\n",
            "|    ep_rew_mean        | -98.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 28900    |\n",
            "|    time_elapsed       | 1054     |\n",
            "|    total_timesteps    | 1156000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.629   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 28899    |\n",
            "|    policy_loss        | -0.16    |\n",
            "|    value_loss         | 4.37     |\n",
            "------------------------------------\n",
            "Num timesteps: 1160000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -100.46\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 476      |\n",
            "|    ep_rew_mean        | -100     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 29000    |\n",
            "|    time_elapsed       | 1058     |\n",
            "|    total_timesteps    | 1160000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.415   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 28999    |\n",
            "|    policy_loss        | -0.0416  |\n",
            "|    value_loss         | 2.48     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 458      |\n",
            "|    ep_rew_mean        | -103     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 29100    |\n",
            "|    time_elapsed       | 1061     |\n",
            "|    total_timesteps    | 1164000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.591   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 29099    |\n",
            "|    policy_loss        | -0.172   |\n",
            "|    value_loss         | 5.26     |\n",
            "------------------------------------\n",
            "Num timesteps: 1168000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -104.21\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 416      |\n",
            "|    ep_rew_mean        | -104     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 29200    |\n",
            "|    time_elapsed       | 1064     |\n",
            "|    total_timesteps    | 1168000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.653   |\n",
            "|    explained_variance | 0.967    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 29199    |\n",
            "|    policy_loss        | -0.279   |\n",
            "|    value_loss         | 6.13     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 427      |\n",
            "|    ep_rew_mean        | -109     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 29300    |\n",
            "|    time_elapsed       | 1068     |\n",
            "|    total_timesteps    | 1172000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.638   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 29299    |\n",
            "|    policy_loss        | -0.319   |\n",
            "|    value_loss         | 4.82     |\n",
            "------------------------------------\n",
            "Num timesteps: 1176000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -111.60\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 429      |\n",
            "|    ep_rew_mean        | -112     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 29400    |\n",
            "|    time_elapsed       | 1072     |\n",
            "|    total_timesteps    | 1176000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.585   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 29399    |\n",
            "|    policy_loss        | -0.443   |\n",
            "|    value_loss         | 3.54     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 423      |\n",
            "|    ep_rew_mean        | -112     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 29500    |\n",
            "|    time_elapsed       | 1076     |\n",
            "|    total_timesteps    | 1180000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.515   |\n",
            "|    explained_variance | 0.91     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 29499    |\n",
            "|    policy_loss        | -0.763   |\n",
            "|    value_loss         | 100      |\n",
            "------------------------------------\n",
            "Num timesteps: 1184000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -111.01\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 410      |\n",
            "|    ep_rew_mean        | -111     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 29600    |\n",
            "|    time_elapsed       | 1078     |\n",
            "|    total_timesteps    | 1184000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.592   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 29599    |\n",
            "|    policy_loss        | -0.226   |\n",
            "|    value_loss         | 3.58     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 392      |\n",
            "|    ep_rew_mean        | -111     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 29700    |\n",
            "|    time_elapsed       | 1081     |\n",
            "|    total_timesteps    | 1188000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.732   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 29699    |\n",
            "|    policy_loss        | -0.183   |\n",
            "|    value_loss         | 4.13     |\n",
            "------------------------------------\n",
            "Num timesteps: 1192000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -112.62\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 376      |\n",
            "|    ep_rew_mean        | -113     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 29800    |\n",
            "|    time_elapsed       | 1084     |\n",
            "|    total_timesteps    | 1192000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.527   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 29799    |\n",
            "|    policy_loss        | 0.168    |\n",
            "|    value_loss         | 5.45     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 360      |\n",
            "|    ep_rew_mean        | -111     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 29900    |\n",
            "|    time_elapsed       | 1087     |\n",
            "|    total_timesteps    | 1196000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.621   |\n",
            "|    explained_variance | 0.962    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 29899    |\n",
            "|    policy_loss        | -0.0951  |\n",
            "|    value_loss         | 7.03     |\n",
            "------------------------------------\n",
            "Num timesteps: 1200000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -110.13\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 363      |\n",
            "|    ep_rew_mean        | -110     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 30000    |\n",
            "|    time_elapsed       | 1091     |\n",
            "|    total_timesteps    | 1200000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.781   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 29999    |\n",
            "|    policy_loss        | 0.172    |\n",
            "|    value_loss         | 3.2      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 368      |\n",
            "|    ep_rew_mean        | -111     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 30100    |\n",
            "|    time_elapsed       | 1095     |\n",
            "|    total_timesteps    | 1204000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.54    |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 30099    |\n",
            "|    policy_loss        | 0.259    |\n",
            "|    value_loss         | 3.64     |\n",
            "------------------------------------\n",
            "Num timesteps: 1208000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -110.27\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 374      |\n",
            "|    ep_rew_mean        | -110     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 30200    |\n",
            "|    time_elapsed       | 1098     |\n",
            "|    total_timesteps    | 1208000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.599   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 30199    |\n",
            "|    policy_loss        | 1.33     |\n",
            "|    value_loss         | 7.2      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 369      |\n",
            "|    ep_rew_mean        | -110     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 30300    |\n",
            "|    time_elapsed       | 1102     |\n",
            "|    total_timesteps    | 1212000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.724   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 30299    |\n",
            "|    policy_loss        | -0.273   |\n",
            "|    value_loss         | 3.23     |\n",
            "------------------------------------\n",
            "Num timesteps: 1216000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -112.19\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 363      |\n",
            "|    ep_rew_mean        | -112     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 30400    |\n",
            "|    time_elapsed       | 1106     |\n",
            "|    total_timesteps    | 1216000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.598   |\n",
            "|    explained_variance | 0.821    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 30399    |\n",
            "|    policy_loss        | 0.88     |\n",
            "|    value_loss         | 10.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 381      |\n",
            "|    ep_rew_mean        | -114     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 30500    |\n",
            "|    time_elapsed       | 1110     |\n",
            "|    total_timesteps    | 1220000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.569   |\n",
            "|    explained_variance | 0.974    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 30499    |\n",
            "|    policy_loss        | 1.4      |\n",
            "|    value_loss         | 13.6     |\n",
            "------------------------------------\n",
            "Num timesteps: 1224000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -113.96\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 378      |\n",
            "|    ep_rew_mean        | -114     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 30600    |\n",
            "|    time_elapsed       | 1113     |\n",
            "|    total_timesteps    | 1224000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.606   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 30599    |\n",
            "|    policy_loss        | 0.308    |\n",
            "|    value_loss         | 3.27     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 376      |\n",
            "|    ep_rew_mean        | -113     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 30700    |\n",
            "|    time_elapsed       | 1117     |\n",
            "|    total_timesteps    | 1228000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.611   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 30699    |\n",
            "|    policy_loss        | -0.576   |\n",
            "|    value_loss         | 6.31     |\n",
            "------------------------------------\n",
            "Num timesteps: 1232000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -114.18\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 384      |\n",
            "|    ep_rew_mean        | -114     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 30800    |\n",
            "|    time_elapsed       | 1120     |\n",
            "|    total_timesteps    | 1232000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.654   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 30799    |\n",
            "|    policy_loss        | -0.0161  |\n",
            "|    value_loss         | 1.82     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 390      |\n",
            "|    ep_rew_mean        | -114     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 30900    |\n",
            "|    time_elapsed       | 1124     |\n",
            "|    total_timesteps    | 1236000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.635   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 30899    |\n",
            "|    policy_loss        | -0.0671  |\n",
            "|    value_loss         | 2.84     |\n",
            "------------------------------------\n",
            "Num timesteps: 1240000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -116.34\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 398      |\n",
            "|    ep_rew_mean        | -116     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 31000    |\n",
            "|    time_elapsed       | 1128     |\n",
            "|    total_timesteps    | 1240000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.605   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 30999    |\n",
            "|    policy_loss        | 0.354    |\n",
            "|    value_loss         | 3.63     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 397      |\n",
            "|    ep_rew_mean        | -116     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 31100    |\n",
            "|    time_elapsed       | 1132     |\n",
            "|    total_timesteps    | 1244000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.655   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 31099    |\n",
            "|    policy_loss        | 0.319    |\n",
            "|    value_loss         | 3.55     |\n",
            "------------------------------------\n",
            "Num timesteps: 1248000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -116.55\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 403      |\n",
            "|    ep_rew_mean        | -117     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 31200    |\n",
            "|    time_elapsed       | 1136     |\n",
            "|    total_timesteps    | 1248000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.63    |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 31199    |\n",
            "|    policy_loss        | -0.451   |\n",
            "|    value_loss         | 8.9      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 389      |\n",
            "|    ep_rew_mean        | -117     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 31300    |\n",
            "|    time_elapsed       | 1140     |\n",
            "|    total_timesteps    | 1252000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.592   |\n",
            "|    explained_variance | 0.871    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 31299    |\n",
            "|    policy_loss        | 0.455    |\n",
            "|    value_loss         | 49.2     |\n",
            "------------------------------------\n",
            "Num timesteps: 1256000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -112.86\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 386      |\n",
            "|    ep_rew_mean        | -113     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 31400    |\n",
            "|    time_elapsed       | 1143     |\n",
            "|    total_timesteps    | 1256000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.556   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 31399    |\n",
            "|    policy_loss        | -0.154   |\n",
            "|    value_loss         | 1.54     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 386      |\n",
            "|    ep_rew_mean        | -111     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 31500    |\n",
            "|    time_elapsed       | 1146     |\n",
            "|    total_timesteps    | 1260000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.576   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 31499    |\n",
            "|    policy_loss        | -0.222   |\n",
            "|    value_loss         | 6.19     |\n",
            "------------------------------------\n",
            "Num timesteps: 1264000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -112.45\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 394      |\n",
            "|    ep_rew_mean        | -112     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 31600    |\n",
            "|    time_elapsed       | 1151     |\n",
            "|    total_timesteps    | 1264000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.568   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 31599    |\n",
            "|    policy_loss        | -0.497   |\n",
            "|    value_loss         | 1.88     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 413      |\n",
            "|    ep_rew_mean        | -113     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 31700    |\n",
            "|    time_elapsed       | 1154     |\n",
            "|    total_timesteps    | 1268000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.55    |\n",
            "|    explained_variance | 0.943    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 31699    |\n",
            "|    policy_loss        | 0.213    |\n",
            "|    value_loss         | 38.6     |\n",
            "------------------------------------\n",
            "Num timesteps: 1272000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -112.56\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 402      |\n",
            "|    ep_rew_mean        | -113     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 31800    |\n",
            "|    time_elapsed       | 1157     |\n",
            "|    total_timesteps    | 1272000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.603   |\n",
            "|    explained_variance | 0.958    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 31799    |\n",
            "|    policy_loss        | 0.483    |\n",
            "|    value_loss         | 8.53     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 408      |\n",
            "|    ep_rew_mean        | -115     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 31900    |\n",
            "|    time_elapsed       | 1161     |\n",
            "|    total_timesteps    | 1276000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.509   |\n",
            "|    explained_variance | 0.968    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 31899    |\n",
            "|    policy_loss        | 0.157    |\n",
            "|    value_loss         | 3.77     |\n",
            "------------------------------------\n",
            "Num timesteps: 1280000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -115.58\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 414      |\n",
            "|    ep_rew_mean        | -116     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 32000    |\n",
            "|    time_elapsed       | 1166     |\n",
            "|    total_timesteps    | 1280000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.551   |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 31999    |\n",
            "|    policy_loss        | -0.279   |\n",
            "|    value_loss         | 3.84     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 397      |\n",
            "|    ep_rew_mean        | -115     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 32100    |\n",
            "|    time_elapsed       | 1169     |\n",
            "|    total_timesteps    | 1284000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.765   |\n",
            "|    explained_variance | 0.858    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 32099    |\n",
            "|    policy_loss        | -2.13    |\n",
            "|    value_loss         | 143      |\n",
            "------------------------------------\n",
            "Num timesteps: 1288000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -113.11\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 380      |\n",
            "|    ep_rew_mean        | -113     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 32200    |\n",
            "|    time_elapsed       | 1172     |\n",
            "|    total_timesteps    | 1288000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.542   |\n",
            "|    explained_variance | 0.971    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 32199    |\n",
            "|    policy_loss        | 0.259    |\n",
            "|    value_loss         | 6.43     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 368      |\n",
            "|    ep_rew_mean        | -114     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 32300    |\n",
            "|    time_elapsed       | 1175     |\n",
            "|    total_timesteps    | 1292000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.625   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 32299    |\n",
            "|    policy_loss        | -0.226   |\n",
            "|    value_loss         | 5.95     |\n",
            "------------------------------------\n",
            "Num timesteps: 1296000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -114.43\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 358      |\n",
            "|    ep_rew_mean        | -114     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 32400    |\n",
            "|    time_elapsed       | 1178     |\n",
            "|    total_timesteps    | 1296000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.68    |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 32399    |\n",
            "|    policy_loss        | -0.264   |\n",
            "|    value_loss         | 4.75     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 332      |\n",
            "|    ep_rew_mean        | -116     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 32500    |\n",
            "|    time_elapsed       | 1181     |\n",
            "|    total_timesteps    | 1300000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.626   |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 32499    |\n",
            "|    policy_loss        | -0.113   |\n",
            "|    value_loss         | 3.12     |\n",
            "------------------------------------\n",
            "Num timesteps: 1304000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -114.08\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 338      |\n",
            "|    ep_rew_mean        | -114     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 32600    |\n",
            "|    time_elapsed       | 1186     |\n",
            "|    total_timesteps    | 1304000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.634   |\n",
            "|    explained_variance | 0.893    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 32599    |\n",
            "|    policy_loss        | -3.04    |\n",
            "|    value_loss         | 52.7     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 354      |\n",
            "|    ep_rew_mean        | -115     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 32700    |\n",
            "|    time_elapsed       | 1189     |\n",
            "|    total_timesteps    | 1308000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.647   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 32699    |\n",
            "|    policy_loss        | 0.668    |\n",
            "|    value_loss         | 2.82     |\n",
            "------------------------------------\n",
            "Num timesteps: 1312000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -115.37\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 336      |\n",
            "|    ep_rew_mean        | -115     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 32800    |\n",
            "|    time_elapsed       | 1193     |\n",
            "|    total_timesteps    | 1312000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.361   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 32799    |\n",
            "|    policy_loss        | -0.469   |\n",
            "|    value_loss         | 4.37     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 334      |\n",
            "|    ep_rew_mean        | -115     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 32900    |\n",
            "|    time_elapsed       | 1198     |\n",
            "|    total_timesteps    | 1316000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.64    |\n",
            "|    explained_variance | 0.98     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 32899    |\n",
            "|    policy_loss        | -0.908   |\n",
            "|    value_loss         | 8.6      |\n",
            "------------------------------------\n",
            "Num timesteps: 1320000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -115.53\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 347      |\n",
            "|    ep_rew_mean        | -116     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 33000    |\n",
            "|    time_elapsed       | 1201     |\n",
            "|    total_timesteps    | 1320000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.634   |\n",
            "|    explained_variance | 0.976    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 32999    |\n",
            "|    policy_loss        | -0.676   |\n",
            "|    value_loss         | 9.37     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 370      |\n",
            "|    ep_rew_mean        | -120     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 33100    |\n",
            "|    time_elapsed       | 1204     |\n",
            "|    total_timesteps    | 1324000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.612   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 33099    |\n",
            "|    policy_loss        | 0.678    |\n",
            "|    value_loss         | 4.77     |\n",
            "------------------------------------\n",
            "Num timesteps: 1328000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -120.46\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 365      |\n",
            "|    ep_rew_mean        | -120     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 33200    |\n",
            "|    time_elapsed       | 1208     |\n",
            "|    total_timesteps    | 1328000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.759   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 33199    |\n",
            "|    policy_loss        | 0.0462   |\n",
            "|    value_loss         | 6.21     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 379      |\n",
            "|    ep_rew_mean        | -122     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 33300    |\n",
            "|    time_elapsed       | 1212     |\n",
            "|    total_timesteps    | 1332000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.627   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 33299    |\n",
            "|    policy_loss        | -0.0417  |\n",
            "|    value_loss         | 1.8      |\n",
            "------------------------------------\n",
            "Num timesteps: 1336000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -123.66\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 405      |\n",
            "|    ep_rew_mean        | -124     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 33400    |\n",
            "|    time_elapsed       | 1216     |\n",
            "|    total_timesteps    | 1336000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.61    |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 33399    |\n",
            "|    policy_loss        | -0.446   |\n",
            "|    value_loss         | 5.07     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 406      |\n",
            "|    ep_rew_mean        | -125     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 33500    |\n",
            "|    time_elapsed       | 1219     |\n",
            "|    total_timesteps    | 1340000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.711   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 33499    |\n",
            "|    policy_loss        | -0.126   |\n",
            "|    value_loss         | 2.79     |\n",
            "------------------------------------\n",
            "Num timesteps: 1344000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -125.51\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 403      |\n",
            "|    ep_rew_mean        | -126     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 33600    |\n",
            "|    time_elapsed       | 1223     |\n",
            "|    total_timesteps    | 1344000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.636   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 33599    |\n",
            "|    policy_loss        | -0.173   |\n",
            "|    value_loss         | 4.02     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 382      |\n",
            "|    ep_rew_mean        | -125     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 33700    |\n",
            "|    time_elapsed       | 1226     |\n",
            "|    total_timesteps    | 1348000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.524   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 33699    |\n",
            "|    policy_loss        | -0.279   |\n",
            "|    value_loss         | 4.89     |\n",
            "------------------------------------\n",
            "Num timesteps: 1352000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -125.72\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 376      |\n",
            "|    ep_rew_mean        | -126     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 33800    |\n",
            "|    time_elapsed       | 1230     |\n",
            "|    total_timesteps    | 1352000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.524   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 33799    |\n",
            "|    policy_loss        | 0.46     |\n",
            "|    value_loss         | 2.05     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 371      |\n",
            "|    ep_rew_mean        | -128     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 33900    |\n",
            "|    time_elapsed       | 1234     |\n",
            "|    total_timesteps    | 1356000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.661   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 33899    |\n",
            "|    policy_loss        | -0.0999  |\n",
            "|    value_loss         | 6.3      |\n",
            "------------------------------------\n",
            "Num timesteps: 1360000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -126.44\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 362      |\n",
            "|    ep_rew_mean        | -126     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 34000    |\n",
            "|    time_elapsed       | 1237     |\n",
            "|    total_timesteps    | 1360000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.597   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 33999    |\n",
            "|    policy_loss        | 0.409    |\n",
            "|    value_loss         | 7.58     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 363      |\n",
            "|    ep_rew_mean        | -125     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 34100    |\n",
            "|    time_elapsed       | 1240     |\n",
            "|    total_timesteps    | 1364000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.627   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 34099    |\n",
            "|    policy_loss        | -0.058   |\n",
            "|    value_loss         | 8.4      |\n",
            "------------------------------------\n",
            "Num timesteps: 1368000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -121.60\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 342      |\n",
            "|    ep_rew_mean        | -122     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 34200    |\n",
            "|    time_elapsed       | 1244     |\n",
            "|    total_timesteps    | 1368000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.537   |\n",
            "|    explained_variance | 0.979    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 34199    |\n",
            "|    policy_loss        | -0.198   |\n",
            "|    value_loss         | 6.34     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 338      |\n",
            "|    ep_rew_mean        | -121     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 34300    |\n",
            "|    time_elapsed       | 1248     |\n",
            "|    total_timesteps    | 1372000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.64    |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 34299    |\n",
            "|    policy_loss        | -0.45    |\n",
            "|    value_loss         | 4.57     |\n",
            "------------------------------------\n",
            "Num timesteps: 1376000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -120.54\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 364      |\n",
            "|    ep_rew_mean        | -121     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 34400    |\n",
            "|    time_elapsed       | 1253     |\n",
            "|    total_timesteps    | 1376000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.628   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 34399    |\n",
            "|    policy_loss        | -0.746   |\n",
            "|    value_loss         | 5.27     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 377      |\n",
            "|    ep_rew_mean        | -121     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 34500    |\n",
            "|    time_elapsed       | 1257     |\n",
            "|    total_timesteps    | 1380000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.517   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 34499    |\n",
            "|    policy_loss        | -0.172   |\n",
            "|    value_loss         | 3.57     |\n",
            "------------------------------------\n",
            "Num timesteps: 1384000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -120.57\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 387      |\n",
            "|    ep_rew_mean        | -121     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 34600    |\n",
            "|    time_elapsed       | 1262     |\n",
            "|    total_timesteps    | 1384000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.621   |\n",
            "|    explained_variance | 0.972    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 34599    |\n",
            "|    policy_loss        | 0.0328   |\n",
            "|    value_loss         | 2.47     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 401      |\n",
            "|    ep_rew_mean        | -121     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 34700    |\n",
            "|    time_elapsed       | 1267     |\n",
            "|    total_timesteps    | 1388000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.649   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 34699    |\n",
            "|    policy_loss        | -0.0892  |\n",
            "|    value_loss         | 2.68     |\n",
            "------------------------------------\n",
            "Num timesteps: 1392000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -121.49\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 423      |\n",
            "|    ep_rew_mean        | -121     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 34800    |\n",
            "|    time_elapsed       | 1271     |\n",
            "|    total_timesteps    | 1392000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.635   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 34799    |\n",
            "|    policy_loss        | -0.247   |\n",
            "|    value_loss         | 2.19     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 436      |\n",
            "|    ep_rew_mean        | -120     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 34900    |\n",
            "|    time_elapsed       | 1275     |\n",
            "|    total_timesteps    | 1396000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.641   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 34899    |\n",
            "|    policy_loss        | -0.332   |\n",
            "|    value_loss         | 6.53     |\n",
            "------------------------------------\n",
            "Num timesteps: 1400000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -118.30\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 436      |\n",
            "|    ep_rew_mean        | -118     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 35000    |\n",
            "|    time_elapsed       | 1278     |\n",
            "|    total_timesteps    | 1400000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.639   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 34999    |\n",
            "|    policy_loss        | -0.0527  |\n",
            "|    value_loss         | 4.61     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 423      |\n",
            "|    ep_rew_mean        | -116     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 35100    |\n",
            "|    time_elapsed       | 1281     |\n",
            "|    total_timesteps    | 1404000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.493   |\n",
            "|    explained_variance | 0.979    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 35099    |\n",
            "|    policy_loss        | 0.431    |\n",
            "|    value_loss         | 19.2     |\n",
            "------------------------------------\n",
            "Num timesteps: 1408000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -114.10\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 423      |\n",
            "|    ep_rew_mean        | -114     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 35200    |\n",
            "|    time_elapsed       | 1283     |\n",
            "|    total_timesteps    | 1408000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.606   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 35199    |\n",
            "|    policy_loss        | -0.69    |\n",
            "|    value_loss         | 5.83     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 437      |\n",
            "|    ep_rew_mean        | -114     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 35300    |\n",
            "|    time_elapsed       | 1287     |\n",
            "|    total_timesteps    | 1412000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.533   |\n",
            "|    explained_variance | 0.971    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 35299    |\n",
            "|    policy_loss        | -0.244   |\n",
            "|    value_loss         | 4.37     |\n",
            "------------------------------------\n",
            "Num timesteps: 1416000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -112.65\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 439      |\n",
            "|    ep_rew_mean        | -113     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 35400    |\n",
            "|    time_elapsed       | 1291     |\n",
            "|    total_timesteps    | 1416000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.511   |\n",
            "|    explained_variance | 0.94     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 35399    |\n",
            "|    policy_loss        | -0.0983  |\n",
            "|    value_loss         | 8.15     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 453      |\n",
            "|    ep_rew_mean        | -111     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 35500    |\n",
            "|    time_elapsed       | 1296     |\n",
            "|    total_timesteps    | 1420000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.544   |\n",
            "|    explained_variance | 0.903    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 35499    |\n",
            "|    policy_loss        | 0.263    |\n",
            "|    value_loss         | 4.24     |\n",
            "------------------------------------\n",
            "Num timesteps: 1424000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -110.96\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 454      |\n",
            "|    ep_rew_mean        | -111     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 35600    |\n",
            "|    time_elapsed       | 1299     |\n",
            "|    total_timesteps    | 1424000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.641   |\n",
            "|    explained_variance | 0.898    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 35599    |\n",
            "|    policy_loss        | -5.81    |\n",
            "|    value_loss         | 156      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 440      |\n",
            "|    ep_rew_mean        | -111     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 35700    |\n",
            "|    time_elapsed       | 1302     |\n",
            "|    total_timesteps    | 1428000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.562   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 35699    |\n",
            "|    policy_loss        | -0.104   |\n",
            "|    value_loss         | 1.98     |\n",
            "------------------------------------\n",
            "Num timesteps: 1432000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -110.48\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 412      |\n",
            "|    ep_rew_mean        | -110     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 35800    |\n",
            "|    time_elapsed       | 1305     |\n",
            "|    total_timesteps    | 1432000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.746   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 35799    |\n",
            "|    policy_loss        | 0.165    |\n",
            "|    value_loss         | 2.01     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 395      |\n",
            "|    ep_rew_mean        | -110     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 35900    |\n",
            "|    time_elapsed       | 1309     |\n",
            "|    total_timesteps    | 1436000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.558   |\n",
            "|    explained_variance | 0.907    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 35899    |\n",
            "|    policy_loss        | 0.443    |\n",
            "|    value_loss         | 30.6     |\n",
            "------------------------------------\n",
            "Num timesteps: 1440000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -108.80\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 409      |\n",
            "|    ep_rew_mean        | -109     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 36000    |\n",
            "|    time_elapsed       | 1313     |\n",
            "|    total_timesteps    | 1440000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.687   |\n",
            "|    explained_variance | 0.922    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 35999    |\n",
            "|    policy_loss        | -0.634   |\n",
            "|    value_loss         | 5.27     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 420      |\n",
            "|    ep_rew_mean        | -110     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 36100    |\n",
            "|    time_elapsed       | 1317     |\n",
            "|    total_timesteps    | 1444000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.649   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 36099    |\n",
            "|    policy_loss        | -0.293   |\n",
            "|    value_loss         | 5.37     |\n",
            "------------------------------------\n",
            "Num timesteps: 1448000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -108.82\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 424      |\n",
            "|    ep_rew_mean        | -109     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 36200    |\n",
            "|    time_elapsed       | 1320     |\n",
            "|    total_timesteps    | 1448000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.506   |\n",
            "|    explained_variance | 0.944    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 36199    |\n",
            "|    policy_loss        | -0.172   |\n",
            "|    value_loss         | 5.39     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 442      |\n",
            "|    ep_rew_mean        | -112     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 36300    |\n",
            "|    time_elapsed       | 1325     |\n",
            "|    total_timesteps    | 1452000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.472   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 36299    |\n",
            "|    policy_loss        | 0.0698   |\n",
            "|    value_loss         | 2.47     |\n",
            "------------------------------------\n",
            "Num timesteps: 1456000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -111.65\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 449      |\n",
            "|    ep_rew_mean        | -112     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 36400    |\n",
            "|    time_elapsed       | 1329     |\n",
            "|    total_timesteps    | 1456000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.579   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 36399    |\n",
            "|    policy_loss        | -0.474   |\n",
            "|    value_loss         | 4.2      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 451      |\n",
            "|    ep_rew_mean        | -113     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 36500    |\n",
            "|    time_elapsed       | 1333     |\n",
            "|    total_timesteps    | 1460000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.555   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 36499    |\n",
            "|    policy_loss        | -0.506   |\n",
            "|    value_loss         | 10.3     |\n",
            "------------------------------------\n",
            "Num timesteps: 1464000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -115.98\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 430      |\n",
            "|    ep_rew_mean        | -116     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 36600    |\n",
            "|    time_elapsed       | 1337     |\n",
            "|    total_timesteps    | 1464000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.511   |\n",
            "|    explained_variance | 0.896    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 36599    |\n",
            "|    policy_loss        | -1.16    |\n",
            "|    value_loss         | 136      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 433      |\n",
            "|    ep_rew_mean        | -116     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 36700    |\n",
            "|    time_elapsed       | 1341     |\n",
            "|    total_timesteps    | 1468000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.468   |\n",
            "|    explained_variance | 0.921    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 36699    |\n",
            "|    policy_loss        | -1.04    |\n",
            "|    value_loss         | 49.7     |\n",
            "------------------------------------\n",
            "Num timesteps: 1472000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -115.92\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 442      |\n",
            "|    ep_rew_mean        | -116     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 36800    |\n",
            "|    time_elapsed       | 1343     |\n",
            "|    total_timesteps    | 1472000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.477   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 36799    |\n",
            "|    policy_loss        | -0.068   |\n",
            "|    value_loss         | 4.11     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 441      |\n",
            "|    ep_rew_mean        | -116     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 36900    |\n",
            "|    time_elapsed       | 1346     |\n",
            "|    total_timesteps    | 1476000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.435   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 36899    |\n",
            "|    policy_loss        | 0.365    |\n",
            "|    value_loss         | 4.89     |\n",
            "------------------------------------\n",
            "Num timesteps: 1480000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -115.74\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 444      |\n",
            "|    ep_rew_mean        | -116     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 37000    |\n",
            "|    time_elapsed       | 1351     |\n",
            "|    total_timesteps    | 1480000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.521   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 36999    |\n",
            "|    policy_loss        | 0.254    |\n",
            "|    value_loss         | 3.77     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 442      |\n",
            "|    ep_rew_mean        | -116     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 37100    |\n",
            "|    time_elapsed       | 1355     |\n",
            "|    total_timesteps    | 1484000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.683   |\n",
            "|    explained_variance | 0.961    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 37099    |\n",
            "|    policy_loss        | 0.646    |\n",
            "|    value_loss         | 2.97     |\n",
            "------------------------------------\n",
            "Num timesteps: 1488000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -114.13\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 449      |\n",
            "|    ep_rew_mean        | -114     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 37200    |\n",
            "|    time_elapsed       | 1359     |\n",
            "|    total_timesteps    | 1488000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.61    |\n",
            "|    explained_variance | 0.961    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 37199    |\n",
            "|    policy_loss        | 0.281    |\n",
            "|    value_loss         | 6.52     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 460      |\n",
            "|    ep_rew_mean        | -113     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 37300    |\n",
            "|    time_elapsed       | 1362     |\n",
            "|    total_timesteps    | 1492000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.617   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 37299    |\n",
            "|    policy_loss        | -0.441   |\n",
            "|    value_loss         | 7.62     |\n",
            "------------------------------------\n",
            "Num timesteps: 1496000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -114.22\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 446      |\n",
            "|    ep_rew_mean        | -114     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 37400    |\n",
            "|    time_elapsed       | 1366     |\n",
            "|    total_timesteps    | 1496000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.53    |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 37399    |\n",
            "|    policy_loss        | 0.0177   |\n",
            "|    value_loss         | 2        |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 433      |\n",
            "|    ep_rew_mean        | -113     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 37500    |\n",
            "|    time_elapsed       | 1369     |\n",
            "|    total_timesteps    | 1500000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.468   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 37499    |\n",
            "|    policy_loss        | 0.0523   |\n",
            "|    value_loss         | 4.88     |\n",
            "------------------------------------\n",
            "Num timesteps: 1504000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -113.11\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 415      |\n",
            "|    ep_rew_mean        | -113     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 37600    |\n",
            "|    time_elapsed       | 1372     |\n",
            "|    total_timesteps    | 1504000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.549   |\n",
            "|    explained_variance | 0.979    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 37599    |\n",
            "|    policy_loss        | 0.291    |\n",
            "|    value_loss         | 7.47     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 410      |\n",
            "|    ep_rew_mean        | -107     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 37700    |\n",
            "|    time_elapsed       | 1376     |\n",
            "|    total_timesteps    | 1508000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.547   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 37699    |\n",
            "|    policy_loss        | -1.11    |\n",
            "|    value_loss         | 11.2     |\n",
            "------------------------------------\n",
            "Num timesteps: 1512000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -105.91\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 398      |\n",
            "|    ep_rew_mean        | -106     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 37800    |\n",
            "|    time_elapsed       | 1378     |\n",
            "|    total_timesteps    | 1512000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.533   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 37799    |\n",
            "|    policy_loss        | -0.17    |\n",
            "|    value_loss         | 3.89     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 406      |\n",
            "|    ep_rew_mean        | -105     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 37900    |\n",
            "|    time_elapsed       | 1381     |\n",
            "|    total_timesteps    | 1516000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.459   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 37899    |\n",
            "|    policy_loss        | -0.268   |\n",
            "|    value_loss         | 3.62     |\n",
            "------------------------------------\n",
            "Num timesteps: 1520000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -100.75\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 370      |\n",
            "|    ep_rew_mean        | -101     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 38000    |\n",
            "|    time_elapsed       | 1383     |\n",
            "|    total_timesteps    | 1520000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.618   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 37999    |\n",
            "|    policy_loss        | -0.92    |\n",
            "|    value_loss         | 2.89     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 363      |\n",
            "|    ep_rew_mean        | -103     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 38100    |\n",
            "|    time_elapsed       | 1388     |\n",
            "|    total_timesteps    | 1524000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.452   |\n",
            "|    explained_variance | 0.62     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 38099    |\n",
            "|    policy_loss        | -1.76    |\n",
            "|    value_loss         | 271      |\n",
            "------------------------------------\n",
            "Num timesteps: 1528000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -102.87\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 349      |\n",
            "|    ep_rew_mean        | -103     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 38200    |\n",
            "|    time_elapsed       | 1392     |\n",
            "|    total_timesteps    | 1528000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.594   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 38199    |\n",
            "|    policy_loss        | 0.343    |\n",
            "|    value_loss         | 7.72     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 361      |\n",
            "|    ep_rew_mean        | -103     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 38300    |\n",
            "|    time_elapsed       | 1395     |\n",
            "|    total_timesteps    | 1532000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.477   |\n",
            "|    explained_variance | 0.979    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 38299    |\n",
            "|    policy_loss        | 0.0914   |\n",
            "|    value_loss         | 5.09     |\n",
            "------------------------------------\n",
            "Num timesteps: 1536000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -102.78\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 366      |\n",
            "|    ep_rew_mean        | -103     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 38400    |\n",
            "|    time_elapsed       | 1399     |\n",
            "|    total_timesteps    | 1536000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.516   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 38399    |\n",
            "|    policy_loss        | 0.44     |\n",
            "|    value_loss         | 4.44     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 389      |\n",
            "|    ep_rew_mean        | -98.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 38500    |\n",
            "|    time_elapsed       | 1403     |\n",
            "|    total_timesteps    | 1540000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.607   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 38499    |\n",
            "|    policy_loss        | -0.237   |\n",
            "|    value_loss         | 3.14     |\n",
            "------------------------------------\n",
            "Num timesteps: 1544000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -93.02\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 399      |\n",
            "|    ep_rew_mean        | -93      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 38600    |\n",
            "|    time_elapsed       | 1407     |\n",
            "|    total_timesteps    | 1544000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.542   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 38599    |\n",
            "|    policy_loss        | 0.348    |\n",
            "|    value_loss         | 3.17     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 400      |\n",
            "|    ep_rew_mean        | -90      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 38700    |\n",
            "|    time_elapsed       | 1410     |\n",
            "|    total_timesteps    | 1548000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.353   |\n",
            "|    explained_variance | 0.979    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 38699    |\n",
            "|    policy_loss        | 0.0312   |\n",
            "|    value_loss         | 8.77     |\n",
            "------------------------------------\n",
            "Num timesteps: 1552000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -88.83\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 410      |\n",
            "|    ep_rew_mean        | -88.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 38800    |\n",
            "|    time_elapsed       | 1414     |\n",
            "|    total_timesteps    | 1552000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.509   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 38799    |\n",
            "|    policy_loss        | 0.307    |\n",
            "|    value_loss         | 3.5      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 426      |\n",
            "|    ep_rew_mean        | -85.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 38900    |\n",
            "|    time_elapsed       | 1419     |\n",
            "|    total_timesteps    | 1556000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.475   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 38899    |\n",
            "|    policy_loss        | -0.235   |\n",
            "|    value_loss         | 1.75     |\n",
            "------------------------------------\n",
            "Num timesteps: 1560000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -80.20\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 458      |\n",
            "|    ep_rew_mean        | -80.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 39000    |\n",
            "|    time_elapsed       | 1423     |\n",
            "|    total_timesteps    | 1560000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.45    |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 38999    |\n",
            "|    policy_loss        | -0.0116  |\n",
            "|    value_loss         | 3.47     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 456      |\n",
            "|    ep_rew_mean        | -81      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 39100    |\n",
            "|    time_elapsed       | 1426     |\n",
            "|    total_timesteps    | 1564000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.481   |\n",
            "|    explained_variance | 0.624    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 39099    |\n",
            "|    policy_loss        | -6.88    |\n",
            "|    value_loss         | 387      |\n",
            "------------------------------------\n",
            "Num timesteps: 1568000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -81.30\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 477      |\n",
            "|    ep_rew_mean        | -81.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 39200    |\n",
            "|    time_elapsed       | 1430     |\n",
            "|    total_timesteps    | 1568000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.461   |\n",
            "|    explained_variance | 0.965    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 39199    |\n",
            "|    policy_loss        | -0.116   |\n",
            "|    value_loss         | 2.51     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 484      |\n",
            "|    ep_rew_mean        | -76.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 39300    |\n",
            "|    time_elapsed       | 1435     |\n",
            "|    total_timesteps    | 1572000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.528   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 39299    |\n",
            "|    policy_loss        | 0.286    |\n",
            "|    value_loss         | 5.74     |\n",
            "------------------------------------\n",
            "Num timesteps: 1576000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -67.45\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 467      |\n",
            "|    ep_rew_mean        | -67.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 39400    |\n",
            "|    time_elapsed       | 1437     |\n",
            "|    total_timesteps    | 1576000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.518   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 39399    |\n",
            "|    policy_loss        | 0.205    |\n",
            "|    value_loss         | 5.69     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 441      |\n",
            "|    ep_rew_mean        | -71.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 39500    |\n",
            "|    time_elapsed       | 1440     |\n",
            "|    total_timesteps    | 1580000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.37    |\n",
            "|    explained_variance | 0.795    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 39499    |\n",
            "|    policy_loss        | 0.131    |\n",
            "|    value_loss         | 173      |\n",
            "------------------------------------\n",
            "Num timesteps: 1584000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -70.19\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 395      |\n",
            "|    ep_rew_mean        | -70.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 39600    |\n",
            "|    time_elapsed       | 1443     |\n",
            "|    total_timesteps    | 1584000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.343   |\n",
            "|    explained_variance | 0.98     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 39599    |\n",
            "|    policy_loss        | 0.786    |\n",
            "|    value_loss         | 8.47     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 388      |\n",
            "|    ep_rew_mean        | -59.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 39700    |\n",
            "|    time_elapsed       | 1446     |\n",
            "|    total_timesteps    | 1588000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.512   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 39699    |\n",
            "|    policy_loss        | -0.498   |\n",
            "|    value_loss         | 6.15     |\n",
            "------------------------------------\n",
            "Num timesteps: 1592000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -58.81\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 399      |\n",
            "|    ep_rew_mean        | -58.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 39800    |\n",
            "|    time_elapsed       | 1449     |\n",
            "|    total_timesteps    | 1592000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.481   |\n",
            "|    explained_variance | 0.957    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 39799    |\n",
            "|    policy_loss        | 0.161    |\n",
            "|    value_loss         | 67.2     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 359      |\n",
            "|    ep_rew_mean        | -64.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 39900    |\n",
            "|    time_elapsed       | 1453     |\n",
            "|    total_timesteps    | 1596000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.49    |\n",
            "|    explained_variance | 0.659    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 39899    |\n",
            "|    policy_loss        | -0.485   |\n",
            "|    value_loss         | 230      |\n",
            "------------------------------------\n",
            "Num timesteps: 1600000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -67.57\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 344      |\n",
            "|    ep_rew_mean        | -67.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 40000    |\n",
            "|    time_elapsed       | 1456     |\n",
            "|    total_timesteps    | 1600000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.496   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 39999    |\n",
            "|    policy_loss        | 0.166    |\n",
            "|    value_loss         | 8.72     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 316      |\n",
            "|    ep_rew_mean        | -70.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 40100    |\n",
            "|    time_elapsed       | 1458     |\n",
            "|    total_timesteps    | 1604000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.391   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 40099    |\n",
            "|    policy_loss        | 0.237    |\n",
            "|    value_loss         | 3.46     |\n",
            "------------------------------------\n",
            "Num timesteps: 1608000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -72.10\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 323      |\n",
            "|    ep_rew_mean        | -72.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 40200    |\n",
            "|    time_elapsed       | 1462     |\n",
            "|    total_timesteps    | 1608000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.572   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 40199    |\n",
            "|    policy_loss        | -0.43    |\n",
            "|    value_loss         | 1.22     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 337      |\n",
            "|    ep_rew_mean        | -72.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 40300    |\n",
            "|    time_elapsed       | 1466     |\n",
            "|    total_timesteps    | 1612000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.437   |\n",
            "|    explained_variance | 0.973    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 40299    |\n",
            "|    policy_loss        | 0.127    |\n",
            "|    value_loss         | 7.66     |\n",
            "------------------------------------\n",
            "Num timesteps: 1616000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -69.65\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 342      |\n",
            "|    ep_rew_mean        | -69.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 40400    |\n",
            "|    time_elapsed       | 1470     |\n",
            "|    total_timesteps    | 1616000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.433   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 40399    |\n",
            "|    policy_loss        | 0.0847   |\n",
            "|    value_loss         | 1.84     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 353      |\n",
            "|    ep_rew_mean        | -76      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 40500    |\n",
            "|    time_elapsed       | 1473     |\n",
            "|    total_timesteps    | 1620000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.561   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 40499    |\n",
            "|    policy_loss        | 0.147    |\n",
            "|    value_loss         | 2.55     |\n",
            "------------------------------------\n",
            "Num timesteps: 1624000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -88.11\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 361      |\n",
            "|    ep_rew_mean        | -88.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 40600    |\n",
            "|    time_elapsed       | 1478     |\n",
            "|    total_timesteps    | 1624000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.397   |\n",
            "|    explained_variance | 0.619    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 40599    |\n",
            "|    policy_loss        | -0.0299  |\n",
            "|    value_loss         | 370      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 378      |\n",
            "|    ep_rew_mean        | -86.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 40700    |\n",
            "|    time_elapsed       | 1482     |\n",
            "|    total_timesteps    | 1628000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.448   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 40699    |\n",
            "|    policy_loss        | -0.0112  |\n",
            "|    value_loss         | 5.05     |\n",
            "------------------------------------\n",
            "Num timesteps: 1632000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -84.34\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 386      |\n",
            "|    ep_rew_mean        | -84.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 40800    |\n",
            "|    time_elapsed       | 1488     |\n",
            "|    total_timesteps    | 1632000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.52    |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 40799    |\n",
            "|    policy_loss        | -0.0265  |\n",
            "|    value_loss         | 3.5      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 389      |\n",
            "|    ep_rew_mean        | -88.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 40900    |\n",
            "|    time_elapsed       | 1491     |\n",
            "|    total_timesteps    | 1636000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.631   |\n",
            "|    explained_variance | 0.98     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 40899    |\n",
            "|    policy_loss        | 0.771    |\n",
            "|    value_loss         | 9.74     |\n",
            "------------------------------------\n",
            "Num timesteps: 1640000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -86.66\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 401      |\n",
            "|    ep_rew_mean        | -86.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 41000    |\n",
            "|    time_elapsed       | 1495     |\n",
            "|    total_timesteps    | 1640000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.574   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 40999    |\n",
            "|    policy_loss        | -0.0192  |\n",
            "|    value_loss         | 3.64     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 434      |\n",
            "|    ep_rew_mean        | -84.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 41100    |\n",
            "|    time_elapsed       | 1499     |\n",
            "|    total_timesteps    | 1644000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.483   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 41099    |\n",
            "|    policy_loss        | 0.575    |\n",
            "|    value_loss         | 7.7      |\n",
            "------------------------------------\n",
            "Num timesteps: 1648000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -82.76\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 439      |\n",
            "|    ep_rew_mean        | -82.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 41200    |\n",
            "|    time_elapsed       | 1504     |\n",
            "|    total_timesteps    | 1648000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.499   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 41199    |\n",
            "|    policy_loss        | 0.273    |\n",
            "|    value_loss         | 6.61     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 474      |\n",
            "|    ep_rew_mean        | -88.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 41300    |\n",
            "|    time_elapsed       | 1507     |\n",
            "|    total_timesteps    | 1652000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.533   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 41299    |\n",
            "|    policy_loss        | 0.481    |\n",
            "|    value_loss         | 6.32     |\n",
            "------------------------------------\n",
            "Num timesteps: 1656000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -89.57\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 454      |\n",
            "|    ep_rew_mean        | -89.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 41400    |\n",
            "|    time_elapsed       | 1510     |\n",
            "|    total_timesteps    | 1656000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.407   |\n",
            "|    explained_variance | 0.976    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 41399    |\n",
            "|    policy_loss        | -0.373   |\n",
            "|    value_loss         | 4.97     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 439      |\n",
            "|    ep_rew_mean        | -92      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 41500    |\n",
            "|    time_elapsed       | 1513     |\n",
            "|    total_timesteps    | 1660000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.705   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 41499    |\n",
            "|    policy_loss        | -0.123   |\n",
            "|    value_loss         | 6.59     |\n",
            "------------------------------------\n",
            "Num timesteps: 1664000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -91.31\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 445      |\n",
            "|    ep_rew_mean        | -91.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 41600    |\n",
            "|    time_elapsed       | 1517     |\n",
            "|    total_timesteps    | 1664000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.495   |\n",
            "|    explained_variance | 0.517    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 41599    |\n",
            "|    policy_loss        | -1.23    |\n",
            "|    value_loss         | 446      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 434      |\n",
            "|    ep_rew_mean        | -86.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 41700    |\n",
            "|    time_elapsed       | 1520     |\n",
            "|    total_timesteps    | 1668000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.569   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 41699    |\n",
            "|    policy_loss        | -0.139   |\n",
            "|    value_loss         | 4.38     |\n",
            "------------------------------------\n",
            "Num timesteps: 1672000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -86.47\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 415      |\n",
            "|    ep_rew_mean        | -86.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 41800    |\n",
            "|    time_elapsed       | 1524     |\n",
            "|    total_timesteps    | 1672000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.42    |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 41799    |\n",
            "|    policy_loss        | 0.222    |\n",
            "|    value_loss         | 1.8      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 413      |\n",
            "|    ep_rew_mean        | -85.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 41900    |\n",
            "|    time_elapsed       | 1528     |\n",
            "|    total_timesteps    | 1676000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.619   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 41899    |\n",
            "|    policy_loss        | -1.64    |\n",
            "|    value_loss         | 9.03     |\n",
            "------------------------------------\n",
            "Num timesteps: 1680000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -87.24\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 389      |\n",
            "|    ep_rew_mean        | -87.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 42000    |\n",
            "|    time_elapsed       | 1531     |\n",
            "|    total_timesteps    | 1680000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.406   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 41999    |\n",
            "|    policy_loss        | -0.122   |\n",
            "|    value_loss         | 12.7     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 370      |\n",
            "|    ep_rew_mean        | -89.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 42100    |\n",
            "|    time_elapsed       | 1534     |\n",
            "|    total_timesteps    | 1684000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.489   |\n",
            "|    explained_variance | 0.976    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 42099    |\n",
            "|    policy_loss        | -0.178   |\n",
            "|    value_loss         | 4.34     |\n",
            "------------------------------------\n",
            "Num timesteps: 1688000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -85.76\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 351      |\n",
            "|    ep_rew_mean        | -85.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 42200    |\n",
            "|    time_elapsed       | 1538     |\n",
            "|    total_timesteps    | 1688000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.581   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 42199    |\n",
            "|    policy_loss        | -0.237   |\n",
            "|    value_loss         | 4.54     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 369      |\n",
            "|    ep_rew_mean        | -85.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 42300    |\n",
            "|    time_elapsed       | 1541     |\n",
            "|    total_timesteps    | 1692000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.559   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 42299    |\n",
            "|    policy_loss        | -0.218   |\n",
            "|    value_loss         | 3.94     |\n",
            "------------------------------------\n",
            "Num timesteps: 1696000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -84.18\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 379      |\n",
            "|    ep_rew_mean        | -84.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 42400    |\n",
            "|    time_elapsed       | 1544     |\n",
            "|    total_timesteps    | 1696000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.528   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 42399    |\n",
            "|    policy_loss        | 0.332    |\n",
            "|    value_loss         | 5.49     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 374      |\n",
            "|    ep_rew_mean        | -83.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 42500    |\n",
            "|    time_elapsed       | 1549     |\n",
            "|    total_timesteps    | 1700000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.646   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 42499    |\n",
            "|    policy_loss        | -0.862   |\n",
            "|    value_loss         | 2.07     |\n",
            "------------------------------------\n",
            "Num timesteps: 1704000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -85.79\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 386      |\n",
            "|    ep_rew_mean        | -85.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 42600    |\n",
            "|    time_elapsed       | 1552     |\n",
            "|    total_timesteps    | 1704000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.58    |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 42599    |\n",
            "|    policy_loss        | -1.46    |\n",
            "|    value_loss         | 3.94     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 380      |\n",
            "|    ep_rew_mean        | -96      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 42700    |\n",
            "|    time_elapsed       | 1556     |\n",
            "|    total_timesteps    | 1708000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.572   |\n",
            "|    explained_variance | 0.979    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 42699    |\n",
            "|    policy_loss        | 0.0511   |\n",
            "|    value_loss         | 1.79     |\n",
            "------------------------------------\n",
            "Num timesteps: 1712000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -98.56\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 377      |\n",
            "|    ep_rew_mean        | -98.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 42800    |\n",
            "|    time_elapsed       | 1559     |\n",
            "|    total_timesteps    | 1712000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.63    |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 42799    |\n",
            "|    policy_loss        | -0.368   |\n",
            "|    value_loss         | 3.14     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 368      |\n",
            "|    ep_rew_mean        | -98.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 42900    |\n",
            "|    time_elapsed       | 1562     |\n",
            "|    total_timesteps    | 1716000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.538   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 42899    |\n",
            "|    policy_loss        | 0.348    |\n",
            "|    value_loss         | 6.21     |\n",
            "------------------------------------\n",
            "Num timesteps: 1720000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -98.84\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 353      |\n",
            "|    ep_rew_mean        | -98.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 43000    |\n",
            "|    time_elapsed       | 1565     |\n",
            "|    total_timesteps    | 1720000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.478   |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 42999    |\n",
            "|    policy_loss        | -0.0142  |\n",
            "|    value_loss         | 7.56     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 360      |\n",
            "|    ep_rew_mean        | -99.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 43100    |\n",
            "|    time_elapsed       | 1568     |\n",
            "|    total_timesteps    | 1724000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.417   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 43099    |\n",
            "|    policy_loss        | -0.135   |\n",
            "|    value_loss         | 3.48     |\n",
            "------------------------------------\n",
            "Num timesteps: 1728000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -98.07\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 376      |\n",
            "|    ep_rew_mean        | -98.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 43200    |\n",
            "|    time_elapsed       | 1572     |\n",
            "|    total_timesteps    | 1728000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.509   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 43199    |\n",
            "|    policy_loss        | 0.263    |\n",
            "|    value_loss         | 3.82     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 381      |\n",
            "|    ep_rew_mean        | -94.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 43300    |\n",
            "|    time_elapsed       | 1576     |\n",
            "|    total_timesteps    | 1732000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.497   |\n",
            "|    explained_variance | 0.909    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 43299    |\n",
            "|    policy_loss        | -0.0829  |\n",
            "|    value_loss         | 4.76     |\n",
            "------------------------------------\n",
            "Num timesteps: 1736000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -92.11\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 371      |\n",
            "|    ep_rew_mean        | -92.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 43400    |\n",
            "|    time_elapsed       | 1579     |\n",
            "|    total_timesteps    | 1736000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.507   |\n",
            "|    explained_variance | 0.942    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 43399    |\n",
            "|    policy_loss        | 0.129    |\n",
            "|    value_loss         | 39.6     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 368      |\n",
            "|    ep_rew_mean        | -93.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 43500    |\n",
            "|    time_elapsed       | 1582     |\n",
            "|    total_timesteps    | 1740000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.564   |\n",
            "|    explained_variance | 0.979    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 43499    |\n",
            "|    policy_loss        | -0.00572 |\n",
            "|    value_loss         | 10.3     |\n",
            "------------------------------------\n",
            "Num timesteps: 1744000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -91.04\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 382      |\n",
            "|    ep_rew_mean        | -91      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 43600    |\n",
            "|    time_elapsed       | 1586     |\n",
            "|    total_timesteps    | 1744000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.465   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 43599    |\n",
            "|    policy_loss        | -0.353   |\n",
            "|    value_loss         | 1.59     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 372      |\n",
            "|    ep_rew_mean        | -83.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 43700    |\n",
            "|    time_elapsed       | 1589     |\n",
            "|    total_timesteps    | 1748000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.579   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 43699    |\n",
            "|    policy_loss        | -0.795   |\n",
            "|    value_loss         | 6.09     |\n",
            "------------------------------------\n",
            "Num timesteps: 1752000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -79.13\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 365      |\n",
            "|    ep_rew_mean        | -79.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 43800    |\n",
            "|    time_elapsed       | 1592     |\n",
            "|    total_timesteps    | 1752000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.494   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 43799    |\n",
            "|    policy_loss        | 0.244    |\n",
            "|    value_loss         | 3.32     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 377      |\n",
            "|    ep_rew_mean        | -77.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 43900    |\n",
            "|    time_elapsed       | 1595     |\n",
            "|    total_timesteps    | 1756000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.531   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 43899    |\n",
            "|    policy_loss        | 0.0907   |\n",
            "|    value_loss         | 4.53     |\n",
            "------------------------------------\n",
            "Num timesteps: 1760000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -77.61\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 376      |\n",
            "|    ep_rew_mean        | -77.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 44000    |\n",
            "|    time_elapsed       | 1598     |\n",
            "|    total_timesteps    | 1760000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.505   |\n",
            "|    explained_variance | 0.633    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 43999    |\n",
            "|    policy_loss        | -0.15    |\n",
            "|    value_loss         | 580      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 361      |\n",
            "|    ep_rew_mean        | -77.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 44100    |\n",
            "|    time_elapsed       | 1603     |\n",
            "|    total_timesteps    | 1764000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.535   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 44099    |\n",
            "|    policy_loss        | 0.151    |\n",
            "|    value_loss         | 3.44     |\n",
            "------------------------------------\n",
            "Num timesteps: 1768000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -82.81\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 372      |\n",
            "|    ep_rew_mean        | -82.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 44200    |\n",
            "|    time_elapsed       | 1607     |\n",
            "|    total_timesteps    | 1768000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.443   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 44199    |\n",
            "|    policy_loss        | -0.272   |\n",
            "|    value_loss         | 14.7     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 353      |\n",
            "|    ep_rew_mean        | -90.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 44300    |\n",
            "|    time_elapsed       | 1609     |\n",
            "|    total_timesteps    | 1772000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.624   |\n",
            "|    explained_variance | 0.958    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 44299    |\n",
            "|    policy_loss        | 0.328    |\n",
            "|    value_loss         | 56.4     |\n",
            "------------------------------------\n",
            "Num timesteps: 1776000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -84.98\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 333      |\n",
            "|    ep_rew_mean        | -85      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 44400    |\n",
            "|    time_elapsed       | 1612     |\n",
            "|    total_timesteps    | 1776000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.467   |\n",
            "|    explained_variance | 0.979    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 44399    |\n",
            "|    policy_loss        | 0.967    |\n",
            "|    value_loss         | 20.7     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 333      |\n",
            "|    ep_rew_mean        | -98.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 44500    |\n",
            "|    time_elapsed       | 1615     |\n",
            "|    total_timesteps    | 1780000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.604   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 44499    |\n",
            "|    policy_loss        | 0.329    |\n",
            "|    value_loss         | 3.38     |\n",
            "------------------------------------\n",
            "Num timesteps: 1784000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -97.55\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 324      |\n",
            "|    ep_rew_mean        | -97.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 44600    |\n",
            "|    time_elapsed       | 1618     |\n",
            "|    total_timesteps    | 1784000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.538   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 44599    |\n",
            "|    policy_loss        | 0.132    |\n",
            "|    value_loss         | 1.6      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 352      |\n",
            "|    ep_rew_mean        | -94      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 44700    |\n",
            "|    time_elapsed       | 1622     |\n",
            "|    total_timesteps    | 1788000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.475   |\n",
            "|    explained_variance | 0.958    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 44699    |\n",
            "|    policy_loss        | -1.52    |\n",
            "|    value_loss         | 48.7     |\n",
            "------------------------------------\n",
            "Num timesteps: 1792000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -97.99\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 366      |\n",
            "|    ep_rew_mean        | -98      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 44800    |\n",
            "|    time_elapsed       | 1626     |\n",
            "|    total_timesteps    | 1792000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.379   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 44799    |\n",
            "|    policy_loss        | -0.292   |\n",
            "|    value_loss         | 6.38     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 372      |\n",
            "|    ep_rew_mean        | -100     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 44900    |\n",
            "|    time_elapsed       | 1629     |\n",
            "|    total_timesteps    | 1796000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.472   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 44899    |\n",
            "|    policy_loss        | -0.032   |\n",
            "|    value_loss         | 2.59     |\n",
            "------------------------------------\n",
            "Num timesteps: 1800000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -95.85\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 364      |\n",
            "|    ep_rew_mean        | -95.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 45000    |\n",
            "|    time_elapsed       | 1632     |\n",
            "|    total_timesteps    | 1800000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.426   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 44999    |\n",
            "|    policy_loss        | -0.367   |\n",
            "|    value_loss         | 2.43     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 353      |\n",
            "|    ep_rew_mean        | -92.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 45100    |\n",
            "|    time_elapsed       | 1636     |\n",
            "|    total_timesteps    | 1804000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.479   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 45099    |\n",
            "|    policy_loss        | 0.734    |\n",
            "|    value_loss         | 7.53     |\n",
            "------------------------------------\n",
            "Num timesteps: 1808000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -91.84\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 371      |\n",
            "|    ep_rew_mean        | -91.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 45200    |\n",
            "|    time_elapsed       | 1640     |\n",
            "|    total_timesteps    | 1808000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.531   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 45199    |\n",
            "|    policy_loss        | -0.341   |\n",
            "|    value_loss         | 4.77     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 375      |\n",
            "|    ep_rew_mean        | -87.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 45300    |\n",
            "|    time_elapsed       | 1643     |\n",
            "|    total_timesteps    | 1812000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.526   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 45299    |\n",
            "|    policy_loss        | 0.158    |\n",
            "|    value_loss         | 3.57     |\n",
            "------------------------------------\n",
            "Num timesteps: 1816000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -85.55\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 382      |\n",
            "|    ep_rew_mean        | -85.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1103     |\n",
            "|    iterations         | 45400    |\n",
            "|    time_elapsed       | 1646     |\n",
            "|    total_timesteps    | 1816000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.517   |\n",
            "|    explained_variance | 0.946    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 45399    |\n",
            "|    policy_loss        | 0.194    |\n",
            "|    value_loss         | 58.2     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 383      |\n",
            "|    ep_rew_mean        | -83.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1103     |\n",
            "|    iterations         | 45500    |\n",
            "|    time_elapsed       | 1648     |\n",
            "|    total_timesteps    | 1820000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.441   |\n",
            "|    explained_variance | 0.977    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 45499    |\n",
            "|    policy_loss        | 0.201    |\n",
            "|    value_loss         | 10.4     |\n",
            "------------------------------------\n",
            "Num timesteps: 1824000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -87.70\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 365      |\n",
            "|    ep_rew_mean        | -87.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1104     |\n",
            "|    iterations         | 45600    |\n",
            "|    time_elapsed       | 1651     |\n",
            "|    total_timesteps    | 1824000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.495   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 45599    |\n",
            "|    policy_loss        | -0.0184  |\n",
            "|    value_loss         | 5.33     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 356      |\n",
            "|    ep_rew_mean        | -85.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1105     |\n",
            "|    iterations         | 45700    |\n",
            "|    time_elapsed       | 1654     |\n",
            "|    total_timesteps    | 1828000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.523   |\n",
            "|    explained_variance | 0.964    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 45699    |\n",
            "|    policy_loss        | 0.0661   |\n",
            "|    value_loss         | 13.6     |\n",
            "------------------------------------\n",
            "Num timesteps: 1832000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -87.73\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 346      |\n",
            "|    ep_rew_mean        | -87.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1105     |\n",
            "|    iterations         | 45800    |\n",
            "|    time_elapsed       | 1657     |\n",
            "|    total_timesteps    | 1832000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.41    |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 45799    |\n",
            "|    policy_loss        | 0.847    |\n",
            "|    value_loss         | 6.19     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 349      |\n",
            "|    ep_rew_mean        | -90.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1105     |\n",
            "|    iterations         | 45900    |\n",
            "|    time_elapsed       | 1660     |\n",
            "|    total_timesteps    | 1836000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.513   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 45899    |\n",
            "|    policy_loss        | -0.158   |\n",
            "|    value_loss         | 8.08     |\n",
            "------------------------------------\n",
            "Num timesteps: 1840000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -92.65\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 354      |\n",
            "|    ep_rew_mean        | -92.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1105     |\n",
            "|    iterations         | 46000    |\n",
            "|    time_elapsed       | 1664     |\n",
            "|    total_timesteps    | 1840000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.579   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 45999    |\n",
            "|    policy_loss        | -0.258   |\n",
            "|    value_loss         | 6.05     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 354      |\n",
            "|    ep_rew_mean        | -92.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1104     |\n",
            "|    iterations         | 46100    |\n",
            "|    time_elapsed       | 1669     |\n",
            "|    total_timesteps    | 1844000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.533   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 46099    |\n",
            "|    policy_loss        | -0.257   |\n",
            "|    value_loss         | 7.67     |\n",
            "------------------------------------\n",
            "Num timesteps: 1848000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -99.05\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 367      |\n",
            "|    ep_rew_mean        | -99      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1104     |\n",
            "|    iterations         | 46200    |\n",
            "|    time_elapsed       | 1673     |\n",
            "|    total_timesteps    | 1848000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.401   |\n",
            "|    explained_variance | 0.969    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 46199    |\n",
            "|    policy_loss        | -0.107   |\n",
            "|    value_loss         | 6.67     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 384      |\n",
            "|    ep_rew_mean        | -99.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1103     |\n",
            "|    iterations         | 46300    |\n",
            "|    time_elapsed       | 1677     |\n",
            "|    total_timesteps    | 1852000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.338   |\n",
            "|    explained_variance | 0.934    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 46299    |\n",
            "|    policy_loss        | -0.0799  |\n",
            "|    value_loss         | 9        |\n",
            "------------------------------------\n",
            "Num timesteps: 1856000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -102.14\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 397      |\n",
            "|    ep_rew_mean        | -102     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1103     |\n",
            "|    iterations         | 46400    |\n",
            "|    time_elapsed       | 1682     |\n",
            "|    total_timesteps    | 1856000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.392   |\n",
            "|    explained_variance | 0.975    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 46399    |\n",
            "|    policy_loss        | 0.599    |\n",
            "|    value_loss         | 8.08     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 404      |\n",
            "|    ep_rew_mean        | -103     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1103     |\n",
            "|    iterations         | 46500    |\n",
            "|    time_elapsed       | 1686     |\n",
            "|    total_timesteps    | 1860000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.487   |\n",
            "|    explained_variance | 0.956    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 46499    |\n",
            "|    policy_loss        | 0.129    |\n",
            "|    value_loss         | 4.24     |\n",
            "------------------------------------\n",
            "Num timesteps: 1864000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -105.64\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 407      |\n",
            "|    ep_rew_mean        | -106     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 46600    |\n",
            "|    time_elapsed       | 1690     |\n",
            "|    total_timesteps    | 1864000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.447   |\n",
            "|    explained_variance | 0.963    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 46599    |\n",
            "|    policy_loss        | 0.325    |\n",
            "|    value_loss         | 6.48     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 424      |\n",
            "|    ep_rew_mean        | -103     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 46700    |\n",
            "|    time_elapsed       | 1694     |\n",
            "|    total_timesteps    | 1868000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.429   |\n",
            "|    explained_variance | 0.93     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 46699    |\n",
            "|    policy_loss        | 0.00882  |\n",
            "|    value_loss         | 6.4      |\n",
            "------------------------------------\n",
            "Num timesteps: 1872000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -101.97\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 457      |\n",
            "|    ep_rew_mean        | -102     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 46800    |\n",
            "|    time_elapsed       | 1699     |\n",
            "|    total_timesteps    | 1872000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.43    |\n",
            "|    explained_variance | 0.876    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 46799    |\n",
            "|    policy_loss        | -0.661   |\n",
            "|    value_loss         | 128      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 467      |\n",
            "|    ep_rew_mean        | -101     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 46900    |\n",
            "|    time_elapsed       | 1702     |\n",
            "|    total_timesteps    | 1876000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.445   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 46899    |\n",
            "|    policy_loss        | 0.169    |\n",
            "|    value_loss         | 2.3      |\n",
            "------------------------------------\n",
            "Num timesteps: 1880000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -98.79\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 481      |\n",
            "|    ep_rew_mean        | -98.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 47000    |\n",
            "|    time_elapsed       | 1707     |\n",
            "|    total_timesteps    | 1880000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.46    |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 46999    |\n",
            "|    policy_loss        | 0.563    |\n",
            "|    value_loss         | 5.41     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 471      |\n",
            "|    ep_rew_mean        | -87.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 47100    |\n",
            "|    time_elapsed       | 1710     |\n",
            "|    total_timesteps    | 1884000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.499   |\n",
            "|    explained_variance | 0.95     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 47099    |\n",
            "|    policy_loss        | -1.25    |\n",
            "|    value_loss         | 49.2     |\n",
            "------------------------------------\n",
            "Num timesteps: 1888000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -86.52\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 455      |\n",
            "|    ep_rew_mean        | -86.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 47200    |\n",
            "|    time_elapsed       | 1713     |\n",
            "|    total_timesteps    | 1888000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.455   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 47199    |\n",
            "|    policy_loss        | -0.0457  |\n",
            "|    value_loss         | 7.64     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 448      |\n",
            "|    ep_rew_mean        | -88.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 47300    |\n",
            "|    time_elapsed       | 1716     |\n",
            "|    total_timesteps    | 1892000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.413   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 47299    |\n",
            "|    policy_loss        | 0.15     |\n",
            "|    value_loss         | 4.8      |\n",
            "------------------------------------\n",
            "Num timesteps: 1896000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -88.11\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 444      |\n",
            "|    ep_rew_mean        | -88.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 47400    |\n",
            "|    time_elapsed       | 1720     |\n",
            "|    total_timesteps    | 1896000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.477   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 47399    |\n",
            "|    policy_loss        | -1.01    |\n",
            "|    value_loss         | 4.93     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 440      |\n",
            "|    ep_rew_mean        | -87.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 47500    |\n",
            "|    time_elapsed       | 1724     |\n",
            "|    total_timesteps    | 1900000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.524   |\n",
            "|    explained_variance | 0.976    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 47499    |\n",
            "|    policy_loss        | 0.134    |\n",
            "|    value_loss         | 26.1     |\n",
            "------------------------------------\n",
            "Num timesteps: 1904000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -83.02\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 422      |\n",
            "|    ep_rew_mean        | -83      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 47600    |\n",
            "|    time_elapsed       | 1727     |\n",
            "|    total_timesteps    | 1904000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.499   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 47599    |\n",
            "|    policy_loss        | -0.65    |\n",
            "|    value_loss         | 17.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 408      |\n",
            "|    ep_rew_mean        | -85.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 47700    |\n",
            "|    time_elapsed       | 1731     |\n",
            "|    total_timesteps    | 1908000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.458   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 47699    |\n",
            "|    policy_loss        | -0.721   |\n",
            "|    value_loss         | 8.29     |\n",
            "------------------------------------\n",
            "Num timesteps: 1912000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -86.95\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 386      |\n",
            "|    ep_rew_mean        | -87      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 47800    |\n",
            "|    time_elapsed       | 1734     |\n",
            "|    total_timesteps    | 1912000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.543   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 47799    |\n",
            "|    policy_loss        | -0.699   |\n",
            "|    value_loss         | 10.9     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 355      |\n",
            "|    ep_rew_mean        | -92.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1103     |\n",
            "|    iterations         | 47900    |\n",
            "|    time_elapsed       | 1736     |\n",
            "|    total_timesteps    | 1916000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.466   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 47899    |\n",
            "|    policy_loss        | 0.209    |\n",
            "|    value_loss         | 1.56     |\n",
            "------------------------------------\n",
            "Num timesteps: 1920000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -100.61\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 348      |\n",
            "|    ep_rew_mean        | -101     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1103     |\n",
            "|    iterations         | 48000    |\n",
            "|    time_elapsed       | 1740     |\n",
            "|    total_timesteps    | 1920000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.438   |\n",
            "|    explained_variance | 0.976    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 47999    |\n",
            "|    policy_loss        | -0.103   |\n",
            "|    value_loss         | 5.29     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 387      |\n",
            "|    ep_rew_mean        | -106     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 48100    |\n",
            "|    time_elapsed       | 1744     |\n",
            "|    total_timesteps    | 1924000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.441   |\n",
            "|    explained_variance | 0.979    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 48099    |\n",
            "|    policy_loss        | 0.671    |\n",
            "|    value_loss         | 14.2     |\n",
            "------------------------------------\n",
            "Num timesteps: 1928000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -105.91\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 379      |\n",
            "|    ep_rew_mean        | -106     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 48200    |\n",
            "|    time_elapsed       | 1748     |\n",
            "|    total_timesteps    | 1928000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.434   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 48199    |\n",
            "|    policy_loss        | 0.192    |\n",
            "|    value_loss         | 5.4      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 374      |\n",
            "|    ep_rew_mean        | -105     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 48300    |\n",
            "|    time_elapsed       | 1751     |\n",
            "|    total_timesteps    | 1932000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.393   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 48299    |\n",
            "|    policy_loss        | -0.395   |\n",
            "|    value_loss         | 4.52     |\n",
            "------------------------------------\n",
            "Num timesteps: 1936000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -103.54\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 383      |\n",
            "|    ep_rew_mean        | -104     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 48400    |\n",
            "|    time_elapsed       | 1755     |\n",
            "|    total_timesteps    | 1936000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.514   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 48399    |\n",
            "|    policy_loss        | -0.201   |\n",
            "|    value_loss         | 1.3      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 394      |\n",
            "|    ep_rew_mean        | -101     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 48500    |\n",
            "|    time_elapsed       | 1760     |\n",
            "|    total_timesteps    | 1940000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.468   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 48499    |\n",
            "|    policy_loss        | -0.00266 |\n",
            "|    value_loss         | 1.97     |\n",
            "------------------------------------\n",
            "Num timesteps: 1944000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -102.09\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 417      |\n",
            "|    ep_rew_mean        | -102     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 48600    |\n",
            "|    time_elapsed       | 1765     |\n",
            "|    total_timesteps    | 1944000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.48    |\n",
            "|    explained_variance | 0.971    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 48599    |\n",
            "|    policy_loss        | -0.745   |\n",
            "|    value_loss         | 33.7     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 421      |\n",
            "|    ep_rew_mean        | -98.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 48700    |\n",
            "|    time_elapsed       | 1768     |\n",
            "|    total_timesteps    | 1948000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.358   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 48699    |\n",
            "|    policy_loss        | 0.0635   |\n",
            "|    value_loss         | 7.44     |\n",
            "------------------------------------\n",
            "Num timesteps: 1952000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -95.62\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 414      |\n",
            "|    ep_rew_mean        | -95.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 48800    |\n",
            "|    time_elapsed       | 1771     |\n",
            "|    total_timesteps    | 1952000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.411   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 48799    |\n",
            "|    policy_loss        | 0.074    |\n",
            "|    value_loss         | 5.57     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 409      |\n",
            "|    ep_rew_mean        | -94.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 48900    |\n",
            "|    time_elapsed       | 1776     |\n",
            "|    total_timesteps    | 1956000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.45    |\n",
            "|    explained_variance | 0.904    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 48899    |\n",
            "|    policy_loss        | -0.0192  |\n",
            "|    value_loss         | 73       |\n",
            "------------------------------------\n",
            "Num timesteps: 1960000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -89.69\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 431      |\n",
            "|    ep_rew_mean        | -89.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 49000    |\n",
            "|    time_elapsed       | 1779     |\n",
            "|    total_timesteps    | 1960000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.58    |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 48999    |\n",
            "|    policy_loss        | 0.364    |\n",
            "|    value_loss         | 3.5      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 408      |\n",
            "|    ep_rew_mean        | -89.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 49100    |\n",
            "|    time_elapsed       | 1783     |\n",
            "|    total_timesteps    | 1964000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.5     |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 49099    |\n",
            "|    policy_loss        | 0.5      |\n",
            "|    value_loss         | 7.9      |\n",
            "------------------------------------\n",
            "Num timesteps: 1968000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -84.21\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 428      |\n",
            "|    ep_rew_mean        | -84.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 49200    |\n",
            "|    time_elapsed       | 1788     |\n",
            "|    total_timesteps    | 1968000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.481   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 49199    |\n",
            "|    policy_loss        | -0.468   |\n",
            "|    value_loss         | 3.95     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 417      |\n",
            "|    ep_rew_mean        | -74.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 49300    |\n",
            "|    time_elapsed       | 1791     |\n",
            "|    total_timesteps    | 1972000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.561   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 49299    |\n",
            "|    policy_loss        | 0.179    |\n",
            "|    value_loss         | 4.08     |\n",
            "------------------------------------\n",
            "Num timesteps: 1976000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -76.17\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 386      |\n",
            "|    ep_rew_mean        | -76.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 49400    |\n",
            "|    time_elapsed       | 1794     |\n",
            "|    total_timesteps    | 1976000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.303   |\n",
            "|    explained_variance | 0.975    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 49399    |\n",
            "|    policy_loss        | -0.0397  |\n",
            "|    value_loss         | 41.1     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 358      |\n",
            "|    ep_rew_mean        | -73.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 49500    |\n",
            "|    time_elapsed       | 1797     |\n",
            "|    total_timesteps    | 1980000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.443   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 49499    |\n",
            "|    policy_loss        | 0.0303   |\n",
            "|    value_loss         | 2.54     |\n",
            "------------------------------------\n",
            "Num timesteps: 1984000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -74.37\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 365      |\n",
            "|    ep_rew_mean        | -74.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 49600    |\n",
            "|    time_elapsed       | 1802     |\n",
            "|    total_timesteps    | 1984000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.456   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 49599    |\n",
            "|    policy_loss        | -0.348   |\n",
            "|    value_loss         | 12.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 362      |\n",
            "|    ep_rew_mean        | -77      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 49700    |\n",
            "|    time_elapsed       | 1806     |\n",
            "|    total_timesteps    | 1988000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.446   |\n",
            "|    explained_variance | 0.962    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 49699    |\n",
            "|    policy_loss        | 0.192    |\n",
            "|    value_loss         | 15.7     |\n",
            "------------------------------------\n",
            "Num timesteps: 1992000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -69.61\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 369      |\n",
            "|    ep_rew_mean        | -69.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 49800    |\n",
            "|    time_elapsed       | 1811     |\n",
            "|    total_timesteps    | 1992000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.547   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 49799    |\n",
            "|    policy_loss        | 0.367    |\n",
            "|    value_loss         | 3.76     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 370      |\n",
            "|    ep_rew_mean        | -70.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 49900    |\n",
            "|    time_elapsed       | 1816     |\n",
            "|    total_timesteps    | 1996000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.378   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 49899    |\n",
            "|    policy_loss        | 0.712    |\n",
            "|    value_loss         | 11.9     |\n",
            "------------------------------------\n",
            "Num timesteps: 2000000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -67.11\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 379      |\n",
            "|    ep_rew_mean        | -67.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 50000    |\n",
            "|    time_elapsed       | 1820     |\n",
            "|    total_timesteps    | 2000000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.425   |\n",
            "|    explained_variance | 0.98     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 49999    |\n",
            "|    policy_loss        | 0.00388  |\n",
            "|    value_loss         | 9.8      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 369      |\n",
            "|    ep_rew_mean        | -64.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 50100    |\n",
            "|    time_elapsed       | 1823     |\n",
            "|    total_timesteps    | 2004000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.409   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 50099    |\n",
            "|    policy_loss        | -0.196   |\n",
            "|    value_loss         | 2.86     |\n",
            "------------------------------------\n",
            "Num timesteps: 2008000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -71.20\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 365      |\n",
            "|    ep_rew_mean        | -71.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 50200    |\n",
            "|    time_elapsed       | 1828     |\n",
            "|    total_timesteps    | 2008000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.538   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 50199    |\n",
            "|    policy_loss        | 0.203    |\n",
            "|    value_loss         | 6.61     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 400      |\n",
            "|    ep_rew_mean        | -70.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 50300    |\n",
            "|    time_elapsed       | 1831     |\n",
            "|    total_timesteps    | 2012000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.365   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 50299    |\n",
            "|    policy_loss        | -0.164   |\n",
            "|    value_loss         | 2.87     |\n",
            "------------------------------------\n",
            "Num timesteps: 2016000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -72.60\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 403      |\n",
            "|    ep_rew_mean        | -72.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 50400    |\n",
            "|    time_elapsed       | 1835     |\n",
            "|    total_timesteps    | 2016000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.443   |\n",
            "|    explained_variance | 0.939    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 50399    |\n",
            "|    policy_loss        | 0.238    |\n",
            "|    value_loss         | 48.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 413      |\n",
            "|    ep_rew_mean        | -73.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 50500    |\n",
            "|    time_elapsed       | 1838     |\n",
            "|    total_timesteps    | 2020000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.538   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 50499    |\n",
            "|    policy_loss        | -0.248   |\n",
            "|    value_loss         | 5.31     |\n",
            "------------------------------------\n",
            "Num timesteps: 2024000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -73.63\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 403      |\n",
            "|    ep_rew_mean        | -73.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 50600    |\n",
            "|    time_elapsed       | 1841     |\n",
            "|    total_timesteps    | 2024000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.529   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 50599    |\n",
            "|    policy_loss        | -0.34    |\n",
            "|    value_loss         | 5.77     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 389      |\n",
            "|    ep_rew_mean        | -73.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 50700    |\n",
            "|    time_elapsed       | 1843     |\n",
            "|    total_timesteps    | 2028000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.383   |\n",
            "|    explained_variance | 0.96     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 50699    |\n",
            "|    policy_loss        | -0.288   |\n",
            "|    value_loss         | 7.05     |\n",
            "------------------------------------\n",
            "Num timesteps: 2032000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -80.34\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 391      |\n",
            "|    ep_rew_mean        | -80.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 50800    |\n",
            "|    time_elapsed       | 1847     |\n",
            "|    total_timesteps    | 2032000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.403   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 50799    |\n",
            "|    policy_loss        | -0.0872  |\n",
            "|    value_loss         | 8.48     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 385      |\n",
            "|    ep_rew_mean        | -82.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 50900    |\n",
            "|    time_elapsed       | 1851     |\n",
            "|    total_timesteps    | 2036000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.505   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 50899    |\n",
            "|    policy_loss        | -0.453   |\n",
            "|    value_loss         | 8.21     |\n",
            "------------------------------------\n",
            "Num timesteps: 2040000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -87.89\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 370      |\n",
            "|    ep_rew_mean        | -87.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 51000    |\n",
            "|    time_elapsed       | 1855     |\n",
            "|    total_timesteps    | 2040000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.552   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 50999    |\n",
            "|    policy_loss        | -0.232   |\n",
            "|    value_loss         | 3.21     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 378      |\n",
            "|    ep_rew_mean        | -90      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 51100    |\n",
            "|    time_elapsed       | 1859     |\n",
            "|    total_timesteps    | 2044000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.451   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 51099    |\n",
            "|    policy_loss        | 0.15     |\n",
            "|    value_loss         | 5.24     |\n",
            "------------------------------------\n",
            "Num timesteps: 2048000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -89.72\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 377      |\n",
            "|    ep_rew_mean        | -89.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 51200    |\n",
            "|    time_elapsed       | 1863     |\n",
            "|    total_timesteps    | 2048000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.378   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 51199    |\n",
            "|    policy_loss        | 0.0719   |\n",
            "|    value_loss         | 1.24     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 382      |\n",
            "|    ep_rew_mean        | -86.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 51300    |\n",
            "|    time_elapsed       | 1867     |\n",
            "|    total_timesteps    | 2052000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.478   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 51299    |\n",
            "|    policy_loss        | 0.299    |\n",
            "|    value_loss         | 3.13     |\n",
            "------------------------------------\n",
            "Num timesteps: 2056000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -84.07\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 399      |\n",
            "|    ep_rew_mean        | -84.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 51400    |\n",
            "|    time_elapsed       | 1871     |\n",
            "|    total_timesteps    | 2056000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.524   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 51399    |\n",
            "|    policy_loss        | -0.00751 |\n",
            "|    value_loss         | 4.93     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 398      |\n",
            "|    ep_rew_mean        | -88.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 51500    |\n",
            "|    time_elapsed       | 1875     |\n",
            "|    total_timesteps    | 2060000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.506   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 51499    |\n",
            "|    policy_loss        | -0.0766  |\n",
            "|    value_loss         | 3.72     |\n",
            "------------------------------------\n",
            "Num timesteps: 2064000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -92.52\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 398      |\n",
            "|    ep_rew_mean        | -92.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 51600    |\n",
            "|    time_elapsed       | 1878     |\n",
            "|    total_timesteps    | 2064000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.422   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 51599    |\n",
            "|    policy_loss        | 0.173    |\n",
            "|    value_loss         | 1.98     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 413      |\n",
            "|    ep_rew_mean        | -93.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 51700    |\n",
            "|    time_elapsed       | 1883     |\n",
            "|    total_timesteps    | 2068000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.509   |\n",
            "|    explained_variance | 0.961    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 51699    |\n",
            "|    policy_loss        | 0.73     |\n",
            "|    value_loss         | 9.16     |\n",
            "------------------------------------\n",
            "Num timesteps: 2072000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -95.58\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 443      |\n",
            "|    ep_rew_mean        | -95.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 51800    |\n",
            "|    time_elapsed       | 1887     |\n",
            "|    total_timesteps    | 2072000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.432   |\n",
            "|    explained_variance | 0.94     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 51799    |\n",
            "|    policy_loss        | 0.851    |\n",
            "|    value_loss         | 7.59     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 462      |\n",
            "|    ep_rew_mean        | -96.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 51900    |\n",
            "|    time_elapsed       | 1891     |\n",
            "|    total_timesteps    | 2076000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.342   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 51899    |\n",
            "|    policy_loss        | 0.181    |\n",
            "|    value_loss         | 2.64     |\n",
            "------------------------------------\n",
            "Num timesteps: 2080000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -90.95\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 457      |\n",
            "|    ep_rew_mean        | -91      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 52000    |\n",
            "|    time_elapsed       | 1894     |\n",
            "|    total_timesteps    | 2080000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.429   |\n",
            "|    explained_variance | 0.978    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 51999    |\n",
            "|    policy_loss        | 0.252    |\n",
            "|    value_loss         | 5.92     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 477      |\n",
            "|    ep_rew_mean        | -86.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 52100    |\n",
            "|    time_elapsed       | 1897     |\n",
            "|    total_timesteps    | 2084000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.397   |\n",
            "|    explained_variance | 0.977    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 52099    |\n",
            "|    policy_loss        | 0.117    |\n",
            "|    value_loss         | 8.67     |\n",
            "------------------------------------\n",
            "Num timesteps: 2088000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -80.36\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 480      |\n",
            "|    ep_rew_mean        | -80.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 52200    |\n",
            "|    time_elapsed       | 1901     |\n",
            "|    total_timesteps    | 2088000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.359   |\n",
            "|    explained_variance | 0.973    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 52199    |\n",
            "|    policy_loss        | -0.126   |\n",
            "|    value_loss         | 22       |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 481      |\n",
            "|    ep_rew_mean        | -64.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 52300    |\n",
            "|    time_elapsed       | 1904     |\n",
            "|    total_timesteps    | 2092000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.331   |\n",
            "|    explained_variance | 0.784    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 52299    |\n",
            "|    policy_loss        | -0.0177  |\n",
            "|    value_loss         | 66.8     |\n",
            "------------------------------------\n",
            "Num timesteps: 2096000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -56.98\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 463      |\n",
            "|    ep_rew_mean        | -57      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 52400    |\n",
            "|    time_elapsed       | 1907     |\n",
            "|    total_timesteps    | 2096000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.461   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 52399    |\n",
            "|    policy_loss        | 0.00458  |\n",
            "|    value_loss         | 7.41     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 435      |\n",
            "|    ep_rew_mean        | -53.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 52500    |\n",
            "|    time_elapsed       | 1910     |\n",
            "|    total_timesteps    | 2100000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.375   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 52499    |\n",
            "|    policy_loss        | 0.0395   |\n",
            "|    value_loss         | 3.36     |\n",
            "------------------------------------\n",
            "Num timesteps: 2104000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -38.98\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 441      |\n",
            "|    ep_rew_mean        | -39      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 52600    |\n",
            "|    time_elapsed       | 1912     |\n",
            "|    total_timesteps    | 2104000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.48    |\n",
            "|    explained_variance | 0.977    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 52599    |\n",
            "|    policy_loss        | 0.678    |\n",
            "|    value_loss         | 14.9     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 412      |\n",
            "|    ep_rew_mean        | -33      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 52700    |\n",
            "|    time_elapsed       | 1915     |\n",
            "|    total_timesteps    | 2108000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.484   |\n",
            "|    explained_variance | 0.359    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 52699    |\n",
            "|    policy_loss        | 0.223    |\n",
            "|    value_loss         | 646      |\n",
            "------------------------------------\n",
            "Num timesteps: 2112000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -18.87\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 385      |\n",
            "|    ep_rew_mean        | -18.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 52800    |\n",
            "|    time_elapsed       | 1918     |\n",
            "|    total_timesteps    | 2112000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.366   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 52799    |\n",
            "|    policy_loss        | -0.00423 |\n",
            "|    value_loss         | 5.49     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 367      |\n",
            "|    ep_rew_mean        | -16.7    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 52900    |\n",
            "|    time_elapsed       | 1921     |\n",
            "|    total_timesteps    | 2116000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.359   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 52899    |\n",
            "|    policy_loss        | -0.0551  |\n",
            "|    value_loss         | 3.72     |\n",
            "------------------------------------\n",
            "Num timesteps: 2120000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -8.88\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 376      |\n",
            "|    ep_rew_mean        | -8.88    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 53000    |\n",
            "|    time_elapsed       | 1925     |\n",
            "|    total_timesteps    | 2120000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.369   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 52999    |\n",
            "|    policy_loss        | 0.403    |\n",
            "|    value_loss         | 5.46     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 371      |\n",
            "|    ep_rew_mean        | -6.11    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 53100    |\n",
            "|    time_elapsed       | 1928     |\n",
            "|    total_timesteps    | 2124000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.408   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 53099    |\n",
            "|    policy_loss        | -0.405   |\n",
            "|    value_loss         | 6.88     |\n",
            "------------------------------------\n",
            "Num timesteps: 2128000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -17.62\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 360      |\n",
            "|    ep_rew_mean        | -17.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 53200    |\n",
            "|    time_elapsed       | 1931     |\n",
            "|    total_timesteps    | 2128000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.273   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 53199    |\n",
            "|    policy_loss        | 0.73     |\n",
            "|    value_loss         | 5.38     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 370      |\n",
            "|    ep_rew_mean        | -16      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 53300    |\n",
            "|    time_elapsed       | 1935     |\n",
            "|    total_timesteps    | 2132000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.355   |\n",
            "|    explained_variance | 0.948    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 53299    |\n",
            "|    policy_loss        | 0.128    |\n",
            "|    value_loss         | 2.96     |\n",
            "------------------------------------\n",
            "Num timesteps: 2136000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -24.52\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 391      |\n",
            "|    ep_rew_mean        | -24.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 53400    |\n",
            "|    time_elapsed       | 1939     |\n",
            "|    total_timesteps    | 2136000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.48    |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 53399    |\n",
            "|    policy_loss        | -0.0723  |\n",
            "|    value_loss         | 6.49     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 392      |\n",
            "|    ep_rew_mean        | -25.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 53500    |\n",
            "|    time_elapsed       | 1943     |\n",
            "|    total_timesteps    | 2140000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.384   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 53499    |\n",
            "|    policy_loss        | 0.0927   |\n",
            "|    value_loss         | 1.94     |\n",
            "------------------------------------\n",
            "Num timesteps: 2144000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -28.60\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 407      |\n",
            "|    ep_rew_mean        | -28.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 53600    |\n",
            "|    time_elapsed       | 1947     |\n",
            "|    total_timesteps    | 2144000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.371   |\n",
            "|    explained_variance | 0.922    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 53599    |\n",
            "|    policy_loss        | 0.199    |\n",
            "|    value_loss         | 65.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 411      |\n",
            "|    ep_rew_mean        | -27.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 53700    |\n",
            "|    time_elapsed       | 1951     |\n",
            "|    total_timesteps    | 2148000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.381   |\n",
            "|    explained_variance | 0.975    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 53699    |\n",
            "|    policy_loss        | -0.0146  |\n",
            "|    value_loss         | 4.19     |\n",
            "------------------------------------\n",
            "Num timesteps: 2152000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -26.89\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 443      |\n",
            "|    ep_rew_mean        | -26.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 53800    |\n",
            "|    time_elapsed       | 1956     |\n",
            "|    total_timesteps    | 2152000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.423   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 53799    |\n",
            "|    policy_loss        | -0.18    |\n",
            "|    value_loss         | 1.38     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 465      |\n",
            "|    ep_rew_mean        | -27.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 53900    |\n",
            "|    time_elapsed       | 1960     |\n",
            "|    total_timesteps    | 2156000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.466   |\n",
            "|    explained_variance | 0.949    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 53899    |\n",
            "|    policy_loss        | 0.0989   |\n",
            "|    value_loss         | 4.4      |\n",
            "------------------------------------\n",
            "Num timesteps: 2160000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -29.28\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 478      |\n",
            "|    ep_rew_mean        | -29.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 54000    |\n",
            "|    time_elapsed       | 1964     |\n",
            "|    total_timesteps    | 2160000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.456   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 53999    |\n",
            "|    policy_loss        | -0.0381  |\n",
            "|    value_loss         | 0.816    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 491      |\n",
            "|    ep_rew_mean        | -31.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 54100    |\n",
            "|    time_elapsed       | 1968     |\n",
            "|    total_timesteps    | 2164000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.494   |\n",
            "|    explained_variance | 0.931    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 54099    |\n",
            "|    policy_loss        | 0.163    |\n",
            "|    value_loss         | 3.06     |\n",
            "------------------------------------\n",
            "Num timesteps: 2168000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -34.76\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 516      |\n",
            "|    ep_rew_mean        | -34.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 54200    |\n",
            "|    time_elapsed       | 1972     |\n",
            "|    total_timesteps    | 2168000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.536   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 54199    |\n",
            "|    policy_loss        | -0.0876  |\n",
            "|    value_loss         | 1.86     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 509      |\n",
            "|    ep_rew_mean        | -35.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 54300    |\n",
            "|    time_elapsed       | 1976     |\n",
            "|    total_timesteps    | 2172000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.438   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 54299    |\n",
            "|    policy_loss        | 0.101    |\n",
            "|    value_loss         | 0.687    |\n",
            "------------------------------------\n",
            "Num timesteps: 2176000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -32.22\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 539      |\n",
            "|    ep_rew_mean        | -32.2    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 54400    |\n",
            "|    time_elapsed       | 1982     |\n",
            "|    total_timesteps    | 2176000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.513   |\n",
            "|    explained_variance | 0.98     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 54399    |\n",
            "|    policy_loss        | -0.572   |\n",
            "|    value_loss         | 4.11     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 544      |\n",
            "|    ep_rew_mean        | -34      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 54500    |\n",
            "|    time_elapsed       | 1986     |\n",
            "|    total_timesteps    | 2180000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.503   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 54499    |\n",
            "|    policy_loss        | 0.0794   |\n",
            "|    value_loss         | 3.37     |\n",
            "------------------------------------\n",
            "Num timesteps: 2184000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -29.86\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 563      |\n",
            "|    ep_rew_mean        | -29.9    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 54600    |\n",
            "|    time_elapsed       | 1989     |\n",
            "|    total_timesteps    | 2184000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.487   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 54599    |\n",
            "|    policy_loss        | -0.181   |\n",
            "|    value_loss         | 6.88     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 564      |\n",
            "|    ep_rew_mean        | -33.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 54700    |\n",
            "|    time_elapsed       | 1994     |\n",
            "|    total_timesteps    | 2188000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.426   |\n",
            "|    explained_variance | 0.976    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 54699    |\n",
            "|    policy_loss        | 0.0215   |\n",
            "|    value_loss         | 2.11     |\n",
            "------------------------------------\n",
            "Num timesteps: 2192000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -33.32\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 579      |\n",
            "|    ep_rew_mean        | -33.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 54800    |\n",
            "|    time_elapsed       | 2000     |\n",
            "|    total_timesteps    | 2192000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.37    |\n",
            "|    explained_variance | 0.798    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 54799    |\n",
            "|    policy_loss        | -0.548   |\n",
            "|    value_loss         | 134      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 599      |\n",
            "|    ep_rew_mean        | -22.5    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 54900    |\n",
            "|    time_elapsed       | 2003     |\n",
            "|    total_timesteps    | 2196000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.262   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 54899    |\n",
            "|    policy_loss        | -0.0692  |\n",
            "|    value_loss         | 0.643    |\n",
            "------------------------------------\n",
            "Num timesteps: 2200000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -19.36\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 597      |\n",
            "|    ep_rew_mean        | -19.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 55000    |\n",
            "|    time_elapsed       | 2006     |\n",
            "|    total_timesteps    | 2200000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.404   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 54999    |\n",
            "|    policy_loss        | -0.172   |\n",
            "|    value_loss         | 1.33     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 596      |\n",
            "|    ep_rew_mean        | -17.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 55100    |\n",
            "|    time_elapsed       | 2009     |\n",
            "|    total_timesteps    | 2204000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.54    |\n",
            "|    explained_variance | 0.978    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 55099    |\n",
            "|    policy_loss        | -0.133   |\n",
            "|    value_loss         | 8.45     |\n",
            "------------------------------------\n",
            "Num timesteps: 2208000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -9.72\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 553      |\n",
            "|    ep_rew_mean        | -9.72    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 55200    |\n",
            "|    time_elapsed       | 2012     |\n",
            "|    total_timesteps    | 2208000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.46    |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 55199    |\n",
            "|    policy_loss        | 0.386    |\n",
            "|    value_loss         | 6.36     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 542      |\n",
            "|    ep_rew_mean        | -13.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 55300    |\n",
            "|    time_elapsed       | 2016     |\n",
            "|    total_timesteps    | 2212000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.282   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 55299    |\n",
            "|    policy_loss        | 0.179    |\n",
            "|    value_loss         | 1.67     |\n",
            "------------------------------------\n",
            "Num timesteps: 2216000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -6.23\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 539      |\n",
            "|    ep_rew_mean        | -6.23    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 55400    |\n",
            "|    time_elapsed       | 2020     |\n",
            "|    total_timesteps    | 2216000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.343   |\n",
            "|    explained_variance | 0.978    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 55399    |\n",
            "|    policy_loss        | -0.288   |\n",
            "|    value_loss         | 5.49     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 527      |\n",
            "|    ep_rew_mean        | -10.3    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 55500    |\n",
            "|    time_elapsed       | 2023     |\n",
            "|    total_timesteps    | 2220000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.272   |\n",
            "|    explained_variance | 0.482    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 55499    |\n",
            "|    policy_loss        | 0.447    |\n",
            "|    value_loss         | 148      |\n",
            "------------------------------------\n",
            "Num timesteps: 2224000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -1.71\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 533      |\n",
            "|    ep_rew_mean        | -1.71    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 55600    |\n",
            "|    time_elapsed       | 2027     |\n",
            "|    total_timesteps    | 2224000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.325   |\n",
            "|    explained_variance | 0.966    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 55599    |\n",
            "|    policy_loss        | -0.0767  |\n",
            "|    value_loss         | 7.46     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 527      |\n",
            "|    ep_rew_mean        | -4.26    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 55700    |\n",
            "|    time_elapsed       | 2032     |\n",
            "|    total_timesteps    | 2228000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.509   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 55699    |\n",
            "|    policy_loss        | -0.0827  |\n",
            "|    value_loss         | 3.6      |\n",
            "------------------------------------\n",
            "Num timesteps: 2232000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -5.95\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 524      |\n",
            "|    ep_rew_mean        | -5.95    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 55800    |\n",
            "|    time_elapsed       | 2036     |\n",
            "|    total_timesteps    | 2232000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.387   |\n",
            "|    explained_variance | 0.624    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 55799    |\n",
            "|    policy_loss        | 0.32     |\n",
            "|    value_loss         | 233      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 530      |\n",
            "|    ep_rew_mean        | 0.45     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 55900    |\n",
            "|    time_elapsed       | 2039     |\n",
            "|    total_timesteps    | 2236000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.319   |\n",
            "|    explained_variance | 0.444    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 55899    |\n",
            "|    policy_loss        | 0.142    |\n",
            "|    value_loss         | 700      |\n",
            "------------------------------------\n",
            "Num timesteps: 2240000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 10.48\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 491      |\n",
            "|    ep_rew_mean        | 10.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 56000    |\n",
            "|    time_elapsed       | 2042     |\n",
            "|    total_timesteps    | 2240000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.239   |\n",
            "|    explained_variance | 0.234    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 55999    |\n",
            "|    policy_loss        | 0.695    |\n",
            "|    value_loss         | 140      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 472      |\n",
            "|    ep_rew_mean        | 9.26     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 56100    |\n",
            "|    time_elapsed       | 2046     |\n",
            "|    total_timesteps    | 2244000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.5     |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 56099    |\n",
            "|    policy_loss        | 0.00198  |\n",
            "|    value_loss         | 1.68     |\n",
            "------------------------------------\n",
            "Num timesteps: 2248000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 12.11\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 488      |\n",
            "|    ep_rew_mean        | 12.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 56200    |\n",
            "|    time_elapsed       | 2050     |\n",
            "|    total_timesteps    | 2248000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.395   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 56199    |\n",
            "|    policy_loss        | -0.0364  |\n",
            "|    value_loss         | 2.89     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 499      |\n",
            "|    ep_rew_mean        | 10.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 56300    |\n",
            "|    time_elapsed       | 2054     |\n",
            "|    total_timesteps    | 2252000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.43    |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 56299    |\n",
            "|    policy_loss        | -0.151   |\n",
            "|    value_loss         | 4.38     |\n",
            "------------------------------------\n",
            "Num timesteps: 2256000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 11.07\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 497      |\n",
            "|    ep_rew_mean        | 11.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 56400    |\n",
            "|    time_elapsed       | 2058     |\n",
            "|    total_timesteps    | 2256000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.369   |\n",
            "|    explained_variance | 0.914    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 56399    |\n",
            "|    policy_loss        | -3.24    |\n",
            "|    value_loss         | 72.1     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 520      |\n",
            "|    ep_rew_mean        | 6.32     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 56500    |\n",
            "|    time_elapsed       | 2063     |\n",
            "|    total_timesteps    | 2260000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.476   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 56499    |\n",
            "|    policy_loss        | -0.289   |\n",
            "|    value_loss         | 8.32     |\n",
            "------------------------------------\n",
            "Num timesteps: 2264000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 5.17\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 529      |\n",
            "|    ep_rew_mean        | 5.17     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 56600    |\n",
            "|    time_elapsed       | 2067     |\n",
            "|    total_timesteps    | 2264000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.421   |\n",
            "|    explained_variance | 0.953    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 56599    |\n",
            "|    policy_loss        | 0.214    |\n",
            "|    value_loss         | 3.29     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 528      |\n",
            "|    ep_rew_mean        | 4.41     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 56700    |\n",
            "|    time_elapsed       | 2071     |\n",
            "|    total_timesteps    | 2268000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.272   |\n",
            "|    explained_variance | 0.87     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 56699    |\n",
            "|    policy_loss        | -0.232   |\n",
            "|    value_loss         | 33.1     |\n",
            "------------------------------------\n",
            "Num timesteps: 2272000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 8.66\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 532      |\n",
            "|    ep_rew_mean        | 8.66     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 56800    |\n",
            "|    time_elapsed       | 2074     |\n",
            "|    total_timesteps    | 2272000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.447   |\n",
            "|    explained_variance | 0.952    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 56799    |\n",
            "|    policy_loss        | 0.0876   |\n",
            "|    value_loss         | 6.76     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 532      |\n",
            "|    ep_rew_mean        | 8.78     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 56900    |\n",
            "|    time_elapsed       | 2079     |\n",
            "|    total_timesteps    | 2276000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.319   |\n",
            "|    explained_variance | 0.87     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 56899    |\n",
            "|    policy_loss        | -0.0356  |\n",
            "|    value_loss         | 65.1     |\n",
            "------------------------------------\n",
            "Num timesteps: 2280000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 9.05\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 533      |\n",
            "|    ep_rew_mean        | 9.05     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 57000    |\n",
            "|    time_elapsed       | 2083     |\n",
            "|    total_timesteps    | 2280000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.291   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 56999    |\n",
            "|    policy_loss        | -0.0918  |\n",
            "|    value_loss         | 3.97     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 510      |\n",
            "|    ep_rew_mean        | 10.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 57100    |\n",
            "|    time_elapsed       | 2086     |\n",
            "|    total_timesteps    | 2284000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.422   |\n",
            "|    explained_variance | 0.525    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 57099    |\n",
            "|    policy_loss        | -0.0636  |\n",
            "|    value_loss         | 34.1     |\n",
            "------------------------------------\n",
            "Num timesteps: 2288000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 15.60\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 504      |\n",
            "|    ep_rew_mean        | 15.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 57200    |\n",
            "|    time_elapsed       | 2089     |\n",
            "|    total_timesteps    | 2288000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.445   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 57199    |\n",
            "|    policy_loss        | -0.0366  |\n",
            "|    value_loss         | 2.32     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 507      |\n",
            "|    ep_rew_mean        | 21.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 57300    |\n",
            "|    time_elapsed       | 2093     |\n",
            "|    total_timesteps    | 2292000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.359   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 57299    |\n",
            "|    policy_loss        | -0.0319  |\n",
            "|    value_loss         | 1.61     |\n",
            "------------------------------------\n",
            "Num timesteps: 2296000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 27.39\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 522      |\n",
            "|    ep_rew_mean        | 27.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 57400    |\n",
            "|    time_elapsed       | 2096     |\n",
            "|    total_timesteps    | 2296000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.375   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 57399    |\n",
            "|    policy_loss        | 0.214    |\n",
            "|    value_loss         | 5.65     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 510      |\n",
            "|    ep_rew_mean        | 37.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 57500    |\n",
            "|    time_elapsed       | 2099     |\n",
            "|    total_timesteps    | 2300000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.425   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 57499    |\n",
            "|    policy_loss        | -0.184   |\n",
            "|    value_loss         | 3.11     |\n",
            "------------------------------------\n",
            "Num timesteps: 2304000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 36.13\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 486      |\n",
            "|    ep_rew_mean        | 36.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 57600    |\n",
            "|    time_elapsed       | 2102     |\n",
            "|    total_timesteps    | 2304000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.386   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 57599    |\n",
            "|    policy_loss        | 0.0922   |\n",
            "|    value_loss         | 2.56     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 472      |\n",
            "|    ep_rew_mean        | 36.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 57700    |\n",
            "|    time_elapsed       | 2106     |\n",
            "|    total_timesteps    | 2308000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.543   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 57699    |\n",
            "|    policy_loss        | -0.239   |\n",
            "|    value_loss         | 2.37     |\n",
            "------------------------------------\n",
            "Num timesteps: 2312000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 35.27\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 459      |\n",
            "|    ep_rew_mean        | 35.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 57800    |\n",
            "|    time_elapsed       | 2111     |\n",
            "|    total_timesteps    | 2312000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.332   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 57799    |\n",
            "|    policy_loss        | -0.242   |\n",
            "|    value_loss         | 1.44     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 470      |\n",
            "|    ep_rew_mean        | 38.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 57900    |\n",
            "|    time_elapsed       | 2116     |\n",
            "|    total_timesteps    | 2316000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.295   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 57899    |\n",
            "|    policy_loss        | 0.21     |\n",
            "|    value_loss         | 4.62     |\n",
            "------------------------------------\n",
            "Num timesteps: 2320000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 40.45\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 458      |\n",
            "|    ep_rew_mean        | 40.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 58000    |\n",
            "|    time_elapsed       | 2118     |\n",
            "|    total_timesteps    | 2320000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.332   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 57999    |\n",
            "|    policy_loss        | -0.106   |\n",
            "|    value_loss         | 2.46     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 448      |\n",
            "|    ep_rew_mean        | 44.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 58100    |\n",
            "|    time_elapsed       | 2123     |\n",
            "|    total_timesteps    | 2324000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.381   |\n",
            "|    explained_variance | 0.907    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 58099    |\n",
            "|    policy_loss        | -0.471   |\n",
            "|    value_loss         | 23.1     |\n",
            "------------------------------------\n",
            "Num timesteps: 2328000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 50.71\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 452      |\n",
            "|    ep_rew_mean        | 50.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 58200    |\n",
            "|    time_elapsed       | 2126     |\n",
            "|    total_timesteps    | 2328000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.328   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 58199    |\n",
            "|    policy_loss        | 0.265    |\n",
            "|    value_loss         | 2.45     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 459      |\n",
            "|    ep_rew_mean        | 48.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 58300    |\n",
            "|    time_elapsed       | 2130     |\n",
            "|    total_timesteps    | 2332000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.5     |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 58299    |\n",
            "|    policy_loss        | 0.0316   |\n",
            "|    value_loss         | 1.8      |\n",
            "------------------------------------\n",
            "Num timesteps: 2336000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 44.03\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 464      |\n",
            "|    ep_rew_mean        | 44       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 58400    |\n",
            "|    time_elapsed       | 2134     |\n",
            "|    total_timesteps    | 2336000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.284   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 58399    |\n",
            "|    policy_loss        | -0.0512  |\n",
            "|    value_loss         | 1.25     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 462      |\n",
            "|    ep_rew_mean        | 31.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 58500    |\n",
            "|    time_elapsed       | 2138     |\n",
            "|    total_timesteps    | 2340000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.362   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 58499    |\n",
            "|    policy_loss        | -0.0312  |\n",
            "|    value_loss         | 1.48     |\n",
            "------------------------------------\n",
            "Num timesteps: 2344000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 18.93\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 464      |\n",
            "|    ep_rew_mean        | 18.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1093     |\n",
            "|    iterations         | 58600    |\n",
            "|    time_elapsed       | 2143     |\n",
            "|    total_timesteps    | 2344000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.364   |\n",
            "|    explained_variance | 0.556    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 58599    |\n",
            "|    policy_loss        | 0.321    |\n",
            "|    value_loss         | 729      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 470      |\n",
            "|    ep_rew_mean        | 9.28     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 58700    |\n",
            "|    time_elapsed       | 2145     |\n",
            "|    total_timesteps    | 2348000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.356   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 58699    |\n",
            "|    policy_loss        | -0.189   |\n",
            "|    value_loss         | 0.917    |\n",
            "------------------------------------\n",
            "Num timesteps: 2352000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 17.02\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 476      |\n",
            "|    ep_rew_mean        | 17       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 58800    |\n",
            "|    time_elapsed       | 2148     |\n",
            "|    total_timesteps    | 2352000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.335   |\n",
            "|    explained_variance | 0.856    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 58799    |\n",
            "|    policy_loss        | -0.11    |\n",
            "|    value_loss         | 64.2     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 469      |\n",
            "|    ep_rew_mean        | 11.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1094     |\n",
            "|    iterations         | 58900    |\n",
            "|    time_elapsed       | 2151     |\n",
            "|    total_timesteps    | 2356000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.433   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 58899    |\n",
            "|    policy_loss        | -0.152   |\n",
            "|    value_loss         | 4.1      |\n",
            "------------------------------------\n",
            "Num timesteps: 2360000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 13.15\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 461      |\n",
            "|    ep_rew_mean        | 13.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 59000    |\n",
            "|    time_elapsed       | 2154     |\n",
            "|    total_timesteps    | 2360000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.351   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 58999    |\n",
            "|    policy_loss        | -0.461   |\n",
            "|    value_loss         | 7.83     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 455      |\n",
            "|    ep_rew_mean        | 9.73     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1095     |\n",
            "|    iterations         | 59100    |\n",
            "|    time_elapsed       | 2157     |\n",
            "|    total_timesteps    | 2364000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.357   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 59099    |\n",
            "|    policy_loss        | -0.358   |\n",
            "|    value_loss         | 3.46     |\n",
            "------------------------------------\n",
            "Num timesteps: 2368000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 4.87\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 446      |\n",
            "|    ep_rew_mean        | 4.87     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 59200    |\n",
            "|    time_elapsed       | 2160     |\n",
            "|    total_timesteps    | 2368000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.508   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 59199    |\n",
            "|    policy_loss        | -0.0994  |\n",
            "|    value_loss         | 1.63     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 413      |\n",
            "|    ep_rew_mean        | -6.82    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 59300    |\n",
            "|    time_elapsed       | 2162     |\n",
            "|    total_timesteps    | 2372000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.494   |\n",
            "|    explained_variance | 0.971    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 59299    |\n",
            "|    policy_loss        | -0.148   |\n",
            "|    value_loss         | 3.04     |\n",
            "------------------------------------\n",
            "Num timesteps: 2376000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -4.96\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 408      |\n",
            "|    ep_rew_mean        | -4.96    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 59400    |\n",
            "|    time_elapsed       | 2166     |\n",
            "|    total_timesteps    | 2376000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.296   |\n",
            "|    explained_variance | 0.977    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 59399    |\n",
            "|    policy_loss        | -0.204   |\n",
            "|    value_loss         | 8.64     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 402      |\n",
            "|    ep_rew_mean        | 0.114    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1096     |\n",
            "|    iterations         | 59500    |\n",
            "|    time_elapsed       | 2169     |\n",
            "|    total_timesteps    | 2380000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.248   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 59499    |\n",
            "|    policy_loss        | 0.0631   |\n",
            "|    value_loss         | 3.87     |\n",
            "------------------------------------\n",
            "Num timesteps: 2384000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 9.31\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 385      |\n",
            "|    ep_rew_mean        | 9.31     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1097     |\n",
            "|    iterations         | 59600    |\n",
            "|    time_elapsed       | 2171     |\n",
            "|    total_timesteps    | 2384000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.568   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 59599    |\n",
            "|    policy_loss        | 0.0536   |\n",
            "|    value_loss         | 2.92     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 377      |\n",
            "|    ep_rew_mean        | 13.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 59700    |\n",
            "|    time_elapsed       | 2174     |\n",
            "|    total_timesteps    | 2388000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.399   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 59699    |\n",
            "|    policy_loss        | 0.0437   |\n",
            "|    value_loss         | 2.9      |\n",
            "------------------------------------\n",
            "Num timesteps: 2392000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 24.52\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 360      |\n",
            "|    ep_rew_mean        | 24.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1098     |\n",
            "|    iterations         | 59800    |\n",
            "|    time_elapsed       | 2177     |\n",
            "|    total_timesteps    | 2392000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.511   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 59799    |\n",
            "|    policy_loss        | -0.233   |\n",
            "|    value_loss         | 1.23     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 352      |\n",
            "|    ep_rew_mean        | 23.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 59900    |\n",
            "|    time_elapsed       | 2180     |\n",
            "|    total_timesteps    | 2396000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.457   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 59899    |\n",
            "|    policy_loss        | -0.708   |\n",
            "|    value_loss         | 5.61     |\n",
            "------------------------------------\n",
            "Num timesteps: 2400000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 23.48\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 343      |\n",
            "|    ep_rew_mean        | 23.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 60000    |\n",
            "|    time_elapsed       | 2182     |\n",
            "|    total_timesteps    | 2400000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.406   |\n",
            "|    explained_variance | 0.955    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 59999    |\n",
            "|    policy_loss        | -0.0629  |\n",
            "|    value_loss         | 31.3     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 346      |\n",
            "|    ep_rew_mean        | 22       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 60100    |\n",
            "|    time_elapsed       | 2186     |\n",
            "|    total_timesteps    | 2404000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.499   |\n",
            "|    explained_variance | 0.969    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 60099    |\n",
            "|    policy_loss        | -0.0107  |\n",
            "|    value_loss         | 6.21     |\n",
            "------------------------------------\n",
            "Num timesteps: 2408000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 14.61\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 356      |\n",
            "|    ep_rew_mean        | 14.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 60200    |\n",
            "|    time_elapsed       | 2189     |\n",
            "|    total_timesteps    | 2408000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.437   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 60199    |\n",
            "|    policy_loss        | -0.122   |\n",
            "|    value_loss         | 3.09     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 356      |\n",
            "|    ep_rew_mean        | 8        |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 60300    |\n",
            "|    time_elapsed       | 2192     |\n",
            "|    total_timesteps    | 2412000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.53    |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 60299    |\n",
            "|    policy_loss        | 0.0657   |\n",
            "|    value_loss         | 4.23     |\n",
            "------------------------------------\n",
            "Num timesteps: 2416000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 7.40\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 352      |\n",
            "|    ep_rew_mean        | 7.4      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 60400    |\n",
            "|    time_elapsed       | 2196     |\n",
            "|    total_timesteps    | 2416000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.428   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 60399    |\n",
            "|    policy_loss        | -0.0713  |\n",
            "|    value_loss         | 2.36     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 395      |\n",
            "|    ep_rew_mean        | 10.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 60500    |\n",
            "|    time_elapsed       | 2200     |\n",
            "|    total_timesteps    | 2420000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.496   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 60499    |\n",
            "|    policy_loss        | -0.584   |\n",
            "|    value_loss         | 2.57     |\n",
            "------------------------------------\n",
            "Num timesteps: 2424000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 3.40\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 392      |\n",
            "|    ep_rew_mean        | 3.4      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 60600    |\n",
            "|    time_elapsed       | 2203     |\n",
            "|    total_timesteps    | 2424000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.326   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 60599    |\n",
            "|    policy_loss        | 0.513    |\n",
            "|    value_loss         | 5.47     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 389      |\n",
            "|    ep_rew_mean        | -4.32    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 60700    |\n",
            "|    time_elapsed       | 2205     |\n",
            "|    total_timesteps    | 2428000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.604   |\n",
            "|    explained_variance | 0.98     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 60699    |\n",
            "|    policy_loss        | -0.0227  |\n",
            "|    value_loss         | 7.77     |\n",
            "------------------------------------\n",
            "Num timesteps: 2432000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -2.68\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 398      |\n",
            "|    ep_rew_mean        | -2.68    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 60800    |\n",
            "|    time_elapsed       | 2208     |\n",
            "|    total_timesteps    | 2432000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.456   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 60799    |\n",
            "|    policy_loss        | 0.281    |\n",
            "|    value_loss         | 4.21     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 402      |\n",
            "|    ep_rew_mean        | -9.59    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 60900    |\n",
            "|    time_elapsed       | 2211     |\n",
            "|    total_timesteps    | 2436000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.529   |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 60899    |\n",
            "|    policy_loss        | 0.35     |\n",
            "|    value_loss         | 6.38     |\n",
            "------------------------------------\n",
            "Num timesteps: 2440000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -5.64\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 410      |\n",
            "|    ep_rew_mean        | -5.64    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 61000    |\n",
            "|    time_elapsed       | 2214     |\n",
            "|    total_timesteps    | 2440000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.392   |\n",
            "|    explained_variance | 0.843    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 60999    |\n",
            "|    policy_loss        | -2.82    |\n",
            "|    value_loss         | 70.7     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 420      |\n",
            "|    ep_rew_mean        | -11.1    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 61100    |\n",
            "|    time_elapsed       | 2218     |\n",
            "|    total_timesteps    | 2444000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.377   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 61099    |\n",
            "|    policy_loss        | -0.214   |\n",
            "|    value_loss         | 3.93     |\n",
            "------------------------------------\n",
            "Num timesteps: 2448000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 1.28\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 425      |\n",
            "|    ep_rew_mean        | 1.28     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 61200    |\n",
            "|    time_elapsed       | 2221     |\n",
            "|    total_timesteps    | 2448000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.363   |\n",
            "|    explained_variance | 0.586    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 61199    |\n",
            "|    policy_loss        | -0.184   |\n",
            "|    value_loss         | 737      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 417      |\n",
            "|    ep_rew_mean        | 13.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 61300    |\n",
            "|    time_elapsed       | 2224     |\n",
            "|    total_timesteps    | 2452000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.353   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 61299    |\n",
            "|    policy_loss        | 0.446    |\n",
            "|    value_loss         | 3.33     |\n",
            "------------------------------------\n",
            "Num timesteps: 2456000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 24.27\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 398      |\n",
            "|    ep_rew_mean        | 24.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 61400    |\n",
            "|    time_elapsed       | 2227     |\n",
            "|    total_timesteps    | 2456000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.359   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 61399    |\n",
            "|    policy_loss        | 0.3      |\n",
            "|    value_loss         | 6.52     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 377      |\n",
            "|    ep_rew_mean        | 26.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1103     |\n",
            "|    iterations         | 61500    |\n",
            "|    time_elapsed       | 2229     |\n",
            "|    total_timesteps    | 2460000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.413   |\n",
            "|    explained_variance | 0.581    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 61499    |\n",
            "|    policy_loss        | 0.295    |\n",
            "|    value_loss         | 947      |\n",
            "------------------------------------\n",
            "Num timesteps: 2464000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 35.34\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 374      |\n",
            "|    ep_rew_mean        | 35.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1104     |\n",
            "|    iterations         | 61600    |\n",
            "|    time_elapsed       | 2231     |\n",
            "|    total_timesteps    | 2464000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.509   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 61599    |\n",
            "|    policy_loss        | -0.581   |\n",
            "|    value_loss         | 5.5      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 347      |\n",
            "|    ep_rew_mean        | 29.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1104     |\n",
            "|    iterations         | 61700    |\n",
            "|    time_elapsed       | 2233     |\n",
            "|    total_timesteps    | 2468000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.508   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 61699    |\n",
            "|    policy_loss        | -0.0642  |\n",
            "|    value_loss         | 2.08     |\n",
            "------------------------------------\n",
            "Num timesteps: 2472000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 35.74\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 329      |\n",
            "|    ep_rew_mean        | 35.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1105     |\n",
            "|    iterations         | 61800    |\n",
            "|    time_elapsed       | 2236     |\n",
            "|    total_timesteps    | 2472000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.536   |\n",
            "|    explained_variance | 0.779    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 61799    |\n",
            "|    policy_loss        | -1.53    |\n",
            "|    value_loss         | 152      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 323      |\n",
            "|    ep_rew_mean        | 42.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1106     |\n",
            "|    iterations         | 61900    |\n",
            "|    time_elapsed       | 2238     |\n",
            "|    total_timesteps    | 2476000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.333   |\n",
            "|    explained_variance | 0.955    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 61899    |\n",
            "|    policy_loss        | 0.378    |\n",
            "|    value_loss         | 6.1      |\n",
            "------------------------------------\n",
            "Num timesteps: 2480000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 37.01\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 305      |\n",
            "|    ep_rew_mean        | 37       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1106     |\n",
            "|    iterations         | 62000    |\n",
            "|    time_elapsed       | 2240     |\n",
            "|    total_timesteps    | 2480000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.483   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 61999    |\n",
            "|    policy_loss        | -0.3     |\n",
            "|    value_loss         | 3.87     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 286      |\n",
            "|    ep_rew_mean        | 25.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1107     |\n",
            "|    iterations         | 62100    |\n",
            "|    time_elapsed       | 2242     |\n",
            "|    total_timesteps    | 2484000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.296   |\n",
            "|    explained_variance | 0.948    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 62099    |\n",
            "|    policy_loss        | -0.394   |\n",
            "|    value_loss         | 102      |\n",
            "------------------------------------\n",
            "Num timesteps: 2488000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 28.77\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 298      |\n",
            "|    ep_rew_mean        | 28.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1107     |\n",
            "|    iterations         | 62200    |\n",
            "|    time_elapsed       | 2245     |\n",
            "|    total_timesteps    | 2488000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.478   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 62199    |\n",
            "|    policy_loss        | -0.677   |\n",
            "|    value_loss         | 1.65     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 310      |\n",
            "|    ep_rew_mean        | 28       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1108     |\n",
            "|    iterations         | 62300    |\n",
            "|    time_elapsed       | 2248     |\n",
            "|    total_timesteps    | 2492000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.458   |\n",
            "|    explained_variance | 0.817    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 62299    |\n",
            "|    policy_loss        | 0.329    |\n",
            "|    value_loss         | 38.4     |\n",
            "------------------------------------\n",
            "Num timesteps: 2496000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 22.28\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 317      |\n",
            "|    ep_rew_mean        | 22.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1108     |\n",
            "|    iterations         | 62400    |\n",
            "|    time_elapsed       | 2250     |\n",
            "|    total_timesteps    | 2496000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.444   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 62399    |\n",
            "|    policy_loss        | 0.0785   |\n",
            "|    value_loss         | 1.78     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 332      |\n",
            "|    ep_rew_mean        | 27.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 62500    |\n",
            "|    time_elapsed       | 2253     |\n",
            "|    total_timesteps    | 2500000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.514   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 62499    |\n",
            "|    policy_loss        | -0.303   |\n",
            "|    value_loss         | 5.1      |\n",
            "------------------------------------\n",
            "Num timesteps: 2504000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 26.50\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 340      |\n",
            "|    ep_rew_mean        | 26.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 62600    |\n",
            "|    time_elapsed       | 2256     |\n",
            "|    total_timesteps    | 2504000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.386   |\n",
            "|    explained_variance | 0.894    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 62599    |\n",
            "|    policy_loss        | -0.0245  |\n",
            "|    value_loss         | 68.7     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 349      |\n",
            "|    ep_rew_mean        | 24.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 62700    |\n",
            "|    time_elapsed       | 2259     |\n",
            "|    total_timesteps    | 2508000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.452   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 62699    |\n",
            "|    policy_loss        | 0.102    |\n",
            "|    value_loss         | 4.97     |\n",
            "------------------------------------\n",
            "Num timesteps: 2512000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 17.86\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 343      |\n",
            "|    ep_rew_mean        | 17.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1110     |\n",
            "|    iterations         | 62800    |\n",
            "|    time_elapsed       | 2262     |\n",
            "|    total_timesteps    | 2512000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.309   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 62799    |\n",
            "|    policy_loss        | -0.199   |\n",
            "|    value_loss         | 14       |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 356      |\n",
            "|    ep_rew_mean        | 22.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1110     |\n",
            "|    iterations         | 62900    |\n",
            "|    time_elapsed       | 2265     |\n",
            "|    total_timesteps    | 2516000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.505   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 62899    |\n",
            "|    policy_loss        | -0.115   |\n",
            "|    value_loss         | 1.49     |\n",
            "------------------------------------\n",
            "Num timesteps: 2520000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 19.91\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 371      |\n",
            "|    ep_rew_mean        | 19.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1110     |\n",
            "|    iterations         | 63000    |\n",
            "|    time_elapsed       | 2269     |\n",
            "|    total_timesteps    | 2520000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.333   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 62999    |\n",
            "|    policy_loss        | -0.317   |\n",
            "|    value_loss         | 3.78     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 388      |\n",
            "|    ep_rew_mean        | 17.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1110     |\n",
            "|    iterations         | 63100    |\n",
            "|    time_elapsed       | 2272     |\n",
            "|    total_timesteps    | 2524000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.469   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 63099    |\n",
            "|    policy_loss        | -0.191   |\n",
            "|    value_loss         | 3.52     |\n",
            "------------------------------------\n",
            "Num timesteps: 2528000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 5.24\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 381      |\n",
            "|    ep_rew_mean        | 5.24     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1110     |\n",
            "|    iterations         | 63200    |\n",
            "|    time_elapsed       | 2276     |\n",
            "|    total_timesteps    | 2528000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.388   |\n",
            "|    explained_variance | 0.944    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 63199    |\n",
            "|    policy_loss        | 1.22     |\n",
            "|    value_loss         | 33       |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 386      |\n",
            "|    ep_rew_mean        | -0.256   |\n",
            "| time/                 |          |\n",
            "|    fps                | 1111     |\n",
            "|    iterations         | 63300    |\n",
            "|    time_elapsed       | 2278     |\n",
            "|    total_timesteps    | 2532000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.439   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 63299    |\n",
            "|    policy_loss        | 0.328    |\n",
            "|    value_loss         | 3.71     |\n",
            "------------------------------------\n",
            "Num timesteps: 2536000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 1.93\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 386      |\n",
            "|    ep_rew_mean        | 1.93     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1110     |\n",
            "|    iterations         | 63400    |\n",
            "|    time_elapsed       | 2282     |\n",
            "|    total_timesteps    | 2536000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.248   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 63399    |\n",
            "|    policy_loss        | 0.0292   |\n",
            "|    value_loss         | 4.01     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 419      |\n",
            "|    ep_rew_mean        | 0.667    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1110     |\n",
            "|    iterations         | 63500    |\n",
            "|    time_elapsed       | 2286     |\n",
            "|    total_timesteps    | 2540000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.548   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 63499    |\n",
            "|    policy_loss        | -0.609   |\n",
            "|    value_loss         | 2.61     |\n",
            "------------------------------------\n",
            "Num timesteps: 2544000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 1.50\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 428      |\n",
            "|    ep_rew_mean        | 1.5      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1110     |\n",
            "|    iterations         | 63600    |\n",
            "|    time_elapsed       | 2289     |\n",
            "|    total_timesteps    | 2544000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.493   |\n",
            "|    explained_variance | 0.978    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 63599    |\n",
            "|    policy_loss        | 0.364    |\n",
            "|    value_loss         | 5.83     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 444      |\n",
            "|    ep_rew_mean        | -2.55    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1110     |\n",
            "|    iterations         | 63700    |\n",
            "|    time_elapsed       | 2293     |\n",
            "|    total_timesteps    | 2548000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.496   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 63699    |\n",
            "|    policy_loss        | -0.192   |\n",
            "|    value_loss         | 1.64     |\n",
            "------------------------------------\n",
            "Num timesteps: 2552000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -0.25\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 460      |\n",
            "|    ep_rew_mean        | -0.25    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1110     |\n",
            "|    iterations         | 63800    |\n",
            "|    time_elapsed       | 2298     |\n",
            "|    total_timesteps    | 2552000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.479   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 63799    |\n",
            "|    policy_loss        | 0.118    |\n",
            "|    value_loss         | 3.39     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 446      |\n",
            "|    ep_rew_mean        | -6.84    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1110     |\n",
            "|    iterations         | 63900    |\n",
            "|    time_elapsed       | 2300     |\n",
            "|    total_timesteps    | 2556000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.536   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 63899    |\n",
            "|    policy_loss        | -0.29    |\n",
            "|    value_loss         | 2.25     |\n",
            "------------------------------------\n",
            "Num timesteps: 2560000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -10.58\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 443      |\n",
            "|    ep_rew_mean        | -10.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1110     |\n",
            "|    iterations         | 64000    |\n",
            "|    time_elapsed       | 2304     |\n",
            "|    total_timesteps    | 2560000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.514   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 63999    |\n",
            "|    policy_loss        | 0.205    |\n",
            "|    value_loss         | 3.71     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 482      |\n",
            "|    ep_rew_mean        | -10.6    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1110     |\n",
            "|    iterations         | 64100    |\n",
            "|    time_elapsed       | 2309     |\n",
            "|    total_timesteps    | 2564000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.477   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 64099    |\n",
            "|    policy_loss        | 0.216    |\n",
            "|    value_loss         | 1.86     |\n",
            "------------------------------------\n",
            "Num timesteps: 2568000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -11.75\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 459      |\n",
            "|    ep_rew_mean        | -11.8    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1110     |\n",
            "|    iterations         | 64200    |\n",
            "|    time_elapsed       | 2313     |\n",
            "|    total_timesteps    | 2568000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.57    |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 64199    |\n",
            "|    policy_loss        | -0.377   |\n",
            "|    value_loss         | 6.58     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 465      |\n",
            "|    ep_rew_mean        | -10.4    |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 64300    |\n",
            "|    time_elapsed       | 2318     |\n",
            "|    total_timesteps    | 2572000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.346   |\n",
            "|    explained_variance | 0.919    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 64299    |\n",
            "|    policy_loss        | -0.0292  |\n",
            "|    value_loss         | 10.9     |\n",
            "------------------------------------\n",
            "Num timesteps: 2576000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: -0.29\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 479      |\n",
            "|    ep_rew_mean        | -0.295   |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 64400    |\n",
            "|    time_elapsed       | 2322     |\n",
            "|    total_timesteps    | 2576000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.334   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 64399    |\n",
            "|    policy_loss        | 0.0687   |\n",
            "|    value_loss         | 11.2     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 469      |\n",
            "|    ep_rew_mean        | 15.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 64500    |\n",
            "|    time_elapsed       | 2324     |\n",
            "|    total_timesteps    | 2580000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.362   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 64499    |\n",
            "|    policy_loss        | -0.385   |\n",
            "|    value_loss         | 3.12     |\n",
            "------------------------------------\n",
            "Num timesteps: 2584000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 18.77\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 445      |\n",
            "|    ep_rew_mean        | 18.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1110     |\n",
            "|    iterations         | 64600    |\n",
            "|    time_elapsed       | 2327     |\n",
            "|    total_timesteps    | 2584000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.473   |\n",
            "|    explained_variance | 0.943    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 64599    |\n",
            "|    policy_loss        | -4.24    |\n",
            "|    value_loss         | 76.6     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 417      |\n",
            "|    ep_rew_mean        | 29.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1110     |\n",
            "|    iterations         | 64700    |\n",
            "|    time_elapsed       | 2330     |\n",
            "|    total_timesteps    | 2588000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.389   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 64699    |\n",
            "|    policy_loss        | 0.0691   |\n",
            "|    value_loss         | 2.57     |\n",
            "------------------------------------\n",
            "Num timesteps: 2592000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 34.79\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 391      |\n",
            "|    ep_rew_mean        | 34.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1110     |\n",
            "|    iterations         | 64800    |\n",
            "|    time_elapsed       | 2333     |\n",
            "|    total_timesteps    | 2592000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.439   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 64799    |\n",
            "|    policy_loss        | 0.256    |\n",
            "|    value_loss         | 3.68     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 405      |\n",
            "|    ep_rew_mean        | 45.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1110     |\n",
            "|    iterations         | 64900    |\n",
            "|    time_elapsed       | 2336     |\n",
            "|    total_timesteps    | 2596000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.587   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 64899    |\n",
            "|    policy_loss        | -0.304   |\n",
            "|    value_loss         | 3.41     |\n",
            "------------------------------------\n",
            "Num timesteps: 2600000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 49.52\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 381      |\n",
            "|    ep_rew_mean        | 49.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1111     |\n",
            "|    iterations         | 65000    |\n",
            "|    time_elapsed       | 2339     |\n",
            "|    total_timesteps    | 2600000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.385   |\n",
            "|    explained_variance | 0.873    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 64999    |\n",
            "|    policy_loss        | -4.07    |\n",
            "|    value_loss         | 167      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 375      |\n",
            "|    ep_rew_mean        | 60.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1110     |\n",
            "|    iterations         | 65100    |\n",
            "|    time_elapsed       | 2344     |\n",
            "|    total_timesteps    | 2604000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.457   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 65099    |\n",
            "|    policy_loss        | -0.159   |\n",
            "|    value_loss         | 0.838    |\n",
            "------------------------------------\n",
            "Num timesteps: 2608000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 80.77\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 385      |\n",
            "|    ep_rew_mean        | 80.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1111     |\n",
            "|    iterations         | 65200    |\n",
            "|    time_elapsed       | 2347     |\n",
            "|    total_timesteps    | 2608000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.51    |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 65199    |\n",
            "|    policy_loss        | 0.131    |\n",
            "|    value_loss         | 1.13     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 368      |\n",
            "|    ep_rew_mean        | 85.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1111     |\n",
            "|    iterations         | 65300    |\n",
            "|    time_elapsed       | 2350     |\n",
            "|    total_timesteps    | 2612000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.406   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 65299    |\n",
            "|    policy_loss        | -0.151   |\n",
            "|    value_loss         | 1.51     |\n",
            "------------------------------------\n",
            "Num timesteps: 2616000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 86.70\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 368      |\n",
            "|    ep_rew_mean        | 86.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1111     |\n",
            "|    iterations         | 65400    |\n",
            "|    time_elapsed       | 2353     |\n",
            "|    total_timesteps    | 2616000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.455   |\n",
            "|    explained_variance | 0.737    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 65399    |\n",
            "|    policy_loss        | -0.378   |\n",
            "|    value_loss         | 172      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 359      |\n",
            "|    ep_rew_mean        | 78.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1111     |\n",
            "|    iterations         | 65500    |\n",
            "|    time_elapsed       | 2356     |\n",
            "|    total_timesteps    | 2620000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.33    |\n",
            "|    explained_variance | 0.776    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 65499    |\n",
            "|    policy_loss        | -0.191   |\n",
            "|    value_loss         | 57.6     |\n",
            "------------------------------------\n",
            "Num timesteps: 2624000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 83.43\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 352      |\n",
            "|    ep_rew_mean        | 83.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1111     |\n",
            "|    iterations         | 65600    |\n",
            "|    time_elapsed       | 2359     |\n",
            "|    total_timesteps    | 2624000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.378   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 65599    |\n",
            "|    policy_loss        | -0.379   |\n",
            "|    value_loss         | 2.25     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 363      |\n",
            "|    ep_rew_mean        | 80.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1112     |\n",
            "|    iterations         | 65700    |\n",
            "|    time_elapsed       | 2362     |\n",
            "|    total_timesteps    | 2628000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.365   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 65699    |\n",
            "|    policy_loss        | -0.29    |\n",
            "|    value_loss         | 2.67     |\n",
            "------------------------------------\n",
            "Num timesteps: 2632000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 84.52\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 349      |\n",
            "|    ep_rew_mean        | 84.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1112     |\n",
            "|    iterations         | 65800    |\n",
            "|    time_elapsed       | 2365     |\n",
            "|    total_timesteps    | 2632000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.364   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 65799    |\n",
            "|    policy_loss        | -0.255   |\n",
            "|    value_loss         | 4.59     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 353      |\n",
            "|    ep_rew_mean        | 83.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1112     |\n",
            "|    iterations         | 65900    |\n",
            "|    time_elapsed       | 2369     |\n",
            "|    total_timesteps    | 2636000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.259   |\n",
            "|    explained_variance | -0.0109  |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 65899    |\n",
            "|    policy_loss        | -0.0602  |\n",
            "|    value_loss         | 1.43e+03 |\n",
            "------------------------------------\n",
            "Num timesteps: 2640000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 72.20\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 342      |\n",
            "|    ep_rew_mean        | 72.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1112     |\n",
            "|    iterations         | 66000    |\n",
            "|    time_elapsed       | 2372     |\n",
            "|    total_timesteps    | 2640000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.479   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 65999    |\n",
            "|    policy_loss        | 1.14     |\n",
            "|    value_loss         | 10.3     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 334      |\n",
            "|    ep_rew_mean        | 60       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1112     |\n",
            "|    iterations         | 66100    |\n",
            "|    time_elapsed       | 2376     |\n",
            "|    total_timesteps    | 2644000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.28    |\n",
            "|    explained_variance | 0.971    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 66099    |\n",
            "|    policy_loss        | -0.00958 |\n",
            "|    value_loss         | 5.48     |\n",
            "------------------------------------\n",
            "Num timesteps: 2648000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 58.58\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 352      |\n",
            "|    ep_rew_mean        | 58.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1112     |\n",
            "|    iterations         | 66200    |\n",
            "|    time_elapsed       | 2381     |\n",
            "|    total_timesteps    | 2648000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.324   |\n",
            "|    explained_variance | 0.325    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 66199    |\n",
            "|    policy_loss        | -0.214   |\n",
            "|    value_loss         | 1.01e+03 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 362      |\n",
            "|    ep_rew_mean        | 53.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1111     |\n",
            "|    iterations         | 66300    |\n",
            "|    time_elapsed       | 2385     |\n",
            "|    total_timesteps    | 2652000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.401   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 66299    |\n",
            "|    policy_loss        | 0.0909   |\n",
            "|    value_loss         | 1.39     |\n",
            "------------------------------------\n",
            "Num timesteps: 2656000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 50.07\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 377      |\n",
            "|    ep_rew_mean        | 50.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1111     |\n",
            "|    iterations         | 66400    |\n",
            "|    time_elapsed       | 2389     |\n",
            "|    total_timesteps    | 2656000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.479   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 66399    |\n",
            "|    policy_loss        | 0.00518  |\n",
            "|    value_loss         | 3.04     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 391      |\n",
            "|    ep_rew_mean        | 49.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1110     |\n",
            "|    iterations         | 66500    |\n",
            "|    time_elapsed       | 2394     |\n",
            "|    total_timesteps    | 2660000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.477   |\n",
            "|    explained_variance | 0.934    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 66499    |\n",
            "|    policy_loss        | -0.107   |\n",
            "|    value_loss         | 26.9     |\n",
            "------------------------------------\n",
            "Num timesteps: 2664000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 38.49\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 417      |\n",
            "|    ep_rew_mean        | 38.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1110     |\n",
            "|    iterations         | 66600    |\n",
            "|    time_elapsed       | 2398     |\n",
            "|    total_timesteps    | 2664000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.445   |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 66599    |\n",
            "|    policy_loss        | -0.116   |\n",
            "|    value_loss         | 5.82     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 428      |\n",
            "|    ep_rew_mean        | 33.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1110     |\n",
            "|    iterations         | 66700    |\n",
            "|    time_elapsed       | 2402     |\n",
            "|    total_timesteps    | 2668000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.396   |\n",
            "|    explained_variance | 0.97     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 66699    |\n",
            "|    policy_loss        | -0.0244  |\n",
            "|    value_loss         | 9.31     |\n",
            "------------------------------------\n",
            "Num timesteps: 2672000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 24.77\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 430      |\n",
            "|    ep_rew_mean        | 24.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1110     |\n",
            "|    iterations         | 66800    |\n",
            "|    time_elapsed       | 2405     |\n",
            "|    total_timesteps    | 2672000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.502   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 66799    |\n",
            "|    policy_loss        | 0.244    |\n",
            "|    value_loss         | 3.21     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 439      |\n",
            "|    ep_rew_mean        | 21.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1110     |\n",
            "|    iterations         | 66900    |\n",
            "|    time_elapsed       | 2409     |\n",
            "|    total_timesteps    | 2676000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.48    |\n",
            "|    explained_variance | 0.975    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 66899    |\n",
            "|    policy_loss        | 0.113    |\n",
            "|    value_loss         | 4        |\n",
            "------------------------------------\n",
            "Num timesteps: 2680000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 10.15\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 467      |\n",
            "|    ep_rew_mean        | 10.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 67000    |\n",
            "|    time_elapsed       | 2414     |\n",
            "|    total_timesteps    | 2680000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.469   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 66999    |\n",
            "|    policy_loss        | -0.431   |\n",
            "|    value_loss         | 1.86     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 483      |\n",
            "|    ep_rew_mean        | 10.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 67100    |\n",
            "|    time_elapsed       | 2418     |\n",
            "|    total_timesteps    | 2684000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.432   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 67099    |\n",
            "|    policy_loss        | 0.0134   |\n",
            "|    value_loss         | 2.64     |\n",
            "------------------------------------\n",
            "Num timesteps: 2688000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 8.68\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 486      |\n",
            "|    ep_rew_mean        | 8.68     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 67200    |\n",
            "|    time_elapsed       | 2422     |\n",
            "|    total_timesteps    | 2688000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.5     |\n",
            "|    explained_variance | 0.98     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 67199    |\n",
            "|    policy_loss        | 0.0784   |\n",
            "|    value_loss         | 5.02     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 473      |\n",
            "|    ep_rew_mean        | 13.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 67300    |\n",
            "|    time_elapsed       | 2425     |\n",
            "|    total_timesteps    | 2692000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.336   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 67299    |\n",
            "|    policy_loss        | 0.154    |\n",
            "|    value_loss         | 1.04     |\n",
            "------------------------------------\n",
            "Num timesteps: 2696000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 19.79\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 460      |\n",
            "|    ep_rew_mean        | 19.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 67400    |\n",
            "|    time_elapsed       | 2429     |\n",
            "|    total_timesteps    | 2696000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.407   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 67399    |\n",
            "|    policy_loss        | 0.131    |\n",
            "|    value_loss         | 2.24     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 444      |\n",
            "|    ep_rew_mean        | 13.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 67500    |\n",
            "|    time_elapsed       | 2433     |\n",
            "|    total_timesteps    | 2700000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.464   |\n",
            "|    explained_variance | 0.963    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 67499    |\n",
            "|    policy_loss        | 0.142    |\n",
            "|    value_loss         | 6.35     |\n",
            "------------------------------------\n",
            "Num timesteps: 2704000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 10.66\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 438      |\n",
            "|    ep_rew_mean        | 10.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 67600    |\n",
            "|    time_elapsed       | 2437     |\n",
            "|    total_timesteps    | 2704000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.4     |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 67599    |\n",
            "|    policy_loss        | 0.0151   |\n",
            "|    value_loss         | 2.4      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 433      |\n",
            "|    ep_rew_mean        | 15       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 67700    |\n",
            "|    time_elapsed       | 2440     |\n",
            "|    total_timesteps    | 2708000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.327   |\n",
            "|    explained_variance | 0.582    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 67699    |\n",
            "|    policy_loss        | -0.1     |\n",
            "|    value_loss         | 245      |\n",
            "------------------------------------\n",
            "Num timesteps: 2712000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 7.70\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 424      |\n",
            "|    ep_rew_mean        | 7.7      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 67800    |\n",
            "|    time_elapsed       | 2444     |\n",
            "|    total_timesteps    | 2712000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.252   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 67799    |\n",
            "|    policy_loss        | 0.0806   |\n",
            "|    value_loss         | 3.21     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 433      |\n",
            "|    ep_rew_mean        | 2.8      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 67900    |\n",
            "|    time_elapsed       | 2448     |\n",
            "|    total_timesteps    | 2716000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.385   |\n",
            "|    explained_variance | 0.948    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 67899    |\n",
            "|    policy_loss        | 0.141    |\n",
            "|    value_loss         | 15.7     |\n",
            "------------------------------------\n",
            "Num timesteps: 2720000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 2.52\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 443      |\n",
            "|    ep_rew_mean        | 2.52     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1108     |\n",
            "|    iterations         | 68000    |\n",
            "|    time_elapsed       | 2453     |\n",
            "|    total_timesteps    | 2720000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.465   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 67999    |\n",
            "|    policy_loss        | -0.16    |\n",
            "|    value_loss         | 2.51     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 449      |\n",
            "|    ep_rew_mean        | 5.02     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1107     |\n",
            "|    iterations         | 68100    |\n",
            "|    time_elapsed       | 2459     |\n",
            "|    total_timesteps    | 2724000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.561   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 68099    |\n",
            "|    policy_loss        | 0.0493   |\n",
            "|    value_loss         | 3.4      |\n",
            "------------------------------------\n",
            "Num timesteps: 2728000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 4.93\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 451      |\n",
            "|    ep_rew_mean        | 4.93     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1106     |\n",
            "|    iterations         | 68200    |\n",
            "|    time_elapsed       | 2464     |\n",
            "|    total_timesteps    | 2728000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.387   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 68199    |\n",
            "|    policy_loss        | -0.0297  |\n",
            "|    value_loss         | 4.83     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 470      |\n",
            "|    ep_rew_mean        | 9.03     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1106     |\n",
            "|    iterations         | 68300    |\n",
            "|    time_elapsed       | 2468     |\n",
            "|    total_timesteps    | 2732000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.379   |\n",
            "|    explained_variance | 0.959    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 68299    |\n",
            "|    policy_loss        | 0.249    |\n",
            "|    value_loss         | 5.72     |\n",
            "------------------------------------\n",
            "Num timesteps: 2736000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 13.01\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 473      |\n",
            "|    ep_rew_mean        | 13       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1106     |\n",
            "|    iterations         | 68400    |\n",
            "|    time_elapsed       | 2472     |\n",
            "|    total_timesteps    | 2736000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.484   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 68399    |\n",
            "|    policy_loss        | -0.0664  |\n",
            "|    value_loss         | 2.7      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 493      |\n",
            "|    ep_rew_mean        | 8.87     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1106     |\n",
            "|    iterations         | 68500    |\n",
            "|    time_elapsed       | 2476     |\n",
            "|    total_timesteps    | 2740000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.53    |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 68499    |\n",
            "|    policy_loss        | 0.343    |\n",
            "|    value_loss         | 1.18     |\n",
            "------------------------------------\n",
            "Num timesteps: 2744000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 11.12\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 502      |\n",
            "|    ep_rew_mean        | 11.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1106     |\n",
            "|    iterations         | 68600    |\n",
            "|    time_elapsed       | 2480     |\n",
            "|    total_timesteps    | 2744000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.371   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 68599    |\n",
            "|    policy_loss        | -0.278   |\n",
            "|    value_loss         | 3.24     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 510      |\n",
            "|    ep_rew_mean        | 10.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1106     |\n",
            "|    iterations         | 68700    |\n",
            "|    time_elapsed       | 2484     |\n",
            "|    total_timesteps    | 2748000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.521   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 68699    |\n",
            "|    policy_loss        | 0.336    |\n",
            "|    value_loss         | 5.57     |\n",
            "------------------------------------\n",
            "Num timesteps: 2752000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 14.33\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 499      |\n",
            "|    ep_rew_mean        | 14.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1106     |\n",
            "|    iterations         | 68800    |\n",
            "|    time_elapsed       | 2487     |\n",
            "|    total_timesteps    | 2752000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.379   |\n",
            "|    explained_variance | 0.957    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 68799    |\n",
            "|    policy_loss        | 0.178    |\n",
            "|    value_loss         | 21.1     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 491      |\n",
            "|    ep_rew_mean        | 19.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1106     |\n",
            "|    iterations         | 68900    |\n",
            "|    time_elapsed       | 2490     |\n",
            "|    total_timesteps    | 2756000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.235   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 68899    |\n",
            "|    policy_loss        | -0.353   |\n",
            "|    value_loss         | 1.96     |\n",
            "------------------------------------\n",
            "Num timesteps: 2760000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 38.29\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 490      |\n",
            "|    ep_rew_mean        | 38.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1106     |\n",
            "|    iterations         | 69000    |\n",
            "|    time_elapsed       | 2493     |\n",
            "|    total_timesteps    | 2760000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.323   |\n",
            "|    explained_variance | 0.975    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 68999    |\n",
            "|    policy_loss        | 0.335    |\n",
            "|    value_loss         | 9.93     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 462      |\n",
            "|    ep_rew_mean        | 52.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1107     |\n",
            "|    iterations         | 69100    |\n",
            "|    time_elapsed       | 2496     |\n",
            "|    total_timesteps    | 2764000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.366   |\n",
            "|    explained_variance | 0.979    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 69099    |\n",
            "|    policy_loss        | -1.07    |\n",
            "|    value_loss         | 13.2     |\n",
            "------------------------------------\n",
            "Num timesteps: 2768000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 65.80\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 437      |\n",
            "|    ep_rew_mean        | 65.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1107     |\n",
            "|    iterations         | 69200    |\n",
            "|    time_elapsed       | 2499     |\n",
            "|    total_timesteps    | 2768000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.449   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 69199    |\n",
            "|    policy_loss        | -0.0979  |\n",
            "|    value_loss         | 3.43     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 396      |\n",
            "|    ep_rew_mean        | 58.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1107     |\n",
            "|    iterations         | 69300    |\n",
            "|    time_elapsed       | 2502     |\n",
            "|    total_timesteps    | 2772000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.434   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 69299    |\n",
            "|    policy_loss        | 0.506    |\n",
            "|    value_loss         | 2.05     |\n",
            "------------------------------------\n",
            "Num timesteps: 2776000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 59.05\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 361      |\n",
            "|    ep_rew_mean        | 59       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1108     |\n",
            "|    iterations         | 69400    |\n",
            "|    time_elapsed       | 2505     |\n",
            "|    total_timesteps    | 2776000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.516   |\n",
            "|    explained_variance | 0.88     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 69399    |\n",
            "|    policy_loss        | 1.72     |\n",
            "|    value_loss         | 152      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 332      |\n",
            "|    ep_rew_mean        | 53       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1108     |\n",
            "|    iterations         | 69500    |\n",
            "|    time_elapsed       | 2508     |\n",
            "|    total_timesteps    | 2780000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.4     |\n",
            "|    explained_variance | 0.486    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 69499    |\n",
            "|    policy_loss        | -0.0371  |\n",
            "|    value_loss         | 1.24e+03 |\n",
            "------------------------------------\n",
            "Num timesteps: 2784000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 51.81\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 330      |\n",
            "|    ep_rew_mean        | 51.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1108     |\n",
            "|    iterations         | 69600    |\n",
            "|    time_elapsed       | 2510     |\n",
            "|    total_timesteps    | 2784000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.403   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 69599    |\n",
            "|    policy_loss        | 0.168    |\n",
            "|    value_loss         | 1.8      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 344      |\n",
            "|    ep_rew_mean        | 56.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1108     |\n",
            "|    iterations         | 69700    |\n",
            "|    time_elapsed       | 2514     |\n",
            "|    total_timesteps    | 2788000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.402   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 69699    |\n",
            "|    policy_loss        | -0.493   |\n",
            "|    value_loss         | 9.47     |\n",
            "------------------------------------\n",
            "Num timesteps: 2792000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 55.92\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 346      |\n",
            "|    ep_rew_mean        | 55.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1108     |\n",
            "|    iterations         | 69800    |\n",
            "|    time_elapsed       | 2518     |\n",
            "|    total_timesteps    | 2792000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.466   |\n",
            "|    explained_variance | 0.842    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 69799    |\n",
            "|    policy_loss        | -0.791   |\n",
            "|    value_loss         | 115      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 350      |\n",
            "|    ep_rew_mean        | 51.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1108     |\n",
            "|    iterations         | 69900    |\n",
            "|    time_elapsed       | 2521     |\n",
            "|    total_timesteps    | 2796000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.487   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 69899    |\n",
            "|    policy_loss        | -0.0936  |\n",
            "|    value_loss         | 1.27     |\n",
            "------------------------------------\n",
            "Num timesteps: 2800000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 38.14\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 346      |\n",
            "|    ep_rew_mean        | 38.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 70000    |\n",
            "|    time_elapsed       | 2524     |\n",
            "|    total_timesteps    | 2800000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.532   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 69999    |\n",
            "|    policy_loss        | 0.506    |\n",
            "|    value_loss         | 6.49     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 330      |\n",
            "|    ep_rew_mean        | 27.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 70100    |\n",
            "|    time_elapsed       | 2526     |\n",
            "|    total_timesteps    | 2804000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.492   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 70099    |\n",
            "|    policy_loss        | 0.00486  |\n",
            "|    value_loss         | 0.949    |\n",
            "------------------------------------\n",
            "Num timesteps: 2808000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 24.91\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 342      |\n",
            "|    ep_rew_mean        | 24.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 70200    |\n",
            "|    time_elapsed       | 2530     |\n",
            "|    total_timesteps    | 2808000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.453   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 70199    |\n",
            "|    policy_loss        | -0.453   |\n",
            "|    value_loss         | 7.53     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 353      |\n",
            "|    ep_rew_mean        | 38.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1110     |\n",
            "|    iterations         | 70300    |\n",
            "|    time_elapsed       | 2532     |\n",
            "|    total_timesteps    | 2812000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.379   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 70299    |\n",
            "|    policy_loss        | 0.259    |\n",
            "|    value_loss         | 5.78     |\n",
            "------------------------------------\n",
            "Num timesteps: 2816000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 38.70\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 353      |\n",
            "|    ep_rew_mean        | 38.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1110     |\n",
            "|    iterations         | 70400    |\n",
            "|    time_elapsed       | 2535     |\n",
            "|    total_timesteps    | 2816000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.346   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 70399    |\n",
            "|    policy_loss        | -0.076   |\n",
            "|    value_loss         | 1.89     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 358      |\n",
            "|    ep_rew_mean        | 44.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1111     |\n",
            "|    iterations         | 70500    |\n",
            "|    time_elapsed       | 2538     |\n",
            "|    total_timesteps    | 2820000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.527   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 70499    |\n",
            "|    policy_loss        | 0.636    |\n",
            "|    value_loss         | 7.06     |\n",
            "------------------------------------\n",
            "Num timesteps: 2824000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 39.83\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 347      |\n",
            "|    ep_rew_mean        | 39.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1111     |\n",
            "|    iterations         | 70600    |\n",
            "|    time_elapsed       | 2540     |\n",
            "|    total_timesteps    | 2824000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.394   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 70599    |\n",
            "|    policy_loss        | -1.05    |\n",
            "|    value_loss         | 3.89     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 333      |\n",
            "|    ep_rew_mean        | 41.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1112     |\n",
            "|    iterations         | 70700    |\n",
            "|    time_elapsed       | 2542     |\n",
            "|    total_timesteps    | 2828000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.4     |\n",
            "|    explained_variance | 0.977    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 70699    |\n",
            "|    policy_loss        | -0.23    |\n",
            "|    value_loss         | 3.92     |\n",
            "------------------------------------\n",
            "Num timesteps: 2832000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 47.89\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 334      |\n",
            "|    ep_rew_mean        | 47.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1112     |\n",
            "|    iterations         | 70800    |\n",
            "|    time_elapsed       | 2545     |\n",
            "|    total_timesteps    | 2832000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.406   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 70799    |\n",
            "|    policy_loss        | -0.0855  |\n",
            "|    value_loss         | 3.13     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 346      |\n",
            "|    ep_rew_mean        | 58.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1113     |\n",
            "|    iterations         | 70900    |\n",
            "|    time_elapsed       | 2547     |\n",
            "|    total_timesteps    | 2836000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.618   |\n",
            "|    explained_variance | 0.955    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 70899    |\n",
            "|    policy_loss        | -0.556   |\n",
            "|    value_loss         | 4.35     |\n",
            "------------------------------------\n",
            "Num timesteps: 2840000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 48.28\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 345      |\n",
            "|    ep_rew_mean        | 48.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1113     |\n",
            "|    iterations         | 71000    |\n",
            "|    time_elapsed       | 2551     |\n",
            "|    total_timesteps    | 2840000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.482   |\n",
            "|    explained_variance | 0.951    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 70999    |\n",
            "|    policy_loss        | 0.25     |\n",
            "|    value_loss         | 1.9      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 328      |\n",
            "|    ep_rew_mean        | 38.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1113     |\n",
            "|    iterations         | 71100    |\n",
            "|    time_elapsed       | 2553     |\n",
            "|    total_timesteps    | 2844000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.439   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 71099    |\n",
            "|    policy_loss        | -0.0688  |\n",
            "|    value_loss         | 1.72     |\n",
            "------------------------------------\n",
            "Num timesteps: 2848000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 35.22\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 334      |\n",
            "|    ep_rew_mean        | 35.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1113     |\n",
            "|    iterations         | 71200    |\n",
            "|    time_elapsed       | 2556     |\n",
            "|    total_timesteps    | 2848000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.43    |\n",
            "|    explained_variance | 0.698    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 71199    |\n",
            "|    policy_loss        | 0.744    |\n",
            "|    value_loss         | 80.3     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 345      |\n",
            "|    ep_rew_mean        | 33.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1113     |\n",
            "|    iterations         | 71300    |\n",
            "|    time_elapsed       | 2560     |\n",
            "|    total_timesteps    | 2852000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.447   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 71299    |\n",
            "|    policy_loss        | -0.391   |\n",
            "|    value_loss         | 3.35     |\n",
            "------------------------------------\n",
            "Num timesteps: 2856000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 30.84\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 353      |\n",
            "|    ep_rew_mean        | 30.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1113     |\n",
            "|    iterations         | 71400    |\n",
            "|    time_elapsed       | 2564     |\n",
            "|    total_timesteps    | 2856000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.37    |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 71399    |\n",
            "|    policy_loss        | 0.349    |\n",
            "|    value_loss         | 0.703    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 365      |\n",
            "|    ep_rew_mean        | 35.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1114     |\n",
            "|    iterations         | 71500    |\n",
            "|    time_elapsed       | 2566     |\n",
            "|    total_timesteps    | 2860000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.397   |\n",
            "|    explained_variance | 0.835    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 71499    |\n",
            "|    policy_loss        | 0.0457   |\n",
            "|    value_loss         | 78.6     |\n",
            "------------------------------------\n",
            "Num timesteps: 2864000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 29.64\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 360      |\n",
            "|    ep_rew_mean        | 29.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1114     |\n",
            "|    iterations         | 71600    |\n",
            "|    time_elapsed       | 2569     |\n",
            "|    total_timesteps    | 2864000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.259   |\n",
            "|    explained_variance | 0.691    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 71599    |\n",
            "|    policy_loss        | -0.112   |\n",
            "|    value_loss         | 162      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 345      |\n",
            "|    ep_rew_mean        | 33.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1115     |\n",
            "|    iterations         | 71700    |\n",
            "|    time_elapsed       | 2571     |\n",
            "|    total_timesteps    | 2868000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.411   |\n",
            "|    explained_variance | 0.969    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 71699    |\n",
            "|    policy_loss        | -0.181   |\n",
            "|    value_loss         | 4.61     |\n",
            "------------------------------------\n",
            "Num timesteps: 2872000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 41.16\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 331      |\n",
            "|    ep_rew_mean        | 41.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1115     |\n",
            "|    iterations         | 71800    |\n",
            "|    time_elapsed       | 2573     |\n",
            "|    total_timesteps    | 2872000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.418   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 71799    |\n",
            "|    policy_loss        | 0.429    |\n",
            "|    value_loss         | 3.76     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 321      |\n",
            "|    ep_rew_mean        | 44.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1116     |\n",
            "|    iterations         | 71900    |\n",
            "|    time_elapsed       | 2575     |\n",
            "|    total_timesteps    | 2876000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.399   |\n",
            "|    explained_variance | 0.974    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 71899    |\n",
            "|    policy_loss        | 1.02     |\n",
            "|    value_loss         | 20.6     |\n",
            "------------------------------------\n",
            "Num timesteps: 2880000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 50.07\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 313      |\n",
            "|    ep_rew_mean        | 50.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1116     |\n",
            "|    iterations         | 72000    |\n",
            "|    time_elapsed       | 2578     |\n",
            "|    total_timesteps    | 2880000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.376   |\n",
            "|    explained_variance | 0.926    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 71999    |\n",
            "|    policy_loss        | -0.38    |\n",
            "|    value_loss         | 27       |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 312      |\n",
            "|    ep_rew_mean        | 59.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 72100    |\n",
            "|    time_elapsed       | 2581     |\n",
            "|    total_timesteps    | 2884000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.217   |\n",
            "|    explained_variance | 0.168    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 72099    |\n",
            "|    policy_loss        | -0.0665  |\n",
            "|    value_loss         | 1.41e+03 |\n",
            "------------------------------------\n",
            "Num timesteps: 2888000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 74.48\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 296      |\n",
            "|    ep_rew_mean        | 74.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 72200    |\n",
            "|    time_elapsed       | 2584     |\n",
            "|    total_timesteps    | 2888000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.363   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 72199    |\n",
            "|    policy_loss        | -0.0775  |\n",
            "|    value_loss         | 6.63     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 293      |\n",
            "|    ep_rew_mean        | 77.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 72300    |\n",
            "|    time_elapsed       | 2587     |\n",
            "|    total_timesteps    | 2892000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.33    |\n",
            "|    explained_variance | 0.687    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 72299    |\n",
            "|    policy_loss        | 0.0502   |\n",
            "|    value_loss         | 157      |\n",
            "------------------------------------\n",
            "Num timesteps: 2896000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 81.54\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 299      |\n",
            "|    ep_rew_mean        | 81.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 72400    |\n",
            "|    time_elapsed       | 2590     |\n",
            "|    total_timesteps    | 2896000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.435   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 72399    |\n",
            "|    policy_loss        | -0.0928  |\n",
            "|    value_loss         | 2.38     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 337      |\n",
            "|    ep_rew_mean        | 86.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1116     |\n",
            "|    iterations         | 72500    |\n",
            "|    time_elapsed       | 2596     |\n",
            "|    total_timesteps    | 2900000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.27    |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 72499    |\n",
            "|    policy_loss        | 0.315    |\n",
            "|    value_loss         | 1.24     |\n",
            "------------------------------------\n",
            "Num timesteps: 2904000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 88.12\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 359      |\n",
            "|    ep_rew_mean        | 88.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1116     |\n",
            "|    iterations         | 72600    |\n",
            "|    time_elapsed       | 2600     |\n",
            "|    total_timesteps    | 2904000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.41    |\n",
            "|    explained_variance | 0.215    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 72599    |\n",
            "|    policy_loss        | 0.723    |\n",
            "|    value_loss         | 884      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 379      |\n",
            "|    ep_rew_mean        | 91       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1116     |\n",
            "|    iterations         | 72700    |\n",
            "|    time_elapsed       | 2603     |\n",
            "|    total_timesteps    | 2908000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.447   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 72699    |\n",
            "|    policy_loss        | 0.0786   |\n",
            "|    value_loss         | 1.91     |\n",
            "------------------------------------\n",
            "Num timesteps: 2912000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 100.12\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 392      |\n",
            "|    ep_rew_mean        | 100      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1116     |\n",
            "|    iterations         | 72800    |\n",
            "|    time_elapsed       | 2607     |\n",
            "|    total_timesteps    | 2912000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.2     |\n",
            "|    explained_variance | 0.216    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 72799    |\n",
            "|    policy_loss        | 0.158    |\n",
            "|    value_loss         | 650      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 406      |\n",
            "|    ep_rew_mean        | 106      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1116     |\n",
            "|    iterations         | 72900    |\n",
            "|    time_elapsed       | 2611     |\n",
            "|    total_timesteps    | 2916000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.274   |\n",
            "|    explained_variance | 0.851    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 72899    |\n",
            "|    policy_loss        | 0.251    |\n",
            "|    value_loss         | 247      |\n",
            "------------------------------------\n",
            "Num timesteps: 2920000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 114.78\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 423      |\n",
            "|    ep_rew_mean        | 115      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 73000    |\n",
            "|    time_elapsed       | 2614     |\n",
            "|    total_timesteps    | 2920000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.379   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 72999    |\n",
            "|    policy_loss        | 0.21     |\n",
            "|    value_loss         | 2.32     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 408      |\n",
            "|    ep_rew_mean        | 118      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 73100    |\n",
            "|    time_elapsed       | 2616     |\n",
            "|    total_timesteps    | 2924000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.353   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 73099    |\n",
            "|    policy_loss        | -0.0516  |\n",
            "|    value_loss         | 1.55     |\n",
            "------------------------------------\n",
            "Num timesteps: 2928000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 116.89\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 406      |\n",
            "|    ep_rew_mean        | 117      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 73200    |\n",
            "|    time_elapsed       | 2620     |\n",
            "|    total_timesteps    | 2928000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.41    |\n",
            "|    explained_variance | 0.98     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 73199    |\n",
            "|    policy_loss        | -0.112   |\n",
            "|    value_loss         | 4.04     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 421      |\n",
            "|    ep_rew_mean        | 121      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1116     |\n",
            "|    iterations         | 73300    |\n",
            "|    time_elapsed       | 2625     |\n",
            "|    total_timesteps    | 2932000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.431   |\n",
            "|    explained_variance | 0.5      |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 73299    |\n",
            "|    policy_loss        | -0.222   |\n",
            "|    value_loss         | 578      |\n",
            "------------------------------------\n",
            "Num timesteps: 2936000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 114.18\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 399      |\n",
            "|    ep_rew_mean        | 114      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 73400    |\n",
            "|    time_elapsed       | 2628     |\n",
            "|    total_timesteps    | 2936000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.411   |\n",
            "|    explained_variance | 0.474    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 73399    |\n",
            "|    policy_loss        | 0.174    |\n",
            "|    value_loss         | 548      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 374      |\n",
            "|    ep_rew_mean        | 115      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 73500    |\n",
            "|    time_elapsed       | 2631     |\n",
            "|    total_timesteps    | 2940000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.461   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 73499    |\n",
            "|    policy_loss        | 0.193    |\n",
            "|    value_loss         | 4.97     |\n",
            "------------------------------------\n",
            "Num timesteps: 2944000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 112.57\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 375      |\n",
            "|    ep_rew_mean        | 113      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 73600    |\n",
            "|    time_elapsed       | 2635     |\n",
            "|    total_timesteps    | 2944000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.47    |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 73599    |\n",
            "|    policy_loss        | 0.0129   |\n",
            "|    value_loss         | 2.26     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 369      |\n",
            "|    ep_rew_mean        | 104      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 73700    |\n",
            "|    time_elapsed       | 2638     |\n",
            "|    total_timesteps    | 2948000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.222   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 73699    |\n",
            "|    policy_loss        | -0.0292  |\n",
            "|    value_loss         | 0.8      |\n",
            "------------------------------------\n",
            "Num timesteps: 2952000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 105.65\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 363      |\n",
            "|    ep_rew_mean        | 106      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 73800    |\n",
            "|    time_elapsed       | 2642     |\n",
            "|    total_timesteps    | 2952000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.546   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 73799    |\n",
            "|    policy_loss        | -0.184   |\n",
            "|    value_loss         | 1.34     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 369      |\n",
            "|    ep_rew_mean        | 101      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 73900    |\n",
            "|    time_elapsed       | 2645     |\n",
            "|    total_timesteps    | 2956000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.244   |\n",
            "|    explained_variance | 0.974    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 73899    |\n",
            "|    policy_loss        | -0.276   |\n",
            "|    value_loss         | 5.41     |\n",
            "------------------------------------\n",
            "Num timesteps: 2960000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 100.73\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 364      |\n",
            "|    ep_rew_mean        | 101      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 74000    |\n",
            "|    time_elapsed       | 2648     |\n",
            "|    total_timesteps    | 2960000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.428   |\n",
            "|    explained_variance | 0.509    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 73999    |\n",
            "|    policy_loss        | 0.116    |\n",
            "|    value_loss         | 911      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 366      |\n",
            "|    ep_rew_mean        | 93.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 74100    |\n",
            "|    time_elapsed       | 2651     |\n",
            "|    total_timesteps    | 2964000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.448   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 74099    |\n",
            "|    policy_loss        | -0.0783  |\n",
            "|    value_loss         | 3.87     |\n",
            "------------------------------------\n",
            "Num timesteps: 2968000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 95.90\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 343      |\n",
            "|    ep_rew_mean        | 95.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1118     |\n",
            "|    iterations         | 74200    |\n",
            "|    time_elapsed       | 2654     |\n",
            "|    total_timesteps    | 2968000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.485   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 74199    |\n",
            "|    policy_loss        | 0.0363   |\n",
            "|    value_loss         | 1.49     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 352      |\n",
            "|    ep_rew_mean        | 90.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1118     |\n",
            "|    iterations         | 74300    |\n",
            "|    time_elapsed       | 2657     |\n",
            "|    total_timesteps    | 2972000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.503   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 74299    |\n",
            "|    policy_loss        | 0.0406   |\n",
            "|    value_loss         | 0.929    |\n",
            "------------------------------------\n",
            "Num timesteps: 2976000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 90.51\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 349      |\n",
            "|    ep_rew_mean        | 90.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1118     |\n",
            "|    iterations         | 74400    |\n",
            "|    time_elapsed       | 2661     |\n",
            "|    total_timesteps    | 2976000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.34    |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 74399    |\n",
            "|    policy_loss        | -0.0328  |\n",
            "|    value_loss         | 2.7      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 344      |\n",
            "|    ep_rew_mean        | 98.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1118     |\n",
            "|    iterations         | 74500    |\n",
            "|    time_elapsed       | 2665     |\n",
            "|    total_timesteps    | 2980000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.227   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 74499    |\n",
            "|    policy_loss        | -0.025   |\n",
            "|    value_loss         | 2.46     |\n",
            "------------------------------------\n",
            "Num timesteps: 2984000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 88.86\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 357      |\n",
            "|    ep_rew_mean        | 88.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1118     |\n",
            "|    iterations         | 74600    |\n",
            "|    time_elapsed       | 2668     |\n",
            "|    total_timesteps    | 2984000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.439   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 74599    |\n",
            "|    policy_loss        | 0.207    |\n",
            "|    value_loss         | 2.06     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 342      |\n",
            "|    ep_rew_mean        | 87       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1118     |\n",
            "|    iterations         | 74700    |\n",
            "|    time_elapsed       | 2671     |\n",
            "|    total_timesteps    | 2988000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.326   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 74699    |\n",
            "|    policy_loss        | -0.534   |\n",
            "|    value_loss         | 2.85     |\n",
            "------------------------------------\n",
            "Num timesteps: 2992000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 89.17\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 343      |\n",
            "|    ep_rew_mean        | 89.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1118     |\n",
            "|    iterations         | 74800    |\n",
            "|    time_elapsed       | 2675     |\n",
            "|    total_timesteps    | 2992000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.303   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 74799    |\n",
            "|    policy_loss        | -0.176   |\n",
            "|    value_loss         | 2.5      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 351      |\n",
            "|    ep_rew_mean        | 85.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 74900    |\n",
            "|    time_elapsed       | 2679     |\n",
            "|    total_timesteps    | 2996000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.374   |\n",
            "|    explained_variance | 0.847    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 74899    |\n",
            "|    policy_loss        | 0.00938  |\n",
            "|    value_loss         | 6.47     |\n",
            "------------------------------------\n",
            "Num timesteps: 3000000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 82.46\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 394      |\n",
            "|    ep_rew_mean        | 82.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 75000    |\n",
            "|    time_elapsed       | 2685     |\n",
            "|    total_timesteps    | 3000000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.316   |\n",
            "|    explained_variance | 0.945    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 74999    |\n",
            "|    policy_loss        | -0.145   |\n",
            "|    value_loss         | 33.9     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 395      |\n",
            "|    ep_rew_mean        | 81.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 75100    |\n",
            "|    time_elapsed       | 2688     |\n",
            "|    total_timesteps    | 3004000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.397   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 75099    |\n",
            "|    policy_loss        | -0.209   |\n",
            "|    value_loss         | 1.81     |\n",
            "------------------------------------\n",
            "Num timesteps: 3008000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 78.41\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 417      |\n",
            "|    ep_rew_mean        | 78.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 75200    |\n",
            "|    time_elapsed       | 2691     |\n",
            "|    total_timesteps    | 3008000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.325   |\n",
            "|    explained_variance | 0.691    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 75199    |\n",
            "|    policy_loss        | -0.359   |\n",
            "|    value_loss         | 260      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 425      |\n",
            "|    ep_rew_mean        | 74       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 75300    |\n",
            "|    time_elapsed       | 2695     |\n",
            "|    total_timesteps    | 3012000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.449   |\n",
            "|    explained_variance | 0.944    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 75299    |\n",
            "|    policy_loss        | -0.174   |\n",
            "|    value_loss         | 4.77     |\n",
            "------------------------------------\n",
            "Num timesteps: 3016000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 70.90\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 446      |\n",
            "|    ep_rew_mean        | 70.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1116     |\n",
            "|    iterations         | 75400    |\n",
            "|    time_elapsed       | 2700     |\n",
            "|    total_timesteps    | 3016000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.446   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 75399    |\n",
            "|    policy_loss        | 0.17     |\n",
            "|    value_loss         | 1.4      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 470      |\n",
            "|    ep_rew_mean        | 72.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1116     |\n",
            "|    iterations         | 75500    |\n",
            "|    time_elapsed       | 2703     |\n",
            "|    total_timesteps    | 3020000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.475   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 75499    |\n",
            "|    policy_loss        | -0.342   |\n",
            "|    value_loss         | 2.02     |\n",
            "------------------------------------\n",
            "Num timesteps: 3024000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 73.91\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 477      |\n",
            "|    ep_rew_mean        | 73.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1116     |\n",
            "|    iterations         | 75600    |\n",
            "|    time_elapsed       | 2708     |\n",
            "|    total_timesteps    | 3024000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.366   |\n",
            "|    explained_variance | 0.951    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 75599    |\n",
            "|    policy_loss        | -0.209   |\n",
            "|    value_loss         | 38.3     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 486      |\n",
            "|    ep_rew_mean        | 73.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1116     |\n",
            "|    iterations         | 75700    |\n",
            "|    time_elapsed       | 2712     |\n",
            "|    total_timesteps    | 3028000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.335   |\n",
            "|    explained_variance | 0.837    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 75699    |\n",
            "|    policy_loss        | 0.162    |\n",
            "|    value_loss         | 202      |\n",
            "------------------------------------\n",
            "Num timesteps: 3032000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 79.77\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 475      |\n",
            "|    ep_rew_mean        | 79.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1116     |\n",
            "|    iterations         | 75800    |\n",
            "|    time_elapsed       | 2714     |\n",
            "|    total_timesteps    | 3032000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.475   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 75799    |\n",
            "|    policy_loss        | -0.0929  |\n",
            "|    value_loss         | 4.69     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 479      |\n",
            "|    ep_rew_mean        | 79.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 75900    |\n",
            "|    time_elapsed       | 2717     |\n",
            "|    total_timesteps    | 3036000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.487   |\n",
            "|    explained_variance | 0.903    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 75899    |\n",
            "|    policy_loss        | 0.389    |\n",
            "|    value_loss         | 54       |\n",
            "------------------------------------\n",
            "Num timesteps: 3040000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 82.41\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 480      |\n",
            "|    ep_rew_mean        | 82.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 76000    |\n",
            "|    time_elapsed       | 2721     |\n",
            "|    total_timesteps    | 3040000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.275   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 75999    |\n",
            "|    policy_loss        | 0.132    |\n",
            "|    value_loss         | 2.48     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 428      |\n",
            "|    ep_rew_mean        | 108      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 76100    |\n",
            "|    time_elapsed       | 2724     |\n",
            "|    total_timesteps    | 3044000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.507   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 76099    |\n",
            "|    policy_loss        | 0.523    |\n",
            "|    value_loss         | 2.36     |\n",
            "------------------------------------\n",
            "Num timesteps: 3048000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 118.37\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 430      |\n",
            "|    ep_rew_mean        | 118      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 76200    |\n",
            "|    time_elapsed       | 2727     |\n",
            "|    total_timesteps    | 3048000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.424   |\n",
            "|    explained_variance | 0.43     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 76199    |\n",
            "|    policy_loss        | -0.44    |\n",
            "|    value_loss         | 148      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 400      |\n",
            "|    ep_rew_mean        | 140      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 76300    |\n",
            "|    time_elapsed       | 2730     |\n",
            "|    total_timesteps    | 3052000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.246   |\n",
            "|    explained_variance | 0.293    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 76299    |\n",
            "|    policy_loss        | 0.669    |\n",
            "|    value_loss         | 751      |\n",
            "------------------------------------\n",
            "Num timesteps: 3056000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 143.77\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 363      |\n",
            "|    ep_rew_mean        | 144      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 76400    |\n",
            "|    time_elapsed       | 2733     |\n",
            "|    total_timesteps    | 3056000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.357   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 76399    |\n",
            "|    policy_loss        | -0.0369  |\n",
            "|    value_loss         | 3.07     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 361      |\n",
            "|    ep_rew_mean        | 146      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 76500    |\n",
            "|    time_elapsed       | 2737     |\n",
            "|    total_timesteps    | 3060000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.462   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 76499    |\n",
            "|    policy_loss        | -0.268   |\n",
            "|    value_loss         | 1.64     |\n",
            "------------------------------------\n",
            "Num timesteps: 3064000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 148.61\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 350      |\n",
            "|    ep_rew_mean        | 149      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 76600    |\n",
            "|    time_elapsed       | 2740     |\n",
            "|    total_timesteps    | 3064000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.225   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 76599    |\n",
            "|    policy_loss        | -0.0242  |\n",
            "|    value_loss         | 1.16     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 361      |\n",
            "|    ep_rew_mean        | 148      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 76700    |\n",
            "|    time_elapsed       | 2744     |\n",
            "|    total_timesteps    | 3068000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.342   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 76699    |\n",
            "|    policy_loss        | -0.345   |\n",
            "|    value_loss         | 1.94     |\n",
            "------------------------------------\n",
            "Num timesteps: 3072000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 147.08\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 380      |\n",
            "|    ep_rew_mean        | 147      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 76800    |\n",
            "|    time_elapsed       | 2749     |\n",
            "|    total_timesteps    | 3072000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.398   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 76799    |\n",
            "|    policy_loss        | -0.507   |\n",
            "|    value_loss         | 4.7      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 401      |\n",
            "|    ep_rew_mean        | 145      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1116     |\n",
            "|    iterations         | 76900    |\n",
            "|    time_elapsed       | 2754     |\n",
            "|    total_timesteps    | 3076000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.376   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 76899    |\n",
            "|    policy_loss        | 0.172    |\n",
            "|    value_loss         | 3.67     |\n",
            "------------------------------------\n",
            "Num timesteps: 3080000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 139.50\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 409      |\n",
            "|    ep_rew_mean        | 139      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1116     |\n",
            "|    iterations         | 77000    |\n",
            "|    time_elapsed       | 2758     |\n",
            "|    total_timesteps    | 3080000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.304   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 76999    |\n",
            "|    policy_loss        | 0.461    |\n",
            "|    value_loss         | 22.6     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 409      |\n",
            "|    ep_rew_mean        | 130      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1116     |\n",
            "|    iterations         | 77100    |\n",
            "|    time_elapsed       | 2762     |\n",
            "|    total_timesteps    | 3084000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.438   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 77099    |\n",
            "|    policy_loss        | -0.421   |\n",
            "|    value_loss         | 10       |\n",
            "------------------------------------\n",
            "Num timesteps: 3088000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 123.54\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 428      |\n",
            "|    ep_rew_mean        | 124      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1115     |\n",
            "|    iterations         | 77200    |\n",
            "|    time_elapsed       | 2767     |\n",
            "|    total_timesteps    | 3088000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.452   |\n",
            "|    explained_variance | 0.967    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 77199    |\n",
            "|    policy_loss        | -0.272   |\n",
            "|    value_loss         | 11       |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 433      |\n",
            "|    ep_rew_mean        | 115      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1115     |\n",
            "|    iterations         | 77300    |\n",
            "|    time_elapsed       | 2771     |\n",
            "|    total_timesteps    | 3092000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.4     |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 77299    |\n",
            "|    policy_loss        | 0.544    |\n",
            "|    value_loss         | 3.32     |\n",
            "------------------------------------\n",
            "Num timesteps: 3096000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 102.71\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 455      |\n",
            "|    ep_rew_mean        | 103      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1115     |\n",
            "|    iterations         | 77400    |\n",
            "|    time_elapsed       | 2776     |\n",
            "|    total_timesteps    | 3096000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.274   |\n",
            "|    explained_variance | 0.071    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 77399    |\n",
            "|    policy_loss        | -0.143   |\n",
            "|    value_loss         | 1.64e+03 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 464      |\n",
            "|    ep_rew_mean        | 92       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1114     |\n",
            "|    iterations         | 77500    |\n",
            "|    time_elapsed       | 2780     |\n",
            "|    total_timesteps    | 3100000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.397   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 77499    |\n",
            "|    policy_loss        | 0.346    |\n",
            "|    value_loss         | 4.31     |\n",
            "------------------------------------\n",
            "Num timesteps: 3104000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 85.46\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 481      |\n",
            "|    ep_rew_mean        | 85.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1114     |\n",
            "|    iterations         | 77600    |\n",
            "|    time_elapsed       | 2784     |\n",
            "|    total_timesteps    | 3104000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.485   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 77599    |\n",
            "|    policy_loss        | 0.373    |\n",
            "|    value_loss         | 3.17     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 483      |\n",
            "|    ep_rew_mean        | 84.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1114     |\n",
            "|    iterations         | 77700    |\n",
            "|    time_elapsed       | 2788     |\n",
            "|    total_timesteps    | 3108000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.222   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 77699    |\n",
            "|    policy_loss        | -0.153   |\n",
            "|    value_loss         | 1.44     |\n",
            "------------------------------------\n",
            "Num timesteps: 3112000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 79.82\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 517      |\n",
            "|    ep_rew_mean        | 79.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1114     |\n",
            "|    iterations         | 77800    |\n",
            "|    time_elapsed       | 2792     |\n",
            "|    total_timesteps    | 3112000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.455   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 77799    |\n",
            "|    policy_loss        | 0.0161   |\n",
            "|    value_loss         | 0.603    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 498      |\n",
            "|    ep_rew_mean        | 78       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1114     |\n",
            "|    iterations         | 77900    |\n",
            "|    time_elapsed       | 2796     |\n",
            "|    total_timesteps    | 3116000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.407   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 77899    |\n",
            "|    policy_loss        | -0.0814  |\n",
            "|    value_loss         | 1.03     |\n",
            "------------------------------------\n",
            "Num timesteps: 3120000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 75.59\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 491      |\n",
            "|    ep_rew_mean        | 75.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1114     |\n",
            "|    iterations         | 78000    |\n",
            "|    time_elapsed       | 2799     |\n",
            "|    total_timesteps    | 3120000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.321   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 77999    |\n",
            "|    policy_loss        | -0.0848  |\n",
            "|    value_loss         | 2.25     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 470      |\n",
            "|    ep_rew_mean        | 76.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1114     |\n",
            "|    iterations         | 78100    |\n",
            "|    time_elapsed       | 2803     |\n",
            "|    total_timesteps    | 3124000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.43    |\n",
            "|    explained_variance | 0.96     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 78099    |\n",
            "|    policy_loss        | -0.273   |\n",
            "|    value_loss         | 6.55     |\n",
            "------------------------------------\n",
            "Num timesteps: 3128000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 75.95\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 474      |\n",
            "|    ep_rew_mean        | 75.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1114     |\n",
            "|    iterations         | 78200    |\n",
            "|    time_elapsed       | 2807     |\n",
            "|    total_timesteps    | 3128000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.496   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 78199    |\n",
            "|    policy_loss        | -0.0953  |\n",
            "|    value_loss         | 0.898    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 486      |\n",
            "|    ep_rew_mean        | 77.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1114     |\n",
            "|    iterations         | 78300    |\n",
            "|    time_elapsed       | 2811     |\n",
            "|    total_timesteps    | 3132000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.399   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 78299    |\n",
            "|    policy_loss        | -0.298   |\n",
            "|    value_loss         | 1.58     |\n",
            "------------------------------------\n",
            "Num timesteps: 3136000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 79.00\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 485      |\n",
            "|    ep_rew_mean        | 79       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1113     |\n",
            "|    iterations         | 78400    |\n",
            "|    time_elapsed       | 2815     |\n",
            "|    total_timesteps    | 3136000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.401   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 78399    |\n",
            "|    policy_loss        | -0.0898  |\n",
            "|    value_loss         | 2.85     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 486      |\n",
            "|    ep_rew_mean        | 77.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1113     |\n",
            "|    iterations         | 78500    |\n",
            "|    time_elapsed       | 2819     |\n",
            "|    total_timesteps    | 3140000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.374   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 78499    |\n",
            "|    policy_loss        | -0.247   |\n",
            "|    value_loss         | 1.25     |\n",
            "------------------------------------\n",
            "Num timesteps: 3144000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 80.21\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 488      |\n",
            "|    ep_rew_mean        | 80.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1113     |\n",
            "|    iterations         | 78600    |\n",
            "|    time_elapsed       | 2823     |\n",
            "|    total_timesteps    | 3144000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.241   |\n",
            "|    explained_variance | 0.933    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 78599    |\n",
            "|    policy_loss        | 0.0711   |\n",
            "|    value_loss         | 15.3     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 479      |\n",
            "|    ep_rew_mean        | 101      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1113     |\n",
            "|    iterations         | 78700    |\n",
            "|    time_elapsed       | 2826     |\n",
            "|    total_timesteps    | 3148000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.242   |\n",
            "|    explained_variance | 0.876    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 78699    |\n",
            "|    policy_loss        | 3.24     |\n",
            "|    value_loss         | 82.7     |\n",
            "------------------------------------\n",
            "Num timesteps: 3152000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 99.46\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 453      |\n",
            "|    ep_rew_mean        | 99.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1113     |\n",
            "|    iterations         | 78800    |\n",
            "|    time_elapsed       | 2829     |\n",
            "|    total_timesteps    | 3152000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.466   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 78799    |\n",
            "|    policy_loss        | -0.0644  |\n",
            "|    value_loss         | 0.917    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 434      |\n",
            "|    ep_rew_mean        | 100      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1113     |\n",
            "|    iterations         | 78900    |\n",
            "|    time_elapsed       | 2833     |\n",
            "|    total_timesteps    | 3156000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.324   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 78899    |\n",
            "|    policy_loss        | 0.299    |\n",
            "|    value_loss         | 3.24     |\n",
            "------------------------------------\n",
            "Num timesteps: 3160000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 99.80\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 448      |\n",
            "|    ep_rew_mean        | 99.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1113     |\n",
            "|    iterations         | 79000    |\n",
            "|    time_elapsed       | 2838     |\n",
            "|    total_timesteps    | 3160000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.371   |\n",
            "|    explained_variance | 0.674    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 78999    |\n",
            "|    policy_loss        | -0.334   |\n",
            "|    value_loss         | 738      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 476      |\n",
            "|    ep_rew_mean        | 95.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1112     |\n",
            "|    iterations         | 79100    |\n",
            "|    time_elapsed       | 2844     |\n",
            "|    total_timesteps    | 3164000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.341   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 79099    |\n",
            "|    policy_loss        | -0.0105  |\n",
            "|    value_loss         | 7.4      |\n",
            "------------------------------------\n",
            "Num timesteps: 3168000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 98.13\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 472      |\n",
            "|    ep_rew_mean        | 98.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1112     |\n",
            "|    iterations         | 79200    |\n",
            "|    time_elapsed       | 2847     |\n",
            "|    total_timesteps    | 3168000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.306   |\n",
            "|    explained_variance | 0.842    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 79199    |\n",
            "|    policy_loss        | -0.216   |\n",
            "|    value_loss         | 29.4     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 481      |\n",
            "|    ep_rew_mean        | 102      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1112     |\n",
            "|    iterations         | 79300    |\n",
            "|    time_elapsed       | 2851     |\n",
            "|    total_timesteps    | 3172000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.428   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 79299    |\n",
            "|    policy_loss        | -0.0845  |\n",
            "|    value_loss         | 3.38     |\n",
            "------------------------------------\n",
            "Num timesteps: 3176000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 99.90\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 472      |\n",
            "|    ep_rew_mean        | 99.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1112     |\n",
            "|    iterations         | 79400    |\n",
            "|    time_elapsed       | 2855     |\n",
            "|    total_timesteps    | 3176000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.368   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 79399    |\n",
            "|    policy_loss        | -0.0216  |\n",
            "|    value_loss         | 1.27     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 479      |\n",
            "|    ep_rew_mean        | 98       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1111     |\n",
            "|    iterations         | 79500    |\n",
            "|    time_elapsed       | 2861     |\n",
            "|    total_timesteps    | 3180000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.3     |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 79499    |\n",
            "|    policy_loss        | -0.309   |\n",
            "|    value_loss         | 5.57     |\n",
            "------------------------------------\n",
            "Num timesteps: 3184000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 87.34\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 491      |\n",
            "|    ep_rew_mean        | 87.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1110     |\n",
            "|    iterations         | 79600    |\n",
            "|    time_elapsed       | 2865     |\n",
            "|    total_timesteps    | 3184000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.355   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 79599    |\n",
            "|    policy_loss        | 0.0974   |\n",
            "|    value_loss         | 2.37     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 502      |\n",
            "|    ep_rew_mean        | 86.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1110     |\n",
            "|    iterations         | 79700    |\n",
            "|    time_elapsed       | 2870     |\n",
            "|    total_timesteps    | 3188000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.405   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 79699    |\n",
            "|    policy_loss        | -0.0351  |\n",
            "|    value_loss         | 2.89     |\n",
            "------------------------------------\n",
            "Num timesteps: 3192000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 84.53\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 482      |\n",
            "|    ep_rew_mean        | 84.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 79800    |\n",
            "|    time_elapsed       | 2875     |\n",
            "|    total_timesteps    | 3192000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.286   |\n",
            "|    explained_variance | 0.98     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 79799    |\n",
            "|    policy_loss        | -0.11    |\n",
            "|    value_loss         | 9.13     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 473      |\n",
            "|    ep_rew_mean        | 65.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1110     |\n",
            "|    iterations         | 79900    |\n",
            "|    time_elapsed       | 2879     |\n",
            "|    total_timesteps    | 3196000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.278   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 79899    |\n",
            "|    policy_loss        | -0.0665  |\n",
            "|    value_loss         | 2.16     |\n",
            "------------------------------------\n",
            "Num timesteps: 3200000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 51.86\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 493      |\n",
            "|    ep_rew_mean        | 51.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 80000    |\n",
            "|    time_elapsed       | 2883     |\n",
            "|    total_timesteps    | 3200000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.335   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 79999    |\n",
            "|    policy_loss        | -0.082   |\n",
            "|    value_loss         | 2.48     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 509      |\n",
            "|    ep_rew_mean        | 44       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 80100    |\n",
            "|    time_elapsed       | 2888     |\n",
            "|    total_timesteps    | 3204000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.306   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 80099    |\n",
            "|    policy_loss        | 0.0485   |\n",
            "|    value_loss         | 3.54     |\n",
            "------------------------------------\n",
            "Num timesteps: 3208000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 38.08\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 504      |\n",
            "|    ep_rew_mean        | 38.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1108     |\n",
            "|    iterations         | 80200    |\n",
            "|    time_elapsed       | 2892     |\n",
            "|    total_timesteps    | 3208000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.253   |\n",
            "|    explained_variance | 0.922    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 80199    |\n",
            "|    policy_loss        | -0.789   |\n",
            "|    value_loss         | 118      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 477      |\n",
            "|    ep_rew_mean        | 41.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1108     |\n",
            "|    iterations         | 80300    |\n",
            "|    time_elapsed       | 2896     |\n",
            "|    total_timesteps    | 3212000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.429   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 80299    |\n",
            "|    policy_loss        | -0.543   |\n",
            "|    value_loss         | 3.15     |\n",
            "------------------------------------\n",
            "Num timesteps: 3216000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 42.63\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 489      |\n",
            "|    ep_rew_mean        | 42.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1107     |\n",
            "|    iterations         | 80400    |\n",
            "|    time_elapsed       | 2902     |\n",
            "|    total_timesteps    | 3216000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.434   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 80399    |\n",
            "|    policy_loss        | 0.0884   |\n",
            "|    value_loss         | 3.35     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 502      |\n",
            "|    ep_rew_mean        | 34.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1106     |\n",
            "|    iterations         | 80500    |\n",
            "|    time_elapsed       | 2909     |\n",
            "|    total_timesteps    | 3220000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.192   |\n",
            "|    explained_variance | 0.684    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 80499    |\n",
            "|    policy_loss        | 0.0641   |\n",
            "|    value_loss         | 450      |\n",
            "------------------------------------\n",
            "Num timesteps: 3224000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 19.34\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 493      |\n",
            "|    ep_rew_mean        | 19.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1106     |\n",
            "|    iterations         | 80600    |\n",
            "|    time_elapsed       | 2914     |\n",
            "|    total_timesteps    | 3224000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.322   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 80599    |\n",
            "|    policy_loss        | 0.127    |\n",
            "|    value_loss         | 2.08     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 434      |\n",
            "|    ep_rew_mean        | 27.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1106     |\n",
            "|    iterations         | 80700    |\n",
            "|    time_elapsed       | 2917     |\n",
            "|    total_timesteps    | 3228000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.336   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 80699    |\n",
            "|    policy_loss        | 0.101    |\n",
            "|    value_loss         | 4.85     |\n",
            "------------------------------------\n",
            "Num timesteps: 3232000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 31.05\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 416      |\n",
            "|    ep_rew_mean        | 31       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1106     |\n",
            "|    iterations         | 80800    |\n",
            "|    time_elapsed       | 2921     |\n",
            "|    total_timesteps    | 3232000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.439   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 80799    |\n",
            "|    policy_loss        | -0.44    |\n",
            "|    value_loss         | 5.97     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 414      |\n",
            "|    ep_rew_mean        | 30.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1105     |\n",
            "|    iterations         | 80900    |\n",
            "|    time_elapsed       | 2926     |\n",
            "|    total_timesteps    | 3236000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.508   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 80899    |\n",
            "|    policy_loss        | 0.129    |\n",
            "|    value_loss         | 8.47     |\n",
            "------------------------------------\n",
            "Num timesteps: 3240000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 41.02\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 420      |\n",
            "|    ep_rew_mean        | 41       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1105     |\n",
            "|    iterations         | 81000    |\n",
            "|    time_elapsed       | 2930     |\n",
            "|    total_timesteps    | 3240000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.484   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 80999    |\n",
            "|    policy_loss        | -0.184   |\n",
            "|    value_loss         | 2.11     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 422      |\n",
            "|    ep_rew_mean        | 49.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1105     |\n",
            "|    iterations         | 81100    |\n",
            "|    time_elapsed       | 2934     |\n",
            "|    total_timesteps    | 3244000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.416   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 81099    |\n",
            "|    policy_loss        | -0.0118  |\n",
            "|    value_loss         | 2.69     |\n",
            "------------------------------------\n",
            "Num timesteps: 3248000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 50.23\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 423      |\n",
            "|    ep_rew_mean        | 50.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1104     |\n",
            "|    iterations         | 81200    |\n",
            "|    time_elapsed       | 2941     |\n",
            "|    total_timesteps    | 3248000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.461   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 81199    |\n",
            "|    policy_loss        | 0.0478   |\n",
            "|    value_loss         | 3.81     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 421      |\n",
            "|    ep_rew_mean        | 47.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1103     |\n",
            "|    iterations         | 81300    |\n",
            "|    time_elapsed       | 2946     |\n",
            "|    total_timesteps    | 3252000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.275   |\n",
            "|    explained_variance | 0.873    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 81299    |\n",
            "|    policy_loss        | 0.143    |\n",
            "|    value_loss         | 1.89     |\n",
            "------------------------------------\n",
            "Num timesteps: 3256000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 42.81\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 452      |\n",
            "|    ep_rew_mean        | 42.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 81400    |\n",
            "|    time_elapsed       | 2952     |\n",
            "|    total_timesteps    | 3256000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.409   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 81399    |\n",
            "|    policy_loss        | -0.298   |\n",
            "|    value_loss         | 2.66     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 475      |\n",
            "|    ep_rew_mean        | 39.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 81500    |\n",
            "|    time_elapsed       | 2957     |\n",
            "|    total_timesteps    | 3260000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.358   |\n",
            "|    explained_variance | 0.96     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 81499    |\n",
            "|    policy_loss        | 0.189    |\n",
            "|    value_loss         | 1.18     |\n",
            "------------------------------------\n",
            "Num timesteps: 3264000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 26.44\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 494      |\n",
            "|    ep_rew_mean        | 26.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 81600    |\n",
            "|    time_elapsed       | 2963     |\n",
            "|    total_timesteps    | 3264000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.207   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 81599    |\n",
            "|    policy_loss        | 0.211    |\n",
            "|    value_loss         | 2.3      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 481      |\n",
            "|    ep_rew_mean        | 35.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 81700    |\n",
            "|    time_elapsed       | 2968     |\n",
            "|    total_timesteps    | 3268000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.175   |\n",
            "|    explained_variance | 0.683    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 81699    |\n",
            "|    policy_loss        | -0.113   |\n",
            "|    value_loss         | 527      |\n",
            "------------------------------------\n",
            "Num timesteps: 3272000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 44.34\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 491      |\n",
            "|    ep_rew_mean        | 44.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 81800    |\n",
            "|    time_elapsed       | 2972     |\n",
            "|    total_timesteps    | 3272000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.327   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 81799    |\n",
            "|    policy_loss        | -0.386   |\n",
            "|    value_loss         | 1.62     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 502      |\n",
            "|    ep_rew_mean        | 51.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 81900    |\n",
            "|    time_elapsed       | 2976     |\n",
            "|    total_timesteps    | 3276000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.395   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 81899    |\n",
            "|    policy_loss        | -0.346   |\n",
            "|    value_loss         | 3.43     |\n",
            "------------------------------------\n",
            "Num timesteps: 3280000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 59.17\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 507      |\n",
            "|    ep_rew_mean        | 59.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 82000    |\n",
            "|    time_elapsed       | 2980     |\n",
            "|    total_timesteps    | 3280000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.372   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 81999    |\n",
            "|    policy_loss        | -0.535   |\n",
            "|    value_loss         | 3.31     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 530      |\n",
            "|    ep_rew_mean        | 53.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 82100    |\n",
            "|    time_elapsed       | 2983     |\n",
            "|    total_timesteps    | 3284000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.3     |\n",
            "|    explained_variance | 0.972    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 82099    |\n",
            "|    policy_loss        | -0.0781  |\n",
            "|    value_loss         | 50.6     |\n",
            "------------------------------------\n",
            "Num timesteps: 3288000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 64.17\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 522      |\n",
            "|    ep_rew_mean        | 64.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 82200    |\n",
            "|    time_elapsed       | 2987     |\n",
            "|    total_timesteps    | 3288000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.277   |\n",
            "|    explained_variance | 0.838    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 82199    |\n",
            "|    policy_loss        | 0.845    |\n",
            "|    value_loss         | 182      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 522      |\n",
            "|    ep_rew_mean        | 57.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 82300    |\n",
            "|    time_elapsed       | 2991     |\n",
            "|    total_timesteps    | 3292000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.449   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 82299    |\n",
            "|    policy_loss        | -0.332   |\n",
            "|    value_loss         | 2.85     |\n",
            "------------------------------------\n",
            "Num timesteps: 3296000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 62.52\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 517      |\n",
            "|    ep_rew_mean        | 62.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 82400    |\n",
            "|    time_elapsed       | 2995     |\n",
            "|    total_timesteps    | 3296000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.414   |\n",
            "|    explained_variance | 0.972    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 82399    |\n",
            "|    policy_loss        | 0.0121   |\n",
            "|    value_loss         | 10.1     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 468      |\n",
            "|    ep_rew_mean        | 67.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 82500    |\n",
            "|    time_elapsed       | 2999     |\n",
            "|    total_timesteps    | 3300000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.339   |\n",
            "|    explained_variance | 0.954    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 82499    |\n",
            "|    policy_loss        | 0.182    |\n",
            "|    value_loss         | 79.2     |\n",
            "------------------------------------\n",
            "Num timesteps: 3304000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 75.95\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 409      |\n",
            "|    ep_rew_mean        | 76       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 82600    |\n",
            "|    time_elapsed       | 3003     |\n",
            "|    total_timesteps    | 3304000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.335   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 82599    |\n",
            "|    policy_loss        | 0.105    |\n",
            "|    value_loss         | 1.16     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 378      |\n",
            "|    ep_rew_mean        | 81.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 82700    |\n",
            "|    time_elapsed       | 3006     |\n",
            "|    total_timesteps    | 3308000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.418   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 82699    |\n",
            "|    policy_loss        | -0.102   |\n",
            "|    value_loss         | 4.9      |\n",
            "------------------------------------\n",
            "Num timesteps: 3312000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 75.81\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 374      |\n",
            "|    ep_rew_mean        | 75.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 82800    |\n",
            "|    time_elapsed       | 3010     |\n",
            "|    total_timesteps    | 3312000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.312   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 82799    |\n",
            "|    policy_loss        | 0.073    |\n",
            "|    value_loss         | 14.9     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 376      |\n",
            "|    ep_rew_mean        | 66       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 82900    |\n",
            "|    time_elapsed       | 3013     |\n",
            "|    total_timesteps    | 3316000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.521   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 82899    |\n",
            "|    policy_loss        | 0.33     |\n",
            "|    value_loss         | 5.29     |\n",
            "------------------------------------\n",
            "Num timesteps: 3320000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 77.80\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 359      |\n",
            "|    ep_rew_mean        | 77.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 83000    |\n",
            "|    time_elapsed       | 3018     |\n",
            "|    total_timesteps    | 3320000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.529   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 82999    |\n",
            "|    policy_loss        | -0.408   |\n",
            "|    value_loss         | 4.43     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 369      |\n",
            "|    ep_rew_mean        | 62.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 83100    |\n",
            "|    time_elapsed       | 3023     |\n",
            "|    total_timesteps    | 3324000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.39    |\n",
            "|    explained_variance | 0.963    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 83099    |\n",
            "|    policy_loss        | 0.531    |\n",
            "|    value_loss         | 36.7     |\n",
            "------------------------------------\n",
            "Num timesteps: 3328000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 56.72\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 362      |\n",
            "|    ep_rew_mean        | 56.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 83200    |\n",
            "|    time_elapsed       | 3027     |\n",
            "|    total_timesteps    | 3328000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.366   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 83199    |\n",
            "|    policy_loss        | 0.0132   |\n",
            "|    value_loss         | 0.939    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 369      |\n",
            "|    ep_rew_mean        | 42.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 83300    |\n",
            "|    time_elapsed       | 3031     |\n",
            "|    total_timesteps    | 3332000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.335   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 83299    |\n",
            "|    policy_loss        | -1.39    |\n",
            "|    value_loss         | 47.4     |\n",
            "------------------------------------\n",
            "Num timesteps: 3336000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 42.44\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 350      |\n",
            "|    ep_rew_mean        | 42.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 83400    |\n",
            "|    time_elapsed       | 3033     |\n",
            "|    total_timesteps    | 3336000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.388   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 83399    |\n",
            "|    policy_loss        | -0.422   |\n",
            "|    value_loss         | 2.29     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 352      |\n",
            "|    ep_rew_mean        | 43.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 83500    |\n",
            "|    time_elapsed       | 3037     |\n",
            "|    total_timesteps    | 3340000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.499   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 83499    |\n",
            "|    policy_loss        | -0.0676  |\n",
            "|    value_loss         | 5.13     |\n",
            "------------------------------------\n",
            "Num timesteps: 3344000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 36.73\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 334      |\n",
            "|    ep_rew_mean        | 36.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1099     |\n",
            "|    iterations         | 83600    |\n",
            "|    time_elapsed       | 3040     |\n",
            "|    total_timesteps    | 3344000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.312   |\n",
            "|    explained_variance | 0.956    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 83599    |\n",
            "|    policy_loss        | -0.0847  |\n",
            "|    value_loss         | 52.2     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 324      |\n",
            "|    ep_rew_mean        | 50.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 83700    |\n",
            "|    time_elapsed       | 3043     |\n",
            "|    total_timesteps    | 3348000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.369   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 83699    |\n",
            "|    policy_loss        | 0.0016   |\n",
            "|    value_loss         | 1.2      |\n",
            "------------------------------------\n",
            "Num timesteps: 3352000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 61.74\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 332      |\n",
            "|    ep_rew_mean        | 61.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 83800    |\n",
            "|    time_elapsed       | 3047     |\n",
            "|    total_timesteps    | 3352000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.343   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 83799    |\n",
            "|    policy_loss        | 0.408    |\n",
            "|    value_loss         | 10.2     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 309      |\n",
            "|    ep_rew_mean        | 73       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 83900    |\n",
            "|    time_elapsed       | 3049     |\n",
            "|    total_timesteps    | 3356000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.287   |\n",
            "|    explained_variance | 0.944    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 83899    |\n",
            "|    policy_loss        | 0.16     |\n",
            "|    value_loss         | 28.9     |\n",
            "------------------------------------\n",
            "Num timesteps: 3360000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 84.14\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 280      |\n",
            "|    ep_rew_mean        | 84.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1100     |\n",
            "|    iterations         | 84000    |\n",
            "|    time_elapsed       | 3052     |\n",
            "|    total_timesteps    | 3360000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.274   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 83999    |\n",
            "|    policy_loss        | -0.615   |\n",
            "|    value_loss         | 25.2     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 276      |\n",
            "|    ep_rew_mean        | 94.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 84100    |\n",
            "|    time_elapsed       | 3055     |\n",
            "|    total_timesteps    | 3364000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.326   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 84099    |\n",
            "|    policy_loss        | -0.15    |\n",
            "|    value_loss         | 2.18     |\n",
            "------------------------------------\n",
            "Num timesteps: 3368000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 94.72\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 271      |\n",
            "|    ep_rew_mean        | 94.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 84200    |\n",
            "|    time_elapsed       | 3058     |\n",
            "|    total_timesteps    | 3368000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.35    |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 84199    |\n",
            "|    policy_loss        | -0.0916  |\n",
            "|    value_loss         | 3.38     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 270      |\n",
            "|    ep_rew_mean        | 101      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 84300    |\n",
            "|    time_elapsed       | 3060     |\n",
            "|    total_timesteps    | 3372000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.454   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 84299    |\n",
            "|    policy_loss        | -0.222   |\n",
            "|    value_loss         | 1.59     |\n",
            "------------------------------------\n",
            "Num timesteps: 3376000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 103.29\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 275      |\n",
            "|    ep_rew_mean        | 103      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 84400    |\n",
            "|    time_elapsed       | 3064     |\n",
            "|    total_timesteps    | 3376000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.39    |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 84399    |\n",
            "|    policy_loss        | 0.0795   |\n",
            "|    value_loss         | 3.79     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 263      |\n",
            "|    ep_rew_mean        | 93.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1101     |\n",
            "|    iterations         | 84500    |\n",
            "|    time_elapsed       | 3067     |\n",
            "|    total_timesteps    | 3380000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.365   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 84499    |\n",
            "|    policy_loss        | 0.0372   |\n",
            "|    value_loss         | 12.3     |\n",
            "------------------------------------\n",
            "Num timesteps: 3384000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 106.11\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 280      |\n",
            "|    ep_rew_mean        | 106      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 84600    |\n",
            "|    time_elapsed       | 3070     |\n",
            "|    total_timesteps    | 3384000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.338   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 84599    |\n",
            "|    policy_loss        | 0.0287   |\n",
            "|    value_loss         | 0.662    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 271      |\n",
            "|    ep_rew_mean        | 91.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 84700    |\n",
            "|    time_elapsed       | 3073     |\n",
            "|    total_timesteps    | 3388000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.409   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 84699    |\n",
            "|    policy_loss        | 0.064    |\n",
            "|    value_loss         | 0.321    |\n",
            "------------------------------------\n",
            "Num timesteps: 3392000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 97.99\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 284      |\n",
            "|    ep_rew_mean        | 98       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 84800    |\n",
            "|    time_elapsed       | 3076     |\n",
            "|    total_timesteps    | 3392000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.388   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 84799    |\n",
            "|    policy_loss        | -0.0196  |\n",
            "|    value_loss         | 3.47     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 274      |\n",
            "|    ep_rew_mean        | 101      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1102     |\n",
            "|    iterations         | 84900    |\n",
            "|    time_elapsed       | 3078     |\n",
            "|    total_timesteps    | 3396000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.3     |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 84899    |\n",
            "|    policy_loss        | 0.0699   |\n",
            "|    value_loss         | 1.2      |\n",
            "------------------------------------\n",
            "Num timesteps: 3400000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 94.12\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 272      |\n",
            "|    ep_rew_mean        | 94.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1103     |\n",
            "|    iterations         | 85000    |\n",
            "|    time_elapsed       | 3081     |\n",
            "|    total_timesteps    | 3400000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.295   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 84999    |\n",
            "|    policy_loss        | -0.294   |\n",
            "|    value_loss         | 12.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 259      |\n",
            "|    ep_rew_mean        | 96.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1103     |\n",
            "|    iterations         | 85100    |\n",
            "|    time_elapsed       | 3084     |\n",
            "|    total_timesteps    | 3404000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.315   |\n",
            "|    explained_variance | 0.868    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 85099    |\n",
            "|    policy_loss        | -0.0894  |\n",
            "|    value_loss         | 84       |\n",
            "------------------------------------\n",
            "Num timesteps: 3408000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 100.24\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 260      |\n",
            "|    ep_rew_mean        | 100      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1104     |\n",
            "|    iterations         | 85200    |\n",
            "|    time_elapsed       | 3086     |\n",
            "|    total_timesteps    | 3408000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.268   |\n",
            "|    explained_variance | 0.933    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 85199    |\n",
            "|    policy_loss        | 0.537    |\n",
            "|    value_loss         | 8.58     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 231      |\n",
            "|    ep_rew_mean        | 89.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1104     |\n",
            "|    iterations         | 85300    |\n",
            "|    time_elapsed       | 3088     |\n",
            "|    total_timesteps    | 3412000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.594   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 85299    |\n",
            "|    policy_loss        | -0.471   |\n",
            "|    value_loss         | 4.01     |\n",
            "------------------------------------\n",
            "Num timesteps: 3416000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 84.90\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 210      |\n",
            "|    ep_rew_mean        | 84.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1105     |\n",
            "|    iterations         | 85400    |\n",
            "|    time_elapsed       | 3091     |\n",
            "|    total_timesteps    | 3416000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.457   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 85399    |\n",
            "|    policy_loss        | 0.073    |\n",
            "|    value_loss         | 2.47     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 217      |\n",
            "|    ep_rew_mean        | 93.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1105     |\n",
            "|    iterations         | 85500    |\n",
            "|    time_elapsed       | 3093     |\n",
            "|    total_timesteps    | 3420000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.521   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 85499    |\n",
            "|    policy_loss        | -0.299   |\n",
            "|    value_loss         | 4.51     |\n",
            "------------------------------------\n",
            "Num timesteps: 3424000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 92.03\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 216      |\n",
            "|    ep_rew_mean        | 92       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1106     |\n",
            "|    iterations         | 85600    |\n",
            "|    time_elapsed       | 3095     |\n",
            "|    total_timesteps    | 3424000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.32    |\n",
            "|    explained_variance | 0.877    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 85599    |\n",
            "|    policy_loss        | 0.0357   |\n",
            "|    value_loss         | 51.7     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 220      |\n",
            "|    ep_rew_mean        | 92.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1106     |\n",
            "|    iterations         | 85700    |\n",
            "|    time_elapsed       | 3098     |\n",
            "|    total_timesteps    | 3428000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.439   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 85699    |\n",
            "|    policy_loss        | 0.206    |\n",
            "|    value_loss         | 1.12     |\n",
            "------------------------------------\n",
            "Num timesteps: 3432000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 95.78\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 228      |\n",
            "|    ep_rew_mean        | 95.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1106     |\n",
            "|    iterations         | 85800    |\n",
            "|    time_elapsed       | 3101     |\n",
            "|    total_timesteps    | 3432000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.186   |\n",
            "|    explained_variance | 0.7      |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 85799    |\n",
            "|    policy_loss        | 0.401    |\n",
            "|    value_loss         | 399      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 246      |\n",
            "|    ep_rew_mean        | 97.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1106     |\n",
            "|    iterations         | 85900    |\n",
            "|    time_elapsed       | 3105     |\n",
            "|    total_timesteps    | 3436000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.404   |\n",
            "|    explained_variance | 0.438    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 85899    |\n",
            "|    policy_loss        | 3.68     |\n",
            "|    value_loss         | 314      |\n",
            "------------------------------------\n",
            "Num timesteps: 3440000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 107.54\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 281      |\n",
            "|    ep_rew_mean        | 108      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1106     |\n",
            "|    iterations         | 86000    |\n",
            "|    time_elapsed       | 3108     |\n",
            "|    total_timesteps    | 3440000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.346   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 85999    |\n",
            "|    policy_loss        | -0.297   |\n",
            "|    value_loss         | 8.22     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 290      |\n",
            "|    ep_rew_mean        | 105      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1106     |\n",
            "|    iterations         | 86100    |\n",
            "|    time_elapsed       | 3111     |\n",
            "|    total_timesteps    | 3444000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.273   |\n",
            "|    explained_variance | 0.947    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 86099    |\n",
            "|    policy_loss        | 1.67     |\n",
            "|    value_loss         | 56.4     |\n",
            "------------------------------------\n",
            "Num timesteps: 3448000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 94.94\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 295      |\n",
            "|    ep_rew_mean        | 94.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1106     |\n",
            "|    iterations         | 86200    |\n",
            "|    time_elapsed       | 3115     |\n",
            "|    total_timesteps    | 3448000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.309   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 86199    |\n",
            "|    policy_loss        | 0.0597   |\n",
            "|    value_loss         | 17.9     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 298      |\n",
            "|    ep_rew_mean        | 91.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1107     |\n",
            "|    iterations         | 86300    |\n",
            "|    time_elapsed       | 3117     |\n",
            "|    total_timesteps    | 3452000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.308   |\n",
            "|    explained_variance | 0.977    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 86299    |\n",
            "|    policy_loss        | 0.218    |\n",
            "|    value_loss         | 8.43     |\n",
            "------------------------------------\n",
            "Num timesteps: 3456000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 87.86\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 304      |\n",
            "|    ep_rew_mean        | 87.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1107     |\n",
            "|    iterations         | 86400    |\n",
            "|    time_elapsed       | 3120     |\n",
            "|    total_timesteps    | 3456000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.216   |\n",
            "|    explained_variance | 0.418    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 86399    |\n",
            "|    policy_loss        | 0.0464   |\n",
            "|    value_loss         | 467      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 300      |\n",
            "|    ep_rew_mean        | 88.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1107     |\n",
            "|    iterations         | 86500    |\n",
            "|    time_elapsed       | 3123     |\n",
            "|    total_timesteps    | 3460000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.333   |\n",
            "|    explained_variance | 0.864    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 86499    |\n",
            "|    policy_loss        | 0.375    |\n",
            "|    value_loss         | 176      |\n",
            "------------------------------------\n",
            "Num timesteps: 3464000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 77.24\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 287      |\n",
            "|    ep_rew_mean        | 77.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1108     |\n",
            "|    iterations         | 86600    |\n",
            "|    time_elapsed       | 3125     |\n",
            "|    total_timesteps    | 3464000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.338   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 86599    |\n",
            "|    policy_loss        | 0.552    |\n",
            "|    value_loss         | 13.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 276      |\n",
            "|    ep_rew_mean        | 70.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1108     |\n",
            "|    iterations         | 86700    |\n",
            "|    time_elapsed       | 3129     |\n",
            "|    total_timesteps    | 3468000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.497   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 86699    |\n",
            "|    policy_loss        | -0.406   |\n",
            "|    value_loss         | 3.23     |\n",
            "------------------------------------\n",
            "Num timesteps: 3472000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 65.92\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 290      |\n",
            "|    ep_rew_mean        | 65.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1108     |\n",
            "|    iterations         | 86800    |\n",
            "|    time_elapsed       | 3132     |\n",
            "|    total_timesteps    | 3472000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.349   |\n",
            "|    explained_variance | 0.405    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 86799    |\n",
            "|    policy_loss        | -0.278   |\n",
            "|    value_loss         | 1.4e+03  |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 290      |\n",
            "|    ep_rew_mean        | 62.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1108     |\n",
            "|    iterations         | 86900    |\n",
            "|    time_elapsed       | 3135     |\n",
            "|    total_timesteps    | 3476000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.438   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 86899    |\n",
            "|    policy_loss        | -0.413   |\n",
            "|    value_loss         | 4.94     |\n",
            "------------------------------------\n",
            "Num timesteps: 3480000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 61.32\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 291      |\n",
            "|    ep_rew_mean        | 61.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1108     |\n",
            "|    iterations         | 87000    |\n",
            "|    time_elapsed       | 3138     |\n",
            "|    total_timesteps    | 3480000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.211   |\n",
            "|    explained_variance | 0.942    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 86999    |\n",
            "|    policy_loss        | 0.314    |\n",
            "|    value_loss         | 112      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 295      |\n",
            "|    ep_rew_mean        | 59.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1108     |\n",
            "|    iterations         | 87100    |\n",
            "|    time_elapsed       | 3142     |\n",
            "|    total_timesteps    | 3484000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.384   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 87099    |\n",
            "|    policy_loss        | -0.5     |\n",
            "|    value_loss         | 3.62     |\n",
            "------------------------------------\n",
            "Num timesteps: 3488000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 57.88\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 306      |\n",
            "|    ep_rew_mean        | 57.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1108     |\n",
            "|    iterations         | 87200    |\n",
            "|    time_elapsed       | 3146     |\n",
            "|    total_timesteps    | 3488000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.337   |\n",
            "|    explained_variance | 0.948    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 87199    |\n",
            "|    policy_loss        | -0.798   |\n",
            "|    value_loss         | 149      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 310      |\n",
            "|    ep_rew_mean        | 59.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1108     |\n",
            "|    iterations         | 87300    |\n",
            "|    time_elapsed       | 3149     |\n",
            "|    total_timesteps    | 3492000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.173   |\n",
            "|    explained_variance | 0.541    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 87299    |\n",
            "|    policy_loss        | 0.848    |\n",
            "|    value_loss         | 1.01e+03 |\n",
            "------------------------------------\n",
            "Num timesteps: 3496000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 69.29\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 322      |\n",
            "|    ep_rew_mean        | 69.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1108     |\n",
            "|    iterations         | 87400    |\n",
            "|    time_elapsed       | 3153     |\n",
            "|    total_timesteps    | 3496000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.313   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 87399    |\n",
            "|    policy_loss        | 0.0743   |\n",
            "|    value_loss         | 6.56     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 319      |\n",
            "|    ep_rew_mean        | 75.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1108     |\n",
            "|    iterations         | 87500    |\n",
            "|    time_elapsed       | 3156     |\n",
            "|    total_timesteps    | 3500000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.378   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 87499    |\n",
            "|    policy_loss        | -0.0357  |\n",
            "|    value_loss         | 2.88     |\n",
            "------------------------------------\n",
            "Num timesteps: 3504000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 84.81\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 311      |\n",
            "|    ep_rew_mean        | 84.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 87600    |\n",
            "|    time_elapsed       | 3159     |\n",
            "|    total_timesteps    | 3504000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.39    |\n",
            "|    explained_variance | 0.873    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 87599    |\n",
            "|    policy_loss        | 1.98     |\n",
            "|    value_loss         | 292      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 299      |\n",
            "|    ep_rew_mean        | 73.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 87700    |\n",
            "|    time_elapsed       | 3161     |\n",
            "|    total_timesteps    | 3508000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.329   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 87699    |\n",
            "|    policy_loss        | 0.0468   |\n",
            "|    value_loss         | 2.01     |\n",
            "------------------------------------\n",
            "Num timesteps: 3512000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 88.53\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 302      |\n",
            "|    ep_rew_mean        | 88.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 87800    |\n",
            "|    time_elapsed       | 3165     |\n",
            "|    total_timesteps    | 3512000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.282   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 87799    |\n",
            "|    policy_loss        | 0.0538   |\n",
            "|    value_loss         | 9.78     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 302      |\n",
            "|    ep_rew_mean        | 94.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 87900    |\n",
            "|    time_elapsed       | 3169     |\n",
            "|    total_timesteps    | 3516000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.252   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 87899    |\n",
            "|    policy_loss        | -0.0503  |\n",
            "|    value_loss         | 1.12     |\n",
            "------------------------------------\n",
            "Num timesteps: 3520000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 101.38\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 308      |\n",
            "|    ep_rew_mean        | 101      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 88000    |\n",
            "|    time_elapsed       | 3173     |\n",
            "|    total_timesteps    | 3520000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.316   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 87999    |\n",
            "|    policy_loss        | -0.284   |\n",
            "|    value_loss         | 1.86     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 310      |\n",
            "|    ep_rew_mean        | 83.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 88100    |\n",
            "|    time_elapsed       | 3176     |\n",
            "|    total_timesteps    | 3524000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.385   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 88099    |\n",
            "|    policy_loss        | 0.0119   |\n",
            "|    value_loss         | 3.58     |\n",
            "------------------------------------\n",
            "Num timesteps: 3528000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 63.62\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 300      |\n",
            "|    ep_rew_mean        | 63.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 88200    |\n",
            "|    time_elapsed       | 3179     |\n",
            "|    total_timesteps    | 3528000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.401   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 88199    |\n",
            "|    policy_loss        | -0.264   |\n",
            "|    value_loss         | 0.93     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 307      |\n",
            "|    ep_rew_mean        | 69.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 88300    |\n",
            "|    time_elapsed       | 3184     |\n",
            "|    total_timesteps    | 3532000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.317   |\n",
            "|    explained_variance | 0.923    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 88299    |\n",
            "|    policy_loss        | 0.3      |\n",
            "|    value_loss         | 43.7     |\n",
            "------------------------------------\n",
            "Num timesteps: 3536000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 72.29\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 311      |\n",
            "|    ep_rew_mean        | 72.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 88400    |\n",
            "|    time_elapsed       | 3187     |\n",
            "|    total_timesteps    | 3536000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.427   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 88399    |\n",
            "|    policy_loss        | 0.309    |\n",
            "|    value_loss         | 8.71     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 322      |\n",
            "|    ep_rew_mean        | 68.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 88500    |\n",
            "|    time_elapsed       | 3191     |\n",
            "|    total_timesteps    | 3540000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.223   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 88499    |\n",
            "|    policy_loss        | -0.123   |\n",
            "|    value_loss         | 2.64     |\n",
            "------------------------------------\n",
            "Num timesteps: 3544000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 62.81\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 321      |\n",
            "|    ep_rew_mean        | 62.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 88600    |\n",
            "|    time_elapsed       | 3194     |\n",
            "|    total_timesteps    | 3544000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.356   |\n",
            "|    explained_variance | 0.971    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 88599    |\n",
            "|    policy_loss        | 0.079    |\n",
            "|    value_loss         | 59.7     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 311      |\n",
            "|    ep_rew_mean        | 59.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 88700    |\n",
            "|    time_elapsed       | 3197     |\n",
            "|    total_timesteps    | 3548000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.361   |\n",
            "|    explained_variance | 0.952    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 88699    |\n",
            "|    policy_loss        | 0.0104   |\n",
            "|    value_loss         | 40.1     |\n",
            "------------------------------------\n",
            "Num timesteps: 3552000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 56.70\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 314      |\n",
            "|    ep_rew_mean        | 56.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 88800    |\n",
            "|    time_elapsed       | 3200     |\n",
            "|    total_timesteps    | 3552000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.513   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 88799    |\n",
            "|    policy_loss        | 0.00905  |\n",
            "|    value_loss         | 4.07     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 300      |\n",
            "|    ep_rew_mean        | 65.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1109     |\n",
            "|    iterations         | 88900    |\n",
            "|    time_elapsed       | 3204     |\n",
            "|    total_timesteps    | 3556000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.308   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 88899    |\n",
            "|    policy_loss        | -0.307   |\n",
            "|    value_loss         | 2.45     |\n",
            "------------------------------------\n",
            "Num timesteps: 3560000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 73.24\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 310      |\n",
            "|    ep_rew_mean        | 73.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1110     |\n",
            "|    iterations         | 89000    |\n",
            "|    time_elapsed       | 3207     |\n",
            "|    total_timesteps    | 3560000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.249   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 88999    |\n",
            "|    policy_loss        | -0.118   |\n",
            "|    value_loss         | 0.875    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 293      |\n",
            "|    ep_rew_mean        | 78.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1110     |\n",
            "|    iterations         | 89100    |\n",
            "|    time_elapsed       | 3209     |\n",
            "|    total_timesteps    | 3564000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.197   |\n",
            "|    explained_variance | 0.962    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 89099    |\n",
            "|    policy_loss        | 1.58     |\n",
            "|    value_loss         | 58.6     |\n",
            "------------------------------------\n",
            "Num timesteps: 3568000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 92.90\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 294      |\n",
            "|    ep_rew_mean        | 92.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1110     |\n",
            "|    iterations         | 89200    |\n",
            "|    time_elapsed       | 3212     |\n",
            "|    total_timesteps    | 3568000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.504   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 89199    |\n",
            "|    policy_loss        | -0.22    |\n",
            "|    value_loss         | 4.53     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 283      |\n",
            "|    ep_rew_mean        | 90.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1111     |\n",
            "|    iterations         | 89300    |\n",
            "|    time_elapsed       | 3215     |\n",
            "|    total_timesteps    | 3572000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.161   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 89299    |\n",
            "|    policy_loss        | 0.085    |\n",
            "|    value_loss         | 0.821    |\n",
            "------------------------------------\n",
            "Num timesteps: 3576000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 99.02\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 276      |\n",
            "|    ep_rew_mean        | 99       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1111     |\n",
            "|    iterations         | 89400    |\n",
            "|    time_elapsed       | 3217     |\n",
            "|    total_timesteps    | 3576000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.326   |\n",
            "|    explained_variance | 0.744    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 89399    |\n",
            "|    policy_loss        | 0.359    |\n",
            "|    value_loss         | 124      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 276      |\n",
            "|    ep_rew_mean        | 108      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1111     |\n",
            "|    iterations         | 89500    |\n",
            "|    time_elapsed       | 3220     |\n",
            "|    total_timesteps    | 3580000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.362   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 89499    |\n",
            "|    policy_loss        | -0.279   |\n",
            "|    value_loss         | 4.25     |\n",
            "------------------------------------\n",
            "Num timesteps: 3584000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 104.92\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 265      |\n",
            "|    ep_rew_mean        | 105      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1111     |\n",
            "|    iterations         | 89600    |\n",
            "|    time_elapsed       | 3223     |\n",
            "|    total_timesteps    | 3584000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.353   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 89599    |\n",
            "|    policy_loss        | 0.185    |\n",
            "|    value_loss         | 6.17     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 263      |\n",
            "|    ep_rew_mean        | 99.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1112     |\n",
            "|    iterations         | 89700    |\n",
            "|    time_elapsed       | 3226     |\n",
            "|    total_timesteps    | 3588000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.484   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 89699    |\n",
            "|    policy_loss        | 0.26     |\n",
            "|    value_loss         | 5.94     |\n",
            "------------------------------------\n",
            "Num timesteps: 3592000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 95.32\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 258      |\n",
            "|    ep_rew_mean        | 95.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1112     |\n",
            "|    iterations         | 89800    |\n",
            "|    time_elapsed       | 3228     |\n",
            "|    total_timesteps    | 3592000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.365   |\n",
            "|    explained_variance | 0.979    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 89799    |\n",
            "|    policy_loss        | 0.108    |\n",
            "|    value_loss         | 14.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 265      |\n",
            "|    ep_rew_mean        | 105      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1112     |\n",
            "|    iterations         | 89900    |\n",
            "|    time_elapsed       | 3232     |\n",
            "|    total_timesteps    | 3596000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.219   |\n",
            "|    explained_variance | 0.429    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 89899    |\n",
            "|    policy_loss        | 0.228    |\n",
            "|    value_loss         | 1.29e+03 |\n",
            "------------------------------------\n",
            "Num timesteps: 3600000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 107.44\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 271      |\n",
            "|    ep_rew_mean        | 107      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1112     |\n",
            "|    iterations         | 90000    |\n",
            "|    time_elapsed       | 3235     |\n",
            "|    total_timesteps    | 3600000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.333   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 89999    |\n",
            "|    policy_loss        | 0.194    |\n",
            "|    value_loss         | 2.81     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 286      |\n",
            "|    ep_rew_mean        | 104      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1112     |\n",
            "|    iterations         | 90100    |\n",
            "|    time_elapsed       | 3239     |\n",
            "|    total_timesteps    | 3604000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.26    |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 90099    |\n",
            "|    policy_loss        | 0.0244   |\n",
            "|    value_loss         | 0.914    |\n",
            "------------------------------------\n",
            "Num timesteps: 3608000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 89.68\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 271      |\n",
            "|    ep_rew_mean        | 89.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1113     |\n",
            "|    iterations         | 90200    |\n",
            "|    time_elapsed       | 3241     |\n",
            "|    total_timesteps    | 3608000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.39    |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 90199    |\n",
            "|    policy_loss        | -0.0513  |\n",
            "|    value_loss         | 4.66     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 270      |\n",
            "|    ep_rew_mean        | 94.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1113     |\n",
            "|    iterations         | 90300    |\n",
            "|    time_elapsed       | 3244     |\n",
            "|    total_timesteps    | 3612000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.324   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 90299    |\n",
            "|    policy_loss        | -0.0305  |\n",
            "|    value_loss         | 0.887    |\n",
            "------------------------------------\n",
            "Num timesteps: 3616000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 96.57\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 290      |\n",
            "|    ep_rew_mean        | 96.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1113     |\n",
            "|    iterations         | 90400    |\n",
            "|    time_elapsed       | 3247     |\n",
            "|    total_timesteps    | 3616000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.408   |\n",
            "|    explained_variance | 0.685    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 90399    |\n",
            "|    policy_loss        | -0.26    |\n",
            "|    value_loss         | 506      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 292      |\n",
            "|    ep_rew_mean        | 98.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1113     |\n",
            "|    iterations         | 90500    |\n",
            "|    time_elapsed       | 3250     |\n",
            "|    total_timesteps    | 3620000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.282   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 90499    |\n",
            "|    policy_loss        | -0.236   |\n",
            "|    value_loss         | 0.976    |\n",
            "------------------------------------\n",
            "Num timesteps: 3624000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 88.03\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 315      |\n",
            "|    ep_rew_mean        | 88       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1113     |\n",
            "|    iterations         | 90600    |\n",
            "|    time_elapsed       | 3254     |\n",
            "|    total_timesteps    | 3624000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.387   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 90599    |\n",
            "|    policy_loss        | 0.0859   |\n",
            "|    value_loss         | 2.02     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 307      |\n",
            "|    ep_rew_mean        | 79.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1113     |\n",
            "|    iterations         | 90700    |\n",
            "|    time_elapsed       | 3257     |\n",
            "|    total_timesteps    | 3628000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.419   |\n",
            "|    explained_variance | 0.519    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 90699    |\n",
            "|    policy_loss        | -0.143   |\n",
            "|    value_loss         | 117      |\n",
            "------------------------------------\n",
            "Num timesteps: 3632000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 75.90\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 312      |\n",
            "|    ep_rew_mean        | 75.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1113     |\n",
            "|    iterations         | 90800    |\n",
            "|    time_elapsed       | 3260     |\n",
            "|    total_timesteps    | 3632000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.388   |\n",
            "|    explained_variance | 0.839    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 90799    |\n",
            "|    policy_loss        | 0.221    |\n",
            "|    value_loss         | 352      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 306      |\n",
            "|    ep_rew_mean        | 75.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1113     |\n",
            "|    iterations         | 90900    |\n",
            "|    time_elapsed       | 3264     |\n",
            "|    total_timesteps    | 3636000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.318   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 90899    |\n",
            "|    policy_loss        | -0.172   |\n",
            "|    value_loss         | 2.71     |\n",
            "------------------------------------\n",
            "Num timesteps: 3640000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 86.96\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 333      |\n",
            "|    ep_rew_mean        | 87       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1113     |\n",
            "|    iterations         | 91000    |\n",
            "|    time_elapsed       | 3268     |\n",
            "|    total_timesteps    | 3640000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.339   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 90999    |\n",
            "|    policy_loss        | 0.568    |\n",
            "|    value_loss         | 2.46     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 346      |\n",
            "|    ep_rew_mean        | 99.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1113     |\n",
            "|    iterations         | 91100    |\n",
            "|    time_elapsed       | 3271     |\n",
            "|    total_timesteps    | 3644000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.282   |\n",
            "|    explained_variance | 0.764    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 91099    |\n",
            "|    policy_loss        | 0.00486  |\n",
            "|    value_loss         | 172      |\n",
            "------------------------------------\n",
            "Num timesteps: 3648000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 101.15\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 326      |\n",
            "|    ep_rew_mean        | 101      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1114     |\n",
            "|    iterations         | 91200    |\n",
            "|    time_elapsed       | 3273     |\n",
            "|    total_timesteps    | 3648000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.418   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 91199    |\n",
            "|    policy_loss        | 0.0561   |\n",
            "|    value_loss         | 1.74     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 335      |\n",
            "|    ep_rew_mean        | 106      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1114     |\n",
            "|    iterations         | 91300    |\n",
            "|    time_elapsed       | 3276     |\n",
            "|    total_timesteps    | 3652000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.301   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 91299    |\n",
            "|    policy_loss        | 0.179    |\n",
            "|    value_loss         | 1.08     |\n",
            "------------------------------------\n",
            "Num timesteps: 3656000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 110.93\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 323      |\n",
            "|    ep_rew_mean        | 111      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1114     |\n",
            "|    iterations         | 91400    |\n",
            "|    time_elapsed       | 3279     |\n",
            "|    total_timesteps    | 3656000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.423   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 91399    |\n",
            "|    policy_loss        | 0.152    |\n",
            "|    value_loss         | 5.53     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 319      |\n",
            "|    ep_rew_mean        | 109      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1114     |\n",
            "|    iterations         | 91500    |\n",
            "|    time_elapsed       | 3283     |\n",
            "|    total_timesteps    | 3660000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.353   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 91499    |\n",
            "|    policy_loss        | -0.13    |\n",
            "|    value_loss         | 7.81     |\n",
            "------------------------------------\n",
            "Num timesteps: 3664000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 108.02\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 317      |\n",
            "|    ep_rew_mean        | 108      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1115     |\n",
            "|    iterations         | 91600    |\n",
            "|    time_elapsed       | 3285     |\n",
            "|    total_timesteps    | 3664000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.335   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 91599    |\n",
            "|    policy_loss        | -0.258   |\n",
            "|    value_loss         | 5.1      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 305      |\n",
            "|    ep_rew_mean        | 100      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1115     |\n",
            "|    iterations         | 91700    |\n",
            "|    time_elapsed       | 3288     |\n",
            "|    total_timesteps    | 3668000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.457   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 91699    |\n",
            "|    policy_loss        | 0.311    |\n",
            "|    value_loss         | 6.62     |\n",
            "------------------------------------\n",
            "Num timesteps: 3672000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 97.00\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 292      |\n",
            "|    ep_rew_mean        | 97       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1115     |\n",
            "|    iterations         | 91800    |\n",
            "|    time_elapsed       | 3292     |\n",
            "|    total_timesteps    | 3672000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.276   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 91799    |\n",
            "|    policy_loss        | 0.197    |\n",
            "|    value_loss         | 0.873    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 315      |\n",
            "|    ep_rew_mean        | 80.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1115     |\n",
            "|    iterations         | 91900    |\n",
            "|    time_elapsed       | 3296     |\n",
            "|    total_timesteps    | 3676000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.345   |\n",
            "|    explained_variance | 0.451    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 91899    |\n",
            "|    policy_loss        | -0.344   |\n",
            "|    value_loss         | 447      |\n",
            "------------------------------------\n",
            "Num timesteps: 3680000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 79.87\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 319      |\n",
            "|    ep_rew_mean        | 79.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1115     |\n",
            "|    iterations         | 92000    |\n",
            "|    time_elapsed       | 3299     |\n",
            "|    total_timesteps    | 3680000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.316   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 91999    |\n",
            "|    policy_loss        | -0.206   |\n",
            "|    value_loss         | 10.8     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 310      |\n",
            "|    ep_rew_mean        | 78.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1115     |\n",
            "|    iterations         | 92100    |\n",
            "|    time_elapsed       | 3302     |\n",
            "|    total_timesteps    | 3684000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.338   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 92099    |\n",
            "|    policy_loss        | -0.335   |\n",
            "|    value_loss         | 1.52     |\n",
            "------------------------------------\n",
            "Num timesteps: 3688000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 77.61\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 314      |\n",
            "|    ep_rew_mean        | 77.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1115     |\n",
            "|    iterations         | 92200    |\n",
            "|    time_elapsed       | 3305     |\n",
            "|    total_timesteps    | 3688000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.328   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 92199    |\n",
            "|    policy_loss        | -0.0229  |\n",
            "|    value_loss         | 10       |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 302      |\n",
            "|    ep_rew_mean        | 80.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1115     |\n",
            "|    iterations         | 92300    |\n",
            "|    time_elapsed       | 3308     |\n",
            "|    total_timesteps    | 3692000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.46    |\n",
            "|    explained_variance | 0.948    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 92299    |\n",
            "|    policy_loss        | 2.17     |\n",
            "|    value_loss         | 115      |\n",
            "------------------------------------\n",
            "Num timesteps: 3696000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 79.83\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 294      |\n",
            "|    ep_rew_mean        | 79.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1116     |\n",
            "|    iterations         | 92400    |\n",
            "|    time_elapsed       | 3311     |\n",
            "|    total_timesteps    | 3696000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.39    |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 92399    |\n",
            "|    policy_loss        | 0.568    |\n",
            "|    value_loss         | 4.95     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 264      |\n",
            "|    ep_rew_mean        | 83.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1116     |\n",
            "|    iterations         | 92500    |\n",
            "|    time_elapsed       | 3313     |\n",
            "|    total_timesteps    | 3700000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.305   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 92499    |\n",
            "|    policy_loss        | 0.666    |\n",
            "|    value_loss         | 5.16     |\n",
            "------------------------------------\n",
            "Num timesteps: 3704000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 80.73\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 252      |\n",
            "|    ep_rew_mean        | 80.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1116     |\n",
            "|    iterations         | 92600    |\n",
            "|    time_elapsed       | 3316     |\n",
            "|    total_timesteps    | 3704000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.35    |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 92599    |\n",
            "|    policy_loss        | -0.0707  |\n",
            "|    value_loss         | 2.33     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 256      |\n",
            "|    ep_rew_mean        | 78       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 92700    |\n",
            "|    time_elapsed       | 3319     |\n",
            "|    total_timesteps    | 3708000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.323   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 92699    |\n",
            "|    policy_loss        | 0.292    |\n",
            "|    value_loss         | 2.75     |\n",
            "------------------------------------\n",
            "Num timesteps: 3712000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 74.23\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 247      |\n",
            "|    ep_rew_mean        | 74.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 92800    |\n",
            "|    time_elapsed       | 3321     |\n",
            "|    total_timesteps    | 3712000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.275   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 92799    |\n",
            "|    policy_loss        | 0.0292   |\n",
            "|    value_loss         | 5.92     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 246      |\n",
            "|    ep_rew_mean        | 72.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 92900    |\n",
            "|    time_elapsed       | 3324     |\n",
            "|    total_timesteps    | 3716000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.381   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 92899    |\n",
            "|    policy_loss        | -0.401   |\n",
            "|    value_loss         | 2.94     |\n",
            "------------------------------------\n",
            "Num timesteps: 3720000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 73.08\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 252      |\n",
            "|    ep_rew_mean        | 73.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 93000    |\n",
            "|    time_elapsed       | 3328     |\n",
            "|    total_timesteps    | 3720000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.226   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 92999    |\n",
            "|    policy_loss        | -0.0794  |\n",
            "|    value_loss         | 2.22     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 277      |\n",
            "|    ep_rew_mean        | 71.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 93100    |\n",
            "|    time_elapsed       | 3332     |\n",
            "|    total_timesteps    | 3724000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.345   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 93099    |\n",
            "|    policy_loss        | 0.146    |\n",
            "|    value_loss         | 5.54     |\n",
            "------------------------------------\n",
            "Num timesteps: 3728000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 73.48\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 299      |\n",
            "|    ep_rew_mean        | 73.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 93200    |\n",
            "|    time_elapsed       | 3335     |\n",
            "|    total_timesteps    | 3728000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.342   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 93199    |\n",
            "|    policy_loss        | -0.138   |\n",
            "|    value_loss         | 8.21     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 316      |\n",
            "|    ep_rew_mean        | 84.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 93300    |\n",
            "|    time_elapsed       | 3338     |\n",
            "|    total_timesteps    | 3732000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.406   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 93299    |\n",
            "|    policy_loss        | 0.368    |\n",
            "|    value_loss         | 2.66     |\n",
            "------------------------------------\n",
            "Num timesteps: 3736000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 91.29\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 321      |\n",
            "|    ep_rew_mean        | 91.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1117     |\n",
            "|    iterations         | 93400    |\n",
            "|    time_elapsed       | 3341     |\n",
            "|    total_timesteps    | 3736000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.407   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 93399    |\n",
            "|    policy_loss        | -0.145   |\n",
            "|    value_loss         | 1.75     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 334      |\n",
            "|    ep_rew_mean        | 103      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1118     |\n",
            "|    iterations         | 93500    |\n",
            "|    time_elapsed       | 3344     |\n",
            "|    total_timesteps    | 3740000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.549   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 93499    |\n",
            "|    policy_loss        | -0.34    |\n",
            "|    value_loss         | 2.73     |\n",
            "------------------------------------\n",
            "Num timesteps: 3744000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 97.89\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 322      |\n",
            "|    ep_rew_mean        | 97.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1118     |\n",
            "|    iterations         | 93600    |\n",
            "|    time_elapsed       | 3347     |\n",
            "|    total_timesteps    | 3744000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.194   |\n",
            "|    explained_variance | 0.973    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 93599    |\n",
            "|    policy_loss        | 0.0724   |\n",
            "|    value_loss         | 78.1     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 327      |\n",
            "|    ep_rew_mean        | 95.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1118     |\n",
            "|    iterations         | 93700    |\n",
            "|    time_elapsed       | 3350     |\n",
            "|    total_timesteps    | 3748000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.245   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 93699    |\n",
            "|    policy_loss        | 0.00825  |\n",
            "|    value_loss         | 18.9     |\n",
            "------------------------------------\n",
            "Num timesteps: 3752000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 109.38\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 294      |\n",
            "|    ep_rew_mean        | 109      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1119     |\n",
            "|    iterations         | 93800    |\n",
            "|    time_elapsed       | 3352     |\n",
            "|    total_timesteps    | 3752000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.315   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 93799    |\n",
            "|    policy_loss        | -0.0488  |\n",
            "|    value_loss         | 1.33     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 272      |\n",
            "|    ep_rew_mean        | 104      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1119     |\n",
            "|    iterations         | 93900    |\n",
            "|    time_elapsed       | 3355     |\n",
            "|    total_timesteps    | 3756000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.438   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 93899    |\n",
            "|    policy_loss        | -0.161   |\n",
            "|    value_loss         | 1.37     |\n",
            "------------------------------------\n",
            "Num timesteps: 3760000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 102.52\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 259      |\n",
            "|    ep_rew_mean        | 103      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1119     |\n",
            "|    iterations         | 94000    |\n",
            "|    time_elapsed       | 3357     |\n",
            "|    total_timesteps    | 3760000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.256   |\n",
            "|    explained_variance | 0.959    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 93999    |\n",
            "|    policy_loss        | 0.0236   |\n",
            "|    value_loss         | 55.7     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 245      |\n",
            "|    ep_rew_mean        | 102      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1120     |\n",
            "|    iterations         | 94100    |\n",
            "|    time_elapsed       | 3359     |\n",
            "|    total_timesteps    | 3764000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.339   |\n",
            "|    explained_variance | 0.93     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 94099    |\n",
            "|    policy_loss        | -0.115   |\n",
            "|    value_loss         | 111      |\n",
            "------------------------------------\n",
            "Num timesteps: 3768000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 112.08\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 251      |\n",
            "|    ep_rew_mean        | 112      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1120     |\n",
            "|    iterations         | 94200    |\n",
            "|    time_elapsed       | 3362     |\n",
            "|    total_timesteps    | 3768000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.277   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 94199    |\n",
            "|    policy_loss        | -0.391   |\n",
            "|    value_loss         | 4.59     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 250      |\n",
            "|    ep_rew_mean        | 111      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1120     |\n",
            "|    iterations         | 94300    |\n",
            "|    time_elapsed       | 3366     |\n",
            "|    total_timesteps    | 3772000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.426   |\n",
            "|    explained_variance | 0.961    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 94299    |\n",
            "|    policy_loss        | 0.0927   |\n",
            "|    value_loss         | 17.9     |\n",
            "------------------------------------\n",
            "Num timesteps: 3776000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 102.04\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 266      |\n",
            "|    ep_rew_mean        | 102      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1120     |\n",
            "|    iterations         | 94400    |\n",
            "|    time_elapsed       | 3369     |\n",
            "|    total_timesteps    | 3776000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.29    |\n",
            "|    explained_variance | 0.951    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 94399    |\n",
            "|    policy_loss        | 0.0412   |\n",
            "|    value_loss         | 65.6     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 267      |\n",
            "|    ep_rew_mean        | 99.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1120     |\n",
            "|    iterations         | 94500    |\n",
            "|    time_elapsed       | 3372     |\n",
            "|    total_timesteps    | 3780000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.266   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 94499    |\n",
            "|    policy_loss        | 0.13     |\n",
            "|    value_loss         | 2.27     |\n",
            "------------------------------------\n",
            "Num timesteps: 3784000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 106.55\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 271      |\n",
            "|    ep_rew_mean        | 107      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1121     |\n",
            "|    iterations         | 94600    |\n",
            "|    time_elapsed       | 3375     |\n",
            "|    total_timesteps    | 3784000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.152   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 94599    |\n",
            "|    policy_loss        | -0.00985 |\n",
            "|    value_loss         | 1.34     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 274      |\n",
            "|    ep_rew_mean        | 112      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1121     |\n",
            "|    iterations         | 94700    |\n",
            "|    time_elapsed       | 3377     |\n",
            "|    total_timesteps    | 3788000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.292   |\n",
            "|    explained_variance | 0.491    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 94699    |\n",
            "|    policy_loss        | -0.146   |\n",
            "|    value_loss         | 1.84e+03 |\n",
            "------------------------------------\n",
            "Num timesteps: 3792000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 112.29\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 275      |\n",
            "|    ep_rew_mean        | 112      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1121     |\n",
            "|    iterations         | 94800    |\n",
            "|    time_elapsed       | 3380     |\n",
            "|    total_timesteps    | 3792000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.256   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 94799    |\n",
            "|    policy_loss        | 0.583    |\n",
            "|    value_loss         | 5.06     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 271      |\n",
            "|    ep_rew_mean        | 111      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1122     |\n",
            "|    iterations         | 94900    |\n",
            "|    time_elapsed       | 3383     |\n",
            "|    total_timesteps    | 3796000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.399   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 94899    |\n",
            "|    policy_loss        | -0.114   |\n",
            "|    value_loss         | 2.28     |\n",
            "------------------------------------\n",
            "Num timesteps: 3800000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 113.85\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 261      |\n",
            "|    ep_rew_mean        | 114      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1122     |\n",
            "|    iterations         | 95000    |\n",
            "|    time_elapsed       | 3385     |\n",
            "|    total_timesteps    | 3800000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.38    |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 94999    |\n",
            "|    policy_loss        | 0.116    |\n",
            "|    value_loss         | 2.47     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 262      |\n",
            "|    ep_rew_mean        | 110      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1122     |\n",
            "|    iterations         | 95100    |\n",
            "|    time_elapsed       | 3389     |\n",
            "|    total_timesteps    | 3804000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.297   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 95099    |\n",
            "|    policy_loss        | 0.13     |\n",
            "|    value_loss         | 1.23     |\n",
            "------------------------------------\n",
            "Num timesteps: 3808000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 115.65\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 254      |\n",
            "|    ep_rew_mean        | 116      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1122     |\n",
            "|    iterations         | 95200    |\n",
            "|    time_elapsed       | 3391     |\n",
            "|    total_timesteps    | 3808000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.366   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 95199    |\n",
            "|    policy_loss        | -0.236   |\n",
            "|    value_loss         | 0.69     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 258      |\n",
            "|    ep_rew_mean        | 113      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1122     |\n",
            "|    iterations         | 95300    |\n",
            "|    time_elapsed       | 3394     |\n",
            "|    total_timesteps    | 3812000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.383   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 95299    |\n",
            "|    policy_loss        | -0.478   |\n",
            "|    value_loss         | 1.87     |\n",
            "------------------------------------\n",
            "Num timesteps: 3816000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 107.76\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 268      |\n",
            "|    ep_rew_mean        | 108      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1123     |\n",
            "|    iterations         | 95400    |\n",
            "|    time_elapsed       | 3397     |\n",
            "|    total_timesteps    | 3816000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.291   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 95399    |\n",
            "|    policy_loss        | -0.073   |\n",
            "|    value_loss         | 11.9     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 263      |\n",
            "|    ep_rew_mean        | 103      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1123     |\n",
            "|    iterations         | 95500    |\n",
            "|    time_elapsed       | 3399     |\n",
            "|    total_timesteps    | 3820000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.334   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 95499    |\n",
            "|    policy_loss        | -0.46    |\n",
            "|    value_loss         | 3.91     |\n",
            "------------------------------------\n",
            "Num timesteps: 3824000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 104.77\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 248      |\n",
            "|    ep_rew_mean        | 105      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1124     |\n",
            "|    iterations         | 95600    |\n",
            "|    time_elapsed       | 3401     |\n",
            "|    total_timesteps    | 3824000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.266   |\n",
            "|    explained_variance | 0.87     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 95599    |\n",
            "|    policy_loss        | 1.48     |\n",
            "|    value_loss         | 366      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 226      |\n",
            "|    ep_rew_mean        | 94.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1124     |\n",
            "|    iterations         | 95700    |\n",
            "|    time_elapsed       | 3404     |\n",
            "|    total_timesteps    | 3828000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.18    |\n",
            "|    explained_variance | 0.0338   |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 95699    |\n",
            "|    policy_loss        | -0.0517  |\n",
            "|    value_loss         | 2.06e+03 |\n",
            "------------------------------------\n",
            "Num timesteps: 3832000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 85.47\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 215      |\n",
            "|    ep_rew_mean        | 85.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1124     |\n",
            "|    iterations         | 95800    |\n",
            "|    time_elapsed       | 3406     |\n",
            "|    total_timesteps    | 3832000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.3     |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 95799    |\n",
            "|    policy_loss        | -0.459   |\n",
            "|    value_loss         | 19.1     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 208      |\n",
            "|    ep_rew_mean        | 95.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1125     |\n",
            "|    iterations         | 95900    |\n",
            "|    time_elapsed       | 3409     |\n",
            "|    total_timesteps    | 3836000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.333   |\n",
            "|    explained_variance | 0.94     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 95899    |\n",
            "|    policy_loss        | -0.424   |\n",
            "|    value_loss         | 47.2     |\n",
            "------------------------------------\n",
            "Num timesteps: 3840000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 92.93\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 211      |\n",
            "|    ep_rew_mean        | 92.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1125     |\n",
            "|    iterations         | 96000    |\n",
            "|    time_elapsed       | 3411     |\n",
            "|    total_timesteps    | 3840000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.326   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 95999    |\n",
            "|    policy_loss        | 0.0381   |\n",
            "|    value_loss         | 1.2      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 213      |\n",
            "|    ep_rew_mean        | 104      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1125     |\n",
            "|    iterations         | 96100    |\n",
            "|    time_elapsed       | 3414     |\n",
            "|    total_timesteps    | 3844000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.476   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 96099    |\n",
            "|    policy_loss        | -0.117   |\n",
            "|    value_loss         | 2.69     |\n",
            "------------------------------------\n",
            "Num timesteps: 3848000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 116.19\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 226      |\n",
            "|    ep_rew_mean        | 116      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1126     |\n",
            "|    iterations         | 96200    |\n",
            "|    time_elapsed       | 3416     |\n",
            "|    total_timesteps    | 3848000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.239   |\n",
            "|    explained_variance | 0.944    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 96199    |\n",
            "|    policy_loss        | 0.00809  |\n",
            "|    value_loss         | 129      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 232      |\n",
            "|    ep_rew_mean        | 118      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1126     |\n",
            "|    iterations         | 96300    |\n",
            "|    time_elapsed       | 3419     |\n",
            "|    total_timesteps    | 3852000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.313   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 96299    |\n",
            "|    policy_loss        | 0.23     |\n",
            "|    value_loss         | 11.4     |\n",
            "------------------------------------\n",
            "Num timesteps: 3856000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 129.96\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 242      |\n",
            "|    ep_rew_mean        | 130      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1126     |\n",
            "|    iterations         | 96400    |\n",
            "|    time_elapsed       | 3422     |\n",
            "|    total_timesteps    | 3856000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.345   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 96399    |\n",
            "|    policy_loss        | -0.221   |\n",
            "|    value_loss         | 1.44     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 246      |\n",
            "|    ep_rew_mean        | 121      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1126     |\n",
            "|    iterations         | 96500    |\n",
            "|    time_elapsed       | 3426     |\n",
            "|    total_timesteps    | 3860000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.332   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 96499    |\n",
            "|    policy_loss        | -0.154   |\n",
            "|    value_loss         | 4.13     |\n",
            "------------------------------------\n",
            "Num timesteps: 3864000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 112.43\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 261      |\n",
            "|    ep_rew_mean        | 112      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1126     |\n",
            "|    iterations         | 96600    |\n",
            "|    time_elapsed       | 3429     |\n",
            "|    total_timesteps    | 3864000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.359   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 96599    |\n",
            "|    policy_loss        | -0.309   |\n",
            "|    value_loss         | 11.7     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 280      |\n",
            "|    ep_rew_mean        | 106      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1126     |\n",
            "|    iterations         | 96700    |\n",
            "|    time_elapsed       | 3433     |\n",
            "|    total_timesteps    | 3868000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.34    |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 96699    |\n",
            "|    policy_loss        | -0.122   |\n",
            "|    value_loss         | 1.92     |\n",
            "------------------------------------\n",
            "Num timesteps: 3872000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 107.83\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 283      |\n",
            "|    ep_rew_mean        | 108      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1126     |\n",
            "|    iterations         | 96800    |\n",
            "|    time_elapsed       | 3436     |\n",
            "|    total_timesteps    | 3872000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.241   |\n",
            "|    explained_variance | 0.502    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 96799    |\n",
            "|    policy_loss        | 0.478    |\n",
            "|    value_loss         | 1.47e+03 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 288      |\n",
            "|    ep_rew_mean        | 97.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1127     |\n",
            "|    iterations         | 96900    |\n",
            "|    time_elapsed       | 3438     |\n",
            "|    total_timesteps    | 3876000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.307   |\n",
            "|    explained_variance | 0.94     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 96899    |\n",
            "|    policy_loss        | -0.148   |\n",
            "|    value_loss         | 63.7     |\n",
            "------------------------------------\n",
            "Num timesteps: 3880000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 96.51\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 298      |\n",
            "|    ep_rew_mean        | 96.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1127     |\n",
            "|    iterations         | 97000    |\n",
            "|    time_elapsed       | 3441     |\n",
            "|    total_timesteps    | 3880000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.453   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 96999    |\n",
            "|    policy_loss        | -0.324   |\n",
            "|    value_loss         | 2.07     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 297      |\n",
            "|    ep_rew_mean        | 91.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1127     |\n",
            "|    iterations         | 97100    |\n",
            "|    time_elapsed       | 3444     |\n",
            "|    total_timesteps    | 3884000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.403   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 97099    |\n",
            "|    policy_loss        | -0.0998  |\n",
            "|    value_loss         | 2.05     |\n",
            "------------------------------------\n",
            "Num timesteps: 3888000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 90.08\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 285      |\n",
            "|    ep_rew_mean        | 90.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1127     |\n",
            "|    iterations         | 97200    |\n",
            "|    time_elapsed       | 3446     |\n",
            "|    total_timesteps    | 3888000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.337   |\n",
            "|    explained_variance | 0.86     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 97199    |\n",
            "|    policy_loss        | 0.391    |\n",
            "|    value_loss         | 84.1     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 276      |\n",
            "|    ep_rew_mean        | 107      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1128     |\n",
            "|    iterations         | 97300    |\n",
            "|    time_elapsed       | 3449     |\n",
            "|    total_timesteps    | 3892000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.33    |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 97299    |\n",
            "|    policy_loss        | -0.393   |\n",
            "|    value_loss         | 3.06     |\n",
            "------------------------------------\n",
            "Num timesteps: 3896000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 127.03\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 262      |\n",
            "|    ep_rew_mean        | 127      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1128     |\n",
            "|    iterations         | 97400    |\n",
            "|    time_elapsed       | 3452     |\n",
            "|    total_timesteps    | 3896000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.366   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 97399    |\n",
            "|    policy_loss        | -0.379   |\n",
            "|    value_loss         | 1.5      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 264      |\n",
            "|    ep_rew_mean        | 125      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1128     |\n",
            "|    iterations         | 97500    |\n",
            "|    time_elapsed       | 3454     |\n",
            "|    total_timesteps    | 3900000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.311   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 97499    |\n",
            "|    policy_loss        | -0.277   |\n",
            "|    value_loss         | 1.85     |\n",
            "------------------------------------\n",
            "Num timesteps: 3904000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 126.14\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 269      |\n",
            "|    ep_rew_mean        | 126      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1129     |\n",
            "|    iterations         | 97600    |\n",
            "|    time_elapsed       | 3457     |\n",
            "|    total_timesteps    | 3904000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.239   |\n",
            "|    explained_variance | 0.971    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 97599    |\n",
            "|    policy_loss        | -0.0965  |\n",
            "|    value_loss         | 21       |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 268      |\n",
            "|    ep_rew_mean        | 140      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1129     |\n",
            "|    iterations         | 97700    |\n",
            "|    time_elapsed       | 3459     |\n",
            "|    total_timesteps    | 3908000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.278   |\n",
            "|    explained_variance | 0.207    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 97699    |\n",
            "|    policy_loss        | 0.00886  |\n",
            "|    value_loss         | 365      |\n",
            "------------------------------------\n",
            "Num timesteps: 3912000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 141.41\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 266      |\n",
            "|    ep_rew_mean        | 141      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1129     |\n",
            "|    iterations         | 97800    |\n",
            "|    time_elapsed       | 3462     |\n",
            "|    total_timesteps    | 3912000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.3     |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 97799    |\n",
            "|    policy_loss        | -0.538   |\n",
            "|    value_loss         | 1.41     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 269      |\n",
            "|    ep_rew_mean        | 145      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1129     |\n",
            "|    iterations         | 97900    |\n",
            "|    time_elapsed       | 3465     |\n",
            "|    total_timesteps    | 3916000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.252   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 97899    |\n",
            "|    policy_loss        | 0.13     |\n",
            "|    value_loss         | 2.09     |\n",
            "------------------------------------\n",
            "Num timesteps: 3920000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 128.73\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 277      |\n",
            "|    ep_rew_mean        | 129      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1130     |\n",
            "|    iterations         | 98000    |\n",
            "|    time_elapsed       | 3468     |\n",
            "|    total_timesteps    | 3920000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.334   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 97999    |\n",
            "|    policy_loss        | -0.236   |\n",
            "|    value_loss         | 4.3      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 270      |\n",
            "|    ep_rew_mean        | 107      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1130     |\n",
            "|    iterations         | 98100    |\n",
            "|    time_elapsed       | 3471     |\n",
            "|    total_timesteps    | 3924000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.243   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 98099    |\n",
            "|    policy_loss        | 0.00855  |\n",
            "|    value_loss         | 1.04     |\n",
            "------------------------------------\n",
            "Num timesteps: 3928000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 117.61\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 282      |\n",
            "|    ep_rew_mean        | 118      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1130     |\n",
            "|    iterations         | 98200    |\n",
            "|    time_elapsed       | 3475     |\n",
            "|    total_timesteps    | 3928000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.221   |\n",
            "|    explained_variance | 0.862    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 98199    |\n",
            "|    policy_loss        | 2.58     |\n",
            "|    value_loss         | 149      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 290      |\n",
            "|    ep_rew_mean        | 124      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1130     |\n",
            "|    iterations         | 98300    |\n",
            "|    time_elapsed       | 3478     |\n",
            "|    total_timesteps    | 3932000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.299   |\n",
            "|    explained_variance | 0.762    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 98299    |\n",
            "|    policy_loss        | 0.845    |\n",
            "|    value_loss         | 169      |\n",
            "------------------------------------\n",
            "Num timesteps: 3936000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 113.31\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 293      |\n",
            "|    ep_rew_mean        | 113      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1130     |\n",
            "|    iterations         | 98400    |\n",
            "|    time_elapsed       | 3481     |\n",
            "|    total_timesteps    | 3936000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.333   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 98399    |\n",
            "|    policy_loss        | -0.0758  |\n",
            "|    value_loss         | 1.09     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 306      |\n",
            "|    ep_rew_mean        | 110      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1130     |\n",
            "|    iterations         | 98500    |\n",
            "|    time_elapsed       | 3485     |\n",
            "|    total_timesteps    | 3940000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.199   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 98499    |\n",
            "|    policy_loss        | 0.143    |\n",
            "|    value_loss         | 8.03     |\n",
            "------------------------------------\n",
            "Num timesteps: 3944000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 100.48\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 312      |\n",
            "|    ep_rew_mean        | 100      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1130     |\n",
            "|    iterations         | 98600    |\n",
            "|    time_elapsed       | 3487     |\n",
            "|    total_timesteps    | 3944000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.293   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 98599    |\n",
            "|    policy_loss        | -0.434   |\n",
            "|    value_loss         | 2.91     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 309      |\n",
            "|    ep_rew_mean        | 97.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1130     |\n",
            "|    iterations         | 98700    |\n",
            "|    time_elapsed       | 3490     |\n",
            "|    total_timesteps    | 3948000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.302   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 98699    |\n",
            "|    policy_loss        | 0.000303 |\n",
            "|    value_loss         | 1.73     |\n",
            "------------------------------------\n",
            "Num timesteps: 3952000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 100.12\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 319      |\n",
            "|    ep_rew_mean        | 100      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1130     |\n",
            "|    iterations         | 98800    |\n",
            "|    time_elapsed       | 3495     |\n",
            "|    total_timesteps    | 3952000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.439   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 98799    |\n",
            "|    policy_loss        | -0.184   |\n",
            "|    value_loss         | 3.14     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 338      |\n",
            "|    ep_rew_mean        | 111      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1130     |\n",
            "|    iterations         | 98900    |\n",
            "|    time_elapsed       | 3499     |\n",
            "|    total_timesteps    | 3956000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.26    |\n",
            "|    explained_variance | 0.974    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 98899    |\n",
            "|    policy_loss        | -0.529   |\n",
            "|    value_loss         | 32.6     |\n",
            "------------------------------------\n",
            "Num timesteps: 3960000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 102.72\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 338      |\n",
            "|    ep_rew_mean        | 103      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1130     |\n",
            "|    iterations         | 99000    |\n",
            "|    time_elapsed       | 3503     |\n",
            "|    total_timesteps    | 3960000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.266   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 98999    |\n",
            "|    policy_loss        | 0.361    |\n",
            "|    value_loss         | 4.65     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 332      |\n",
            "|    ep_rew_mean        | 89.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1130     |\n",
            "|    iterations         | 99100    |\n",
            "|    time_elapsed       | 3505     |\n",
            "|    total_timesteps    | 3964000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.283   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 99099    |\n",
            "|    policy_loss        | -0.216   |\n",
            "|    value_loss         | 1.4      |\n",
            "------------------------------------\n",
            "Num timesteps: 3968000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 96.22\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 326      |\n",
            "|    ep_rew_mean        | 96.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1130     |\n",
            "|    iterations         | 99200    |\n",
            "|    time_elapsed       | 3508     |\n",
            "|    total_timesteps    | 3968000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.381   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 99199    |\n",
            "|    policy_loss        | 0.426    |\n",
            "|    value_loss         | 9.7      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 331      |\n",
            "|    ep_rew_mean        | 99.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1130     |\n",
            "|    iterations         | 99300    |\n",
            "|    time_elapsed       | 3512     |\n",
            "|    total_timesteps    | 3972000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.282   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 99299    |\n",
            "|    policy_loss        | -0.116   |\n",
            "|    value_loss         | 3.38     |\n",
            "------------------------------------\n",
            "Num timesteps: 3976000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 97.16\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 333      |\n",
            "|    ep_rew_mean        | 97.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1130     |\n",
            "|    iterations         | 99400    |\n",
            "|    time_elapsed       | 3515     |\n",
            "|    total_timesteps    | 3976000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.306   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 99399    |\n",
            "|    policy_loss        | -0.0367  |\n",
            "|    value_loss         | 3.48     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 341      |\n",
            "|    ep_rew_mean        | 83.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1130     |\n",
            "|    iterations         | 99500    |\n",
            "|    time_elapsed       | 3519     |\n",
            "|    total_timesteps    | 3980000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.33    |\n",
            "|    explained_variance | 0.956    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 99499    |\n",
            "|    policy_loss        | 0.26     |\n",
            "|    value_loss         | 41       |\n",
            "------------------------------------\n",
            "Num timesteps: 3984000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 81.54\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 323      |\n",
            "|    ep_rew_mean        | 81.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1131     |\n",
            "|    iterations         | 99600    |\n",
            "|    time_elapsed       | 3521     |\n",
            "|    total_timesteps    | 3984000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.262   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 99599    |\n",
            "|    policy_loss        | -0.0949  |\n",
            "|    value_loss         | 3.22     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 307      |\n",
            "|    ep_rew_mean        | 86.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1131     |\n",
            "|    iterations         | 99700    |\n",
            "|    time_elapsed       | 3524     |\n",
            "|    total_timesteps    | 3988000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.268   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 99699    |\n",
            "|    policy_loss        | -0.288   |\n",
            "|    value_loss         | 7.59     |\n",
            "------------------------------------\n",
            "Num timesteps: 3992000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 91.91\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 303      |\n",
            "|    ep_rew_mean        | 91.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1131     |\n",
            "|    iterations         | 99800    |\n",
            "|    time_elapsed       | 3527     |\n",
            "|    total_timesteps    | 3992000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.427   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 99799    |\n",
            "|    policy_loss        | 0.23     |\n",
            "|    value_loss         | 2.14     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 309      |\n",
            "|    ep_rew_mean        | 84.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1131     |\n",
            "|    iterations         | 99900    |\n",
            "|    time_elapsed       | 3531     |\n",
            "|    total_timesteps    | 3996000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.303   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 99899    |\n",
            "|    policy_loss        | -0.115   |\n",
            "|    value_loss         | 1.71     |\n",
            "------------------------------------\n",
            "Num timesteps: 4000000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 87.87\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 314      |\n",
            "|    ep_rew_mean        | 87.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1131     |\n",
            "|    iterations         | 100000   |\n",
            "|    time_elapsed       | 3533     |\n",
            "|    total_timesteps    | 4000000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.123   |\n",
            "|    explained_variance | 0.674    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 99999    |\n",
            "|    policy_loss        | -0.382   |\n",
            "|    value_loss         | 366      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 285      |\n",
            "|    ep_rew_mean        | 93.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1132     |\n",
            "|    iterations         | 100100   |\n",
            "|    time_elapsed       | 3535     |\n",
            "|    total_timesteps    | 4004000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.325   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 100099   |\n",
            "|    policy_loss        | -0.207   |\n",
            "|    value_loss         | 4.01     |\n",
            "------------------------------------\n",
            "Num timesteps: 4008000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 107.28\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 274      |\n",
            "|    ep_rew_mean        | 107      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1132     |\n",
            "|    iterations         | 100200   |\n",
            "|    time_elapsed       | 3538     |\n",
            "|    total_timesteps    | 4008000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.342   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 100199   |\n",
            "|    policy_loss        | -0.349   |\n",
            "|    value_loss         | 7.59     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 274      |\n",
            "|    ep_rew_mean        | 110      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1132     |\n",
            "|    iterations         | 100300   |\n",
            "|    time_elapsed       | 3542     |\n",
            "|    total_timesteps    | 4012000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.437   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 100299   |\n",
            "|    policy_loss        | 0.221    |\n",
            "|    value_loss         | 5.06     |\n",
            "------------------------------------\n",
            "Num timesteps: 4016000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 108.91\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 276      |\n",
            "|    ep_rew_mean        | 109      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1132     |\n",
            "|    iterations         | 100400   |\n",
            "|    time_elapsed       | 3545     |\n",
            "|    total_timesteps    | 4016000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.346   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 100399   |\n",
            "|    policy_loss        | 0.134    |\n",
            "|    value_loss         | 3.15     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 266      |\n",
            "|    ep_rew_mean        | 104      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1132     |\n",
            "|    iterations         | 100500   |\n",
            "|    time_elapsed       | 3548     |\n",
            "|    total_timesteps    | 4020000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.294   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 100499   |\n",
            "|    policy_loss        | 0.0506   |\n",
            "|    value_loss         | 2.63     |\n",
            "------------------------------------\n",
            "Num timesteps: 4024000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 97.37\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 258      |\n",
            "|    ep_rew_mean        | 97.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1133     |\n",
            "|    iterations         | 100600   |\n",
            "|    time_elapsed       | 3550     |\n",
            "|    total_timesteps    | 4024000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.323   |\n",
            "|    explained_variance | 0.98     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 100599   |\n",
            "|    policy_loss        | -0.21    |\n",
            "|    value_loss         | 17.6     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 256      |\n",
            "|    ep_rew_mean        | 99.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1133     |\n",
            "|    iterations         | 100700   |\n",
            "|    time_elapsed       | 3554     |\n",
            "|    total_timesteps    | 4028000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.286   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 100699   |\n",
            "|    policy_loss        | -0.135   |\n",
            "|    value_loss         | 1.66     |\n",
            "------------------------------------\n",
            "Num timesteps: 4032000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 112.35\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 280      |\n",
            "|    ep_rew_mean        | 112      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1133     |\n",
            "|    iterations         | 100800   |\n",
            "|    time_elapsed       | 3557     |\n",
            "|    total_timesteps    | 4032000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.406   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 100799   |\n",
            "|    policy_loss        | 0.704    |\n",
            "|    value_loss         | 6.83     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 289      |\n",
            "|    ep_rew_mean        | 112      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1133     |\n",
            "|    iterations         | 100900   |\n",
            "|    time_elapsed       | 3560     |\n",
            "|    total_timesteps    | 4036000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.407   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 100899   |\n",
            "|    policy_loss        | -0.202   |\n",
            "|    value_loss         | 4.32     |\n",
            "------------------------------------\n",
            "Num timesteps: 4040000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 107.27\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 284      |\n",
            "|    ep_rew_mean        | 107      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1133     |\n",
            "|    iterations         | 101000   |\n",
            "|    time_elapsed       | 3563     |\n",
            "|    total_timesteps    | 4040000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.374   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 100999   |\n",
            "|    policy_loss        | -0.0754  |\n",
            "|    value_loss         | 0.9      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 286      |\n",
            "|    ep_rew_mean        | 114      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1134     |\n",
            "|    iterations         | 101100   |\n",
            "|    time_elapsed       | 3565     |\n",
            "|    total_timesteps    | 4044000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.359   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 101099   |\n",
            "|    policy_loss        | 0.534    |\n",
            "|    value_loss         | 7.57     |\n",
            "------------------------------------\n",
            "Num timesteps: 4048000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 128.82\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 285      |\n",
            "|    ep_rew_mean        | 129      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1134     |\n",
            "|    iterations         | 101200   |\n",
            "|    time_elapsed       | 3568     |\n",
            "|    total_timesteps    | 4048000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.23    |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 101199   |\n",
            "|    policy_loss        | -0.212   |\n",
            "|    value_loss         | 0.891    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 288      |\n",
            "|    ep_rew_mean        | 133      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1134     |\n",
            "|    iterations         | 101300   |\n",
            "|    time_elapsed       | 3571     |\n",
            "|    total_timesteps    | 4052000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.436   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 101299   |\n",
            "|    policy_loss        | -0.28    |\n",
            "|    value_loss         | 5.13     |\n",
            "------------------------------------\n",
            "Num timesteps: 4056000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 119.34\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 267      |\n",
            "|    ep_rew_mean        | 119      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1134     |\n",
            "|    iterations         | 101400   |\n",
            "|    time_elapsed       | 3574     |\n",
            "|    total_timesteps    | 4056000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.256   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 101399   |\n",
            "|    policy_loss        | 0.243    |\n",
            "|    value_loss         | 1.83     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 264      |\n",
            "|    ep_rew_mean        | 120      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1134     |\n",
            "|    iterations         | 101500   |\n",
            "|    time_elapsed       | 3577     |\n",
            "|    total_timesteps    | 4060000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.301   |\n",
            "|    explained_variance | 0.652    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 101499   |\n",
            "|    policy_loss        | -0.0634  |\n",
            "|    value_loss         | 1.21e+03 |\n",
            "------------------------------------\n",
            "Num timesteps: 4064000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 109.46\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 263      |\n",
            "|    ep_rew_mean        | 109      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1135     |\n",
            "|    iterations         | 101600   |\n",
            "|    time_elapsed       | 3579     |\n",
            "|    total_timesteps    | 4064000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.33    |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 101599   |\n",
            "|    policy_loss        | 0.355    |\n",
            "|    value_loss         | 2.73     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 261      |\n",
            "|    ep_rew_mean        | 115      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1135     |\n",
            "|    iterations         | 101700   |\n",
            "|    time_elapsed       | 3582     |\n",
            "|    total_timesteps    | 4068000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.357   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 101699   |\n",
            "|    policy_loss        | 0.136    |\n",
            "|    value_loss         | 2.77     |\n",
            "------------------------------------\n",
            "Num timesteps: 4072000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 116.64\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 255      |\n",
            "|    ep_rew_mean        | 117      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1135     |\n",
            "|    iterations         | 101800   |\n",
            "|    time_elapsed       | 3584     |\n",
            "|    total_timesteps    | 4072000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.243   |\n",
            "|    explained_variance | 0.947    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 101799   |\n",
            "|    policy_loss        | -0.0965  |\n",
            "|    value_loss         | 113      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 245      |\n",
            "|    ep_rew_mean        | 109      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1136     |\n",
            "|    iterations         | 101900   |\n",
            "|    time_elapsed       | 3587     |\n",
            "|    total_timesteps    | 4076000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.292   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 101899   |\n",
            "|    policy_loss        | -0.156   |\n",
            "|    value_loss         | 2.74     |\n",
            "------------------------------------\n",
            "Num timesteps: 4080000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 101.91\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 246      |\n",
            "|    ep_rew_mean        | 102      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1136     |\n",
            "|    iterations         | 102000   |\n",
            "|    time_elapsed       | 3590     |\n",
            "|    total_timesteps    | 4080000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.362   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 101999   |\n",
            "|    policy_loss        | 0.239    |\n",
            "|    value_loss         | 2.63     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 241      |\n",
            "|    ep_rew_mean        | 99.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1136     |\n",
            "|    iterations         | 102100   |\n",
            "|    time_elapsed       | 3593     |\n",
            "|    total_timesteps    | 4084000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.317   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 102099   |\n",
            "|    policy_loss        | -0.288   |\n",
            "|    value_loss         | 6.45     |\n",
            "------------------------------------\n",
            "Num timesteps: 4088000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 101.01\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 241      |\n",
            "|    ep_rew_mean        | 101      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1136     |\n",
            "|    iterations         | 102200   |\n",
            "|    time_elapsed       | 3596     |\n",
            "|    total_timesteps    | 4088000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.206   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 102199   |\n",
            "|    policy_loss        | -0.0789  |\n",
            "|    value_loss         | 5.14     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 257      |\n",
            "|    ep_rew_mean        | 102      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1136     |\n",
            "|    iterations         | 102300   |\n",
            "|    time_elapsed       | 3599     |\n",
            "|    total_timesteps    | 4092000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.319   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 102299   |\n",
            "|    policy_loss        | 0.0528   |\n",
            "|    value_loss         | 2.95     |\n",
            "------------------------------------\n",
            "Num timesteps: 4096000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 115.40\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 251      |\n",
            "|    ep_rew_mean        | 115      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1137     |\n",
            "|    iterations         | 102400   |\n",
            "|    time_elapsed       | 3602     |\n",
            "|    total_timesteps    | 4096000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.355   |\n",
            "|    explained_variance | 0.974    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 102399   |\n",
            "|    policy_loss        | -0.102   |\n",
            "|    value_loss         | 71.1     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 256      |\n",
            "|    ep_rew_mean        | 109      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1137     |\n",
            "|    iterations         | 102500   |\n",
            "|    time_elapsed       | 3604     |\n",
            "|    total_timesteps    | 4100000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.261   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 102499   |\n",
            "|    policy_loss        | -0.0282  |\n",
            "|    value_loss         | 6.46     |\n",
            "------------------------------------\n",
            "Num timesteps: 4104000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 109.55\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 257      |\n",
            "|    ep_rew_mean        | 110      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1137     |\n",
            "|    iterations         | 102600   |\n",
            "|    time_elapsed       | 3606     |\n",
            "|    total_timesteps    | 4104000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.334   |\n",
            "|    explained_variance | 0.362    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 102599   |\n",
            "|    policy_loss        | -0.168   |\n",
            "|    value_loss         | 447      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 259      |\n",
            "|    ep_rew_mean        | 123      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1138     |\n",
            "|    iterations         | 102700   |\n",
            "|    time_elapsed       | 3609     |\n",
            "|    total_timesteps    | 4108000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.273   |\n",
            "|    explained_variance | 0.639    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 102699   |\n",
            "|    policy_loss        | -0.0741  |\n",
            "|    value_loss         | 1.49e+03 |\n",
            "------------------------------------\n",
            "Num timesteps: 4112000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 121.33\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 259      |\n",
            "|    ep_rew_mean        | 121      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1138     |\n",
            "|    iterations         | 102800   |\n",
            "|    time_elapsed       | 3612     |\n",
            "|    total_timesteps    | 4112000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.337   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 102799   |\n",
            "|    policy_loss        | 0.27     |\n",
            "|    value_loss         | 4.59     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 244      |\n",
            "|    ep_rew_mean        | 105      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1138     |\n",
            "|    iterations         | 102900   |\n",
            "|    time_elapsed       | 3614     |\n",
            "|    total_timesteps    | 4116000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.383   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 102899   |\n",
            "|    policy_loss        | -0.105   |\n",
            "|    value_loss         | 3.41     |\n",
            "------------------------------------\n",
            "Num timesteps: 4120000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 97.54\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 247      |\n",
            "|    ep_rew_mean        | 97.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1138     |\n",
            "|    iterations         | 103000   |\n",
            "|    time_elapsed       | 3618     |\n",
            "|    total_timesteps    | 4120000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.395   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 102999   |\n",
            "|    policy_loss        | 0.0116   |\n",
            "|    value_loss         | 5.74     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 274      |\n",
            "|    ep_rew_mean        | 86.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1138     |\n",
            "|    iterations         | 103100   |\n",
            "|    time_elapsed       | 3621     |\n",
            "|    total_timesteps    | 4124000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.325   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 103099   |\n",
            "|    policy_loss        | -0.127   |\n",
            "|    value_loss         | 6.08     |\n",
            "------------------------------------\n",
            "Num timesteps: 4128000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 101.39\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 275      |\n",
            "|    ep_rew_mean        | 101      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1138     |\n",
            "|    iterations         | 103200   |\n",
            "|    time_elapsed       | 3624     |\n",
            "|    total_timesteps    | 4128000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.353   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 103199   |\n",
            "|    policy_loss        | -0.0141  |\n",
            "|    value_loss         | 7.04     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 270      |\n",
            "|    ep_rew_mean        | 96.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1139     |\n",
            "|    iterations         | 103300   |\n",
            "|    time_elapsed       | 3627     |\n",
            "|    total_timesteps    | 4132000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.235   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 103299   |\n",
            "|    policy_loss        | -0.0202  |\n",
            "|    value_loss         | 1.56     |\n",
            "------------------------------------\n",
            "Num timesteps: 4136000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 84.00\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 272      |\n",
            "|    ep_rew_mean        | 84       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1139     |\n",
            "|    iterations         | 103400   |\n",
            "|    time_elapsed       | 3629     |\n",
            "|    total_timesteps    | 4136000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.306   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 103399   |\n",
            "|    policy_loss        | -0.196   |\n",
            "|    value_loss         | 7.32     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 262      |\n",
            "|    ep_rew_mean        | 94.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1139     |\n",
            "|    iterations         | 103500   |\n",
            "|    time_elapsed       | 3632     |\n",
            "|    total_timesteps    | 4140000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.258   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 103499   |\n",
            "|    policy_loss        | -0.107   |\n",
            "|    value_loss         | 3.56     |\n",
            "------------------------------------\n",
            "Num timesteps: 4144000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 98.90\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 266      |\n",
            "|    ep_rew_mean        | 98.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1139     |\n",
            "|    iterations         | 103600   |\n",
            "|    time_elapsed       | 3635     |\n",
            "|    total_timesteps    | 4144000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.361   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 103599   |\n",
            "|    policy_loss        | 0.205    |\n",
            "|    value_loss         | 2.05     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 254      |\n",
            "|    ep_rew_mean        | 109      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1139     |\n",
            "|    iterations         | 103700   |\n",
            "|    time_elapsed       | 3638     |\n",
            "|    total_timesteps    | 4148000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.358   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 103699   |\n",
            "|    policy_loss        | 0.305    |\n",
            "|    value_loss         | 6.26     |\n",
            "------------------------------------\n",
            "Num timesteps: 4152000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 94.61\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 250      |\n",
            "|    ep_rew_mean        | 94.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1140     |\n",
            "|    iterations         | 103800   |\n",
            "|    time_elapsed       | 3640     |\n",
            "|    total_timesteps    | 4152000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.106   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 103799   |\n",
            "|    policy_loss        | -0.014   |\n",
            "|    value_loss         | 1.97     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 251      |\n",
            "|    ep_rew_mean        | 102      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1140     |\n",
            "|    iterations         | 103900   |\n",
            "|    time_elapsed       | 3643     |\n",
            "|    total_timesteps    | 4156000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.293   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 103899   |\n",
            "|    policy_loss        | -0.0193  |\n",
            "|    value_loss         | 4.45     |\n",
            "------------------------------------\n",
            "Num timesteps: 4160000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 103.95\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 259      |\n",
            "|    ep_rew_mean        | 104      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1140     |\n",
            "|    iterations         | 104000   |\n",
            "|    time_elapsed       | 3646     |\n",
            "|    total_timesteps    | 4160000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.341   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 103999   |\n",
            "|    policy_loss        | 0.373    |\n",
            "|    value_loss         | 2.69     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 266      |\n",
            "|    ep_rew_mean        | 114      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1141     |\n",
            "|    iterations         | 104100   |\n",
            "|    time_elapsed       | 3649     |\n",
            "|    total_timesteps    | 4164000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.249   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 104099   |\n",
            "|    policy_loss        | -0.194   |\n",
            "|    value_loss         | 6.8      |\n",
            "------------------------------------\n",
            "Num timesteps: 4168000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 109.95\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 258      |\n",
            "|    ep_rew_mean        | 110      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1141     |\n",
            "|    iterations         | 104200   |\n",
            "|    time_elapsed       | 3652     |\n",
            "|    total_timesteps    | 4168000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.261   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 104199   |\n",
            "|    policy_loss        | -0.0638  |\n",
            "|    value_loss         | 2.17     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 244      |\n",
            "|    ep_rew_mean        | 116      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1141     |\n",
            "|    iterations         | 104300   |\n",
            "|    time_elapsed       | 3654     |\n",
            "|    total_timesteps    | 4172000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.322   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 104299   |\n",
            "|    policy_loss        | -0.00527 |\n",
            "|    value_loss         | 0.908    |\n",
            "------------------------------------\n",
            "Num timesteps: 4176000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 127.33\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 258      |\n",
            "|    ep_rew_mean        | 127      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1141     |\n",
            "|    iterations         | 104400   |\n",
            "|    time_elapsed       | 3657     |\n",
            "|    total_timesteps    | 4176000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.321   |\n",
            "|    explained_variance | 0.818    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 104399   |\n",
            "|    policy_loss        | -6.68    |\n",
            "|    value_loss         | 387      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 266      |\n",
            "|    ep_rew_mean        | 126      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1141     |\n",
            "|    iterations         | 104500   |\n",
            "|    time_elapsed       | 3660     |\n",
            "|    total_timesteps    | 4180000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.321   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 104499   |\n",
            "|    policy_loss        | -0.0142  |\n",
            "|    value_loss         | 3.75     |\n",
            "------------------------------------\n",
            "Num timesteps: 4184000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 131.60\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 268      |\n",
            "|    ep_rew_mean        | 132      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1142     |\n",
            "|    iterations         | 104600   |\n",
            "|    time_elapsed       | 3663     |\n",
            "|    total_timesteps    | 4184000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.269   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 104599   |\n",
            "|    policy_loss        | -0.271   |\n",
            "|    value_loss         | 4.17     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 262      |\n",
            "|    ep_rew_mean        | 138      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1142     |\n",
            "|    iterations         | 104700   |\n",
            "|    time_elapsed       | 3666     |\n",
            "|    total_timesteps    | 4188000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.374   |\n",
            "|    explained_variance | 0.785    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 104699   |\n",
            "|    policy_loss        | 0.315    |\n",
            "|    value_loss         | 596      |\n",
            "------------------------------------\n",
            "Num timesteps: 4192000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 140.04\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 267      |\n",
            "|    ep_rew_mean        | 140      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1142     |\n",
            "|    iterations         | 104800   |\n",
            "|    time_elapsed       | 3669     |\n",
            "|    total_timesteps    | 4192000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.29    |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 104799   |\n",
            "|    policy_loss        | 0.484    |\n",
            "|    value_loss         | 8.82     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 273      |\n",
            "|    ep_rew_mean        | 137      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1142     |\n",
            "|    iterations         | 104900   |\n",
            "|    time_elapsed       | 3672     |\n",
            "|    total_timesteps    | 4196000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.304   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 104899   |\n",
            "|    policy_loss        | -0.193   |\n",
            "|    value_loss         | 8.9      |\n",
            "------------------------------------\n",
            "Num timesteps: 4200000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 140.21\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 270      |\n",
            "|    ep_rew_mean        | 140      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1142     |\n",
            "|    iterations         | 105000   |\n",
            "|    time_elapsed       | 3674     |\n",
            "|    total_timesteps    | 4200000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.21    |\n",
            "|    explained_variance | 0.972    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 104999   |\n",
            "|    policy_loss        | 1.64     |\n",
            "|    value_loss         | 83       |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 263      |\n",
            "|    ep_rew_mean        | 129      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1143     |\n",
            "|    iterations         | 105100   |\n",
            "|    time_elapsed       | 3677     |\n",
            "|    total_timesteps    | 4204000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.224   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 105099   |\n",
            "|    policy_loss        | 0.229    |\n",
            "|    value_loss         | 2.67     |\n",
            "------------------------------------\n",
            "Num timesteps: 4208000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 118.42\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 259      |\n",
            "|    ep_rew_mean        | 118      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1143     |\n",
            "|    iterations         | 105200   |\n",
            "|    time_elapsed       | 3680     |\n",
            "|    total_timesteps    | 4208000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.331   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 105199   |\n",
            "|    policy_loss        | 0.199    |\n",
            "|    value_loss         | 2.31     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 241      |\n",
            "|    ep_rew_mean        | 106      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1143     |\n",
            "|    iterations         | 105300   |\n",
            "|    time_elapsed       | 3682     |\n",
            "|    total_timesteps    | 4212000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.192   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 105299   |\n",
            "|    policy_loss        | 0.0822   |\n",
            "|    value_loss         | 0.863    |\n",
            "------------------------------------\n",
            "Num timesteps: 4216000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 113.02\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 234      |\n",
            "|    ep_rew_mean        | 113      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1144     |\n",
            "|    iterations         | 105400   |\n",
            "|    time_elapsed       | 3684     |\n",
            "|    total_timesteps    | 4216000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.365   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 105399   |\n",
            "|    policy_loss        | -0.566   |\n",
            "|    value_loss         | 8.75     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 231      |\n",
            "|    ep_rew_mean        | 112      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1144     |\n",
            "|    iterations         | 105500   |\n",
            "|    time_elapsed       | 3687     |\n",
            "|    total_timesteps    | 4220000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.369   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 105499   |\n",
            "|    policy_loss        | -0.0394  |\n",
            "|    value_loss         | 1.86     |\n",
            "------------------------------------\n",
            "Num timesteps: 4224000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 100.05\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 240      |\n",
            "|    ep_rew_mean        | 100      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1144     |\n",
            "|    iterations         | 105600   |\n",
            "|    time_elapsed       | 3691     |\n",
            "|    total_timesteps    | 4224000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.327   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 105599   |\n",
            "|    policy_loss        | -0.0518  |\n",
            "|    value_loss         | 6.34     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 250      |\n",
            "|    ep_rew_mean        | 93.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1144     |\n",
            "|    iterations         | 105700   |\n",
            "|    time_elapsed       | 3694     |\n",
            "|    total_timesteps    | 4228000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.383   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 105699   |\n",
            "|    policy_loss        | 0.37     |\n",
            "|    value_loss         | 3.95     |\n",
            "------------------------------------\n",
            "Num timesteps: 4232000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 91.27\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 255      |\n",
            "|    ep_rew_mean        | 91.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1144     |\n",
            "|    iterations         | 105800   |\n",
            "|    time_elapsed       | 3698     |\n",
            "|    total_timesteps    | 4232000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.263   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 105799   |\n",
            "|    policy_loss        | 0.0723   |\n",
            "|    value_loss         | 2.46     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 285      |\n",
            "|    ep_rew_mean        | 89.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1144     |\n",
            "|    iterations         | 105900   |\n",
            "|    time_elapsed       | 3702     |\n",
            "|    total_timesteps    | 4236000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.157   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 105899   |\n",
            "|    policy_loss        | 0.067    |\n",
            "|    value_loss         | 2.71     |\n",
            "------------------------------------\n",
            "Num timesteps: 4240000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 97.21\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 293      |\n",
            "|    ep_rew_mean        | 97.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1144     |\n",
            "|    iterations         | 106000   |\n",
            "|    time_elapsed       | 3705     |\n",
            "|    total_timesteps    | 4240000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.42    |\n",
            "|    explained_variance | 0.891    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 105999   |\n",
            "|    policy_loss        | 0.285    |\n",
            "|    value_loss         | 231      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 285      |\n",
            "|    ep_rew_mean        | 95.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1144     |\n",
            "|    iterations         | 106100   |\n",
            "|    time_elapsed       | 3708     |\n",
            "|    total_timesteps    | 4244000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.364   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 106099   |\n",
            "|    policy_loss        | -0.359   |\n",
            "|    value_loss         | 4.17     |\n",
            "------------------------------------\n",
            "Num timesteps: 4248000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 73.49\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 308      |\n",
            "|    ep_rew_mean        | 73.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1144     |\n",
            "|    iterations         | 106200   |\n",
            "|    time_elapsed       | 3713     |\n",
            "|    total_timesteps    | 4248000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.424   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 106199   |\n",
            "|    policy_loss        | -0.0358  |\n",
            "|    value_loss         | 7.77     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 287      |\n",
            "|    ep_rew_mean        | 72.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1144     |\n",
            "|    iterations         | 106300   |\n",
            "|    time_elapsed       | 3715     |\n",
            "|    total_timesteps    | 4252000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.352   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 106299   |\n",
            "|    policy_loss        | -0.282   |\n",
            "|    value_loss         | 6.27     |\n",
            "------------------------------------\n",
            "Num timesteps: 4256000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 74.26\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 272      |\n",
            "|    ep_rew_mean        | 74.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1144     |\n",
            "|    iterations         | 106400   |\n",
            "|    time_elapsed       | 3719     |\n",
            "|    total_timesteps    | 4256000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.332   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 106399   |\n",
            "|    policy_loss        | 0.274    |\n",
            "|    value_loss         | 16.4     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 269      |\n",
            "|    ep_rew_mean        | 94.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1144     |\n",
            "|    iterations         | 106500   |\n",
            "|    time_elapsed       | 3723     |\n",
            "|    total_timesteps    | 4260000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.332   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 106499   |\n",
            "|    policy_loss        | -0.414   |\n",
            "|    value_loss         | 5.12     |\n",
            "------------------------------------\n",
            "Num timesteps: 4264000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 89.41\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 263      |\n",
            "|    ep_rew_mean        | 89.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1144     |\n",
            "|    iterations         | 106600   |\n",
            "|    time_elapsed       | 3725     |\n",
            "|    total_timesteps    | 4264000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.377   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 106599   |\n",
            "|    policy_loss        | 0.0483   |\n",
            "|    value_loss         | 2.67     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 263      |\n",
            "|    ep_rew_mean        | 98.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1144     |\n",
            "|    iterations         | 106700   |\n",
            "|    time_elapsed       | 3728     |\n",
            "|    total_timesteps    | 4268000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.31    |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 106699   |\n",
            "|    policy_loss        | -0.129   |\n",
            "|    value_loss         | 1.6      |\n",
            "------------------------------------\n",
            "Num timesteps: 4272000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 93.74\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 289      |\n",
            "|    ep_rew_mean        | 93.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1144     |\n",
            "|    iterations         | 106800   |\n",
            "|    time_elapsed       | 3732     |\n",
            "|    total_timesteps    | 4272000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.364   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 106799   |\n",
            "|    policy_loss        | -0.472   |\n",
            "|    value_loss         | 1.97     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 274      |\n",
            "|    ep_rew_mean        | 95.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1144     |\n",
            "|    iterations         | 106900   |\n",
            "|    time_elapsed       | 3736     |\n",
            "|    total_timesteps    | 4276000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.348   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 106899   |\n",
            "|    policy_loss        | 0.000765 |\n",
            "|    value_loss         | 2.47     |\n",
            "------------------------------------\n",
            "Num timesteps: 4280000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 95.23\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 290      |\n",
            "|    ep_rew_mean        | 95.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1144     |\n",
            "|    iterations         | 107000   |\n",
            "|    time_elapsed       | 3740     |\n",
            "|    total_timesteps    | 4280000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.265   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 106999   |\n",
            "|    policy_loss        | 0.92     |\n",
            "|    value_loss         | 22.7     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 313      |\n",
            "|    ep_rew_mean        | 107      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1144     |\n",
            "|    iterations         | 107100   |\n",
            "|    time_elapsed       | 3743     |\n",
            "|    total_timesteps    | 4284000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.331   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 107099   |\n",
            "|    policy_loss        | 0.364    |\n",
            "|    value_loss         | 20.2     |\n",
            "------------------------------------\n",
            "Num timesteps: 4288000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 106.07\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 295      |\n",
            "|    ep_rew_mean        | 106      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1144     |\n",
            "|    iterations         | 107200   |\n",
            "|    time_elapsed       | 3746     |\n",
            "|    total_timesteps    | 4288000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.355   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 107199   |\n",
            "|    policy_loss        | -0.257   |\n",
            "|    value_loss         | 1.56     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 289      |\n",
            "|    ep_rew_mean        | 112      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1144     |\n",
            "|    iterations         | 107300   |\n",
            "|    time_elapsed       | 3748     |\n",
            "|    total_timesteps    | 4292000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.345   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 107299   |\n",
            "|    policy_loss        | 0.396    |\n",
            "|    value_loss         | 10.2     |\n",
            "------------------------------------\n",
            "Num timesteps: 4296000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 114.19\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 288      |\n",
            "|    ep_rew_mean        | 114      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1144     |\n",
            "|    iterations         | 107400   |\n",
            "|    time_elapsed       | 3752     |\n",
            "|    total_timesteps    | 4296000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.282   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 107399   |\n",
            "|    policy_loss        | -0.0763  |\n",
            "|    value_loss         | 6.6      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 275      |\n",
            "|    ep_rew_mean        | 107      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1144     |\n",
            "|    iterations         | 107500   |\n",
            "|    time_elapsed       | 3756     |\n",
            "|    total_timesteps    | 4300000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.335   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 107499   |\n",
            "|    policy_loss        | 0.0734   |\n",
            "|    value_loss         | 17       |\n",
            "------------------------------------\n",
            "Num timesteps: 4304000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 119.56\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 274      |\n",
            "|    ep_rew_mean        | 120      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1144     |\n",
            "|    iterations         | 107600   |\n",
            "|    time_elapsed       | 3759     |\n",
            "|    total_timesteps    | 4304000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.29    |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 107599   |\n",
            "|    policy_loss        | 0.122    |\n",
            "|    value_loss         | 3.77     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 244      |\n",
            "|    ep_rew_mean        | 124      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1144     |\n",
            "|    iterations         | 107700   |\n",
            "|    time_elapsed       | 3763     |\n",
            "|    total_timesteps    | 4308000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.235   |\n",
            "|    explained_variance | 0.899    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 107699   |\n",
            "|    policy_loss        | 2.26     |\n",
            "|    value_loss         | 137      |\n",
            "------------------------------------\n",
            "Num timesteps: 4312000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 115.00\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 262      |\n",
            "|    ep_rew_mean        | 115      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1145     |\n",
            "|    iterations         | 107800   |\n",
            "|    time_elapsed       | 3765     |\n",
            "|    total_timesteps    | 4312000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.373   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 107799   |\n",
            "|    policy_loss        | 0.319    |\n",
            "|    value_loss         | 3.92     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 272      |\n",
            "|    ep_rew_mean        | 112      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1145     |\n",
            "|    iterations         | 107900   |\n",
            "|    time_elapsed       | 3768     |\n",
            "|    total_timesteps    | 4316000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.187   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 107899   |\n",
            "|    policy_loss        | 0.0017   |\n",
            "|    value_loss         | 1.38     |\n",
            "------------------------------------\n",
            "Num timesteps: 4320000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 122.28\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 265      |\n",
            "|    ep_rew_mean        | 122      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1145     |\n",
            "|    iterations         | 108000   |\n",
            "|    time_elapsed       | 3770     |\n",
            "|    total_timesteps    | 4320000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.293   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 107999   |\n",
            "|    policy_loss        | -0.148   |\n",
            "|    value_loss         | 1.96     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 246      |\n",
            "|    ep_rew_mean        | 128      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1145     |\n",
            "|    iterations         | 108100   |\n",
            "|    time_elapsed       | 3773     |\n",
            "|    total_timesteps    | 4324000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.158   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 108099   |\n",
            "|    policy_loss        | -0.0343  |\n",
            "|    value_loss         | 0.796    |\n",
            "------------------------------------\n",
            "Num timesteps: 4328000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 134.24\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 242      |\n",
            "|    ep_rew_mean        | 134      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1145     |\n",
            "|    iterations         | 108200   |\n",
            "|    time_elapsed       | 3776     |\n",
            "|    total_timesteps    | 4328000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.291   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 108199   |\n",
            "|    policy_loss        | 0.262    |\n",
            "|    value_loss         | 2.74     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 255      |\n",
            "|    ep_rew_mean        | 134      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1145     |\n",
            "|    iterations         | 108300   |\n",
            "|    time_elapsed       | 3780     |\n",
            "|    total_timesteps    | 4332000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.209   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 108299   |\n",
            "|    policy_loss        | 0.0658   |\n",
            "|    value_loss         | 7.13     |\n",
            "------------------------------------\n",
            "Num timesteps: 4336000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 112.63\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 253      |\n",
            "|    ep_rew_mean        | 113      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1146     |\n",
            "|    iterations         | 108400   |\n",
            "|    time_elapsed       | 3783     |\n",
            "|    total_timesteps    | 4336000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.457   |\n",
            "|    explained_variance | 0.901    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 108399   |\n",
            "|    policy_loss        | -2.62    |\n",
            "|    value_loss         | 94.7     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 245      |\n",
            "|    ep_rew_mean        | 113      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1146     |\n",
            "|    iterations         | 108500   |\n",
            "|    time_elapsed       | 3785     |\n",
            "|    total_timesteps    | 4340000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.307   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 108499   |\n",
            "|    policy_loss        | -0.00448 |\n",
            "|    value_loss         | 3.35     |\n",
            "------------------------------------\n",
            "Num timesteps: 4344000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 105.20\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 240      |\n",
            "|    ep_rew_mean        | 105      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1146     |\n",
            "|    iterations         | 108600   |\n",
            "|    time_elapsed       | 3788     |\n",
            "|    total_timesteps    | 4344000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.296   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 108599   |\n",
            "|    policy_loss        | 0.0054   |\n",
            "|    value_loss         | 2.32     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 246      |\n",
            "|    ep_rew_mean        | 90.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1146     |\n",
            "|    iterations         | 108700   |\n",
            "|    time_elapsed       | 3791     |\n",
            "|    total_timesteps    | 4348000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.237   |\n",
            "|    explained_variance | 0.966    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 108699   |\n",
            "|    policy_loss        | -0.151   |\n",
            "|    value_loss         | 37.2     |\n",
            "------------------------------------\n",
            "Num timesteps: 4352000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 101.13\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 243      |\n",
            "|    ep_rew_mean        | 101      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1147     |\n",
            "|    iterations         | 108800   |\n",
            "|    time_elapsed       | 3793     |\n",
            "|    total_timesteps    | 4352000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.3     |\n",
            "|    explained_variance | 0.789    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 108799   |\n",
            "|    policy_loss        | 0.252    |\n",
            "|    value_loss         | 546      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 241      |\n",
            "|    ep_rew_mean        | 104      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1147     |\n",
            "|    iterations         | 108900   |\n",
            "|    time_elapsed       | 3796     |\n",
            "|    total_timesteps    | 4356000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.221   |\n",
            "|    explained_variance | 0.892    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 108899   |\n",
            "|    policy_loss        | 1.4      |\n",
            "|    value_loss         | 183      |\n",
            "------------------------------------\n",
            "Num timesteps: 4360000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 129.32\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 234      |\n",
            "|    ep_rew_mean        | 129      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1147     |\n",
            "|    iterations         | 109000   |\n",
            "|    time_elapsed       | 3799     |\n",
            "|    total_timesteps    | 4360000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.212   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 108999   |\n",
            "|    policy_loss        | -0.0835  |\n",
            "|    value_loss         | 1.62     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 234      |\n",
            "|    ep_rew_mean        | 125      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1147     |\n",
            "|    iterations         | 109100   |\n",
            "|    time_elapsed       | 3801     |\n",
            "|    total_timesteps    | 4364000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.274   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 109099   |\n",
            "|    policy_loss        | 0.132    |\n",
            "|    value_loss         | 2.23     |\n",
            "------------------------------------\n",
            "Num timesteps: 4368000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 117.86\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 255      |\n",
            "|    ep_rew_mean        | 118      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1147     |\n",
            "|    iterations         | 109200   |\n",
            "|    time_elapsed       | 3804     |\n",
            "|    total_timesteps    | 4368000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.246   |\n",
            "|    explained_variance | -0.361   |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 109199   |\n",
            "|    policy_loss        | 0.0714   |\n",
            "|    value_loss         | 1.61e+03 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 241      |\n",
            "|    ep_rew_mean        | 122      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1148     |\n",
            "|    iterations         | 109300   |\n",
            "|    time_elapsed       | 3806     |\n",
            "|    total_timesteps    | 4372000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.198   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 109299   |\n",
            "|    policy_loss        | 0.0752   |\n",
            "|    value_loss         | 10.1     |\n",
            "------------------------------------\n",
            "Num timesteps: 4376000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 132.90\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 231      |\n",
            "|    ep_rew_mean        | 133      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1148     |\n",
            "|    iterations         | 109400   |\n",
            "|    time_elapsed       | 3809     |\n",
            "|    total_timesteps    | 4376000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.229   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 109399   |\n",
            "|    policy_loss        | 0.00445  |\n",
            "|    value_loss         | 3.82     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 226      |\n",
            "|    ep_rew_mean        | 121      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1149     |\n",
            "|    iterations         | 109500   |\n",
            "|    time_elapsed       | 3811     |\n",
            "|    total_timesteps    | 4380000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.316   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 109499   |\n",
            "|    policy_loss        | 0.248    |\n",
            "|    value_loss         | 2.61     |\n",
            "------------------------------------\n",
            "Num timesteps: 4384000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 104.44\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 224      |\n",
            "|    ep_rew_mean        | 104      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1149     |\n",
            "|    iterations         | 109600   |\n",
            "|    time_elapsed       | 3814     |\n",
            "|    total_timesteps    | 4384000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.342   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 109599   |\n",
            "|    policy_loss        | -0.0724  |\n",
            "|    value_loss         | 4.79     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 214      |\n",
            "|    ep_rew_mean        | 112      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1149     |\n",
            "|    iterations         | 109700   |\n",
            "|    time_elapsed       | 3817     |\n",
            "|    total_timesteps    | 4388000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.343   |\n",
            "|    explained_variance | 0.883    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 109699   |\n",
            "|    policy_loss        | -1.66    |\n",
            "|    value_loss         | 185      |\n",
            "------------------------------------\n",
            "Num timesteps: 4392000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 123.36\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 212      |\n",
            "|    ep_rew_mean        | 123      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1149     |\n",
            "|    iterations         | 109800   |\n",
            "|    time_elapsed       | 3819     |\n",
            "|    total_timesteps    | 4392000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.343   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 109799   |\n",
            "|    policy_loss        | 0.206    |\n",
            "|    value_loss         | 6.3      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 220      |\n",
            "|    ep_rew_mean        | 120      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1150     |\n",
            "|    iterations         | 109900   |\n",
            "|    time_elapsed       | 3822     |\n",
            "|    total_timesteps    | 4396000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.3     |\n",
            "|    explained_variance | 0.896    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 109899   |\n",
            "|    policy_loss        | 0.0224   |\n",
            "|    value_loss         | 92.7     |\n",
            "------------------------------------\n",
            "Num timesteps: 4400000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 127.18\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 221      |\n",
            "|    ep_rew_mean        | 127      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1150     |\n",
            "|    iterations         | 110000   |\n",
            "|    time_elapsed       | 3824     |\n",
            "|    total_timesteps    | 4400000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.283   |\n",
            "|    explained_variance | 0.968    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 109999   |\n",
            "|    policy_loss        | 2.58     |\n",
            "|    value_loss         | 47.4     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 221      |\n",
            "|    ep_rew_mean        | 134      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1150     |\n",
            "|    iterations         | 110100   |\n",
            "|    time_elapsed       | 3827     |\n",
            "|    total_timesteps    | 4404000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.329   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 110099   |\n",
            "|    policy_loss        | -0.0241  |\n",
            "|    value_loss         | 1.24     |\n",
            "------------------------------------\n",
            "Num timesteps: 4408000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 142.80\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 222      |\n",
            "|    ep_rew_mean        | 143      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1150     |\n",
            "|    iterations         | 110200   |\n",
            "|    time_elapsed       | 3830     |\n",
            "|    total_timesteps    | 4408000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.28    |\n",
            "|    explained_variance | 0.979    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 110199   |\n",
            "|    policy_loss        | -0.095   |\n",
            "|    value_loss         | 65.9     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 227      |\n",
            "|    ep_rew_mean        | 142      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1151     |\n",
            "|    iterations         | 110300   |\n",
            "|    time_elapsed       | 3832     |\n",
            "|    total_timesteps    | 4412000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.292   |\n",
            "|    explained_variance | 0.349    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 110299   |\n",
            "|    policy_loss        | -0.309   |\n",
            "|    value_loss         | 703      |\n",
            "------------------------------------\n",
            "Num timesteps: 4416000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 148.42\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 218      |\n",
            "|    ep_rew_mean        | 148      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1151     |\n",
            "|    iterations         | 110400   |\n",
            "|    time_elapsed       | 3834     |\n",
            "|    total_timesteps    | 4416000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.206   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 110399   |\n",
            "|    policy_loss        | -0.0596  |\n",
            "|    value_loss         | 2.51     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 209      |\n",
            "|    ep_rew_mean        | 142      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1151     |\n",
            "|    iterations         | 110500   |\n",
            "|    time_elapsed       | 3837     |\n",
            "|    total_timesteps    | 4420000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.204   |\n",
            "|    explained_variance | 0.828    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 110499   |\n",
            "|    policy_loss        | -0.178   |\n",
            "|    value_loss         | 563      |\n",
            "------------------------------------\n",
            "Num timesteps: 4424000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 143.36\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 220      |\n",
            "|    ep_rew_mean        | 143      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1151     |\n",
            "|    iterations         | 110600   |\n",
            "|    time_elapsed       | 3840     |\n",
            "|    total_timesteps    | 4424000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.293   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 110599   |\n",
            "|    policy_loss        | 0.212    |\n",
            "|    value_loss         | 4.24     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 209      |\n",
            "|    ep_rew_mean        | 146      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1152     |\n",
            "|    iterations         | 110700   |\n",
            "|    time_elapsed       | 3843     |\n",
            "|    total_timesteps    | 4428000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.292   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 110699   |\n",
            "|    policy_loss        | -0.268   |\n",
            "|    value_loss         | 3.39     |\n",
            "------------------------------------\n",
            "Num timesteps: 4432000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 149.07\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 221      |\n",
            "|    ep_rew_mean        | 149      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1151     |\n",
            "|    iterations         | 110800   |\n",
            "|    time_elapsed       | 3847     |\n",
            "|    total_timesteps    | 4432000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.247   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 110799   |\n",
            "|    policy_loss        | 0.104    |\n",
            "|    value_loss         | 11.7     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 242      |\n",
            "|    ep_rew_mean        | 151      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1151     |\n",
            "|    iterations         | 110900   |\n",
            "|    time_elapsed       | 3851     |\n",
            "|    total_timesteps    | 4436000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.422   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 110899   |\n",
            "|    policy_loss        | 0.0385   |\n",
            "|    value_loss         | 4.47     |\n",
            "------------------------------------\n",
            "Num timesteps: 4440000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 144.99\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 243      |\n",
            "|    ep_rew_mean        | 145      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1151     |\n",
            "|    iterations         | 111000   |\n",
            "|    time_elapsed       | 3854     |\n",
            "|    total_timesteps    | 4440000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.314   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 110999   |\n",
            "|    policy_loss        | 0.221    |\n",
            "|    value_loss         | 1.87     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 252      |\n",
            "|    ep_rew_mean        | 147      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1151     |\n",
            "|    iterations         | 111100   |\n",
            "|    time_elapsed       | 3858     |\n",
            "|    total_timesteps    | 4444000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.382   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 111099   |\n",
            "|    policy_loss        | -0.4     |\n",
            "|    value_loss         | 1.23     |\n",
            "------------------------------------\n",
            "Num timesteps: 4448000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 128.88\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 295      |\n",
            "|    ep_rew_mean        | 129      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1150     |\n",
            "|    iterations         | 111200   |\n",
            "|    time_elapsed       | 3864     |\n",
            "|    total_timesteps    | 4448000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.27    |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 111199   |\n",
            "|    policy_loss        | -0.343   |\n",
            "|    value_loss         | 2.46     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 314      |\n",
            "|    ep_rew_mean        | 132      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1151     |\n",
            "|    iterations         | 111300   |\n",
            "|    time_elapsed       | 3867     |\n",
            "|    total_timesteps    | 4452000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.45    |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 111299   |\n",
            "|    policy_loss        | -0.145   |\n",
            "|    value_loss         | 5.9      |\n",
            "------------------------------------\n",
            "Num timesteps: 4456000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 131.48\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 313      |\n",
            "|    ep_rew_mean        | 131      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1151     |\n",
            "|    iterations         | 111400   |\n",
            "|    time_elapsed       | 3870     |\n",
            "|    total_timesteps    | 4456000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.331   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 111399   |\n",
            "|    policy_loss        | -0.0282  |\n",
            "|    value_loss         | 8.19     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 308      |\n",
            "|    ep_rew_mean        | 124      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1151     |\n",
            "|    iterations         | 111500   |\n",
            "|    time_elapsed       | 3873     |\n",
            "|    total_timesteps    | 4460000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.306   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 111499   |\n",
            "|    policy_loss        | 0.352    |\n",
            "|    value_loss         | 5.4      |\n",
            "------------------------------------\n",
            "Num timesteps: 4464000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 119.99\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 273      |\n",
            "|    ep_rew_mean        | 120      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1151     |\n",
            "|    iterations         | 111600   |\n",
            "|    time_elapsed       | 3875     |\n",
            "|    total_timesteps    | 4464000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.254   |\n",
            "|    explained_variance | 0.812    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 111599   |\n",
            "|    policy_loss        | -0.0364  |\n",
            "|    value_loss         | 478      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 261      |\n",
            "|    ep_rew_mean        | 113      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1152     |\n",
            "|    iterations         | 111700   |\n",
            "|    time_elapsed       | 3878     |\n",
            "|    total_timesteps    | 4468000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.359   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 111699   |\n",
            "|    policy_loss        | -0.275   |\n",
            "|    value_loss         | 4.51     |\n",
            "------------------------------------\n",
            "Num timesteps: 4472000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 118.57\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 228      |\n",
            "|    ep_rew_mean        | 119      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1152     |\n",
            "|    iterations         | 111800   |\n",
            "|    time_elapsed       | 3881     |\n",
            "|    total_timesteps    | 4472000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.302   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 111799   |\n",
            "|    policy_loss        | -0.1     |\n",
            "|    value_loss         | 1.03     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 220      |\n",
            "|    ep_rew_mean        | 123      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1152     |\n",
            "|    iterations         | 111900   |\n",
            "|    time_elapsed       | 3883     |\n",
            "|    total_timesteps    | 4476000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.295   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 111899   |\n",
            "|    policy_loss        | 0.0339   |\n",
            "|    value_loss         | 6.82     |\n",
            "------------------------------------\n",
            "Num timesteps: 4480000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 127.90\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 223      |\n",
            "|    ep_rew_mean        | 128      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1152     |\n",
            "|    iterations         | 112000   |\n",
            "|    time_elapsed       | 3887     |\n",
            "|    total_timesteps    | 4480000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.317   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 111999   |\n",
            "|    policy_loss        | -0.122   |\n",
            "|    value_loss         | 3.63     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 247      |\n",
            "|    ep_rew_mean        | 118      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1152     |\n",
            "|    iterations         | 112100   |\n",
            "|    time_elapsed       | 3890     |\n",
            "|    total_timesteps    | 4484000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.243   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 112099   |\n",
            "|    policy_loss        | 0.233    |\n",
            "|    value_loss         | 2.46     |\n",
            "------------------------------------\n",
            "Num timesteps: 4488000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 109.78\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 249      |\n",
            "|    ep_rew_mean        | 110      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1152     |\n",
            "|    iterations         | 112200   |\n",
            "|    time_elapsed       | 3892     |\n",
            "|    total_timesteps    | 4488000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.456   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 112199   |\n",
            "|    policy_loss        | 0.0769   |\n",
            "|    value_loss         | 1.63     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 256      |\n",
            "|    ep_rew_mean        | 127      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1153     |\n",
            "|    iterations         | 112300   |\n",
            "|    time_elapsed       | 3895     |\n",
            "|    total_timesteps    | 4492000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.315   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 112299   |\n",
            "|    policy_loss        | -0.274   |\n",
            "|    value_loss         | 4.8      |\n",
            "------------------------------------\n",
            "Num timesteps: 4496000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 124.87\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 248      |\n",
            "|    ep_rew_mean        | 125      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1153     |\n",
            "|    iterations         | 112400   |\n",
            "|    time_elapsed       | 3897     |\n",
            "|    total_timesteps    | 4496000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.191   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 112399   |\n",
            "|    policy_loss        | 0.196    |\n",
            "|    value_loss         | 0.977    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 248      |\n",
            "|    ep_rew_mean        | 115      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1153     |\n",
            "|    iterations         | 112500   |\n",
            "|    time_elapsed       | 3899     |\n",
            "|    total_timesteps    | 4500000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.406   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 112499   |\n",
            "|    policy_loss        | 0.292    |\n",
            "|    value_loss         | 2.44     |\n",
            "------------------------------------\n",
            "Num timesteps: 4504000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 126.19\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 231      |\n",
            "|    ep_rew_mean        | 126      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1154     |\n",
            "|    iterations         | 112600   |\n",
            "|    time_elapsed       | 3902     |\n",
            "|    total_timesteps    | 4504000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.336   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 112599   |\n",
            "|    policy_loss        | 0.0358   |\n",
            "|    value_loss         | 3.01     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 224      |\n",
            "|    ep_rew_mean        | 115      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1154     |\n",
            "|    iterations         | 112700   |\n",
            "|    time_elapsed       | 3905     |\n",
            "|    total_timesteps    | 4508000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.389   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 112699   |\n",
            "|    policy_loss        | -0.0995  |\n",
            "|    value_loss         | 4.88     |\n",
            "------------------------------------\n",
            "Num timesteps: 4512000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 114.39\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 246      |\n",
            "|    ep_rew_mean        | 114      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1154     |\n",
            "|    iterations         | 112800   |\n",
            "|    time_elapsed       | 3908     |\n",
            "|    total_timesteps    | 4512000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.451   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 112799   |\n",
            "|    policy_loss        | -0.0583  |\n",
            "|    value_loss         | 0.27     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 247      |\n",
            "|    ep_rew_mean        | 111      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1154     |\n",
            "|    iterations         | 112900   |\n",
            "|    time_elapsed       | 3911     |\n",
            "|    total_timesteps    | 4516000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.39    |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 112899   |\n",
            "|    policy_loss        | -0.663   |\n",
            "|    value_loss         | 14       |\n",
            "------------------------------------\n",
            "Num timesteps: 4520000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 99.69\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 262      |\n",
            "|    ep_rew_mean        | 99.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1154     |\n",
            "|    iterations         | 113000   |\n",
            "|    time_elapsed       | 3914     |\n",
            "|    total_timesteps    | 4520000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.489   |\n",
            "|    explained_variance | 0.772    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 112999   |\n",
            "|    policy_loss        | -0.25    |\n",
            "|    value_loss         | 265      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 272      |\n",
            "|    ep_rew_mean        | 104      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1154     |\n",
            "|    iterations         | 113100   |\n",
            "|    time_elapsed       | 3917     |\n",
            "|    total_timesteps    | 4524000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.509   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 113099   |\n",
            "|    policy_loss        | -0.242   |\n",
            "|    value_loss         | 1.37     |\n",
            "------------------------------------\n",
            "Num timesteps: 4528000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 120.72\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 272      |\n",
            "|    ep_rew_mean        | 121      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1154     |\n",
            "|    iterations         | 113200   |\n",
            "|    time_elapsed       | 3920     |\n",
            "|    total_timesteps    | 4528000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.363   |\n",
            "|    explained_variance | 0.963    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 113199   |\n",
            "|    policy_loss        | 0.213    |\n",
            "|    value_loss         | 39.7     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 274      |\n",
            "|    ep_rew_mean        | 103      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1155     |\n",
            "|    iterations         | 113300   |\n",
            "|    time_elapsed       | 3923     |\n",
            "|    total_timesteps    | 4532000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.44    |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 113299   |\n",
            "|    policy_loss        | 0.457    |\n",
            "|    value_loss         | 3.27     |\n",
            "------------------------------------\n",
            "Num timesteps: 4536000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 113.92\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 253      |\n",
            "|    ep_rew_mean        | 114      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1155     |\n",
            "|    iterations         | 113400   |\n",
            "|    time_elapsed       | 3925     |\n",
            "|    total_timesteps    | 4536000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.458   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 113399   |\n",
            "|    policy_loss        | 0.217    |\n",
            "|    value_loss         | 24.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 232      |\n",
            "|    ep_rew_mean        | 116      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1155     |\n",
            "|    iterations         | 113500   |\n",
            "|    time_elapsed       | 3927     |\n",
            "|    total_timesteps    | 4540000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.249   |\n",
            "|    explained_variance | 0.789    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 113499   |\n",
            "|    policy_loss        | -0.193   |\n",
            "|    value_loss         | 309      |\n",
            "------------------------------------\n",
            "Num timesteps: 4544000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 124.02\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 208      |\n",
            "|    ep_rew_mean        | 124      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1156     |\n",
            "|    iterations         | 113600   |\n",
            "|    time_elapsed       | 3930     |\n",
            "|    total_timesteps    | 4544000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.377   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 113599   |\n",
            "|    policy_loss        | -0.153   |\n",
            "|    value_loss         | 33.9     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 204      |\n",
            "|    ep_rew_mean        | 108      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1156     |\n",
            "|    iterations         | 113700   |\n",
            "|    time_elapsed       | 3932     |\n",
            "|    total_timesteps    | 4548000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.236   |\n",
            "|    explained_variance | 0.761    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 113699   |\n",
            "|    policy_loss        | 0.152    |\n",
            "|    value_loss         | 512      |\n",
            "------------------------------------\n",
            "Num timesteps: 4552000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 106.17\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 209      |\n",
            "|    ep_rew_mean        | 106      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1156     |\n",
            "|    iterations         | 113800   |\n",
            "|    time_elapsed       | 3935     |\n",
            "|    total_timesteps    | 4552000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.343   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 113799   |\n",
            "|    policy_loss        | -0.414   |\n",
            "|    value_loss         | 4.97     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 219      |\n",
            "|    ep_rew_mean        | 105      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1157     |\n",
            "|    iterations         | 113900   |\n",
            "|    time_elapsed       | 3937     |\n",
            "|    total_timesteps    | 4556000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.383   |\n",
            "|    explained_variance | 0.957    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 113899   |\n",
            "|    policy_loss        | 0.856    |\n",
            "|    value_loss         | 39.7     |\n",
            "------------------------------------\n",
            "Num timesteps: 4560000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 93.63\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 212      |\n",
            "|    ep_rew_mean        | 93.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1157     |\n",
            "|    iterations         | 114000   |\n",
            "|    time_elapsed       | 3939     |\n",
            "|    total_timesteps    | 4560000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.202   |\n",
            "|    explained_variance | 0.749    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 113999   |\n",
            "|    policy_loss        | -0.113   |\n",
            "|    value_loss         | 157      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 214      |\n",
            "|    ep_rew_mean        | 89.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1157     |\n",
            "|    iterations         | 114100   |\n",
            "|    time_elapsed       | 3942     |\n",
            "|    total_timesteps    | 4564000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.286   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 114099   |\n",
            "|    policy_loss        | -0.449   |\n",
            "|    value_loss         | 5.1      |\n",
            "------------------------------------\n",
            "Num timesteps: 4568000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 96.22\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 216      |\n",
            "|    ep_rew_mean        | 96.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1157     |\n",
            "|    iterations         | 114200   |\n",
            "|    time_elapsed       | 3945     |\n",
            "|    total_timesteps    | 4568000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.332   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 114199   |\n",
            "|    policy_loss        | 0.126    |\n",
            "|    value_loss         | 1        |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 221      |\n",
            "|    ep_rew_mean        | 99.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1158     |\n",
            "|    iterations         | 114300   |\n",
            "|    time_elapsed       | 3948     |\n",
            "|    total_timesteps    | 4572000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.399   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 114299   |\n",
            "|    policy_loss        | -0.0158  |\n",
            "|    value_loss         | 1.66     |\n",
            "------------------------------------\n",
            "Num timesteps: 4576000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 115.06\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 205      |\n",
            "|    ep_rew_mean        | 115      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1158     |\n",
            "|    iterations         | 114400   |\n",
            "|    time_elapsed       | 3950     |\n",
            "|    total_timesteps    | 4576000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.413   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 114399   |\n",
            "|    policy_loss        | -0.31    |\n",
            "|    value_loss         | 3        |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 205      |\n",
            "|    ep_rew_mean        | 117      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1158     |\n",
            "|    iterations         | 114500   |\n",
            "|    time_elapsed       | 3952     |\n",
            "|    total_timesteps    | 4580000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.298   |\n",
            "|    explained_variance | 0.918    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 114499   |\n",
            "|    policy_loss        | -0.0883  |\n",
            "|    value_loss         | 170      |\n",
            "------------------------------------\n",
            "Num timesteps: 4584000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 109.54\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 202      |\n",
            "|    ep_rew_mean        | 110      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1159     |\n",
            "|    iterations         | 114600   |\n",
            "|    time_elapsed       | 3955     |\n",
            "|    total_timesteps    | 4584000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.265   |\n",
            "|    explained_variance | 0.974    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 114599   |\n",
            "|    policy_loss        | 0.313    |\n",
            "|    value_loss         | 33.8     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 220      |\n",
            "|    ep_rew_mean        | 106      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1159     |\n",
            "|    iterations         | 114700   |\n",
            "|    time_elapsed       | 3958     |\n",
            "|    total_timesteps    | 4588000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.371   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 114699   |\n",
            "|    policy_loss        | -0.309   |\n",
            "|    value_loss         | 3.76     |\n",
            "------------------------------------\n",
            "Num timesteps: 4592000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 107.54\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 211      |\n",
            "|    ep_rew_mean        | 108      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1159     |\n",
            "|    iterations         | 114800   |\n",
            "|    time_elapsed       | 3960     |\n",
            "|    total_timesteps    | 4592000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.496   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 114799   |\n",
            "|    policy_loss        | 0.154    |\n",
            "|    value_loss         | 4.32     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 211      |\n",
            "|    ep_rew_mean        | 95.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1159     |\n",
            "|    iterations         | 114900   |\n",
            "|    time_elapsed       | 3963     |\n",
            "|    total_timesteps    | 4596000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.355   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 114899   |\n",
            "|    policy_loss        | -0.0126  |\n",
            "|    value_loss         | 3.57     |\n",
            "------------------------------------\n",
            "Num timesteps: 4600000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 99.38\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 229      |\n",
            "|    ep_rew_mean        | 99.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1159     |\n",
            "|    iterations         | 115000   |\n",
            "|    time_elapsed       | 3966     |\n",
            "|    total_timesteps    | 4600000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.484   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 114999   |\n",
            "|    policy_loss        | -0.0351  |\n",
            "|    value_loss         | 2.57     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 252      |\n",
            "|    ep_rew_mean        | 94       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1159     |\n",
            "|    iterations         | 115100   |\n",
            "|    time_elapsed       | 3969     |\n",
            "|    total_timesteps    | 4604000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.362   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 115099   |\n",
            "|    policy_loss        | 0.0403   |\n",
            "|    value_loss         | 2.92     |\n",
            "------------------------------------\n",
            "Num timesteps: 4608000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 76.43\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 256      |\n",
            "|    ep_rew_mean        | 76.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1160     |\n",
            "|    iterations         | 115200   |\n",
            "|    time_elapsed       | 3971     |\n",
            "|    total_timesteps    | 4608000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.469   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 115199   |\n",
            "|    policy_loss        | -0.329   |\n",
            "|    value_loss         | 6.61     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 250      |\n",
            "|    ep_rew_mean        | 74.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1160     |\n",
            "|    iterations         | 115300   |\n",
            "|    time_elapsed       | 3974     |\n",
            "|    total_timesteps    | 4612000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.4     |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 115299   |\n",
            "|    policy_loss        | 0.984    |\n",
            "|    value_loss         | 9.95     |\n",
            "------------------------------------\n",
            "Num timesteps: 4616000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 67.86\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 253      |\n",
            "|    ep_rew_mean        | 67.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1160     |\n",
            "|    iterations         | 115400   |\n",
            "|    time_elapsed       | 3976     |\n",
            "|    total_timesteps    | 4616000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.449   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 115399   |\n",
            "|    policy_loss        | 0.248    |\n",
            "|    value_loss         | 4.45     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 259      |\n",
            "|    ep_rew_mean        | 70.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1160     |\n",
            "|    iterations         | 115500   |\n",
            "|    time_elapsed       | 3979     |\n",
            "|    total_timesteps    | 4620000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.376   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 115499   |\n",
            "|    policy_loss        | -0.192   |\n",
            "|    value_loss         | 6.61     |\n",
            "------------------------------------\n",
            "Num timesteps: 4624000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 70.83\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 284      |\n",
            "|    ep_rew_mean        | 70.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1160     |\n",
            "|    iterations         | 115600   |\n",
            "|    time_elapsed       | 3983     |\n",
            "|    total_timesteps    | 4624000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.364   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 115599   |\n",
            "|    policy_loss        | 0.0766   |\n",
            "|    value_loss         | 3.38     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 281      |\n",
            "|    ep_rew_mean        | 56.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1160     |\n",
            "|    iterations         | 115700   |\n",
            "|    time_elapsed       | 3986     |\n",
            "|    total_timesteps    | 4628000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.552   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 115699   |\n",
            "|    policy_loss        | -0.171   |\n",
            "|    value_loss         | 7.3      |\n",
            "------------------------------------\n",
            "Num timesteps: 4632000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 65.15\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 263      |\n",
            "|    ep_rew_mean        | 65.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1161     |\n",
            "|    iterations         | 115800   |\n",
            "|    time_elapsed       | 3988     |\n",
            "|    total_timesteps    | 4632000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.278   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 115799   |\n",
            "|    policy_loss        | -1.28    |\n",
            "|    value_loss         | 49.8     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 262      |\n",
            "|    ep_rew_mean        | 74.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1161     |\n",
            "|    iterations         | 115900   |\n",
            "|    time_elapsed       | 3991     |\n",
            "|    total_timesteps    | 4636000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.287   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 115899   |\n",
            "|    policy_loss        | 0.409    |\n",
            "|    value_loss         | 3.86     |\n",
            "------------------------------------\n",
            "Num timesteps: 4640000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 73.60\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 272      |\n",
            "|    ep_rew_mean        | 73.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1161     |\n",
            "|    iterations         | 116000   |\n",
            "|    time_elapsed       | 3994     |\n",
            "|    total_timesteps    | 4640000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.232   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 115999   |\n",
            "|    policy_loss        | -0.0272  |\n",
            "|    value_loss         | 1.07     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 272      |\n",
            "|    ep_rew_mean        | 81.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1161     |\n",
            "|    iterations         | 116100   |\n",
            "|    time_elapsed       | 3997     |\n",
            "|    total_timesteps    | 4644000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.153   |\n",
            "|    explained_variance | 0.878    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 116099   |\n",
            "|    policy_loss        | 0.118    |\n",
            "|    value_loss         | 390      |\n",
            "------------------------------------\n",
            "Num timesteps: 4648000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 75.59\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 244      |\n",
            "|    ep_rew_mean        | 75.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1161     |\n",
            "|    iterations         | 116200   |\n",
            "|    time_elapsed       | 4000     |\n",
            "|    total_timesteps    | 4648000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.272   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 116199   |\n",
            "|    policy_loss        | -0.382   |\n",
            "|    value_loss         | 13.6     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 236      |\n",
            "|    ep_rew_mean        | 89.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1161     |\n",
            "|    iterations         | 116300   |\n",
            "|    time_elapsed       | 4003     |\n",
            "|    total_timesteps    | 4652000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.289   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 116299   |\n",
            "|    policy_loss        | -0.0925  |\n",
            "|    value_loss         | 5.19     |\n",
            "------------------------------------\n",
            "Num timesteps: 4656000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 93.31\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 241      |\n",
            "|    ep_rew_mean        | 93.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1162     |\n",
            "|    iterations         | 116400   |\n",
            "|    time_elapsed       | 4006     |\n",
            "|    total_timesteps    | 4656000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.324   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 116399   |\n",
            "|    policy_loss        | 0.259    |\n",
            "|    value_loss         | 0.721    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 247      |\n",
            "|    ep_rew_mean        | 86.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1162     |\n",
            "|    iterations         | 116500   |\n",
            "|    time_elapsed       | 4009     |\n",
            "|    total_timesteps    | 4660000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.363   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 116499   |\n",
            "|    policy_loss        | -0.409   |\n",
            "|    value_loss         | 8.37     |\n",
            "------------------------------------\n",
            "Num timesteps: 4664000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 75.35\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 231      |\n",
            "|    ep_rew_mean        | 75.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1162     |\n",
            "|    iterations         | 116600   |\n",
            "|    time_elapsed       | 4012     |\n",
            "|    total_timesteps    | 4664000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.358   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 116599   |\n",
            "|    policy_loss        | -0.112   |\n",
            "|    value_loss         | 8.49     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 227      |\n",
            "|    ep_rew_mean        | 79       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1162     |\n",
            "|    iterations         | 116700   |\n",
            "|    time_elapsed       | 4014     |\n",
            "|    total_timesteps    | 4668000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.385   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 116699   |\n",
            "|    policy_loss        | -0.098   |\n",
            "|    value_loss         | 3.45     |\n",
            "------------------------------------\n",
            "Num timesteps: 4672000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 89.92\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 226      |\n",
            "|    ep_rew_mean        | 89.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1163     |\n",
            "|    iterations         | 116800   |\n",
            "|    time_elapsed       | 4017     |\n",
            "|    total_timesteps    | 4672000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.186   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 116799   |\n",
            "|    policy_loss        | 0.0511   |\n",
            "|    value_loss         | 47.4     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 216      |\n",
            "|    ep_rew_mean        | 81.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1163     |\n",
            "|    iterations         | 116900   |\n",
            "|    time_elapsed       | 4020     |\n",
            "|    total_timesteps    | 4676000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.395   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 116899   |\n",
            "|    policy_loss        | 0.0158   |\n",
            "|    value_loss         | 0.656    |\n",
            "------------------------------------\n",
            "Num timesteps: 4680000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 80.90\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 189      |\n",
            "|    ep_rew_mean        | 80.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1163     |\n",
            "|    iterations         | 117000   |\n",
            "|    time_elapsed       | 4022     |\n",
            "|    total_timesteps    | 4680000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.338   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 116999   |\n",
            "|    policy_loss        | 0.404    |\n",
            "|    value_loss         | 8.56     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 165      |\n",
            "|    ep_rew_mean        | 75.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1163     |\n",
            "|    iterations         | 117100   |\n",
            "|    time_elapsed       | 4024     |\n",
            "|    total_timesteps    | 4684000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.348   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 117099   |\n",
            "|    policy_loss        | 0.216    |\n",
            "|    value_loss         | 6.36     |\n",
            "------------------------------------\n",
            "Num timesteps: 4688000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 65.48\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 146      |\n",
            "|    ep_rew_mean        | 65.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1164     |\n",
            "|    iterations         | 117200   |\n",
            "|    time_elapsed       | 4026     |\n",
            "|    total_timesteps    | 4688000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.356   |\n",
            "|    explained_variance | 0.739    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 117199   |\n",
            "|    policy_loss        | -0.203   |\n",
            "|    value_loss         | 1.15e+03 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 150      |\n",
            "|    ep_rew_mean        | 78.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1164     |\n",
            "|    iterations         | 117300   |\n",
            "|    time_elapsed       | 4029     |\n",
            "|    total_timesteps    | 4692000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.249   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 117299   |\n",
            "|    policy_loss        | -0.202   |\n",
            "|    value_loss         | 4.8      |\n",
            "------------------------------------\n",
            "Num timesteps: 4696000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 85.20\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 155      |\n",
            "|    ep_rew_mean        | 85.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1164     |\n",
            "|    iterations         | 117400   |\n",
            "|    time_elapsed       | 4032     |\n",
            "|    total_timesteps    | 4696000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.414   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 117399   |\n",
            "|    policy_loss        | 0.088    |\n",
            "|    value_loss         | 3.77     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 197      |\n",
            "|    ep_rew_mean        | 83.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1164     |\n",
            "|    iterations         | 117500   |\n",
            "|    time_elapsed       | 4036     |\n",
            "|    total_timesteps    | 4700000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.346   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 117499   |\n",
            "|    policy_loss        | -0.294   |\n",
            "|    value_loss         | 4.99     |\n",
            "------------------------------------\n",
            "Num timesteps: 4704000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 79.86\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 212      |\n",
            "|    ep_rew_mean        | 79.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1164     |\n",
            "|    iterations         | 117600   |\n",
            "|    time_elapsed       | 4039     |\n",
            "|    total_timesteps    | 4704000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.32    |\n",
            "|    explained_variance | 0.794    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 117599   |\n",
            "|    policy_loss        | 0.326    |\n",
            "|    value_loss         | 681      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 237      |\n",
            "|    ep_rew_mean        | 87.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1164     |\n",
            "|    iterations         | 117700   |\n",
            "|    time_elapsed       | 4042     |\n",
            "|    total_timesteps    | 4708000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.23    |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 117699   |\n",
            "|    policy_loss        | 0.157    |\n",
            "|    value_loss         | 1.21     |\n",
            "------------------------------------\n",
            "Num timesteps: 4712000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 95.99\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 250      |\n",
            "|    ep_rew_mean        | 96       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1164     |\n",
            "|    iterations         | 117800   |\n",
            "|    time_elapsed       | 4045     |\n",
            "|    total_timesteps    | 4712000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.328   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 117799   |\n",
            "|    policy_loss        | 0.253    |\n",
            "|    value_loss         | 7.3      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 260      |\n",
            "|    ep_rew_mean        | 76.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1164     |\n",
            "|    iterations         | 117900   |\n",
            "|    time_elapsed       | 4048     |\n",
            "|    total_timesteps    | 4716000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.278   |\n",
            "|    explained_variance | 0.841    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 117899   |\n",
            "|    policy_loss        | -0.306   |\n",
            "|    value_loss         | 596      |\n",
            "------------------------------------\n",
            "Num timesteps: 4720000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 53.24\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 265      |\n",
            "|    ep_rew_mean        | 53.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1165     |\n",
            "|    iterations         | 118000   |\n",
            "|    time_elapsed       | 4051     |\n",
            "|    total_timesteps    | 4720000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.329   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 117999   |\n",
            "|    policy_loss        | 0.422    |\n",
            "|    value_loss         | 38.6     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 240      |\n",
            "|    ep_rew_mean        | 77.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1165     |\n",
            "|    iterations         | 118100   |\n",
            "|    time_elapsed       | 4053     |\n",
            "|    total_timesteps    | 4724000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.323   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 118099   |\n",
            "|    policy_loss        | 0.353    |\n",
            "|    value_loss         | 8.43     |\n",
            "------------------------------------\n",
            "Num timesteps: 4728000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 86.14\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 235      |\n",
            "|    ep_rew_mean        | 86.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1165     |\n",
            "|    iterations         | 118200   |\n",
            "|    time_elapsed       | 4056     |\n",
            "|    total_timesteps    | 4728000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.45    |\n",
            "|    explained_variance | 0.958    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 118199   |\n",
            "|    policy_loss        | 0.0624   |\n",
            "|    value_loss         | 119      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 254      |\n",
            "|    ep_rew_mean        | 91.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1165     |\n",
            "|    iterations         | 118300   |\n",
            "|    time_elapsed       | 4059     |\n",
            "|    total_timesteps    | 4732000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.251   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 118299   |\n",
            "|    policy_loss        | -0.0596  |\n",
            "|    value_loss         | 4.58     |\n",
            "------------------------------------\n",
            "Num timesteps: 4736000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 89.97\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 245      |\n",
            "|    ep_rew_mean        | 90       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1165     |\n",
            "|    iterations         | 118400   |\n",
            "|    time_elapsed       | 4062     |\n",
            "|    total_timesteps    | 4736000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.41    |\n",
            "|    explained_variance | 0.954    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 118399   |\n",
            "|    policy_loss        | -1.54    |\n",
            "|    value_loss         | 98.9     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 245      |\n",
            "|    ep_rew_mean        | 97.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1166     |\n",
            "|    iterations         | 118500   |\n",
            "|    time_elapsed       | 4064     |\n",
            "|    total_timesteps    | 4740000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.478   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 118499   |\n",
            "|    policy_loss        | -0.14    |\n",
            "|    value_loss         | 10.9     |\n",
            "------------------------------------\n",
            "Num timesteps: 4744000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 111.47\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 242      |\n",
            "|    ep_rew_mean        | 111      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1166     |\n",
            "|    iterations         | 118600   |\n",
            "|    time_elapsed       | 4068     |\n",
            "|    total_timesteps    | 4744000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.312   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 118599   |\n",
            "|    policy_loss        | -0.227   |\n",
            "|    value_loss         | 11.8     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 263      |\n",
            "|    ep_rew_mean        | 98.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1165     |\n",
            "|    iterations         | 118700   |\n",
            "|    time_elapsed       | 4073     |\n",
            "|    total_timesteps    | 4748000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.216   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 118699   |\n",
            "|    policy_loss        | 0.0389   |\n",
            "|    value_loss         | 2.87     |\n",
            "------------------------------------\n",
            "Num timesteps: 4752000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 85.14\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 261      |\n",
            "|    ep_rew_mean        | 85.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1165     |\n",
            "|    iterations         | 118800   |\n",
            "|    time_elapsed       | 4076     |\n",
            "|    total_timesteps    | 4752000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.41    |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 118799   |\n",
            "|    policy_loss        | -0.287   |\n",
            "|    value_loss         | 5.23     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 262      |\n",
            "|    ep_rew_mean        | 68.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1165     |\n",
            "|    iterations         | 118900   |\n",
            "|    time_elapsed       | 4079     |\n",
            "|    total_timesteps    | 4756000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.432   |\n",
            "|    explained_variance | 0.951    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 118899   |\n",
            "|    policy_loss        | -0.989   |\n",
            "|    value_loss         | 215      |\n",
            "------------------------------------\n",
            "Num timesteps: 4760000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 50.52\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 256      |\n",
            "|    ep_rew_mean        | 50.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1166     |\n",
            "|    iterations         | 119000   |\n",
            "|    time_elapsed       | 4082     |\n",
            "|    total_timesteps    | 4760000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.393   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 118999   |\n",
            "|    policy_loss        | 0.104    |\n",
            "|    value_loss         | 13.2     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 261      |\n",
            "|    ep_rew_mean        | 61       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1166     |\n",
            "|    iterations         | 119100   |\n",
            "|    time_elapsed       | 4085     |\n",
            "|    total_timesteps    | 4764000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.321   |\n",
            "|    explained_variance | 0.959    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 119099   |\n",
            "|    policy_loss        | 0.0756   |\n",
            "|    value_loss         | 169      |\n",
            "------------------------------------\n",
            "Num timesteps: 4768000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 66.30\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 272      |\n",
            "|    ep_rew_mean        | 66.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1166     |\n",
            "|    iterations         | 119200   |\n",
            "|    time_elapsed       | 4088     |\n",
            "|    total_timesteps    | 4768000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.348   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 119199   |\n",
            "|    policy_loss        | -0.0761  |\n",
            "|    value_loss         | 4.83     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 261      |\n",
            "|    ep_rew_mean        | 61.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1166     |\n",
            "|    iterations         | 119300   |\n",
            "|    time_elapsed       | 4091     |\n",
            "|    total_timesteps    | 4772000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.416   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 119299   |\n",
            "|    policy_loss        | -0.304   |\n",
            "|    value_loss         | 9.89     |\n",
            "------------------------------------\n",
            "Num timesteps: 4776000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 78.30\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 248      |\n",
            "|    ep_rew_mean        | 78.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1166     |\n",
            "|    iterations         | 119400   |\n",
            "|    time_elapsed       | 4094     |\n",
            "|    total_timesteps    | 4776000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.401   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 119399   |\n",
            "|    policy_loss        | -0.0675  |\n",
            "|    value_loss         | 4.24     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 256      |\n",
            "|    ep_rew_mean        | 78.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1166     |\n",
            "|    iterations         | 119500   |\n",
            "|    time_elapsed       | 4097     |\n",
            "|    total_timesteps    | 4780000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.318   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 119499   |\n",
            "|    policy_loss        | 0.184    |\n",
            "|    value_loss         | 4.6      |\n",
            "------------------------------------\n",
            "Num timesteps: 4784000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 80.05\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 252      |\n",
            "|    ep_rew_mean        | 80       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1166     |\n",
            "|    iterations         | 119600   |\n",
            "|    time_elapsed       | 4100     |\n",
            "|    total_timesteps    | 4784000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.371   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 119599   |\n",
            "|    policy_loss        | -0.0388  |\n",
            "|    value_loss         | 3.33     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 261      |\n",
            "|    ep_rew_mean        | 74.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1166     |\n",
            "|    iterations         | 119700   |\n",
            "|    time_elapsed       | 4103     |\n",
            "|    total_timesteps    | 4788000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.457   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 119699   |\n",
            "|    policy_loss        | 0.409    |\n",
            "|    value_loss         | 4.96     |\n",
            "------------------------------------\n",
            "Num timesteps: 4792000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 70.30\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 262      |\n",
            "|    ep_rew_mean        | 70.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1166     |\n",
            "|    iterations         | 119800   |\n",
            "|    time_elapsed       | 4107     |\n",
            "|    total_timesteps    | 4792000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.31    |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 119799   |\n",
            "|    policy_loss        | -0.0125  |\n",
            "|    value_loss         | 1.27     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 277      |\n",
            "|    ep_rew_mean        | 67.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1166     |\n",
            "|    iterations         | 119900   |\n",
            "|    time_elapsed       | 4110     |\n",
            "|    total_timesteps    | 4796000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.333   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 119899   |\n",
            "|    policy_loss        | 0.203    |\n",
            "|    value_loss         | 4.23     |\n",
            "------------------------------------\n",
            "Num timesteps: 4800000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 75.09\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 271      |\n",
            "|    ep_rew_mean        | 75.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1166     |\n",
            "|    iterations         | 120000   |\n",
            "|    time_elapsed       | 4114     |\n",
            "|    total_timesteps    | 4800000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.283   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 119999   |\n",
            "|    policy_loss        | 0.0981   |\n",
            "|    value_loss         | 0.673    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 289      |\n",
            "|    ep_rew_mean        | 67.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1166     |\n",
            "|    iterations         | 120100   |\n",
            "|    time_elapsed       | 4117     |\n",
            "|    total_timesteps    | 4804000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.452   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 120099   |\n",
            "|    policy_loss        | -0.134   |\n",
            "|    value_loss         | 6.49     |\n",
            "------------------------------------\n",
            "Num timesteps: 4808000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 73.26\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 282      |\n",
            "|    ep_rew_mean        | 73.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1166     |\n",
            "|    iterations         | 120200   |\n",
            "|    time_elapsed       | 4120     |\n",
            "|    total_timesteps    | 4808000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.277   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 120199   |\n",
            "|    policy_loss        | -0.307   |\n",
            "|    value_loss         | 1.8      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 284      |\n",
            "|    ep_rew_mean        | 96.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1167     |\n",
            "|    iterations         | 120300   |\n",
            "|    time_elapsed       | 4123     |\n",
            "|    total_timesteps    | 4812000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.341   |\n",
            "|    explained_variance | 0.916    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 120299   |\n",
            "|    policy_loss        | 0.327    |\n",
            "|    value_loss         | 324      |\n",
            "------------------------------------\n",
            "Num timesteps: 4816000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 92.24\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 282      |\n",
            "|    ep_rew_mean        | 92.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1167     |\n",
            "|    iterations         | 120400   |\n",
            "|    time_elapsed       | 4126     |\n",
            "|    total_timesteps    | 4816000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.26    |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 120399   |\n",
            "|    policy_loss        | 0.178    |\n",
            "|    value_loss         | 1.78     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 281      |\n",
            "|    ep_rew_mean        | 88.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1167     |\n",
            "|    iterations         | 120500   |\n",
            "|    time_elapsed       | 4129     |\n",
            "|    total_timesteps    | 4820000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.362   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 120499   |\n",
            "|    policy_loss        | -0.227   |\n",
            "|    value_loss         | 14.3     |\n",
            "------------------------------------\n",
            "Num timesteps: 4824000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 95.83\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 262      |\n",
            "|    ep_rew_mean        | 95.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1167     |\n",
            "|    iterations         | 120600   |\n",
            "|    time_elapsed       | 4131     |\n",
            "|    total_timesteps    | 4824000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.408   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 120599   |\n",
            "|    policy_loss        | 0.442    |\n",
            "|    value_loss         | 8.65     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 259      |\n",
            "|    ep_rew_mean        | 96.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1167     |\n",
            "|    iterations         | 120700   |\n",
            "|    time_elapsed       | 4135     |\n",
            "|    total_timesteps    | 4828000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.299   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 120699   |\n",
            "|    policy_loss        | 0.398    |\n",
            "|    value_loss         | 8.8      |\n",
            "------------------------------------\n",
            "Num timesteps: 4832000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 89.64\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 269      |\n",
            "|    ep_rew_mean        | 89.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1167     |\n",
            "|    iterations         | 120800   |\n",
            "|    time_elapsed       | 4138     |\n",
            "|    total_timesteps    | 4832000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.385   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 120799   |\n",
            "|    policy_loss        | -0.954   |\n",
            "|    value_loss         | 16.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 292      |\n",
            "|    ep_rew_mean        | 78.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1167     |\n",
            "|    iterations         | 120900   |\n",
            "|    time_elapsed       | 4142     |\n",
            "|    total_timesteps    | 4836000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.406   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 120899   |\n",
            "|    policy_loss        | 0.117    |\n",
            "|    value_loss         | 5.3      |\n",
            "------------------------------------\n",
            "Num timesteps: 4840000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 38.85\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 291      |\n",
            "|    ep_rew_mean        | 38.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1167     |\n",
            "|    iterations         | 121000   |\n",
            "|    time_elapsed       | 4145     |\n",
            "|    total_timesteps    | 4840000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.401   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 120999   |\n",
            "|    policy_loss        | -0.161   |\n",
            "|    value_loss         | 3.99     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 287      |\n",
            "|    ep_rew_mean        | 35.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1167     |\n",
            "|    iterations         | 121100   |\n",
            "|    time_elapsed       | 4148     |\n",
            "|    total_timesteps    | 4844000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.27    |\n",
            "|    explained_variance | 0.828    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 121099   |\n",
            "|    policy_loss        | -0.173   |\n",
            "|    value_loss         | 533      |\n",
            "------------------------------------\n",
            "Num timesteps: 4848000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 44.49\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 287      |\n",
            "|    ep_rew_mean        | 44.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1167     |\n",
            "|    iterations         | 121200   |\n",
            "|    time_elapsed       | 4151     |\n",
            "|    total_timesteps    | 4848000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.516   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 121199   |\n",
            "|    policy_loss        | -0.305   |\n",
            "|    value_loss         | 8.79     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 288      |\n",
            "|    ep_rew_mean        | 54       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1167     |\n",
            "|    iterations         | 121300   |\n",
            "|    time_elapsed       | 4155     |\n",
            "|    total_timesteps    | 4852000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.446   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 121299   |\n",
            "|    policy_loss        | -0.486   |\n",
            "|    value_loss         | 9.3      |\n",
            "------------------------------------\n",
            "Num timesteps: 4856000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 55.84\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 284      |\n",
            "|    ep_rew_mean        | 55.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1167     |\n",
            "|    iterations         | 121400   |\n",
            "|    time_elapsed       | 4158     |\n",
            "|    total_timesteps    | 4856000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.219   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 121399   |\n",
            "|    policy_loss        | -0.101   |\n",
            "|    value_loss         | 2.55     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 265      |\n",
            "|    ep_rew_mean        | 83.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1167     |\n",
            "|    iterations         | 121500   |\n",
            "|    time_elapsed       | 4161     |\n",
            "|    total_timesteps    | 4860000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.362   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 121499   |\n",
            "|    policy_loss        | -0.0155  |\n",
            "|    value_loss         | 2.53     |\n",
            "------------------------------------\n",
            "Num timesteps: 4864000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 118.44\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 247      |\n",
            "|    ep_rew_mean        | 118      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1168     |\n",
            "|    iterations         | 121600   |\n",
            "|    time_elapsed       | 4163     |\n",
            "|    total_timesteps    | 4864000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.434   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 121599   |\n",
            "|    policy_loss        | -0.106   |\n",
            "|    value_loss         | 5.79     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 245      |\n",
            "|    ep_rew_mean        | 132      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1168     |\n",
            "|    iterations         | 121700   |\n",
            "|    time_elapsed       | 4165     |\n",
            "|    total_timesteps    | 4868000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.334   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 121699   |\n",
            "|    policy_loss        | 0.624    |\n",
            "|    value_loss         | 12.6     |\n",
            "------------------------------------\n",
            "Num timesteps: 4872000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 117.60\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 236      |\n",
            "|    ep_rew_mean        | 118      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1168     |\n",
            "|    iterations         | 121800   |\n",
            "|    time_elapsed       | 4169     |\n",
            "|    total_timesteps    | 4872000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.339   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 121799   |\n",
            "|    policy_loss        | -0.176   |\n",
            "|    value_loss         | 4.08     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 225      |\n",
            "|    ep_rew_mean        | 105      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1168     |\n",
            "|    iterations         | 121900   |\n",
            "|    time_elapsed       | 4172     |\n",
            "|    total_timesteps    | 4876000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.435   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 121899   |\n",
            "|    policy_loss        | -0.237   |\n",
            "|    value_loss         | 7.89     |\n",
            "------------------------------------\n",
            "Num timesteps: 4880000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 80.77\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 240      |\n",
            "|    ep_rew_mean        | 80.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1168     |\n",
            "|    iterations         | 122000   |\n",
            "|    time_elapsed       | 4175     |\n",
            "|    total_timesteps    | 4880000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.35    |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 121999   |\n",
            "|    policy_loss        | 0.215    |\n",
            "|    value_loss         | 4.36     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 232      |\n",
            "|    ep_rew_mean        | 57.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1168     |\n",
            "|    iterations         | 122100   |\n",
            "|    time_elapsed       | 4178     |\n",
            "|    total_timesteps    | 4884000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.479   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 122099   |\n",
            "|    policy_loss        | -0.47    |\n",
            "|    value_loss         | 8.26     |\n",
            "------------------------------------\n",
            "Num timesteps: 4888000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 54.86\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 234      |\n",
            "|    ep_rew_mean        | 54.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1169     |\n",
            "|    iterations         | 122200   |\n",
            "|    time_elapsed       | 4181     |\n",
            "|    total_timesteps    | 4888000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.342   |\n",
            "|    explained_variance | 0.65     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 122199   |\n",
            "|    policy_loss        | 0.0306   |\n",
            "|    value_loss         | 404      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 235      |\n",
            "|    ep_rew_mean        | 49.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1169     |\n",
            "|    iterations         | 122300   |\n",
            "|    time_elapsed       | 4183     |\n",
            "|    total_timesteps    | 4892000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.51    |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 122299   |\n",
            "|    policy_loss        | -0.0717  |\n",
            "|    value_loss         | 5.49     |\n",
            "------------------------------------\n",
            "Num timesteps: 4896000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 58.66\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 222      |\n",
            "|    ep_rew_mean        | 58.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1169     |\n",
            "|    iterations         | 122400   |\n",
            "|    time_elapsed       | 4185     |\n",
            "|    total_timesteps    | 4896000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.303   |\n",
            "|    explained_variance | 0.929    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 122399   |\n",
            "|    policy_loss        | -0.339   |\n",
            "|    value_loss         | 99.4     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 219      |\n",
            "|    ep_rew_mean        | 86.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1170     |\n",
            "|    iterations         | 122500   |\n",
            "|    time_elapsed       | 4188     |\n",
            "|    total_timesteps    | 4900000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.238   |\n",
            "|    explained_variance | 0.671    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 122499   |\n",
            "|    policy_loss        | 0.113    |\n",
            "|    value_loss         | 521      |\n",
            "------------------------------------\n",
            "Num timesteps: 4904000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 98.92\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 215      |\n",
            "|    ep_rew_mean        | 98.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1170     |\n",
            "|    iterations         | 122600   |\n",
            "|    time_elapsed       | 4190     |\n",
            "|    total_timesteps    | 4904000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.309   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 122599   |\n",
            "|    policy_loss        | 0.477    |\n",
            "|    value_loss         | 16.6     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 206      |\n",
            "|    ep_rew_mean        | 108      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1170     |\n",
            "|    iterations         | 122700   |\n",
            "|    time_elapsed       | 4192     |\n",
            "|    total_timesteps    | 4908000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.319   |\n",
            "|    explained_variance | 0.913    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 122699   |\n",
            "|    policy_loss        | 0.032    |\n",
            "|    value_loss         | 151      |\n",
            "------------------------------------\n",
            "Num timesteps: 4912000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 125.42\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 199      |\n",
            "|    ep_rew_mean        | 125      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1170     |\n",
            "|    iterations         | 122800   |\n",
            "|    time_elapsed       | 4194     |\n",
            "|    total_timesteps    | 4912000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.377   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 122799   |\n",
            "|    policy_loss        | 0.0856   |\n",
            "|    value_loss         | 0.803    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 202      |\n",
            "|    ep_rew_mean        | 135      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1171     |\n",
            "|    iterations         | 122900   |\n",
            "|    time_elapsed       | 4197     |\n",
            "|    total_timesteps    | 4916000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.278   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 122899   |\n",
            "|    policy_loss        | -0.164   |\n",
            "|    value_loss         | 6.42     |\n",
            "------------------------------------\n",
            "Num timesteps: 4920000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 134.18\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 195      |\n",
            "|    ep_rew_mean        | 134      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1171     |\n",
            "|    iterations         | 123000   |\n",
            "|    time_elapsed       | 4199     |\n",
            "|    total_timesteps    | 4920000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.323   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 122999   |\n",
            "|    policy_loss        | 0.131    |\n",
            "|    value_loss         | 8.42     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 189      |\n",
            "|    ep_rew_mean        | 130      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1171     |\n",
            "|    iterations         | 123100   |\n",
            "|    time_elapsed       | 4201     |\n",
            "|    total_timesteps    | 4924000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.375   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 123099   |\n",
            "|    policy_loss        | -0.123   |\n",
            "|    value_loss         | 1.87     |\n",
            "------------------------------------\n",
            "Num timesteps: 4928000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 121.88\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 185      |\n",
            "|    ep_rew_mean        | 122      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1172     |\n",
            "|    iterations         | 123200   |\n",
            "|    time_elapsed       | 4204     |\n",
            "|    total_timesteps    | 4928000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.129   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 123199   |\n",
            "|    policy_loss        | -0.0657  |\n",
            "|    value_loss         | 29.8     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 184      |\n",
            "|    ep_rew_mean        | 112      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1172     |\n",
            "|    iterations         | 123300   |\n",
            "|    time_elapsed       | 4206     |\n",
            "|    total_timesteps    | 4932000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.37    |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 123299   |\n",
            "|    policy_loss        | 0.00405  |\n",
            "|    value_loss         | 1.69     |\n",
            "------------------------------------\n",
            "Num timesteps: 4936000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 102.44\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 190      |\n",
            "|    ep_rew_mean        | 102      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1172     |\n",
            "|    iterations         | 123400   |\n",
            "|    time_elapsed       | 4208     |\n",
            "|    total_timesteps    | 4936000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.338   |\n",
            "|    explained_variance | 0.958    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 123399   |\n",
            "|    policy_loss        | -2.95    |\n",
            "|    value_loss         | 132      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 181      |\n",
            "|    ep_rew_mean        | 108      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1173     |\n",
            "|    iterations         | 123500   |\n",
            "|    time_elapsed       | 4211     |\n",
            "|    total_timesteps    | 4940000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.242   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 123499   |\n",
            "|    policy_loss        | 0.84     |\n",
            "|    value_loss         | 8.48     |\n",
            "------------------------------------\n",
            "Num timesteps: 4944000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 111.99\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 185      |\n",
            "|    ep_rew_mean        | 112      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1173     |\n",
            "|    iterations         | 123600   |\n",
            "|    time_elapsed       | 4213     |\n",
            "|    total_timesteps    | 4944000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.209   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 123599   |\n",
            "|    policy_loss        | 0.0521   |\n",
            "|    value_loss         | 0.322    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 209      |\n",
            "|    ep_rew_mean        | 120      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1173     |\n",
            "|    iterations         | 123700   |\n",
            "|    time_elapsed       | 4216     |\n",
            "|    total_timesteps    | 4948000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.355   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 123699   |\n",
            "|    policy_loss        | -0.755   |\n",
            "|    value_loss         | 9.16     |\n",
            "------------------------------------\n",
            "Num timesteps: 4952000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 128.64\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 221      |\n",
            "|    ep_rew_mean        | 129      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1173     |\n",
            "|    iterations         | 123800   |\n",
            "|    time_elapsed       | 4218     |\n",
            "|    total_timesteps    | 4952000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.386   |\n",
            "|    explained_variance | 0.637    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 123799   |\n",
            "|    policy_loss        | 0.246    |\n",
            "|    value_loss         | 359      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 203      |\n",
            "|    ep_rew_mean        | 112      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1174     |\n",
            "|    iterations         | 123900   |\n",
            "|    time_elapsed       | 4221     |\n",
            "|    total_timesteps    | 4956000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.163   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 123899   |\n",
            "|    policy_loss        | -0.122   |\n",
            "|    value_loss         | 2.44     |\n",
            "------------------------------------\n",
            "Num timesteps: 4960000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 100.55\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 200      |\n",
            "|    ep_rew_mean        | 101      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1174     |\n",
            "|    iterations         | 124000   |\n",
            "|    time_elapsed       | 4223     |\n",
            "|    total_timesteps    | 4960000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.267   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 123999   |\n",
            "|    policy_loss        | -0.0602  |\n",
            "|    value_loss         | 6.6      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 191      |\n",
            "|    ep_rew_mean        | 101      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1174     |\n",
            "|    iterations         | 124100   |\n",
            "|    time_elapsed       | 4225     |\n",
            "|    total_timesteps    | 4964000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.317   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 124099   |\n",
            "|    policy_loss        | 0.101    |\n",
            "|    value_loss         | 3.01     |\n",
            "------------------------------------\n",
            "Num timesteps: 4968000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 92.04\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 162      |\n",
            "|    ep_rew_mean        | 92       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1175     |\n",
            "|    iterations         | 124200   |\n",
            "|    time_elapsed       | 4227     |\n",
            "|    total_timesteps    | 4968000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.416   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 124199   |\n",
            "|    policy_loss        | -0.661   |\n",
            "|    value_loss         | 12.7     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 166      |\n",
            "|    ep_rew_mean        | 96.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1175     |\n",
            "|    iterations         | 124300   |\n",
            "|    time_elapsed       | 4229     |\n",
            "|    total_timesteps    | 4972000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.363   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 124299   |\n",
            "|    policy_loss        | -0.212   |\n",
            "|    value_loss         | 10.1     |\n",
            "------------------------------------\n",
            "Num timesteps: 4976000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 112.34\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 186      |\n",
            "|    ep_rew_mean        | 112      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1175     |\n",
            "|    iterations         | 124400   |\n",
            "|    time_elapsed       | 4232     |\n",
            "|    total_timesteps    | 4976000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.406   |\n",
            "|    explained_variance | 0.923    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 124399   |\n",
            "|    policy_loss        | -4.56    |\n",
            "|    value_loss         | 318      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 204      |\n",
            "|    ep_rew_mean        | 133      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1175     |\n",
            "|    iterations         | 124500   |\n",
            "|    time_elapsed       | 4235     |\n",
            "|    total_timesteps    | 4980000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.373   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 124499   |\n",
            "|    policy_loss        | -0.274   |\n",
            "|    value_loss         | 5.15     |\n",
            "------------------------------------\n",
            "Num timesteps: 4984000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 138.58\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 206      |\n",
            "|    ep_rew_mean        | 139      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1176     |\n",
            "|    iterations         | 124600   |\n",
            "|    time_elapsed       | 4237     |\n",
            "|    total_timesteps    | 4984000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.264   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 124599   |\n",
            "|    policy_loss        | -0.285   |\n",
            "|    value_loss         | 3.55     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 221      |\n",
            "|    ep_rew_mean        | 154      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1176     |\n",
            "|    iterations         | 124700   |\n",
            "|    time_elapsed       | 4241     |\n",
            "|    total_timesteps    | 4988000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.404   |\n",
            "|    explained_variance | 0.687    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 124699   |\n",
            "|    policy_loss        | 0.0453   |\n",
            "|    value_loss         | 488      |\n",
            "------------------------------------\n",
            "Num timesteps: 4992000\n",
            "Best mean reward: 169.94 - Last mean reward per episode: 174.31\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 234      |\n",
            "|    ep_rew_mean        | 174      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1176     |\n",
            "|    iterations         | 124800   |\n",
            "|    time_elapsed       | 4244     |\n",
            "|    total_timesteps    | 4992000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.402   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 124799   |\n",
            "|    policy_loss        | 0.104    |\n",
            "|    value_loss         | 1.27     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 258      |\n",
            "|    ep_rew_mean        | 169      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1176     |\n",
            "|    iterations         | 124900   |\n",
            "|    time_elapsed       | 4247     |\n",
            "|    total_timesteps    | 4996000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.276   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 124899   |\n",
            "|    policy_loss        | -0.0925  |\n",
            "|    value_loss         | 0.538    |\n",
            "------------------------------------\n",
            "Num timesteps: 5000000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 168.41\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 267      |\n",
            "|    ep_rew_mean        | 168      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1176     |\n",
            "|    iterations         | 125000   |\n",
            "|    time_elapsed       | 4250     |\n",
            "|    total_timesteps    | 5000000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.295   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 124999   |\n",
            "|    policy_loss        | -0.616   |\n",
            "|    value_loss         | 22.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 255      |\n",
            "|    ep_rew_mean        | 163      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1176     |\n",
            "|    iterations         | 125100   |\n",
            "|    time_elapsed       | 4253     |\n",
            "|    total_timesteps    | 5004000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.279   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 125099   |\n",
            "|    policy_loss        | 0.0383   |\n",
            "|    value_loss         | 17.9     |\n",
            "------------------------------------\n",
            "Num timesteps: 5008000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 151.72\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 265      |\n",
            "|    ep_rew_mean        | 152      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1176     |\n",
            "|    iterations         | 125200   |\n",
            "|    time_elapsed       | 4256     |\n",
            "|    total_timesteps    | 5008000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.396   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 125199   |\n",
            "|    policy_loss        | 0.427    |\n",
            "|    value_loss         | 10.8     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 271      |\n",
            "|    ep_rew_mean        | 137      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1176     |\n",
            "|    iterations         | 125300   |\n",
            "|    time_elapsed       | 4260     |\n",
            "|    total_timesteps    | 5012000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.392   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 125299   |\n",
            "|    policy_loss        | -0.499   |\n",
            "|    value_loss         | 7.1      |\n",
            "------------------------------------\n",
            "Num timesteps: 5016000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 123.80\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 291      |\n",
            "|    ep_rew_mean        | 124      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1176     |\n",
            "|    iterations         | 125400   |\n",
            "|    time_elapsed       | 4264     |\n",
            "|    total_timesteps    | 5016000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.505   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 125399   |\n",
            "|    policy_loss        | -0.00719 |\n",
            "|    value_loss         | 4.43     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 296      |\n",
            "|    ep_rew_mean        | 111      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1176     |\n",
            "|    iterations         | 125500   |\n",
            "|    time_elapsed       | 4267     |\n",
            "|    total_timesteps    | 5020000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.487   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 125499   |\n",
            "|    policy_loss        | 0.202    |\n",
            "|    value_loss         | 6.73     |\n",
            "------------------------------------\n",
            "Num timesteps: 5024000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 106.38\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 281      |\n",
            "|    ep_rew_mean        | 106      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1176     |\n",
            "|    iterations         | 125600   |\n",
            "|    time_elapsed       | 4270     |\n",
            "|    total_timesteps    | 5024000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.238   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 125599   |\n",
            "|    policy_loss        | 0.0536   |\n",
            "|    value_loss         | 2.04     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 283      |\n",
            "|    ep_rew_mean        | 103      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1176     |\n",
            "|    iterations         | 125700   |\n",
            "|    time_elapsed       | 4273     |\n",
            "|    total_timesteps    | 5028000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.354   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 125699   |\n",
            "|    policy_loss        | 0.092    |\n",
            "|    value_loss         | 10.1     |\n",
            "------------------------------------\n",
            "Num timesteps: 5032000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 106.06\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 265      |\n",
            "|    ep_rew_mean        | 106      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1176     |\n",
            "|    iterations         | 125800   |\n",
            "|    time_elapsed       | 4275     |\n",
            "|    total_timesteps    | 5032000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.34    |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 125799   |\n",
            "|    policy_loss        | 0.0838   |\n",
            "|    value_loss         | 2.07     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 258      |\n",
            "|    ep_rew_mean        | 102      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1177     |\n",
            "|    iterations         | 125900   |\n",
            "|    time_elapsed       | 4278     |\n",
            "|    total_timesteps    | 5036000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.244   |\n",
            "|    explained_variance | 0.938    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 125899   |\n",
            "|    policy_loss        | -0.19    |\n",
            "|    value_loss         | 207      |\n",
            "------------------------------------\n",
            "Num timesteps: 5040000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 97.85\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 204      |\n",
            "|    ep_rew_mean        | 97.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1177     |\n",
            "|    iterations         | 126000   |\n",
            "|    time_elapsed       | 4280     |\n",
            "|    total_timesteps    | 5040000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.267   |\n",
            "|    explained_variance | 0.962    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 125999   |\n",
            "|    policy_loss        | -0.00506 |\n",
            "|    value_loss         | 70.2     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 190      |\n",
            "|    ep_rew_mean        | 85.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1177     |\n",
            "|    iterations         | 126100   |\n",
            "|    time_elapsed       | 4281     |\n",
            "|    total_timesteps    | 5044000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.269   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 126099   |\n",
            "|    policy_loss        | -0.244   |\n",
            "|    value_loss         | 3.82     |\n",
            "------------------------------------\n",
            "Num timesteps: 5048000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 88.24\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 180      |\n",
            "|    ep_rew_mean        | 88.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1178     |\n",
            "|    iterations         | 126200   |\n",
            "|    time_elapsed       | 4284     |\n",
            "|    total_timesteps    | 5048000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.256   |\n",
            "|    explained_variance | 0.946    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 126199   |\n",
            "|    policy_loss        | -0.229   |\n",
            "|    value_loss         | 183      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 176      |\n",
            "|    ep_rew_mean        | 95.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1178     |\n",
            "|    iterations         | 126300   |\n",
            "|    time_elapsed       | 4286     |\n",
            "|    total_timesteps    | 5052000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.375   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 126299   |\n",
            "|    policy_loss        | 0.0148   |\n",
            "|    value_loss         | 0.864    |\n",
            "------------------------------------\n",
            "Num timesteps: 5056000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 112.96\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 180      |\n",
            "|    ep_rew_mean        | 113      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1178     |\n",
            "|    iterations         | 126400   |\n",
            "|    time_elapsed       | 4289     |\n",
            "|    total_timesteps    | 5056000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.249   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 126399   |\n",
            "|    policy_loss        | 0.25     |\n",
            "|    value_loss         | 2.42     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 197      |\n",
            "|    ep_rew_mean        | 122      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1179     |\n",
            "|    iterations         | 126500   |\n",
            "|    time_elapsed       | 4291     |\n",
            "|    total_timesteps    | 5060000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.265   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 126499   |\n",
            "|    policy_loss        | -0.0672  |\n",
            "|    value_loss         | 2.24     |\n",
            "------------------------------------\n",
            "Num timesteps: 5064000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 128.30\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 196      |\n",
            "|    ep_rew_mean        | 128      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1179     |\n",
            "|    iterations         | 126600   |\n",
            "|    time_elapsed       | 4293     |\n",
            "|    total_timesteps    | 5064000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.237   |\n",
            "|    explained_variance | 0.966    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 126599   |\n",
            "|    policy_loss        | 0.173    |\n",
            "|    value_loss         | 138      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 195      |\n",
            "|    ep_rew_mean        | 125      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1179     |\n",
            "|    iterations         | 126700   |\n",
            "|    time_elapsed       | 4295     |\n",
            "|    total_timesteps    | 5068000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.275   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 126699   |\n",
            "|    policy_loss        | 0.0551   |\n",
            "|    value_loss         | 3.1      |\n",
            "------------------------------------\n",
            "Num timesteps: 5072000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 118.85\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 190      |\n",
            "|    ep_rew_mean        | 119      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1180     |\n",
            "|    iterations         | 126800   |\n",
            "|    time_elapsed       | 4297     |\n",
            "|    total_timesteps    | 5072000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.244   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 126799   |\n",
            "|    policy_loss        | -0.208   |\n",
            "|    value_loss         | 12.7     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 186      |\n",
            "|    ep_rew_mean        | 109      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1180     |\n",
            "|    iterations         | 126900   |\n",
            "|    time_elapsed       | 4300     |\n",
            "|    total_timesteps    | 5076000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.335   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 126899   |\n",
            "|    policy_loss        | 0.12     |\n",
            "|    value_loss         | 2.1      |\n",
            "------------------------------------\n",
            "Num timesteps: 5080000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 114.86\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 187      |\n",
            "|    ep_rew_mean        | 115      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1180     |\n",
            "|    iterations         | 127000   |\n",
            "|    time_elapsed       | 4302     |\n",
            "|    total_timesteps    | 5080000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.271   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 126999   |\n",
            "|    policy_loss        | -0.302   |\n",
            "|    value_loss         | 7.58     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 195      |\n",
            "|    ep_rew_mean        | 101      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1180     |\n",
            "|    iterations         | 127100   |\n",
            "|    time_elapsed       | 4305     |\n",
            "|    total_timesteps    | 5084000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.345   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 127099   |\n",
            "|    policy_loss        | -0.0952  |\n",
            "|    value_loss         | 9.67     |\n",
            "------------------------------------\n",
            "Num timesteps: 5088000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 94.65\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 198      |\n",
            "|    ep_rew_mean        | 94.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1181     |\n",
            "|    iterations         | 127200   |\n",
            "|    time_elapsed       | 4307     |\n",
            "|    total_timesteps    | 5088000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.383   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 127199   |\n",
            "|    policy_loss        | 0.383    |\n",
            "|    value_loss         | 4.6      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 209      |\n",
            "|    ep_rew_mean        | 98.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1181     |\n",
            "|    iterations         | 127300   |\n",
            "|    time_elapsed       | 4310     |\n",
            "|    total_timesteps    | 5092000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.221   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 127299   |\n",
            "|    policy_loss        | -0.091   |\n",
            "|    value_loss         | 35.7     |\n",
            "------------------------------------\n",
            "Num timesteps: 5096000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 92.38\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 212      |\n",
            "|    ep_rew_mean        | 92.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1181     |\n",
            "|    iterations         | 127400   |\n",
            "|    time_elapsed       | 4312     |\n",
            "|    total_timesteps    | 5096000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.434   |\n",
            "|    explained_variance | 0.96     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 127399   |\n",
            "|    policy_loss        | 0.0455   |\n",
            "|    value_loss         | 93.1     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 225      |\n",
            "|    ep_rew_mean        | 113      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1181     |\n",
            "|    iterations         | 127500   |\n",
            "|    time_elapsed       | 4315     |\n",
            "|    total_timesteps    | 5100000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.322   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 127499   |\n",
            "|    policy_loss        | -0.0371  |\n",
            "|    value_loss         | 2.26     |\n",
            "------------------------------------\n",
            "Num timesteps: 5104000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 112.14\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 223      |\n",
            "|    ep_rew_mean        | 112      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1182     |\n",
            "|    iterations         | 127600   |\n",
            "|    time_elapsed       | 4317     |\n",
            "|    total_timesteps    | 5104000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.423   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 127599   |\n",
            "|    policy_loss        | -0.0289  |\n",
            "|    value_loss         | 9.71     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 234      |\n",
            "|    ep_rew_mean        | 137      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1182     |\n",
            "|    iterations         | 127700   |\n",
            "|    time_elapsed       | 4320     |\n",
            "|    total_timesteps    | 5108000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.312   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 127699   |\n",
            "|    policy_loss        | -0.0991  |\n",
            "|    value_loss         | 6.36     |\n",
            "------------------------------------\n",
            "Num timesteps: 5112000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 140.85\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 227      |\n",
            "|    ep_rew_mean        | 141      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1182     |\n",
            "|    iterations         | 127800   |\n",
            "|    time_elapsed       | 4322     |\n",
            "|    total_timesteps    | 5112000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.281   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 127799   |\n",
            "|    policy_loss        | -0.419   |\n",
            "|    value_loss         | 15       |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 215      |\n",
            "|    ep_rew_mean        | 117      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1182     |\n",
            "|    iterations         | 127900   |\n",
            "|    time_elapsed       | 4324     |\n",
            "|    total_timesteps    | 5116000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.409   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 127899   |\n",
            "|    policy_loss        | -0.223   |\n",
            "|    value_loss         | 4.96     |\n",
            "------------------------------------\n",
            "Num timesteps: 5120000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 106.27\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 199      |\n",
            "|    ep_rew_mean        | 106      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1183     |\n",
            "|    iterations         | 128000   |\n",
            "|    time_elapsed       | 4327     |\n",
            "|    total_timesteps    | 5120000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.353   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 127999   |\n",
            "|    policy_loss        | -0.0282  |\n",
            "|    value_loss         | 0.641    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 212      |\n",
            "|    ep_rew_mean        | 101      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1183     |\n",
            "|    iterations         | 128100   |\n",
            "|    time_elapsed       | 4330     |\n",
            "|    total_timesteps    | 5124000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.324   |\n",
            "|    explained_variance | 0.452    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 128099   |\n",
            "|    policy_loss        | 0.353    |\n",
            "|    value_loss         | 1.19e+03 |\n",
            "------------------------------------\n",
            "Num timesteps: 5128000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 106.97\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 226      |\n",
            "|    ep_rew_mean        | 107      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1183     |\n",
            "|    iterations         | 128200   |\n",
            "|    time_elapsed       | 4333     |\n",
            "|    total_timesteps    | 5128000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.447   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 128199   |\n",
            "|    policy_loss        | -0.707   |\n",
            "|    value_loss         | 4.75     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 212      |\n",
            "|    ep_rew_mean        | 101      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1183     |\n",
            "|    iterations         | 128300   |\n",
            "|    time_elapsed       | 4335     |\n",
            "|    total_timesteps    | 5132000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.388   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 128299   |\n",
            "|    policy_loss        | 0.195    |\n",
            "|    value_loss         | 6.67     |\n",
            "------------------------------------\n",
            "Num timesteps: 5136000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 106.45\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 214      |\n",
            "|    ep_rew_mean        | 106      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1183     |\n",
            "|    iterations         | 128400   |\n",
            "|    time_elapsed       | 4338     |\n",
            "|    total_timesteps    | 5136000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.253   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 128399   |\n",
            "|    policy_loss        | 0.145    |\n",
            "|    value_loss         | 5.23     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 228      |\n",
            "|    ep_rew_mean        | 120      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1184     |\n",
            "|    iterations         | 128500   |\n",
            "|    time_elapsed       | 4341     |\n",
            "|    total_timesteps    | 5140000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.401   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 128499   |\n",
            "|    policy_loss        | 0.244    |\n",
            "|    value_loss         | 6.86     |\n",
            "------------------------------------\n",
            "Num timesteps: 5144000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 96.16\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 219      |\n",
            "|    ep_rew_mean        | 96.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1184     |\n",
            "|    iterations         | 128600   |\n",
            "|    time_elapsed       | 4343     |\n",
            "|    total_timesteps    | 5144000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.386   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 128599   |\n",
            "|    policy_loss        | -0.754   |\n",
            "|    value_loss         | 12.1     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 200      |\n",
            "|    ep_rew_mean        | 88.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1184     |\n",
            "|    iterations         | 128700   |\n",
            "|    time_elapsed       | 4345     |\n",
            "|    total_timesteps    | 5148000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.317   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 128699   |\n",
            "|    policy_loss        | -0.353   |\n",
            "|    value_loss         | 5.12     |\n",
            "------------------------------------\n",
            "Num timesteps: 5152000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 92.64\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 196      |\n",
            "|    ep_rew_mean        | 92.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1184     |\n",
            "|    iterations         | 128800   |\n",
            "|    time_elapsed       | 4348     |\n",
            "|    total_timesteps    | 5152000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.395   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 128799   |\n",
            "|    policy_loss        | -0.402   |\n",
            "|    value_loss         | 16.8     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 201      |\n",
            "|    ep_rew_mean        | 84.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1185     |\n",
            "|    iterations         | 128900   |\n",
            "|    time_elapsed       | 4350     |\n",
            "|    total_timesteps    | 5156000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.336   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 128899   |\n",
            "|    policy_loss        | -0.292   |\n",
            "|    value_loss         | 2.55     |\n",
            "------------------------------------\n",
            "Num timesteps: 5160000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 74.28\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 208      |\n",
            "|    ep_rew_mean        | 74.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1185     |\n",
            "|    iterations         | 129000   |\n",
            "|    time_elapsed       | 4354     |\n",
            "|    total_timesteps    | 5160000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.347   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 128999   |\n",
            "|    policy_loss        | 0.45     |\n",
            "|    value_loss         | 7.77     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 218      |\n",
            "|    ep_rew_mean        | 70.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1185     |\n",
            "|    iterations         | 129100   |\n",
            "|    time_elapsed       | 4357     |\n",
            "|    total_timesteps    | 5164000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.273   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 129099   |\n",
            "|    policy_loss        | 0.525    |\n",
            "|    value_loss         | 10.9     |\n",
            "------------------------------------\n",
            "Num timesteps: 5168000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 76.03\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 226      |\n",
            "|    ep_rew_mean        | 76       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1185     |\n",
            "|    iterations         | 129200   |\n",
            "|    time_elapsed       | 4359     |\n",
            "|    total_timesteps    | 5168000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.51    |\n",
            "|    explained_variance | 0.978    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 129199   |\n",
            "|    policy_loss        | -0.532   |\n",
            "|    value_loss         | 118      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 237      |\n",
            "|    ep_rew_mean        | 79.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1185     |\n",
            "|    iterations         | 129300   |\n",
            "|    time_elapsed       | 4362     |\n",
            "|    total_timesteps    | 5172000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.249   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 129299   |\n",
            "|    policy_loss        | 0.193    |\n",
            "|    value_loss         | 3.4      |\n",
            "------------------------------------\n",
            "Num timesteps: 5176000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 79.14\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 245      |\n",
            "|    ep_rew_mean        | 79.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1185     |\n",
            "|    iterations         | 129400   |\n",
            "|    time_elapsed       | 4365     |\n",
            "|    total_timesteps    | 5176000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.329   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 129399   |\n",
            "|    policy_loss        | 0.352    |\n",
            "|    value_loss         | 6.59     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 250      |\n",
            "|    ep_rew_mean        | 81.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1185     |\n",
            "|    iterations         | 129500   |\n",
            "|    time_elapsed       | 4368     |\n",
            "|    total_timesteps    | 5180000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.359   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 129499   |\n",
            "|    policy_loss        | -0.642   |\n",
            "|    value_loss         | 31.1     |\n",
            "------------------------------------\n",
            "Num timesteps: 5184000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 82.60\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 258      |\n",
            "|    ep_rew_mean        | 82.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1185     |\n",
            "|    iterations         | 129600   |\n",
            "|    time_elapsed       | 4371     |\n",
            "|    total_timesteps    | 5184000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.351   |\n",
            "|    explained_variance | 0.488    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 129599   |\n",
            "|    policy_loss        | 0.0226   |\n",
            "|    value_loss         | 2.02e+03 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 246      |\n",
            "|    ep_rew_mean        | 91.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1185     |\n",
            "|    iterations         | 129700   |\n",
            "|    time_elapsed       | 4374     |\n",
            "|    total_timesteps    | 5188000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.313   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 129699   |\n",
            "|    policy_loss        | -0.299   |\n",
            "|    value_loss         | 8.95     |\n",
            "------------------------------------\n",
            "Num timesteps: 5192000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 94.64\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 248      |\n",
            "|    ep_rew_mean        | 94.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1185     |\n",
            "|    iterations         | 129800   |\n",
            "|    time_elapsed       | 4377     |\n",
            "|    total_timesteps    | 5192000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.3     |\n",
            "|    explained_variance | 0.971    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 129799   |\n",
            "|    policy_loss        | -0.95    |\n",
            "|    value_loss         | 76.6     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 263      |\n",
            "|    ep_rew_mean        | 91.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1186     |\n",
            "|    iterations         | 129900   |\n",
            "|    time_elapsed       | 4380     |\n",
            "|    total_timesteps    | 5196000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.31    |\n",
            "|    explained_variance | 0.679    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 129899   |\n",
            "|    policy_loss        | -0.72    |\n",
            "|    value_loss         | 1.33e+03 |\n",
            "------------------------------------\n",
            "Num timesteps: 5200000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 91.45\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 267      |\n",
            "|    ep_rew_mean        | 91.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1186     |\n",
            "|    iterations         | 130000   |\n",
            "|    time_elapsed       | 4383     |\n",
            "|    total_timesteps    | 5200000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.447   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 129999   |\n",
            "|    policy_loss        | 0.029    |\n",
            "|    value_loss         | 1.1      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 275      |\n",
            "|    ep_rew_mean        | 99.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1186     |\n",
            "|    iterations         | 130100   |\n",
            "|    time_elapsed       | 4387     |\n",
            "|    total_timesteps    | 5204000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.395   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 130099   |\n",
            "|    policy_loss        | 0.313    |\n",
            "|    value_loss         | 7.19     |\n",
            "------------------------------------\n",
            "Num timesteps: 5208000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 89.25\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 298      |\n",
            "|    ep_rew_mean        | 89.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1185     |\n",
            "|    iterations         | 130200   |\n",
            "|    time_elapsed       | 4391     |\n",
            "|    total_timesteps    | 5208000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.456   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 130199   |\n",
            "|    policy_loss        | -0.0663  |\n",
            "|    value_loss         | 9.4      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 286      |\n",
            "|    ep_rew_mean        | 99.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1185     |\n",
            "|    iterations         | 130300   |\n",
            "|    time_elapsed       | 4394     |\n",
            "|    total_timesteps    | 5212000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.375   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 130299   |\n",
            "|    policy_loss        | 0.12     |\n",
            "|    value_loss         | 3.65     |\n",
            "------------------------------------\n",
            "Num timesteps: 5216000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 109.02\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 289      |\n",
            "|    ep_rew_mean        | 109      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1186     |\n",
            "|    iterations         | 130400   |\n",
            "|    time_elapsed       | 4397     |\n",
            "|    total_timesteps    | 5216000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.398   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 130399   |\n",
            "|    policy_loss        | 0.0878   |\n",
            "|    value_loss         | 4.19     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 269      |\n",
            "|    ep_rew_mean        | 114      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1186     |\n",
            "|    iterations         | 130500   |\n",
            "|    time_elapsed       | 4400     |\n",
            "|    total_timesteps    | 5220000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.376   |\n",
            "|    explained_variance | 0.949    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 130499   |\n",
            "|    policy_loss        | -0.153   |\n",
            "|    value_loss         | 230      |\n",
            "------------------------------------\n",
            "Num timesteps: 5224000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 116.51\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 278      |\n",
            "|    ep_rew_mean        | 117      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1186     |\n",
            "|    iterations         | 130600   |\n",
            "|    time_elapsed       | 4402     |\n",
            "|    total_timesteps    | 5224000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.456   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 130599   |\n",
            "|    policy_loss        | 0.13     |\n",
            "|    value_loss         | 3.57     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 257      |\n",
            "|    ep_rew_mean        | 114      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1186     |\n",
            "|    iterations         | 130700   |\n",
            "|    time_elapsed       | 4405     |\n",
            "|    total_timesteps    | 5228000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.308   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 130699   |\n",
            "|    policy_loss        | -0.177   |\n",
            "|    value_loss         | 3.08     |\n",
            "------------------------------------\n",
            "Num timesteps: 5232000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 119.47\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 243      |\n",
            "|    ep_rew_mean        | 119      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1186     |\n",
            "|    iterations         | 130800   |\n",
            "|    time_elapsed       | 4408     |\n",
            "|    total_timesteps    | 5232000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.35    |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 130799   |\n",
            "|    policy_loss        | 0.313    |\n",
            "|    value_loss         | 3.96     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 234      |\n",
            "|    ep_rew_mean        | 119      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1187     |\n",
            "|    iterations         | 130900   |\n",
            "|    time_elapsed       | 4410     |\n",
            "|    total_timesteps    | 5236000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.318   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 130899   |\n",
            "|    policy_loss        | 0.215    |\n",
            "|    value_loss         | 7.43     |\n",
            "------------------------------------\n",
            "Num timesteps: 5240000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 116.59\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 235      |\n",
            "|    ep_rew_mean        | 117      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1187     |\n",
            "|    iterations         | 131000   |\n",
            "|    time_elapsed       | 4413     |\n",
            "|    total_timesteps    | 5240000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.353   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 130999   |\n",
            "|    policy_loss        | -0.0254  |\n",
            "|    value_loss         | 3.01     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 225      |\n",
            "|    ep_rew_mean        | 105      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1187     |\n",
            "|    iterations         | 131100   |\n",
            "|    time_elapsed       | 4415     |\n",
            "|    total_timesteps    | 5244000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.346   |\n",
            "|    explained_variance | 0.948    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 131099   |\n",
            "|    policy_loss        | 0.0209   |\n",
            "|    value_loss         | 233      |\n",
            "------------------------------------\n",
            "Num timesteps: 5248000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 112.08\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 211      |\n",
            "|    ep_rew_mean        | 112      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1187     |\n",
            "|    iterations         | 131200   |\n",
            "|    time_elapsed       | 4417     |\n",
            "|    total_timesteps    | 5248000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.197   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 131199   |\n",
            "|    policy_loss        | -0.217   |\n",
            "|    value_loss         | 6.39     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 200      |\n",
            "|    ep_rew_mean        | 116      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1188     |\n",
            "|    iterations         | 131300   |\n",
            "|    time_elapsed       | 4420     |\n",
            "|    total_timesteps    | 5252000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.347   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 131299   |\n",
            "|    policy_loss        | -0.49    |\n",
            "|    value_loss         | 15       |\n",
            "------------------------------------\n",
            "Num timesteps: 5256000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 113.76\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 196      |\n",
            "|    ep_rew_mean        | 114      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1188     |\n",
            "|    iterations         | 131400   |\n",
            "|    time_elapsed       | 4422     |\n",
            "|    total_timesteps    | 5256000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.221   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 131399   |\n",
            "|    policy_loss        | -0.124   |\n",
            "|    value_loss         | 9.12     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 204      |\n",
            "|    ep_rew_mean        | 109      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1188     |\n",
            "|    iterations         | 131500   |\n",
            "|    time_elapsed       | 4425     |\n",
            "|    total_timesteps    | 5260000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.443   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 131499   |\n",
            "|    policy_loss        | -0.213   |\n",
            "|    value_loss         | 5.82     |\n",
            "------------------------------------\n",
            "Num timesteps: 5264000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 119.10\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 206      |\n",
            "|    ep_rew_mean        | 119      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1188     |\n",
            "|    iterations         | 131600   |\n",
            "|    time_elapsed       | 4428     |\n",
            "|    total_timesteps    | 5264000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.238   |\n",
            "|    explained_variance | 0.935    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 131599   |\n",
            "|    policy_loss        | -1.16    |\n",
            "|    value_loss         | 194      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 207      |\n",
            "|    ep_rew_mean        | 120      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1188     |\n",
            "|    iterations         | 131700   |\n",
            "|    time_elapsed       | 4431     |\n",
            "|    total_timesteps    | 5268000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.352   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 131699   |\n",
            "|    policy_loss        | 0.228    |\n",
            "|    value_loss         | 12.3     |\n",
            "------------------------------------\n",
            "Num timesteps: 5272000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 104.23\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 223      |\n",
            "|    ep_rew_mean        | 104      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1188     |\n",
            "|    iterations         | 131800   |\n",
            "|    time_elapsed       | 4435     |\n",
            "|    total_timesteps    | 5272000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.287   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 131799   |\n",
            "|    policy_loss        | -0.0389  |\n",
            "|    value_loss         | 2.37     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 232      |\n",
            "|    ep_rew_mean        | 104      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1188     |\n",
            "|    iterations         | 131900   |\n",
            "|    time_elapsed       | 4438     |\n",
            "|    total_timesteps    | 5276000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.279   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 131899   |\n",
            "|    policy_loss        | 0.0897   |\n",
            "|    value_loss         | 4.89     |\n",
            "------------------------------------\n",
            "Num timesteps: 5280000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 107.07\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 237      |\n",
            "|    ep_rew_mean        | 107      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1188     |\n",
            "|    iterations         | 132000   |\n",
            "|    time_elapsed       | 4440     |\n",
            "|    total_timesteps    | 5280000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.241   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 131999   |\n",
            "|    policy_loss        | -0.456   |\n",
            "|    value_loss         | 4.26     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 238      |\n",
            "|    ep_rew_mean        | 116      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1188     |\n",
            "|    iterations         | 132100   |\n",
            "|    time_elapsed       | 4444     |\n",
            "|    total_timesteps    | 5284000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.217   |\n",
            "|    explained_variance | 0.457    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 132099   |\n",
            "|    policy_loss        | -0.366   |\n",
            "|    value_loss         | 888      |\n",
            "------------------------------------\n",
            "Num timesteps: 5288000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 126.88\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 244      |\n",
            "|    ep_rew_mean        | 127      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1189     |\n",
            "|    iterations         | 132200   |\n",
            "|    time_elapsed       | 4447     |\n",
            "|    total_timesteps    | 5288000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.294   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 132199   |\n",
            "|    policy_loss        | -0.0837  |\n",
            "|    value_loss         | 7.88     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 239      |\n",
            "|    ep_rew_mean        | 136      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1189     |\n",
            "|    iterations         | 132300   |\n",
            "|    time_elapsed       | 4449     |\n",
            "|    total_timesteps    | 5292000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.233   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 132299   |\n",
            "|    policy_loss        | 0.0255   |\n",
            "|    value_loss         | 4.52     |\n",
            "------------------------------------\n",
            "Num timesteps: 5296000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 142.56\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 233      |\n",
            "|    ep_rew_mean        | 143      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1189     |\n",
            "|    iterations         | 132400   |\n",
            "|    time_elapsed       | 4452     |\n",
            "|    total_timesteps    | 5296000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.456   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 132399   |\n",
            "|    policy_loss        | -0.143   |\n",
            "|    value_loss         | 2.88     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 217      |\n",
            "|    ep_rew_mean        | 139      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1189     |\n",
            "|    iterations         | 132500   |\n",
            "|    time_elapsed       | 4455     |\n",
            "|    total_timesteps    | 5300000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.367   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 132499   |\n",
            "|    policy_loss        | -0.398   |\n",
            "|    value_loss         | 11.1     |\n",
            "------------------------------------\n",
            "Num timesteps: 5304000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 150.57\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 221      |\n",
            "|    ep_rew_mean        | 151      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1189     |\n",
            "|    iterations         | 132600   |\n",
            "|    time_elapsed       | 4458     |\n",
            "|    total_timesteps    | 5304000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.312   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 132599   |\n",
            "|    policy_loss        | 0.0409   |\n",
            "|    value_loss         | 3.89     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 212      |\n",
            "|    ep_rew_mean        | 140      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1189     |\n",
            "|    iterations         | 132700   |\n",
            "|    time_elapsed       | 4460     |\n",
            "|    total_timesteps    | 5308000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.234   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 132699   |\n",
            "|    policy_loss        | 0.342    |\n",
            "|    value_loss         | 13.6     |\n",
            "------------------------------------\n",
            "Num timesteps: 5312000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 144.82\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 209      |\n",
            "|    ep_rew_mean        | 145      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1190     |\n",
            "|    iterations         | 132800   |\n",
            "|    time_elapsed       | 4463     |\n",
            "|    total_timesteps    | 5312000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.252   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 132799   |\n",
            "|    policy_loss        | -0.352   |\n",
            "|    value_loss         | 1.31     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 216      |\n",
            "|    ep_rew_mean        | 137      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1190     |\n",
            "|    iterations         | 132900   |\n",
            "|    time_elapsed       | 4465     |\n",
            "|    total_timesteps    | 5316000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.304   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 132899   |\n",
            "|    policy_loss        | 0.0491   |\n",
            "|    value_loss         | 4.77     |\n",
            "------------------------------------\n",
            "Num timesteps: 5320000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 136.34\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 219      |\n",
            "|    ep_rew_mean        | 136      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1190     |\n",
            "|    iterations         | 133000   |\n",
            "|    time_elapsed       | 4468     |\n",
            "|    total_timesteps    | 5320000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.36    |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 132999   |\n",
            "|    policy_loss        | 0.205    |\n",
            "|    value_loss         | 2.23     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 219      |\n",
            "|    ep_rew_mean        | 134      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1190     |\n",
            "|    iterations         | 133100   |\n",
            "|    time_elapsed       | 4470     |\n",
            "|    total_timesteps    | 5324000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.204   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 133099   |\n",
            "|    policy_loss        | 0.0242   |\n",
            "|    value_loss         | 6.77     |\n",
            "------------------------------------\n",
            "Num timesteps: 5328000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 116.25\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 212      |\n",
            "|    ep_rew_mean        | 116      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 133200   |\n",
            "|    time_elapsed       | 4473     |\n",
            "|    total_timesteps    | 5328000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.185   |\n",
            "|    explained_variance | 0.791    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 133199   |\n",
            "|    policy_loss        | 0.39     |\n",
            "|    value_loss         | 444      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 210      |\n",
            "|    ep_rew_mean        | 125      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 133300   |\n",
            "|    time_elapsed       | 4475     |\n",
            "|    total_timesteps    | 5332000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.34    |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 133299   |\n",
            "|    policy_loss        | 0.397    |\n",
            "|    value_loss         | 6.89     |\n",
            "------------------------------------\n",
            "Num timesteps: 5336000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 121.78\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 209      |\n",
            "|    ep_rew_mean        | 122      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 133400   |\n",
            "|    time_elapsed       | 4478     |\n",
            "|    total_timesteps    | 5336000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.388   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 133399   |\n",
            "|    policy_loss        | 0.643    |\n",
            "|    value_loss         | 15.9     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 217      |\n",
            "|    ep_rew_mean        | 127      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 133500   |\n",
            "|    time_elapsed       | 4481     |\n",
            "|    total_timesteps    | 5340000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.304   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 133499   |\n",
            "|    policy_loss        | -0.0638  |\n",
            "|    value_loss         | 12.5     |\n",
            "------------------------------------\n",
            "Num timesteps: 5344000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 128.31\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 229      |\n",
            "|    ep_rew_mean        | 128      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 133600   |\n",
            "|    time_elapsed       | 4483     |\n",
            "|    total_timesteps    | 5344000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.297   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 133599   |\n",
            "|    policy_loss        | 0.518    |\n",
            "|    value_loss         | 5.23     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 218      |\n",
            "|    ep_rew_mean        | 136      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 133700   |\n",
            "|    time_elapsed       | 4486     |\n",
            "|    total_timesteps    | 5348000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.307   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 133699   |\n",
            "|    policy_loss        | 0.358    |\n",
            "|    value_loss         | 5.78     |\n",
            "------------------------------------\n",
            "Num timesteps: 5352000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 138.60\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 224      |\n",
            "|    ep_rew_mean        | 139      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 133800   |\n",
            "|    time_elapsed       | 4491     |\n",
            "|    total_timesteps    | 5352000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.307   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 133799   |\n",
            "|    policy_loss        | -0.218   |\n",
            "|    value_loss         | 12.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 266      |\n",
            "|    ep_rew_mean        | 133      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 133900   |\n",
            "|    time_elapsed       | 4495     |\n",
            "|    total_timesteps    | 5356000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.192   |\n",
            "|    explained_variance | 0.55     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 133899   |\n",
            "|    policy_loss        | -0.0789  |\n",
            "|    value_loss         | 595      |\n",
            "------------------------------------\n",
            "Num timesteps: 5360000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 142.09\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 269      |\n",
            "|    ep_rew_mean        | 142      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 134000   |\n",
            "|    time_elapsed       | 4498     |\n",
            "|    total_timesteps    | 5360000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.341   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 133999   |\n",
            "|    policy_loss        | -0.541   |\n",
            "|    value_loss         | 6.97     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 281      |\n",
            "|    ep_rew_mean        | 135      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 134100   |\n",
            "|    time_elapsed       | 4501     |\n",
            "|    total_timesteps    | 5364000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.243   |\n",
            "|    explained_variance | 0.934    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 134099   |\n",
            "|    policy_loss        | 0.159    |\n",
            "|    value_loss         | 127      |\n",
            "------------------------------------\n",
            "Num timesteps: 5368000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 128.95\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 284      |\n",
            "|    ep_rew_mean        | 129      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 134200   |\n",
            "|    time_elapsed       | 4504     |\n",
            "|    total_timesteps    | 5368000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.384   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 134199   |\n",
            "|    policy_loss        | 0.0447   |\n",
            "|    value_loss         | 12       |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 272      |\n",
            "|    ep_rew_mean        | 129      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 134300   |\n",
            "|    time_elapsed       | 4507     |\n",
            "|    total_timesteps    | 5372000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.282   |\n",
            "|    explained_variance | 0.663    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 134299   |\n",
            "|    policy_loss        | 0.0593   |\n",
            "|    value_loss         | 852      |\n",
            "------------------------------------\n",
            "Num timesteps: 5376000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 115.08\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 287      |\n",
            "|    ep_rew_mean        | 115      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 134400   |\n",
            "|    time_elapsed       | 4512     |\n",
            "|    total_timesteps    | 5376000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.352   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 134399   |\n",
            "|    policy_loss        | 0.14     |\n",
            "|    value_loss         | 1.37     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 294      |\n",
            "|    ep_rew_mean        | 115      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 134500   |\n",
            "|    time_elapsed       | 4514     |\n",
            "|    total_timesteps    | 5380000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.385   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 134499   |\n",
            "|    policy_loss        | 0.00186  |\n",
            "|    value_loss         | 4.03     |\n",
            "------------------------------------\n",
            "Num timesteps: 5384000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 96.28\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 266      |\n",
            "|    ep_rew_mean        | 96.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 134600   |\n",
            "|    time_elapsed       | 4518     |\n",
            "|    total_timesteps    | 5384000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.361   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 134599   |\n",
            "|    policy_loss        | 0.279    |\n",
            "|    value_loss         | 3.48     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 275      |\n",
            "|    ep_rew_mean        | 98.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 134700   |\n",
            "|    time_elapsed       | 4520     |\n",
            "|    total_timesteps    | 5388000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.431   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 134699   |\n",
            "|    policy_loss        | 0.151    |\n",
            "|    value_loss         | 11.7     |\n",
            "------------------------------------\n",
            "Num timesteps: 5392000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 94.92\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 272      |\n",
            "|    ep_rew_mean        | 94.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 134800   |\n",
            "|    time_elapsed       | 4523     |\n",
            "|    total_timesteps    | 5392000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.256   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 134799   |\n",
            "|    policy_loss        | -0.0123  |\n",
            "|    value_loss         | 5.98     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 263      |\n",
            "|    ep_rew_mean        | 103      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 134900   |\n",
            "|    time_elapsed       | 4527     |\n",
            "|    total_timesteps    | 5396000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.389   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 134899   |\n",
            "|    policy_loss        | 0.0487   |\n",
            "|    value_loss         | 1.23     |\n",
            "------------------------------------\n",
            "Num timesteps: 5400000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 109.66\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 278      |\n",
            "|    ep_rew_mean        | 110      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 135000   |\n",
            "|    time_elapsed       | 4532     |\n",
            "|    total_timesteps    | 5400000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.331   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 134999   |\n",
            "|    policy_loss        | -0.145   |\n",
            "|    value_loss         | 2.28     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 300      |\n",
            "|    ep_rew_mean        | 102      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 135100   |\n",
            "|    time_elapsed       | 4535     |\n",
            "|    total_timesteps    | 5404000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.327   |\n",
            "|    explained_variance | 0.957    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 135099   |\n",
            "|    policy_loss        | -0.167   |\n",
            "|    value_loss         | 126      |\n",
            "------------------------------------\n",
            "Num timesteps: 5408000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 118.17\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 283      |\n",
            "|    ep_rew_mean        | 118      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 135200   |\n",
            "|    time_elapsed       | 4538     |\n",
            "|    total_timesteps    | 5408000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.432   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 135199   |\n",
            "|    policy_loss        | 0.456    |\n",
            "|    value_loss         | 7.42     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 287      |\n",
            "|    ep_rew_mean        | 114      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 135300   |\n",
            "|    time_elapsed       | 4542     |\n",
            "|    total_timesteps    | 5412000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.385   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 135299   |\n",
            "|    policy_loss        | -0.342   |\n",
            "|    value_loss         | 4.55     |\n",
            "------------------------------------\n",
            "Num timesteps: 5416000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 109.56\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 305      |\n",
            "|    ep_rew_mean        | 110      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1190     |\n",
            "|    iterations         | 135400   |\n",
            "|    time_elapsed       | 4548     |\n",
            "|    total_timesteps    | 5416000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.402   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 135399   |\n",
            "|    policy_loss        | 0.741    |\n",
            "|    value_loss         | 4.3      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 298      |\n",
            "|    ep_rew_mean        | 106      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1190     |\n",
            "|    iterations         | 135500   |\n",
            "|    time_elapsed       | 4551     |\n",
            "|    total_timesteps    | 5420000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.28    |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 135499   |\n",
            "|    policy_loss        | 0.342    |\n",
            "|    value_loss         | 3.5      |\n",
            "------------------------------------\n",
            "Num timesteps: 5424000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 102.87\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 320      |\n",
            "|    ep_rew_mean        | 103      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1190     |\n",
            "|    iterations         | 135600   |\n",
            "|    time_elapsed       | 4554     |\n",
            "|    total_timesteps    | 5424000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.406   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 135599   |\n",
            "|    policy_loss        | -0.227   |\n",
            "|    value_loss         | 6.73     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 316      |\n",
            "|    ep_rew_mean        | 93.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1190     |\n",
            "|    iterations         | 135700   |\n",
            "|    time_elapsed       | 4557     |\n",
            "|    total_timesteps    | 5428000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.345   |\n",
            "|    explained_variance | 0.931    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 135699   |\n",
            "|    policy_loss        | 0.0711   |\n",
            "|    value_loss         | 173      |\n",
            "------------------------------------\n",
            "Num timesteps: 5432000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 80.20\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 304      |\n",
            "|    ep_rew_mean        | 80.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1190     |\n",
            "|    iterations         | 135800   |\n",
            "|    time_elapsed       | 4561     |\n",
            "|    total_timesteps    | 5432000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.315   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 135799   |\n",
            "|    policy_loss        | -0.137   |\n",
            "|    value_loss         | 1.85     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 311      |\n",
            "|    ep_rew_mean        | 80.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1190     |\n",
            "|    iterations         | 135900   |\n",
            "|    time_elapsed       | 4565     |\n",
            "|    total_timesteps    | 5436000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.323   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 135899   |\n",
            "|    policy_loss        | 0.466    |\n",
            "|    value_loss         | 5.97     |\n",
            "------------------------------------\n",
            "Num timesteps: 5440000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 63.76\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 323      |\n",
            "|    ep_rew_mean        | 63.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1190     |\n",
            "|    iterations         | 136000   |\n",
            "|    time_elapsed       | 4569     |\n",
            "|    total_timesteps    | 5440000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.303   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 135999   |\n",
            "|    policy_loss        | 0.249    |\n",
            "|    value_loss         | 4.11     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 305      |\n",
            "|    ep_rew_mean        | 67.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1190     |\n",
            "|    iterations         | 136100   |\n",
            "|    time_elapsed       | 4572     |\n",
            "|    total_timesteps    | 5444000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.269   |\n",
            "|    explained_variance | 0.774    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 136099   |\n",
            "|    policy_loss        | -0.00441 |\n",
            "|    value_loss         | 626      |\n",
            "------------------------------------\n",
            "Num timesteps: 5448000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 64.53\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 302      |\n",
            "|    ep_rew_mean        | 64.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1190     |\n",
            "|    iterations         | 136200   |\n",
            "|    time_elapsed       | 4575     |\n",
            "|    total_timesteps    | 5448000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.382   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 136199   |\n",
            "|    policy_loss        | -0.143   |\n",
            "|    value_loss         | 7.11     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 282      |\n",
            "|    ep_rew_mean        | 62       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1190     |\n",
            "|    iterations         | 136300   |\n",
            "|    time_elapsed       | 4578     |\n",
            "|    total_timesteps    | 5452000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.276   |\n",
            "|    explained_variance | 0.776    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 136299   |\n",
            "|    policy_loss        | 0.0235   |\n",
            "|    value_loss         | 613      |\n",
            "------------------------------------\n",
            "Num timesteps: 5456000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 71.72\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 276      |\n",
            "|    ep_rew_mean        | 71.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1190     |\n",
            "|    iterations         | 136400   |\n",
            "|    time_elapsed       | 4582     |\n",
            "|    total_timesteps    | 5456000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.235   |\n",
            "|    explained_variance | 0.315    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 136399   |\n",
            "|    policy_loss        | -2.77    |\n",
            "|    value_loss         | 2.68e+03 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 262      |\n",
            "|    ep_rew_mean        | 84.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1190     |\n",
            "|    iterations         | 136500   |\n",
            "|    time_elapsed       | 4585     |\n",
            "|    total_timesteps    | 5460000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.268   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 136499   |\n",
            "|    policy_loss        | -0.249   |\n",
            "|    value_loss         | 25.6     |\n",
            "------------------------------------\n",
            "Num timesteps: 5464000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 81.28\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 255      |\n",
            "|    ep_rew_mean        | 81.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1190     |\n",
            "|    iterations         | 136600   |\n",
            "|    time_elapsed       | 4588     |\n",
            "|    total_timesteps    | 5464000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.263   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 136599   |\n",
            "|    policy_loss        | 0.628    |\n",
            "|    value_loss         | 23.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 240      |\n",
            "|    ep_rew_mean        | 87.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 136700   |\n",
            "|    time_elapsed       | 4590     |\n",
            "|    total_timesteps    | 5468000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.346   |\n",
            "|    explained_variance | 0.917    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 136699   |\n",
            "|    policy_loss        | -0.159   |\n",
            "|    value_loss         | 241      |\n",
            "------------------------------------\n",
            "Num timesteps: 5472000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 81.34\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 232      |\n",
            "|    ep_rew_mean        | 81.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 136800   |\n",
            "|    time_elapsed       | 4593     |\n",
            "|    total_timesteps    | 5472000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.327   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 136799   |\n",
            "|    policy_loss        | -0.0694  |\n",
            "|    value_loss         | 9.28     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 238      |\n",
            "|    ep_rew_mean        | 78.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 136900   |\n",
            "|    time_elapsed       | 4596     |\n",
            "|    total_timesteps    | 5476000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.397   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 136899   |\n",
            "|    policy_loss        | -0.313   |\n",
            "|    value_loss         | 4.98     |\n",
            "------------------------------------\n",
            "Num timesteps: 5480000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 67.00\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 235      |\n",
            "|    ep_rew_mean        | 67       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 137000   |\n",
            "|    time_elapsed       | 4599     |\n",
            "|    total_timesteps    | 5480000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.414   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 136999   |\n",
            "|    policy_loss        | 0.243    |\n",
            "|    value_loss         | 3.76     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 236      |\n",
            "|    ep_rew_mean        | 67.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 137100   |\n",
            "|    time_elapsed       | 4602     |\n",
            "|    total_timesteps    | 5484000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.358   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 137099   |\n",
            "|    policy_loss        | 0.134    |\n",
            "|    value_loss         | 1.15     |\n",
            "------------------------------------\n",
            "Num timesteps: 5488000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 68.56\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 239      |\n",
            "|    ep_rew_mean        | 68.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 137200   |\n",
            "|    time_elapsed       | 4605     |\n",
            "|    total_timesteps    | 5488000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.389   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 137199   |\n",
            "|    policy_loss        | 0.26     |\n",
            "|    value_loss         | 6.88     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 237      |\n",
            "|    ep_rew_mean        | 72       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 137300   |\n",
            "|    time_elapsed       | 4608     |\n",
            "|    total_timesteps    | 5492000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.303   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 137299   |\n",
            "|    policy_loss        | 0.00218  |\n",
            "|    value_loss         | 1.11     |\n",
            "------------------------------------\n",
            "Num timesteps: 5496000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 75.56\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 264      |\n",
            "|    ep_rew_mean        | 75.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 137400   |\n",
            "|    time_elapsed       | 4612     |\n",
            "|    total_timesteps    | 5496000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.293   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 137399   |\n",
            "|    policy_loss        | 0.0197   |\n",
            "|    value_loss         | 6.88     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 267      |\n",
            "|    ep_rew_mean        | 92.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 137500   |\n",
            "|    time_elapsed       | 4614     |\n",
            "|    total_timesteps    | 5500000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.262   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 137499   |\n",
            "|    policy_loss        | 0.0621   |\n",
            "|    value_loss         | 41.2     |\n",
            "------------------------------------\n",
            "Num timesteps: 5504000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 101.94\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 275      |\n",
            "|    ep_rew_mean        | 102      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 137600   |\n",
            "|    time_elapsed       | 4617     |\n",
            "|    total_timesteps    | 5504000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.444   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 137599   |\n",
            "|    policy_loss        | 0.0314   |\n",
            "|    value_loss         | 11.1     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 258      |\n",
            "|    ep_rew_mean        | 94.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 137700   |\n",
            "|    time_elapsed       | 4621     |\n",
            "|    total_timesteps    | 5508000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.289   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 137699   |\n",
            "|    policy_loss        | -0.142   |\n",
            "|    value_loss         | 2.66     |\n",
            "------------------------------------\n",
            "Num timesteps: 5512000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 86.62\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 266      |\n",
            "|    ep_rew_mean        | 86.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 137800   |\n",
            "|    time_elapsed       | 4624     |\n",
            "|    total_timesteps    | 5512000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.31    |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 137799   |\n",
            "|    policy_loss        | 0.0906   |\n",
            "|    value_loss         | 15       |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 258      |\n",
            "|    ep_rew_mean        | 96       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 137900   |\n",
            "|    time_elapsed       | 4626     |\n",
            "|    total_timesteps    | 5516000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.38    |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 137899   |\n",
            "|    policy_loss        | 0.316    |\n",
            "|    value_loss         | 1.7      |\n",
            "------------------------------------\n",
            "Num timesteps: 5520000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 87.76\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 241      |\n",
            "|    ep_rew_mean        | 87.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 138000   |\n",
            "|    time_elapsed       | 4629     |\n",
            "|    total_timesteps    | 5520000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.312   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 137999   |\n",
            "|    policy_loss        | 0.416    |\n",
            "|    value_loss         | 19.6     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 229      |\n",
            "|    ep_rew_mean        | 93.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 138100   |\n",
            "|    time_elapsed       | 4631     |\n",
            "|    total_timesteps    | 5524000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.33    |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 138099   |\n",
            "|    policy_loss        | -0.0145  |\n",
            "|    value_loss         | 4.15     |\n",
            "------------------------------------\n",
            "Num timesteps: 5528000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 99.53\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 227      |\n",
            "|    ep_rew_mean        | 99.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 138200   |\n",
            "|    time_elapsed       | 4635     |\n",
            "|    total_timesteps    | 5528000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.387   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 138199   |\n",
            "|    policy_loss        | -0.606   |\n",
            "|    value_loss         | 13.8     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 224      |\n",
            "|    ep_rew_mean        | 108      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 138300   |\n",
            "|    time_elapsed       | 4637     |\n",
            "|    total_timesteps    | 5532000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.298   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 138299   |\n",
            "|    policy_loss        | 0.293    |\n",
            "|    value_loss         | 3.11     |\n",
            "------------------------------------\n",
            "Num timesteps: 5536000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 113.80\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 221      |\n",
            "|    ep_rew_mean        | 114      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 138400   |\n",
            "|    time_elapsed       | 4640     |\n",
            "|    total_timesteps    | 5536000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.328   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 138399   |\n",
            "|    policy_loss        | -0.55    |\n",
            "|    value_loss         | 5.71     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 224      |\n",
            "|    ep_rew_mean        | 113      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 138500   |\n",
            "|    time_elapsed       | 4643     |\n",
            "|    total_timesteps    | 5540000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.223   |\n",
            "|    explained_variance | 0.779    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 138499   |\n",
            "|    policy_loss        | 0.226    |\n",
            "|    value_loss         | 213      |\n",
            "------------------------------------\n",
            "Num timesteps: 5544000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 113.69\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 234      |\n",
            "|    ep_rew_mean        | 114      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 138600   |\n",
            "|    time_elapsed       | 4646     |\n",
            "|    total_timesteps    | 5544000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.395   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 138599   |\n",
            "|    policy_loss        | -0.159   |\n",
            "|    value_loss         | 25.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 246      |\n",
            "|    ep_rew_mean        | 96.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 138700   |\n",
            "|    time_elapsed       | 4648     |\n",
            "|    total_timesteps    | 5548000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.37    |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 138699   |\n",
            "|    policy_loss        | -0.335   |\n",
            "|    value_loss         | 9.83     |\n",
            "------------------------------------\n",
            "Num timesteps: 5552000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 87.07\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 239      |\n",
            "|    ep_rew_mean        | 87.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 138800   |\n",
            "|    time_elapsed       | 4651     |\n",
            "|    total_timesteps    | 5552000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.385   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 138799   |\n",
            "|    policy_loss        | 0.00222  |\n",
            "|    value_loss         | 5.71     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 247      |\n",
            "|    ep_rew_mean        | 85.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 138900   |\n",
            "|    time_elapsed       | 4654     |\n",
            "|    total_timesteps    | 5556000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.386   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 138899   |\n",
            "|    policy_loss        | 0.121    |\n",
            "|    value_loss         | 5.52     |\n",
            "------------------------------------\n",
            "Num timesteps: 5560000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 74.28\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 255      |\n",
            "|    ep_rew_mean        | 74.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 139000   |\n",
            "|    time_elapsed       | 4657     |\n",
            "|    total_timesteps    | 5560000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.321   |\n",
            "|    explained_variance | 0.92     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 138999   |\n",
            "|    policy_loss        | 0.162    |\n",
            "|    value_loss         | 189      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 263      |\n",
            "|    ep_rew_mean        | 76.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 139100   |\n",
            "|    time_elapsed       | 4661     |\n",
            "|    total_timesteps    | 5564000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.25    |\n",
            "|    explained_variance | 0.885    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 139099   |\n",
            "|    policy_loss        | 0.913    |\n",
            "|    value_loss         | 371      |\n",
            "------------------------------------\n",
            "Num timesteps: 5568000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 63.89\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 270      |\n",
            "|    ep_rew_mean        | 63.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 139200   |\n",
            "|    time_elapsed       | 4665     |\n",
            "|    total_timesteps    | 5568000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.328   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 139199   |\n",
            "|    policy_loss        | 0.273    |\n",
            "|    value_loss         | 6.62     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 275      |\n",
            "|    ep_rew_mean        | 58.3     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 139300   |\n",
            "|    time_elapsed       | 4668     |\n",
            "|    total_timesteps    | 5572000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.31    |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 139299   |\n",
            "|    policy_loss        | -0.631   |\n",
            "|    value_loss         | 4.4      |\n",
            "------------------------------------\n",
            "Num timesteps: 5576000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 68.73\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 280      |\n",
            "|    ep_rew_mean        | 68.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 139400   |\n",
            "|    time_elapsed       | 4672     |\n",
            "|    total_timesteps    | 5576000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.373   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 139399   |\n",
            "|    policy_loss        | -0.266   |\n",
            "|    value_loss         | 3.34     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 282      |\n",
            "|    ep_rew_mean        | 78.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 139500   |\n",
            "|    time_elapsed       | 4675     |\n",
            "|    total_timesteps    | 5580000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.414   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 139499   |\n",
            "|    policy_loss        | -0.582   |\n",
            "|    value_loss         | 9.88     |\n",
            "------------------------------------\n",
            "Num timesteps: 5584000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 72.79\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 278      |\n",
            "|    ep_rew_mean        | 72.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 139600   |\n",
            "|    time_elapsed       | 4678     |\n",
            "|    total_timesteps    | 5584000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.378   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 139599   |\n",
            "|    policy_loss        | -0.00716 |\n",
            "|    value_loss         | 5.39     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 287      |\n",
            "|    ep_rew_mean        | 73.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 139700   |\n",
            "|    time_elapsed       | 4681     |\n",
            "|    total_timesteps    | 5588000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.384   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 139699   |\n",
            "|    policy_loss        | 0.569    |\n",
            "|    value_loss         | 28.6     |\n",
            "------------------------------------\n",
            "Num timesteps: 5592000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 63.14\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 282      |\n",
            "|    ep_rew_mean        | 63.1     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 139800   |\n",
            "|    time_elapsed       | 4684     |\n",
            "|    total_timesteps    | 5592000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.39    |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 139799   |\n",
            "|    policy_loss        | -0.0329  |\n",
            "|    value_loss         | 3.14     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 256      |\n",
            "|    ep_rew_mean        | 94.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 139900   |\n",
            "|    time_elapsed       | 4686     |\n",
            "|    total_timesteps    | 5596000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.326   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 139899   |\n",
            "|    policy_loss        | -0.202   |\n",
            "|    value_loss         | 1.71     |\n",
            "------------------------------------\n",
            "Num timesteps: 5600000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 88.94\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 257      |\n",
            "|    ep_rew_mean        | 88.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 140000   |\n",
            "|    time_elapsed       | 4689     |\n",
            "|    total_timesteps    | 5600000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.401   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 139999   |\n",
            "|    policy_loss        | 0.164    |\n",
            "|    value_loss         | 2.77     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 276      |\n",
            "|    ep_rew_mean        | 82.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 140100   |\n",
            "|    time_elapsed       | 4693     |\n",
            "|    total_timesteps    | 5604000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.318   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 140099   |\n",
            "|    policy_loss        | -0.126   |\n",
            "|    value_loss         | 4.96     |\n",
            "------------------------------------\n",
            "Num timesteps: 5608000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 86.51\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 279      |\n",
            "|    ep_rew_mean        | 86.5     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 140200   |\n",
            "|    time_elapsed       | 4697     |\n",
            "|    total_timesteps    | 5608000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.389   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 140199   |\n",
            "|    policy_loss        | 0.674    |\n",
            "|    value_loss         | 13.1     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 265      |\n",
            "|    ep_rew_mean        | 96       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 140300   |\n",
            "|    time_elapsed       | 4699     |\n",
            "|    total_timesteps    | 5612000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.486   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 140299   |\n",
            "|    policy_loss        | -0.212   |\n",
            "|    value_loss         | 2.19     |\n",
            "------------------------------------\n",
            "Num timesteps: 5616000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 98.37\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 262      |\n",
            "|    ep_rew_mean        | 98.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 140400   |\n",
            "|    time_elapsed       | 4702     |\n",
            "|    total_timesteps    | 5616000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.274   |\n",
            "|    explained_variance | 0.98     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 140399   |\n",
            "|    policy_loss        | -0.292   |\n",
            "|    value_loss         | 22       |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 266      |\n",
            "|    ep_rew_mean        | 101      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 140500   |\n",
            "|    time_elapsed       | 4705     |\n",
            "|    total_timesteps    | 5620000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.447   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 140499   |\n",
            "|    policy_loss        | -0.0692  |\n",
            "|    value_loss         | 2.07     |\n",
            "------------------------------------\n",
            "Num timesteps: 5624000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 76.42\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 270      |\n",
            "|    ep_rew_mean        | 76.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 140600   |\n",
            "|    time_elapsed       | 4707     |\n",
            "|    total_timesteps    | 5624000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.273   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 140599   |\n",
            "|    policy_loss        | -0.0152  |\n",
            "|    value_loss         | 3.1      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 240      |\n",
            "|    ep_rew_mean        | 92.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 140700   |\n",
            "|    time_elapsed       | 4710     |\n",
            "|    total_timesteps    | 5628000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.232   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 140699   |\n",
            "|    policy_loss        | 0.118    |\n",
            "|    value_loss         | 10.2     |\n",
            "------------------------------------\n",
            "Num timesteps: 5632000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 102.34\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 226      |\n",
            "|    ep_rew_mean        | 102      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 140800   |\n",
            "|    time_elapsed       | 4713     |\n",
            "|    total_timesteps    | 5632000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.247   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 140799   |\n",
            "|    policy_loss        | 0.0942   |\n",
            "|    value_loss         | 2.5      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 223      |\n",
            "|    ep_rew_mean        | 89.7     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 140900   |\n",
            "|    time_elapsed       | 4716     |\n",
            "|    total_timesteps    | 5636000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.353   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 140899   |\n",
            "|    policy_loss        | -0.416   |\n",
            "|    value_loss         | 3.45     |\n",
            "------------------------------------\n",
            "Num timesteps: 5640000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 83.55\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 238      |\n",
            "|    ep_rew_mean        | 83.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 141000   |\n",
            "|    time_elapsed       | 4722     |\n",
            "|    total_timesteps    | 5640000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.334   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 140999   |\n",
            "|    policy_loss        | -0.0695  |\n",
            "|    value_loss         | 2.24     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 261      |\n",
            "|    ep_rew_mean        | 81       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 141100   |\n",
            "|    time_elapsed       | 4727     |\n",
            "|    total_timesteps    | 5644000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.467   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 141099   |\n",
            "|    policy_loss        | -0.262   |\n",
            "|    value_loss         | 4.74     |\n",
            "------------------------------------\n",
            "Num timesteps: 5648000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 80.64\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 286      |\n",
            "|    ep_rew_mean        | 80.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 141200   |\n",
            "|    time_elapsed       | 4732     |\n",
            "|    total_timesteps    | 5648000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.384   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 141199   |\n",
            "|    policy_loss        | -0.408   |\n",
            "|    value_loss         | 6.27     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 294      |\n",
            "|    ep_rew_mean        | 88.8     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 141300   |\n",
            "|    time_elapsed       | 4736     |\n",
            "|    total_timesteps    | 5652000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.325   |\n",
            "|    explained_variance | 0.949    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 141299   |\n",
            "|    policy_loss        | -0.0648  |\n",
            "|    value_loss         | 85.9     |\n",
            "------------------------------------\n",
            "Num timesteps: 5656000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 89.94\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 303      |\n",
            "|    ep_rew_mean        | 89.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 141400   |\n",
            "|    time_elapsed       | 4740     |\n",
            "|    total_timesteps    | 5656000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.388   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 141399   |\n",
            "|    policy_loss        | 0.0513   |\n",
            "|    value_loss         | 1.15     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 318      |\n",
            "|    ep_rew_mean        | 93.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 141500   |\n",
            "|    time_elapsed       | 4743     |\n",
            "|    total_timesteps    | 5660000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.293   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 141499   |\n",
            "|    policy_loss        | 0.0465   |\n",
            "|    value_loss         | 2.59     |\n",
            "------------------------------------\n",
            "Num timesteps: 5664000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 104.69\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 321      |\n",
            "|    ep_rew_mean        | 105      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 141600   |\n",
            "|    time_elapsed       | 4747     |\n",
            "|    total_timesteps    | 5664000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.465   |\n",
            "|    explained_variance | 0.914    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 141599   |\n",
            "|    policy_loss        | 1.17     |\n",
            "|    value_loss         | 89.2     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 339      |\n",
            "|    ep_rew_mean        | 102      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 141700   |\n",
            "|    time_elapsed       | 4751     |\n",
            "|    total_timesteps    | 5668000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.482   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 141699   |\n",
            "|    policy_loss        | 0.507    |\n",
            "|    value_loss         | 2.06     |\n",
            "------------------------------------\n",
            "Num timesteps: 5672000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 103.76\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 356      |\n",
            "|    ep_rew_mean        | 104      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 141800   |\n",
            "|    time_elapsed       | 4755     |\n",
            "|    total_timesteps    | 5672000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.397   |\n",
            "|    explained_variance | 0.98     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 141799   |\n",
            "|    policy_loss        | 0.139    |\n",
            "|    value_loss         | 67.1     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 352      |\n",
            "|    ep_rew_mean        | 122      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 141900   |\n",
            "|    time_elapsed       | 4758     |\n",
            "|    total_timesteps    | 5676000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.47    |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 141899   |\n",
            "|    policy_loss        | 0.123    |\n",
            "|    value_loss         | 8.43     |\n",
            "------------------------------------\n",
            "Num timesteps: 5680000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 129.73\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 336      |\n",
            "|    ep_rew_mean        | 130      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 142000   |\n",
            "|    time_elapsed       | 4762     |\n",
            "|    total_timesteps    | 5680000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.376   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 141999   |\n",
            "|    policy_loss        | -0.361   |\n",
            "|    value_loss         | 1.89     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 339      |\n",
            "|    ep_rew_mean        | 134      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 142100   |\n",
            "|    time_elapsed       | 4766     |\n",
            "|    total_timesteps    | 5684000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.355   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 142099   |\n",
            "|    policy_loss        | -0.282   |\n",
            "|    value_loss         | 1.44     |\n",
            "------------------------------------\n",
            "Num timesteps: 5688000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 144.26\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 346      |\n",
            "|    ep_rew_mean        | 144      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 142200   |\n",
            "|    time_elapsed       | 4770     |\n",
            "|    total_timesteps    | 5688000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.289   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 142199   |\n",
            "|    policy_loss        | -0.336   |\n",
            "|    value_loss         | 4.82     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 348      |\n",
            "|    ep_rew_mean        | 152      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 142300   |\n",
            "|    time_elapsed       | 4773     |\n",
            "|    total_timesteps    | 5692000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.24    |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 142299   |\n",
            "|    policy_loss        | -0.145   |\n",
            "|    value_loss         | 3.44     |\n",
            "------------------------------------\n",
            "Num timesteps: 5696000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 144.88\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 362      |\n",
            "|    ep_rew_mean        | 145      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 142400   |\n",
            "|    time_elapsed       | 4778     |\n",
            "|    total_timesteps    | 5696000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.292   |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 142399   |\n",
            "|    policy_loss        | 0.943    |\n",
            "|    value_loss         | 20.6     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 367      |\n",
            "|    ep_rew_mean        | 153      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 142500   |\n",
            "|    time_elapsed       | 4782     |\n",
            "|    total_timesteps    | 5700000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.425   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 142499   |\n",
            "|    policy_loss        | -0.232   |\n",
            "|    value_loss         | 4.56     |\n",
            "------------------------------------\n",
            "Num timesteps: 5704000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 153.72\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 368      |\n",
            "|    ep_rew_mean        | 154      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 142600   |\n",
            "|    time_elapsed       | 4785     |\n",
            "|    total_timesteps    | 5704000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.307   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 142599   |\n",
            "|    policy_loss        | 0.0072   |\n",
            "|    value_loss         | 4.3      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 356      |\n",
            "|    ep_rew_mean        | 157      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 142700   |\n",
            "|    time_elapsed       | 4788     |\n",
            "|    total_timesteps    | 5708000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.382   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 142699   |\n",
            "|    policy_loss        | 0.715    |\n",
            "|    value_loss         | 11.5     |\n",
            "------------------------------------\n",
            "Num timesteps: 5712000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 155.65\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 356      |\n",
            "|    ep_rew_mean        | 156      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 142800   |\n",
            "|    time_elapsed       | 4791     |\n",
            "|    total_timesteps    | 5712000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.312   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 142799   |\n",
            "|    policy_loss        | 0.173    |\n",
            "|    value_loss         | 0.302    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 353      |\n",
            "|    ep_rew_mean        | 161      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 142900   |\n",
            "|    time_elapsed       | 4794     |\n",
            "|    total_timesteps    | 5716000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.345   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 142899   |\n",
            "|    policy_loss        | 0.375    |\n",
            "|    value_loss         | 2.49     |\n",
            "------------------------------------\n",
            "Num timesteps: 5720000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 157.17\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 336      |\n",
            "|    ep_rew_mean        | 157      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 143000   |\n",
            "|    time_elapsed       | 4796     |\n",
            "|    total_timesteps    | 5720000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.327   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 142999   |\n",
            "|    policy_loss        | 0.153    |\n",
            "|    value_loss         | 5.45     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 341      |\n",
            "|    ep_rew_mean        | 160      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 143100   |\n",
            "|    time_elapsed       | 4800     |\n",
            "|    total_timesteps    | 5724000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.392   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 143099   |\n",
            "|    policy_loss        | -0.133   |\n",
            "|    value_loss         | 1.58     |\n",
            "------------------------------------\n",
            "Num timesteps: 5728000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 162.09\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 344      |\n",
            "|    ep_rew_mean        | 162      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 143200   |\n",
            "|    time_elapsed       | 4804     |\n",
            "|    total_timesteps    | 5728000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.401   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 143199   |\n",
            "|    policy_loss        | -0.0557  |\n",
            "|    value_loss         | 1.61     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 352      |\n",
            "|    ep_rew_mean        | 158      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 143300   |\n",
            "|    time_elapsed       | 4808     |\n",
            "|    total_timesteps    | 5732000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.375   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 143299   |\n",
            "|    policy_loss        | -0.226   |\n",
            "|    value_loss         | 1.86     |\n",
            "------------------------------------\n",
            "Num timesteps: 5736000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 154.68\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 324      |\n",
            "|    ep_rew_mean        | 155      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 143400   |\n",
            "|    time_elapsed       | 4811     |\n",
            "|    total_timesteps    | 5736000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.418   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 143399   |\n",
            "|    policy_loss        | 0.0347   |\n",
            "|    value_loss         | 1.62     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 307      |\n",
            "|    ep_rew_mean        | 162      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 143500   |\n",
            "|    time_elapsed       | 4814     |\n",
            "|    total_timesteps    | 5740000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.409   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 143499   |\n",
            "|    policy_loss        | -0.0461  |\n",
            "|    value_loss         | 1.76     |\n",
            "------------------------------------\n",
            "Num timesteps: 5744000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 161.98\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 303      |\n",
            "|    ep_rew_mean        | 162      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 143600   |\n",
            "|    time_elapsed       | 4817     |\n",
            "|    total_timesteps    | 5744000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.242   |\n",
            "|    explained_variance | 0.978    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 143599   |\n",
            "|    policy_loss        | 0.0747   |\n",
            "|    value_loss         | 30.1     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 292      |\n",
            "|    ep_rew_mean        | 149      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 143700   |\n",
            "|    time_elapsed       | 4820     |\n",
            "|    total_timesteps    | 5748000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.383   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 143699   |\n",
            "|    policy_loss        | -0.144   |\n",
            "|    value_loss         | 11.6     |\n",
            "------------------------------------\n",
            "Num timesteps: 5752000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 136.00\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 268      |\n",
            "|    ep_rew_mean        | 136      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 143800   |\n",
            "|    time_elapsed       | 4822     |\n",
            "|    total_timesteps    | 5752000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.319   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 143799   |\n",
            "|    policy_loss        | -0.242   |\n",
            "|    value_loss         | 4.16     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 242      |\n",
            "|    ep_rew_mean        | 137      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 143900   |\n",
            "|    time_elapsed       | 4825     |\n",
            "|    total_timesteps    | 5756000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.341   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 143899   |\n",
            "|    policy_loss        | -0.195   |\n",
            "|    value_loss         | 1.3      |\n",
            "------------------------------------\n",
            "Num timesteps: 5760000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 142.34\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 245      |\n",
            "|    ep_rew_mean        | 142      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 144000   |\n",
            "|    time_elapsed       | 4828     |\n",
            "|    total_timesteps    | 5760000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.306   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 143999   |\n",
            "|    policy_loss        | -0.191   |\n",
            "|    value_loss         | 10.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 245      |\n",
            "|    ep_rew_mean        | 148      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 144100   |\n",
            "|    time_elapsed       | 4831     |\n",
            "|    total_timesteps    | 5764000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.209   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 144099   |\n",
            "|    policy_loss        | -0.262   |\n",
            "|    value_loss         | 3.81     |\n",
            "------------------------------------\n",
            "Num timesteps: 5768000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 151.18\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 241      |\n",
            "|    ep_rew_mean        | 151      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 144200   |\n",
            "|    time_elapsed       | 4833     |\n",
            "|    total_timesteps    | 5768000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.431   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 144199   |\n",
            "|    policy_loss        | 0.715    |\n",
            "|    value_loss         | 6.43     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 235      |\n",
            "|    ep_rew_mean        | 142      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 144300   |\n",
            "|    time_elapsed       | 4836     |\n",
            "|    total_timesteps    | 5772000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.308   |\n",
            "|    explained_variance | 0.792    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 144299   |\n",
            "|    policy_loss        | 0.277    |\n",
            "|    value_loss         | 205      |\n",
            "------------------------------------\n",
            "Num timesteps: 5776000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 137.76\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 229      |\n",
            "|    ep_rew_mean        | 138      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 144400   |\n",
            "|    time_elapsed       | 4839     |\n",
            "|    total_timesteps    | 5776000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.445   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 144399   |\n",
            "|    policy_loss        | -0.272   |\n",
            "|    value_loss         | 9.07     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 223      |\n",
            "|    ep_rew_mean        | 140      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 144500   |\n",
            "|    time_elapsed       | 4841     |\n",
            "|    total_timesteps    | 5780000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.301   |\n",
            "|    explained_variance | 0.974    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 144499   |\n",
            "|    policy_loss        | 1.19     |\n",
            "|    value_loss         | 31       |\n",
            "------------------------------------\n",
            "Num timesteps: 5784000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 134.90\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 222      |\n",
            "|    ep_rew_mean        | 135      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 144600   |\n",
            "|    time_elapsed       | 4845     |\n",
            "|    total_timesteps    | 5784000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.468   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 144599   |\n",
            "|    policy_loss        | -0.042   |\n",
            "|    value_loss         | 4.55     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 213      |\n",
            "|    ep_rew_mean        | 136      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 144700   |\n",
            "|    time_elapsed       | 4848     |\n",
            "|    total_timesteps    | 5788000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.319   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 144699   |\n",
            "|    policy_loss        | 0.0635   |\n",
            "|    value_loss         | 2.66     |\n",
            "------------------------------------\n",
            "Num timesteps: 5792000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 143.61\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 217      |\n",
            "|    ep_rew_mean        | 144      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 144800   |\n",
            "|    time_elapsed       | 4850     |\n",
            "|    total_timesteps    | 5792000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.363   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 144799   |\n",
            "|    policy_loss        | -0.39    |\n",
            "|    value_loss         | 5.46     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 224      |\n",
            "|    ep_rew_mean        | 151      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 144900   |\n",
            "|    time_elapsed       | 4853     |\n",
            "|    total_timesteps    | 5796000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.312   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 144899   |\n",
            "|    policy_loss        | -0.191   |\n",
            "|    value_loss         | 3.74     |\n",
            "------------------------------------\n",
            "Num timesteps: 5800000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 162.67\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 236      |\n",
            "|    ep_rew_mean        | 163      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 145000   |\n",
            "|    time_elapsed       | 4856     |\n",
            "|    total_timesteps    | 5800000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.371   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 144999   |\n",
            "|    policy_loss        | -0.141   |\n",
            "|    value_loss         | 3.24     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 222      |\n",
            "|    ep_rew_mean        | 164      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 145100   |\n",
            "|    time_elapsed       | 4858     |\n",
            "|    total_timesteps    | 5804000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.307   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 145099   |\n",
            "|    policy_loss        | 0.0275   |\n",
            "|    value_loss         | 2.38     |\n",
            "------------------------------------\n",
            "Num timesteps: 5808000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 148.40\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 222      |\n",
            "|    ep_rew_mean        | 148      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 145200   |\n",
            "|    time_elapsed       | 4861     |\n",
            "|    total_timesteps    | 5808000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.294   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 145199   |\n",
            "|    policy_loss        | -0.146   |\n",
            "|    value_loss         | 2.23     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 214      |\n",
            "|    ep_rew_mean        | 152      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 145300   |\n",
            "|    time_elapsed       | 4863     |\n",
            "|    total_timesteps    | 5812000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.221   |\n",
            "|    explained_variance | 0.782    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 145299   |\n",
            "|    policy_loss        | 0.0105   |\n",
            "|    value_loss         | 438      |\n",
            "------------------------------------\n",
            "Num timesteps: 5816000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 144.71\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 203      |\n",
            "|    ep_rew_mean        | 145      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 145400   |\n",
            "|    time_elapsed       | 4866     |\n",
            "|    total_timesteps    | 5816000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.254   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 145399   |\n",
            "|    policy_loss        | 0.0878   |\n",
            "|    value_loss         | 14.4     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 197      |\n",
            "|    ep_rew_mean        | 143      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 145500   |\n",
            "|    time_elapsed       | 4869     |\n",
            "|    total_timesteps    | 5820000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.349   |\n",
            "|    explained_variance | 0.391    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 145499   |\n",
            "|    policy_loss        | 0.0244   |\n",
            "|    value_loss         | 1.94e+03 |\n",
            "------------------------------------\n",
            "Num timesteps: 5824000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 140.45\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 216      |\n",
            "|    ep_rew_mean        | 140      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 145600   |\n",
            "|    time_elapsed       | 4872     |\n",
            "|    total_timesteps    | 5824000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.315   |\n",
            "|    explained_variance | 0.974    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 145599   |\n",
            "|    policy_loss        | 0.0175   |\n",
            "|    value_loss         | 44.9     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 218      |\n",
            "|    ep_rew_mean        | 141      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 145700   |\n",
            "|    time_elapsed       | 4874     |\n",
            "|    total_timesteps    | 5828000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.297   |\n",
            "|    explained_variance | 0.934    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 145699   |\n",
            "|    policy_loss        | -0.681   |\n",
            "|    value_loss         | 85.3     |\n",
            "------------------------------------\n",
            "Num timesteps: 5832000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 150.68\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 216      |\n",
            "|    ep_rew_mean        | 151      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 145800   |\n",
            "|    time_elapsed       | 4878     |\n",
            "|    total_timesteps    | 5832000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.427   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 145799   |\n",
            "|    policy_loss        | -0.00437 |\n",
            "|    value_loss         | 1.6      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 236      |\n",
            "|    ep_rew_mean        | 144      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 145900   |\n",
            "|    time_elapsed       | 4881     |\n",
            "|    total_timesteps    | 5836000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.379   |\n",
            "|    explained_variance | 0.785    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 145899   |\n",
            "|    policy_loss        | 0.347    |\n",
            "|    value_loss         | 152      |\n",
            "------------------------------------\n",
            "Num timesteps: 5840000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 156.29\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 244      |\n",
            "|    ep_rew_mean        | 156      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 146000   |\n",
            "|    time_elapsed       | 4883     |\n",
            "|    total_timesteps    | 5840000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.286   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 145999   |\n",
            "|    policy_loss        | 0.335    |\n",
            "|    value_loss         | 9.48     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 239      |\n",
            "|    ep_rew_mean        | 149      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 146100   |\n",
            "|    time_elapsed       | 4886     |\n",
            "|    total_timesteps    | 5844000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.315   |\n",
            "|    explained_variance | 0.821    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 146099   |\n",
            "|    policy_loss        | -0.262   |\n",
            "|    value_loss         | 524      |\n",
            "------------------------------------\n",
            "Num timesteps: 5848000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 149.02\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 220      |\n",
            "|    ep_rew_mean        | 149      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 146200   |\n",
            "|    time_elapsed       | 4888     |\n",
            "|    total_timesteps    | 5848000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.36    |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 146199   |\n",
            "|    policy_loss        | -0.326   |\n",
            "|    value_loss         | 1.43     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 237      |\n",
            "|    ep_rew_mean        | 155      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 146300   |\n",
            "|    time_elapsed       | 4891     |\n",
            "|    total_timesteps    | 5852000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.42    |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 146299   |\n",
            "|    policy_loss        | 0.216    |\n",
            "|    value_loss         | 3.42     |\n",
            "------------------------------------\n",
            "Num timesteps: 5856000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 147.25\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 230      |\n",
            "|    ep_rew_mean        | 147      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 146400   |\n",
            "|    time_elapsed       | 4895     |\n",
            "|    total_timesteps    | 5856000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.211   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 146399   |\n",
            "|    policy_loss        | -0.227   |\n",
            "|    value_loss         | 3.66     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 236      |\n",
            "|    ep_rew_mean        | 145      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 146500   |\n",
            "|    time_elapsed       | 4897     |\n",
            "|    total_timesteps    | 5860000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.384   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 146499   |\n",
            "|    policy_loss        | -0.0889  |\n",
            "|    value_loss         | 18.2     |\n",
            "------------------------------------\n",
            "Num timesteps: 5864000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 149.05\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 241      |\n",
            "|    ep_rew_mean        | 149      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 146600   |\n",
            "|    time_elapsed       | 4899     |\n",
            "|    total_timesteps    | 5864000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.395   |\n",
            "|    explained_variance | 0.679    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 146599   |\n",
            "|    policy_loss        | -2.05    |\n",
            "|    value_loss         | 659      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 252      |\n",
            "|    ep_rew_mean        | 174      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1197     |\n",
            "|    iterations         | 146700   |\n",
            "|    time_elapsed       | 4902     |\n",
            "|    total_timesteps    | 5868000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.296   |\n",
            "|    explained_variance | 0.767    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 146699   |\n",
            "|    policy_loss        | 0.00817  |\n",
            "|    value_loss         | 384      |\n",
            "------------------------------------\n",
            "Num timesteps: 5872000\n",
            "Best mean reward: 174.31 - Last mean reward per episode: 184.31\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 244      |\n",
            "|    ep_rew_mean        | 184      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1197     |\n",
            "|    iterations         | 146800   |\n",
            "|    time_elapsed       | 4904     |\n",
            "|    total_timesteps    | 5872000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.273   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 146799   |\n",
            "|    policy_loss        | -0.133   |\n",
            "|    value_loss         | 6.37     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 233      |\n",
            "|    ep_rew_mean        | 190      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1197     |\n",
            "|    iterations         | 146900   |\n",
            "|    time_elapsed       | 4907     |\n",
            "|    total_timesteps    | 5876000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.38    |\n",
            "|    explained_variance | 0.975    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 146899   |\n",
            "|    policy_loss        | 0.211    |\n",
            "|    value_loss         | 9.89     |\n",
            "------------------------------------\n",
            "Num timesteps: 5880000\n",
            "Best mean reward: 184.31 - Last mean reward per episode: 198.74\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 232      |\n",
            "|    ep_rew_mean        | 199      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1197     |\n",
            "|    iterations         | 147000   |\n",
            "|    time_elapsed       | 4910     |\n",
            "|    total_timesteps    | 5880000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.158   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 146999   |\n",
            "|    policy_loss        | 0.0266   |\n",
            "|    value_loss         | 1.1      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 238      |\n",
            "|    ep_rew_mean        | 197      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1197     |\n",
            "|    iterations         | 147100   |\n",
            "|    time_elapsed       | 4913     |\n",
            "|    total_timesteps    | 5884000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.416   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 147099   |\n",
            "|    policy_loss        | -0.00839 |\n",
            "|    value_loss         | 1.69     |\n",
            "------------------------------------\n",
            "Num timesteps: 5888000\n",
            "Best mean reward: 198.74 - Last mean reward per episode: 190.60\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 260      |\n",
            "|    ep_rew_mean        | 191      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1197     |\n",
            "|    iterations         | 147200   |\n",
            "|    time_elapsed       | 4917     |\n",
            "|    total_timesteps    | 5888000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.3     |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 147199   |\n",
            "|    policy_loss        | 0.0238   |\n",
            "|    value_loss         | 1.35     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 268      |\n",
            "|    ep_rew_mean        | 193      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1197     |\n",
            "|    iterations         | 147300   |\n",
            "|    time_elapsed       | 4920     |\n",
            "|    total_timesteps    | 5892000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.326   |\n",
            "|    explained_variance | 0.924    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 147299   |\n",
            "|    policy_loss        | -0.1     |\n",
            "|    value_loss         | 139      |\n",
            "------------------------------------\n",
            "Num timesteps: 5896000\n",
            "Best mean reward: 198.74 - Last mean reward per episode: 190.90\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 286      |\n",
            "|    ep_rew_mean        | 191      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1197     |\n",
            "|    iterations         | 147400   |\n",
            "|    time_elapsed       | 4923     |\n",
            "|    total_timesteps    | 5896000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.184   |\n",
            "|    explained_variance | 0.869    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 147399   |\n",
            "|    policy_loss        | -0.00199 |\n",
            "|    value_loss         | 252      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 289      |\n",
            "|    ep_rew_mean        | 201      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1197     |\n",
            "|    iterations         | 147500   |\n",
            "|    time_elapsed       | 4925     |\n",
            "|    total_timesteps    | 5900000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.362   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 147499   |\n",
            "|    policy_loss        | -0.0488  |\n",
            "|    value_loss         | 2.26     |\n",
            "------------------------------------\n",
            "Num timesteps: 5904000\n",
            "Best mean reward: 198.74 - Last mean reward per episode: 203.42\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 289      |\n",
            "|    ep_rew_mean        | 203      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1197     |\n",
            "|    iterations         | 147600   |\n",
            "|    time_elapsed       | 4929     |\n",
            "|    total_timesteps    | 5904000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.262   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 147599   |\n",
            "|    policy_loss        | -0.31    |\n",
            "|    value_loss         | 1.24     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 315      |\n",
            "|    ep_rew_mean        | 197      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1197     |\n",
            "|    iterations         | 147700   |\n",
            "|    time_elapsed       | 4933     |\n",
            "|    total_timesteps    | 5908000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.416   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 147699   |\n",
            "|    policy_loss        | 0.174    |\n",
            "|    value_loss         | 3.45     |\n",
            "------------------------------------\n",
            "Num timesteps: 5912000\n",
            "Best mean reward: 203.42 - Last mean reward per episode: 190.50\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 335      |\n",
            "|    ep_rew_mean        | 190      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1197     |\n",
            "|    iterations         | 147800   |\n",
            "|    time_elapsed       | 4938     |\n",
            "|    total_timesteps    | 5912000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.229   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 147799   |\n",
            "|    policy_loss        | -0.0365  |\n",
            "|    value_loss         | 3.56     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 348      |\n",
            "|    ep_rew_mean        | 183      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 147900   |\n",
            "|    time_elapsed       | 4943     |\n",
            "|    total_timesteps    | 5916000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.379   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 147899   |\n",
            "|    policy_loss        | -0.384   |\n",
            "|    value_loss         | 3.12     |\n",
            "------------------------------------\n",
            "Num timesteps: 5920000\n",
            "Best mean reward: 203.42 - Last mean reward per episode: 180.86\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 360      |\n",
            "|    ep_rew_mean        | 181      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 148000   |\n",
            "|    time_elapsed       | 4947     |\n",
            "|    total_timesteps    | 5920000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.289   |\n",
            "|    explained_variance | 0.976    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 147999   |\n",
            "|    policy_loss        | -0.23    |\n",
            "|    value_loss         | 2.09     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 392      |\n",
            "|    ep_rew_mean        | 173      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 148100   |\n",
            "|    time_elapsed       | 4953     |\n",
            "|    total_timesteps    | 5924000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.324   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 148099   |\n",
            "|    policy_loss        | -0.634   |\n",
            "|    value_loss         | 4.28     |\n",
            "------------------------------------\n",
            "Num timesteps: 5928000\n",
            "Best mean reward: 203.42 - Last mean reward per episode: 167.59\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 392      |\n",
            "|    ep_rew_mean        | 168      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 148200   |\n",
            "|    time_elapsed       | 4958     |\n",
            "|    total_timesteps    | 5928000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.293   |\n",
            "|    explained_variance | 0.586    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 148199   |\n",
            "|    policy_loss        | -0.906   |\n",
            "|    value_loss         | 1.17e+03 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 408      |\n",
            "|    ep_rew_mean        | 169      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 148300   |\n",
            "|    time_elapsed       | 4962     |\n",
            "|    total_timesteps    | 5932000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.246   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 148299   |\n",
            "|    policy_loss        | -0.17    |\n",
            "|    value_loss         | 1.62     |\n",
            "------------------------------------\n",
            "Num timesteps: 5936000\n",
            "Best mean reward: 203.42 - Last mean reward per episode: 169.90\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 416      |\n",
            "|    ep_rew_mean        | 170      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 148400   |\n",
            "|    time_elapsed       | 4966     |\n",
            "|    total_timesteps    | 5936000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.363   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 148399   |\n",
            "|    policy_loss        | 0.072    |\n",
            "|    value_loss         | 1.65     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 412      |\n",
            "|    ep_rew_mean        | 165      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 148500   |\n",
            "|    time_elapsed       | 4971     |\n",
            "|    total_timesteps    | 5940000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.328   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 148499   |\n",
            "|    policy_loss        | 0.108    |\n",
            "|    value_loss         | 7.63     |\n",
            "------------------------------------\n",
            "Num timesteps: 5944000\n",
            "Best mean reward: 203.42 - Last mean reward per episode: 143.78\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 436      |\n",
            "|    ep_rew_mean        | 144      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 148600   |\n",
            "|    time_elapsed       | 4974     |\n",
            "|    total_timesteps    | 5944000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.295   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 148599   |\n",
            "|    policy_loss        | 0.207    |\n",
            "|    value_loss         | 2.22     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 460      |\n",
            "|    ep_rew_mean        | 137      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 148700   |\n",
            "|    time_elapsed       | 4979     |\n",
            "|    total_timesteps    | 5948000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.311   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 148699   |\n",
            "|    policy_loss        | -0.428   |\n",
            "|    value_loss         | 2.32     |\n",
            "------------------------------------\n",
            "Num timesteps: 5952000\n",
            "Best mean reward: 203.42 - Last mean reward per episode: 130.85\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 477      |\n",
            "|    ep_rew_mean        | 131      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 148800   |\n",
            "|    time_elapsed       | 4984     |\n",
            "|    total_timesteps    | 5952000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.242   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 148799   |\n",
            "|    policy_loss        | 0.00194  |\n",
            "|    value_loss         | 5.42     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 476      |\n",
            "|    ep_rew_mean        | 125      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 148900   |\n",
            "|    time_elapsed       | 4988     |\n",
            "|    total_timesteps    | 5956000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.474   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 148899   |\n",
            "|    policy_loss        | -0.321   |\n",
            "|    value_loss         | 1.74     |\n",
            "------------------------------------\n",
            "Num timesteps: 5960000\n",
            "Best mean reward: 203.42 - Last mean reward per episode: 129.61\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 461      |\n",
            "|    ep_rew_mean        | 130      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 149000   |\n",
            "|    time_elapsed       | 4992     |\n",
            "|    total_timesteps    | 5960000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.319   |\n",
            "|    explained_variance | 0.97     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 148999   |\n",
            "|    policy_loss        | -0.469   |\n",
            "|    value_loss         | 5.17     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 452      |\n",
            "|    ep_rew_mean        | 142      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 149100   |\n",
            "|    time_elapsed       | 4995     |\n",
            "|    total_timesteps    | 5964000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.373   |\n",
            "|    explained_variance | 0.973    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 149099   |\n",
            "|    policy_loss        | 0.119    |\n",
            "|    value_loss         | 6.33     |\n",
            "------------------------------------\n",
            "Num timesteps: 5968000\n",
            "Best mean reward: 203.42 - Last mean reward per episode: 141.47\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 456      |\n",
            "|    ep_rew_mean        | 141      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 149200   |\n",
            "|    time_elapsed       | 5001     |\n",
            "|    total_timesteps    | 5968000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.34    |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 149199   |\n",
            "|    policy_loss        | -0.0321  |\n",
            "|    value_loss         | 1.72     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 467      |\n",
            "|    ep_rew_mean        | 142      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 149300   |\n",
            "|    time_elapsed       | 5006     |\n",
            "|    total_timesteps    | 5972000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.337   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 149299   |\n",
            "|    policy_loss        | -0.0366  |\n",
            "|    value_loss         | 12.2     |\n",
            "------------------------------------\n",
            "Num timesteps: 5976000\n",
            "Best mean reward: 203.42 - Last mean reward per episode: 141.44\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 466      |\n",
            "|    ep_rew_mean        | 141      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 149400   |\n",
            "|    time_elapsed       | 5012     |\n",
            "|    total_timesteps    | 5976000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.254   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 149399   |\n",
            "|    policy_loss        | -0.158   |\n",
            "|    value_loss         | 1.3      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 496      |\n",
            "|    ep_rew_mean        | 136      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 149500   |\n",
            "|    time_elapsed       | 5016     |\n",
            "|    total_timesteps    | 5980000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.337   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 149499   |\n",
            "|    policy_loss        | 0.0937   |\n",
            "|    value_loss         | 8.71     |\n",
            "------------------------------------\n",
            "Num timesteps: 5984000\n",
            "Best mean reward: 203.42 - Last mean reward per episode: 141.72\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 489      |\n",
            "|    ep_rew_mean        | 142      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 149600   |\n",
            "|    time_elapsed       | 5021     |\n",
            "|    total_timesteps    | 5984000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.356   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 149599   |\n",
            "|    policy_loss        | 0.151    |\n",
            "|    value_loss         | 2.43     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 501      |\n",
            "|    ep_rew_mean        | 136      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 149700   |\n",
            "|    time_elapsed       | 5025     |\n",
            "|    total_timesteps    | 5988000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.296   |\n",
            "|    explained_variance | 0.661    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 149699   |\n",
            "|    policy_loss        | -0.388   |\n",
            "|    value_loss         | 465      |\n",
            "------------------------------------\n",
            "Num timesteps: 5992000\n",
            "Best mean reward: 203.42 - Last mean reward per episode: 143.51\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 471      |\n",
            "|    ep_rew_mean        | 144      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 149800   |\n",
            "|    time_elapsed       | 5029     |\n",
            "|    total_timesteps    | 5992000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.344   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 149799   |\n",
            "|    policy_loss        | -0.198   |\n",
            "|    value_loss         | 5.72     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 431      |\n",
            "|    ep_rew_mean        | 145      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 149900   |\n",
            "|    time_elapsed       | 5032     |\n",
            "|    total_timesteps    | 5996000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.253   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 149899   |\n",
            "|    policy_loss        | -0.0306  |\n",
            "|    value_loss         | 21.2     |\n",
            "------------------------------------\n",
            "Num timesteps: 6000000\n",
            "Best mean reward: 203.42 - Last mean reward per episode: 144.09\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        | 395       |\n",
            "|    ep_rew_mean        | 144       |\n",
            "| time/                 |           |\n",
            "|    fps                | 1191      |\n",
            "|    iterations         | 150000    |\n",
            "|    time_elapsed       | 5035      |\n",
            "|    total_timesteps    | 6000000   |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.376    |\n",
            "|    explained_variance | 0.999     |\n",
            "|    learning_rate      | 0.00083   |\n",
            "|    n_updates          | 149999    |\n",
            "|    policy_loss        | -0.000295 |\n",
            "|    value_loss         | 1.53      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 360      |\n",
            "|    ep_rew_mean        | 143      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 150100   |\n",
            "|    time_elapsed       | 5038     |\n",
            "|    total_timesteps    | 6004000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.344   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 150099   |\n",
            "|    policy_loss        | -0.13    |\n",
            "|    value_loss         | 9.93     |\n",
            "------------------------------------\n",
            "Num timesteps: 6008000\n",
            "Best mean reward: 203.42 - Last mean reward per episode: 155.27\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 300      |\n",
            "|    ep_rew_mean        | 155      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 150200   |\n",
            "|    time_elapsed       | 5041     |\n",
            "|    total_timesteps    | 6008000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.343   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 150199   |\n",
            "|    policy_loss        | -0.116   |\n",
            "|    value_loss         | 16.1     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 269      |\n",
            "|    ep_rew_mean        | 163      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 150300   |\n",
            "|    time_elapsed       | 5044     |\n",
            "|    total_timesteps    | 6012000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.314   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 150299   |\n",
            "|    policy_loss        | -0.0306  |\n",
            "|    value_loss         | 2.36     |\n",
            "------------------------------------\n",
            "Num timesteps: 6016000\n",
            "Best mean reward: 203.42 - Last mean reward per episode: 164.18\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 263      |\n",
            "|    ep_rew_mean        | 164      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 150400   |\n",
            "|    time_elapsed       | 5048     |\n",
            "|    total_timesteps    | 6016000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.251   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 150399   |\n",
            "|    policy_loss        | -0.123   |\n",
            "|    value_loss         | 10.3     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 274      |\n",
            "|    ep_rew_mean        | 180      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1191     |\n",
            "|    iterations         | 150500   |\n",
            "|    time_elapsed       | 5050     |\n",
            "|    total_timesteps    | 6020000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.368   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 150499   |\n",
            "|    policy_loss        | -0.163   |\n",
            "|    value_loss         | 0.724    |\n",
            "------------------------------------\n",
            "Num timesteps: 6024000\n",
            "Best mean reward: 203.42 - Last mean reward per episode: 186.10\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 277      |\n",
            "|    ep_rew_mean        | 186      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 150600   |\n",
            "|    time_elapsed       | 5053     |\n",
            "|    total_timesteps    | 6024000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.364   |\n",
            "|    explained_variance | 0.801    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 150599   |\n",
            "|    policy_loss        | 0.0419   |\n",
            "|    value_loss         | 372      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 272      |\n",
            "|    ep_rew_mean        | 188      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 150700   |\n",
            "|    time_elapsed       | 5056     |\n",
            "|    total_timesteps    | 6028000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.415   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 150699   |\n",
            "|    policy_loss        | -0.0303  |\n",
            "|    value_loss         | 0.911    |\n",
            "------------------------------------\n",
            "Num timesteps: 6032000\n",
            "Best mean reward: 203.42 - Last mean reward per episode: 200.52\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 271      |\n",
            "|    ep_rew_mean        | 201      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 150800   |\n",
            "|    time_elapsed       | 5059     |\n",
            "|    total_timesteps    | 6032000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.303   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 150799   |\n",
            "|    policy_loss        | -0.0795  |\n",
            "|    value_loss         | 1.35     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 276      |\n",
            "|    ep_rew_mean        | 183      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 150900   |\n",
            "|    time_elapsed       | 5062     |\n",
            "|    total_timesteps    | 6036000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.414   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 150899   |\n",
            "|    policy_loss        | -0.178   |\n",
            "|    value_loss         | 5.42     |\n",
            "------------------------------------\n",
            "Num timesteps: 6040000\n",
            "Best mean reward: 203.42 - Last mean reward per episode: 162.21\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 247      |\n",
            "|    ep_rew_mean        | 162      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 151000   |\n",
            "|    time_elapsed       | 5064     |\n",
            "|    total_timesteps    | 6040000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.34    |\n",
            "|    explained_variance | 0.816    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 150999   |\n",
            "|    policy_loss        | -0.092   |\n",
            "|    value_loss         | 389      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 236      |\n",
            "|    ep_rew_mean        | 164      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 151100   |\n",
            "|    time_elapsed       | 5067     |\n",
            "|    total_timesteps    | 6044000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.297   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 151099   |\n",
            "|    policy_loss        | 0.338    |\n",
            "|    value_loss         | 4.17     |\n",
            "------------------------------------\n",
            "Num timesteps: 6048000\n",
            "Best mean reward: 203.42 - Last mean reward per episode: 166.19\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 242      |\n",
            "|    ep_rew_mean        | 166      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 151200   |\n",
            "|    time_elapsed       | 5070     |\n",
            "|    total_timesteps    | 6048000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.259   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 151199   |\n",
            "|    policy_loss        | 0.117    |\n",
            "|    value_loss         | 14.2     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 242      |\n",
            "|    ep_rew_mean        | 168      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1192     |\n",
            "|    iterations         | 151300   |\n",
            "|    time_elapsed       | 5073     |\n",
            "|    total_timesteps    | 6052000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.405   |\n",
            "|    explained_variance | 0.975    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 151299   |\n",
            "|    policy_loss        | -0.0143  |\n",
            "|    value_loss         | 41.3     |\n",
            "------------------------------------\n",
            "Num timesteps: 6056000\n",
            "Best mean reward: 203.42 - Last mean reward per episode: 163.41\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 232      |\n",
            "|    ep_rew_mean        | 163      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 151400   |\n",
            "|    time_elapsed       | 5076     |\n",
            "|    total_timesteps    | 6056000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.39    |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 151399   |\n",
            "|    policy_loss        | -0.47    |\n",
            "|    value_loss         | 4.7      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 226      |\n",
            "|    ep_rew_mean        | 167      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 151500   |\n",
            "|    time_elapsed       | 5078     |\n",
            "|    total_timesteps    | 6060000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.362   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 151499   |\n",
            "|    policy_loss        | 0.106    |\n",
            "|    value_loss         | 6.71     |\n",
            "------------------------------------\n",
            "Num timesteps: 6064000\n",
            "Best mean reward: 203.42 - Last mean reward per episode: 177.67\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 232      |\n",
            "|    ep_rew_mean        | 178      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 151600   |\n",
            "|    time_elapsed       | 5081     |\n",
            "|    total_timesteps    | 6064000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.3     |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 151599   |\n",
            "|    policy_loss        | 0.152    |\n",
            "|    value_loss         | 29.7     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 244      |\n",
            "|    ep_rew_mean        | 162      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 151700   |\n",
            "|    time_elapsed       | 5083     |\n",
            "|    total_timesteps    | 6068000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.372   |\n",
            "|    explained_variance | 0.87     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 151699   |\n",
            "|    policy_loss        | -0.0535  |\n",
            "|    value_loss         | 200      |\n",
            "------------------------------------\n",
            "Num timesteps: 6072000\n",
            "Best mean reward: 203.42 - Last mean reward per episode: 149.11\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 217      |\n",
            "|    ep_rew_mean        | 149      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1193     |\n",
            "|    iterations         | 151800   |\n",
            "|    time_elapsed       | 5085     |\n",
            "|    total_timesteps    | 6072000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.305   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 151799   |\n",
            "|    policy_loss        | -0.153   |\n",
            "|    value_loss         | 8.07     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 203      |\n",
            "|    ep_rew_mean        | 148      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 151900   |\n",
            "|    time_elapsed       | 5087     |\n",
            "|    total_timesteps    | 6076000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.274   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 151899   |\n",
            "|    policy_loss        | 0.314    |\n",
            "|    value_loss         | 7.95     |\n",
            "------------------------------------\n",
            "Num timesteps: 6080000\n",
            "Best mean reward: 203.42 - Last mean reward per episode: 146.85\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 196      |\n",
            "|    ep_rew_mean        | 147      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 152000   |\n",
            "|    time_elapsed       | 5089     |\n",
            "|    total_timesteps    | 6080000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.306   |\n",
            "|    explained_variance | 0.975    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 151999   |\n",
            "|    policy_loss        | -0.0418  |\n",
            "|    value_loss         | 29.7     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 176      |\n",
            "|    ep_rew_mean        | 132      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 152100   |\n",
            "|    time_elapsed       | 5092     |\n",
            "|    total_timesteps    | 6084000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.441   |\n",
            "|    explained_variance | 0.655    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 152099   |\n",
            "|    policy_loss        | -0.102   |\n",
            "|    value_loss         | 988      |\n",
            "------------------------------------\n",
            "Num timesteps: 6088000\n",
            "Best mean reward: 203.42 - Last mean reward per episode: 137.26\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 172      |\n",
            "|    ep_rew_mean        | 137      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 152200   |\n",
            "|    time_elapsed       | 5095     |\n",
            "|    total_timesteps    | 6088000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.498   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 152199   |\n",
            "|    policy_loss        | 0.0816   |\n",
            "|    value_loss         | 1.68     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 198      |\n",
            "|    ep_rew_mean        | 134      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1194     |\n",
            "|    iterations         | 152300   |\n",
            "|    time_elapsed       | 5098     |\n",
            "|    total_timesteps    | 6092000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.367   |\n",
            "|    explained_variance | 0.607    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 152299   |\n",
            "|    policy_loss        | 0.594    |\n",
            "|    value_loss         | 923      |\n",
            "------------------------------------\n",
            "Num timesteps: 6096000\n",
            "Best mean reward: 203.42 - Last mean reward per episode: 117.36\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 200      |\n",
            "|    ep_rew_mean        | 117      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 152400   |\n",
            "|    time_elapsed       | 5100     |\n",
            "|    total_timesteps    | 6096000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.132   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 152399   |\n",
            "|    policy_loss        | -0.00773 |\n",
            "|    value_loss         | 3.92     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 198      |\n",
            "|    ep_rew_mean        | 100      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 152500   |\n",
            "|    time_elapsed       | 5103     |\n",
            "|    total_timesteps    | 6100000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.46    |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 152499   |\n",
            "|    policy_loss        | 0.612    |\n",
            "|    value_loss         | 1.57     |\n",
            "------------------------------------\n",
            "Num timesteps: 6104000\n",
            "Best mean reward: 203.42 - Last mean reward per episode: 90.97\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 209      |\n",
            "|    ep_rew_mean        | 91       |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 152600   |\n",
            "|    time_elapsed       | 5105     |\n",
            "|    total_timesteps    | 6104000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.316   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 152599   |\n",
            "|    policy_loss        | 0.014    |\n",
            "|    value_loss         | 2.23     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 220      |\n",
            "|    ep_rew_mean        | 90.6     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 152700   |\n",
            "|    time_elapsed       | 5108     |\n",
            "|    total_timesteps    | 6108000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.421   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 152699   |\n",
            "|    policy_loss        | -0.105   |\n",
            "|    value_loss         | 0.801    |\n",
            "------------------------------------\n",
            "Num timesteps: 6112000\n",
            "Best mean reward: 203.42 - Last mean reward per episode: 84.22\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 204      |\n",
            "|    ep_rew_mean        | 84.2     |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 152800   |\n",
            "|    time_elapsed       | 5111     |\n",
            "|    total_timesteps    | 6112000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.343   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 152799   |\n",
            "|    policy_loss        | -0.0757  |\n",
            "|    value_loss         | 6.4      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 224      |\n",
            "|    ep_rew_mean        | 102      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 152900   |\n",
            "|    time_elapsed       | 5114     |\n",
            "|    total_timesteps    | 6116000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.242   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 152899   |\n",
            "|    policy_loss        | 0.0756   |\n",
            "|    value_loss         | 1.75     |\n",
            "------------------------------------\n",
            "Num timesteps: 6120000\n",
            "Best mean reward: 203.42 - Last mean reward per episode: 118.94\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 229      |\n",
            "|    ep_rew_mean        | 119      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1195     |\n",
            "|    iterations         | 153000   |\n",
            "|    time_elapsed       | 5117     |\n",
            "|    total_timesteps    | 6120000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.483   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 152999   |\n",
            "|    policy_loss        | -0.528   |\n",
            "|    value_loss         | 28.8     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 254      |\n",
            "|    ep_rew_mean        | 129      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 153100   |\n",
            "|    time_elapsed       | 5120     |\n",
            "|    total_timesteps    | 6124000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.354   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 153099   |\n",
            "|    policy_loss        | -0.11    |\n",
            "|    value_loss         | 8.66     |\n",
            "------------------------------------\n",
            "Num timesteps: 6128000\n",
            "Best mean reward: 203.42 - Last mean reward per episode: 154.20\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 275      |\n",
            "|    ep_rew_mean        | 154      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 153200   |\n",
            "|    time_elapsed       | 5123     |\n",
            "|    total_timesteps    | 6128000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.316   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 153199   |\n",
            "|    policy_loss        | -0.537   |\n",
            "|    value_loss         | 10.1     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 272      |\n",
            "|    ep_rew_mean        | 173      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 153300   |\n",
            "|    time_elapsed       | 5126     |\n",
            "|    total_timesteps    | 6132000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.284   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 153299   |\n",
            "|    policy_loss        | -0.0785  |\n",
            "|    value_loss         | 0.445    |\n",
            "------------------------------------\n",
            "Num timesteps: 6136000\n",
            "Best mean reward: 203.42 - Last mean reward per episode: 196.50\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 297      |\n",
            "|    ep_rew_mean        | 196      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 153400   |\n",
            "|    time_elapsed       | 5129     |\n",
            "|    total_timesteps    | 6136000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.486   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 153399   |\n",
            "|    policy_loss        | -0.254   |\n",
            "|    value_loss         | 8.99     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 297      |\n",
            "|    ep_rew_mean        | 210      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 153500   |\n",
            "|    time_elapsed       | 5131     |\n",
            "|    total_timesteps    | 6140000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.44    |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 153499   |\n",
            "|    policy_loss        | -0.223   |\n",
            "|    value_loss         | 5.72     |\n",
            "------------------------------------\n",
            "Num timesteps: 6144000\n",
            "Best mean reward: 203.42 - Last mean reward per episode: 213.64\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 302      |\n",
            "|    ep_rew_mean        | 214      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 153600   |\n",
            "|    time_elapsed       | 5134     |\n",
            "|    total_timesteps    | 6144000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.514   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 153599   |\n",
            "|    policy_loss        | 0.778    |\n",
            "|    value_loss         | 5.35     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 279      |\n",
            "|    ep_rew_mean        | 200      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 153700   |\n",
            "|    time_elapsed       | 5137     |\n",
            "|    total_timesteps    | 6148000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.291   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 153699   |\n",
            "|    policy_loss        | 0.0119   |\n",
            "|    value_loss         | 0.883    |\n",
            "------------------------------------\n",
            "Num timesteps: 6152000\n",
            "Best mean reward: 213.64 - Last mean reward per episode: 189.52\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 252      |\n",
            "|    ep_rew_mean        | 190      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1196     |\n",
            "|    iterations         | 153800   |\n",
            "|    time_elapsed       | 5139     |\n",
            "|    total_timesteps    | 6152000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.364   |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 153799   |\n",
            "|    policy_loss        | -1.28    |\n",
            "|    value_loss         | 21.6     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 237      |\n",
            "|    ep_rew_mean        | 177      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1197     |\n",
            "|    iterations         | 153900   |\n",
            "|    time_elapsed       | 5142     |\n",
            "|    total_timesteps    | 6156000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.257   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 153899   |\n",
            "|    policy_loss        | 0.142    |\n",
            "|    value_loss         | 1.08     |\n",
            "------------------------------------\n",
            "Num timesteps: 6160000\n",
            "Best mean reward: 213.64 - Last mean reward per episode: 147.36\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 213      |\n",
            "|    ep_rew_mean        | 147      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1197     |\n",
            "|    iterations         | 154000   |\n",
            "|    time_elapsed       | 5144     |\n",
            "|    total_timesteps    | 6160000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.435   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 153999   |\n",
            "|    policy_loss        | 0.13     |\n",
            "|    value_loss         | 2.53     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 194      |\n",
            "|    ep_rew_mean        | 134      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1197     |\n",
            "|    iterations         | 154100   |\n",
            "|    time_elapsed       | 5147     |\n",
            "|    total_timesteps    | 6164000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.387   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 154099   |\n",
            "|    policy_loss        | 0.0345   |\n",
            "|    value_loss         | 1.84     |\n",
            "------------------------------------\n",
            "Num timesteps: 6168000\n",
            "Best mean reward: 213.64 - Last mean reward per episode: 141.69\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 191      |\n",
            "|    ep_rew_mean        | 142      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1197     |\n",
            "|    iterations         | 154200   |\n",
            "|    time_elapsed       | 5149     |\n",
            "|    total_timesteps    | 6168000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.303   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 154199   |\n",
            "|    policy_loss        | -0.111   |\n",
            "|    value_loss         | 1.71     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 195      |\n",
            "|    ep_rew_mean        | 163      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1198     |\n",
            "|    iterations         | 154300   |\n",
            "|    time_elapsed       | 5151     |\n",
            "|    total_timesteps    | 6172000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.246   |\n",
            "|    explained_variance | 0.982    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 154299   |\n",
            "|    policy_loss        | 0.368    |\n",
            "|    value_loss         | 15.2     |\n",
            "------------------------------------\n",
            "Num timesteps: 6176000\n",
            "Best mean reward: 213.64 - Last mean reward per episode: 158.93\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 189      |\n",
            "|    ep_rew_mean        | 159      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1198     |\n",
            "|    iterations         | 154400   |\n",
            "|    time_elapsed       | 5153     |\n",
            "|    total_timesteps    | 6176000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.308   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 154399   |\n",
            "|    policy_loss        | 0.497    |\n",
            "|    value_loss         | 4.92     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 193      |\n",
            "|    ep_rew_mean        | 163      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1198     |\n",
            "|    iterations         | 154500   |\n",
            "|    time_elapsed       | 5155     |\n",
            "|    total_timesteps    | 6180000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.329   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 154499   |\n",
            "|    policy_loss        | -0.0447  |\n",
            "|    value_loss         | 0.958    |\n",
            "------------------------------------\n",
            "Num timesteps: 6184000\n",
            "Best mean reward: 213.64 - Last mean reward per episode: 166.63\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 191      |\n",
            "|    ep_rew_mean        | 167      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1198     |\n",
            "|    iterations         | 154600   |\n",
            "|    time_elapsed       | 5158     |\n",
            "|    total_timesteps    | 6184000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.366   |\n",
            "|    explained_variance | 0.727    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 154599   |\n",
            "|    policy_loss        | -0.163   |\n",
            "|    value_loss         | 797      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 186      |\n",
            "|    ep_rew_mean        | 150      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1199     |\n",
            "|    iterations         | 154700   |\n",
            "|    time_elapsed       | 5160     |\n",
            "|    total_timesteps    | 6188000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.419   |\n",
            "|    explained_variance | 0.666    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 154699   |\n",
            "|    policy_loss        | 0.0223   |\n",
            "|    value_loss         | 525      |\n",
            "------------------------------------\n",
            "Num timesteps: 6192000\n",
            "Best mean reward: 213.64 - Last mean reward per episode: 127.35\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 190      |\n",
            "|    ep_rew_mean        | 127      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1199     |\n",
            "|    iterations         | 154800   |\n",
            "|    time_elapsed       | 5162     |\n",
            "|    total_timesteps    | 6192000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.313   |\n",
            "|    explained_variance | 0.905    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 154799   |\n",
            "|    policy_loss        | -0.0627  |\n",
            "|    value_loss         | 249      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 199      |\n",
            "|    ep_rew_mean        | 133      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1199     |\n",
            "|    iterations         | 154900   |\n",
            "|    time_elapsed       | 5165     |\n",
            "|    total_timesteps    | 6196000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.362   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 154899   |\n",
            "|    policy_loss        | -0.166   |\n",
            "|    value_loss         | 0.922    |\n",
            "------------------------------------\n",
            "Num timesteps: 6200000\n",
            "Best mean reward: 213.64 - Last mean reward per episode: 133.61\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 214      |\n",
            "|    ep_rew_mean        | 134      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1199     |\n",
            "|    iterations         | 155000   |\n",
            "|    time_elapsed       | 5168     |\n",
            "|    total_timesteps    | 6200000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.319   |\n",
            "|    explained_variance | 0.873    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 154999   |\n",
            "|    policy_loss        | 0.577    |\n",
            "|    value_loss         | 29       |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 229      |\n",
            "|    ep_rew_mean        | 147      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1199     |\n",
            "|    iterations         | 155100   |\n",
            "|    time_elapsed       | 5171     |\n",
            "|    total_timesteps    | 6204000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.399   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 155099   |\n",
            "|    policy_loss        | -0.196   |\n",
            "|    value_loss         | 2.08     |\n",
            "------------------------------------\n",
            "Num timesteps: 6208000\n",
            "Best mean reward: 213.64 - Last mean reward per episode: 171.47\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 245      |\n",
            "|    ep_rew_mean        | 171      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1199     |\n",
            "|    iterations         | 155200   |\n",
            "|    time_elapsed       | 5173     |\n",
            "|    total_timesteps    | 6208000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.379   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 155199   |\n",
            "|    policy_loss        | -0.351   |\n",
            "|    value_loss         | 1.81     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 249      |\n",
            "|    ep_rew_mean        | 191      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1200     |\n",
            "|    iterations         | 155300   |\n",
            "|    time_elapsed       | 5176     |\n",
            "|    total_timesteps    | 6212000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.267   |\n",
            "|    explained_variance | 0.626    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 155299   |\n",
            "|    policy_loss        | 0.0925   |\n",
            "|    value_loss         | 779      |\n",
            "------------------------------------\n",
            "Num timesteps: 6216000\n",
            "Best mean reward: 213.64 - Last mean reward per episode: 190.64\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 239      |\n",
            "|    ep_rew_mean        | 191      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1200     |\n",
            "|    iterations         | 155400   |\n",
            "|    time_elapsed       | 5178     |\n",
            "|    total_timesteps    | 6216000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.396   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 155399   |\n",
            "|    policy_loss        | -0.196   |\n",
            "|    value_loss         | 7.62     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 213      |\n",
            "|    ep_rew_mean        | 176      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1200     |\n",
            "|    iterations         | 155500   |\n",
            "|    time_elapsed       | 5180     |\n",
            "|    total_timesteps    | 6220000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.396   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 155499   |\n",
            "|    policy_loss        | -0.225   |\n",
            "|    value_loss         | 7.41     |\n",
            "------------------------------------\n",
            "Num timesteps: 6224000\n",
            "Best mean reward: 213.64 - Last mean reward per episode: 163.63\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 185      |\n",
            "|    ep_rew_mean        | 164      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1201     |\n",
            "|    iterations         | 155600   |\n",
            "|    time_elapsed       | 5182     |\n",
            "|    total_timesteps    | 6224000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.332   |\n",
            "|    explained_variance | 0.896    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 155599   |\n",
            "|    policy_loss        | 0.0124   |\n",
            "|    value_loss         | 114      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 181      |\n",
            "|    ep_rew_mean        | 148      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1201     |\n",
            "|    iterations         | 155700   |\n",
            "|    time_elapsed       | 5183     |\n",
            "|    total_timesteps    | 6228000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.518   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 155699   |\n",
            "|    policy_loss        | -0.141   |\n",
            "|    value_loss         | 3.29     |\n",
            "------------------------------------\n",
            "Num timesteps: 6232000\n",
            "Best mean reward: 213.64 - Last mean reward per episode: 144.56\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 182      |\n",
            "|    ep_rew_mean        | 145      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1201     |\n",
            "|    iterations         | 155800   |\n",
            "|    time_elapsed       | 5186     |\n",
            "|    total_timesteps    | 6232000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.423   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 155799   |\n",
            "|    policy_loss        | 0.0749   |\n",
            "|    value_loss         | 3.51     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 183      |\n",
            "|    ep_rew_mean        | 143      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1201     |\n",
            "|    iterations         | 155900   |\n",
            "|    time_elapsed       | 5188     |\n",
            "|    total_timesteps    | 6236000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.457   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 155899   |\n",
            "|    policy_loss        | -0.144   |\n",
            "|    value_loss         | 0.4      |\n",
            "------------------------------------\n",
            "Num timesteps: 6240000\n",
            "Best mean reward: 213.64 - Last mean reward per episode: 140.53\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 188      |\n",
            "|    ep_rew_mean        | 141      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1202     |\n",
            "|    iterations         | 156000   |\n",
            "|    time_elapsed       | 5190     |\n",
            "|    total_timesteps    | 6240000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.403   |\n",
            "|    explained_variance | 0.523    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 155999   |\n",
            "|    policy_loss        | 0.185    |\n",
            "|    value_loss         | 351      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 192      |\n",
            "|    ep_rew_mean        | 135      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1202     |\n",
            "|    iterations         | 156100   |\n",
            "|    time_elapsed       | 5192     |\n",
            "|    total_timesteps    | 6244000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.255   |\n",
            "|    explained_variance | 0.828    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 156099   |\n",
            "|    policy_loss        | -0.291   |\n",
            "|    value_loss         | 159      |\n",
            "------------------------------------\n",
            "Num timesteps: 6248000\n",
            "Best mean reward: 213.64 - Last mean reward per episode: 135.35\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 196      |\n",
            "|    ep_rew_mean        | 135      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1202     |\n",
            "|    iterations         | 156200   |\n",
            "|    time_elapsed       | 5195     |\n",
            "|    total_timesteps    | 6248000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.425   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 156199   |\n",
            "|    policy_loss        | -0.119   |\n",
            "|    value_loss         | 7.09     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 204      |\n",
            "|    ep_rew_mean        | 156      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1202     |\n",
            "|    iterations         | 156300   |\n",
            "|    time_elapsed       | 5197     |\n",
            "|    total_timesteps    | 6252000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.397   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 156299   |\n",
            "|    policy_loss        | -0.0819  |\n",
            "|    value_loss         | 8.58     |\n",
            "------------------------------------\n",
            "Num timesteps: 6256000\n",
            "Best mean reward: 213.64 - Last mean reward per episode: 159.23\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 211      |\n",
            "|    ep_rew_mean        | 159      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1203     |\n",
            "|    iterations         | 156400   |\n",
            "|    time_elapsed       | 5200     |\n",
            "|    total_timesteps    | 6256000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.313   |\n",
            "|    explained_variance | 0.468    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 156399   |\n",
            "|    policy_loss        | 0.386    |\n",
            "|    value_loss         | 528      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 215      |\n",
            "|    ep_rew_mean        | 168      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1203     |\n",
            "|    iterations         | 156500   |\n",
            "|    time_elapsed       | 5202     |\n",
            "|    total_timesteps    | 6260000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.439   |\n",
            "|    explained_variance | 0.976    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 156499   |\n",
            "|    policy_loss        | -0.581   |\n",
            "|    value_loss         | 6.95     |\n",
            "------------------------------------\n",
            "Num timesteps: 6264000\n",
            "Best mean reward: 213.64 - Last mean reward per episode: 186.35\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 225      |\n",
            "|    ep_rew_mean        | 186      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1203     |\n",
            "|    iterations         | 156600   |\n",
            "|    time_elapsed       | 5205     |\n",
            "|    total_timesteps    | 6264000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.317   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 156599   |\n",
            "|    policy_loss        | -0.115   |\n",
            "|    value_loss         | 3.01     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 233      |\n",
            "|    ep_rew_mean        | 196      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1203     |\n",
            "|    iterations         | 156700   |\n",
            "|    time_elapsed       | 5207     |\n",
            "|    total_timesteps    | 6268000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.305   |\n",
            "|    explained_variance | 0.941    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 156699   |\n",
            "|    policy_loss        | 0.668    |\n",
            "|    value_loss         | 41.6     |\n",
            "------------------------------------\n",
            "Num timesteps: 6272000\n",
            "Best mean reward: 213.64 - Last mean reward per episode: 199.01\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 230      |\n",
            "|    ep_rew_mean        | 199      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1203     |\n",
            "|    iterations         | 156800   |\n",
            "|    time_elapsed       | 5209     |\n",
            "|    total_timesteps    | 6272000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.297   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 156799   |\n",
            "|    policy_loss        | -0.099   |\n",
            "|    value_loss         | 7.83     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 229      |\n",
            "|    ep_rew_mean        | 200      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1204     |\n",
            "|    iterations         | 156900   |\n",
            "|    time_elapsed       | 5212     |\n",
            "|    total_timesteps    | 6276000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.447   |\n",
            "|    explained_variance | 0.944    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 156899   |\n",
            "|    policy_loss        | -0.0547  |\n",
            "|    value_loss         | 46.8     |\n",
            "------------------------------------\n",
            "Num timesteps: 6280000\n",
            "Best mean reward: 213.64 - Last mean reward per episode: 207.43\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 231      |\n",
            "|    ep_rew_mean        | 207      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1204     |\n",
            "|    iterations         | 157000   |\n",
            "|    time_elapsed       | 5214     |\n",
            "|    total_timesteps    | 6280000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.452   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 156999   |\n",
            "|    policy_loss        | 0.191    |\n",
            "|    value_loss         | 9.1      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 239      |\n",
            "|    ep_rew_mean        | 215      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1204     |\n",
            "|    iterations         | 157100   |\n",
            "|    time_elapsed       | 5216     |\n",
            "|    total_timesteps    | 6284000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.499   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 157099   |\n",
            "|    policy_loss        | 0.118    |\n",
            "|    value_loss         | 5.25     |\n",
            "------------------------------------\n",
            "Num timesteps: 6288000\n",
            "Best mean reward: 213.64 - Last mean reward per episode: 218.66\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 236      |\n",
            "|    ep_rew_mean        | 219      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1204     |\n",
            "|    iterations         | 157200   |\n",
            "|    time_elapsed       | 5219     |\n",
            "|    total_timesteps    | 6288000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.268   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 157199   |\n",
            "|    policy_loss        | 0.169    |\n",
            "|    value_loss         | 3.61     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 236      |\n",
            "|    ep_rew_mean        | 224      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1205     |\n",
            "|    iterations         | 157300   |\n",
            "|    time_elapsed       | 5221     |\n",
            "|    total_timesteps    | 6292000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.511   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 157299   |\n",
            "|    policy_loss        | -0.166   |\n",
            "|    value_loss         | 4.01     |\n",
            "------------------------------------\n",
            "Num timesteps: 6296000\n",
            "Best mean reward: 218.66 - Last mean reward per episode: 232.57\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 234      |\n",
            "|    ep_rew_mean        | 233      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1205     |\n",
            "|    iterations         | 157400   |\n",
            "|    time_elapsed       | 5223     |\n",
            "|    total_timesteps    | 6296000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.448   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 157399   |\n",
            "|    policy_loss        | 0.153    |\n",
            "|    value_loss         | 2.01     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 234      |\n",
            "|    ep_rew_mean        | 232      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1205     |\n",
            "|    iterations         | 157500   |\n",
            "|    time_elapsed       | 5225     |\n",
            "|    total_timesteps    | 6300000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.415   |\n",
            "|    explained_variance | 0.956    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 157499   |\n",
            "|    policy_loss        | -0.663   |\n",
            "|    value_loss         | 19.3     |\n",
            "------------------------------------\n",
            "Num timesteps: 6304000\n",
            "Best mean reward: 232.57 - Last mean reward per episode: 235.08\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 237      |\n",
            "|    ep_rew_mean        | 235      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1205     |\n",
            "|    iterations         | 157600   |\n",
            "|    time_elapsed       | 5228     |\n",
            "|    total_timesteps    | 6304000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.288   |\n",
            "|    explained_variance | 0.978    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 157599   |\n",
            "|    policy_loss        | -0.349   |\n",
            "|    value_loss         | 2.07     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 228      |\n",
            "|    ep_rew_mean        | 226      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1206     |\n",
            "|    iterations         | 157700   |\n",
            "|    time_elapsed       | 5230     |\n",
            "|    total_timesteps    | 6308000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.292   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 157699   |\n",
            "|    policy_loss        | 0.0478   |\n",
            "|    value_loss         | 7.3      |\n",
            "------------------------------------\n",
            "Num timesteps: 6312000\n",
            "Best mean reward: 235.08 - Last mean reward per episode: 219.18\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 232      |\n",
            "|    ep_rew_mean        | 219      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1206     |\n",
            "|    iterations         | 157800   |\n",
            "|    time_elapsed       | 5233     |\n",
            "|    total_timesteps    | 6312000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.367   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 157799   |\n",
            "|    policy_loss        | -0.453   |\n",
            "|    value_loss         | 7.09     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 224      |\n",
            "|    ep_rew_mean        | 209      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1206     |\n",
            "|    iterations         | 157900   |\n",
            "|    time_elapsed       | 5235     |\n",
            "|    total_timesteps    | 6316000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.312   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 157899   |\n",
            "|    policy_loss        | -0.216   |\n",
            "|    value_loss         | 3.26     |\n",
            "------------------------------------\n",
            "Num timesteps: 6320000\n",
            "Best mean reward: 235.08 - Last mean reward per episode: 203.66\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 226      |\n",
            "|    ep_rew_mean        | 204      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1206     |\n",
            "|    iterations         | 158000   |\n",
            "|    time_elapsed       | 5237     |\n",
            "|    total_timesteps    | 6320000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.369   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 157999   |\n",
            "|    policy_loss        | 0.125    |\n",
            "|    value_loss         | 8.07     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 223      |\n",
            "|    ep_rew_mean        | 202      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1206     |\n",
            "|    iterations         | 158100   |\n",
            "|    time_elapsed       | 5239     |\n",
            "|    total_timesteps    | 6324000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.481   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 158099   |\n",
            "|    policy_loss        | -0.278   |\n",
            "|    value_loss         | 2.61     |\n",
            "------------------------------------\n",
            "Num timesteps: 6328000\n",
            "Best mean reward: 235.08 - Last mean reward per episode: 196.60\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 216      |\n",
            "|    ep_rew_mean        | 197      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1207     |\n",
            "|    iterations         | 158200   |\n",
            "|    time_elapsed       | 5241     |\n",
            "|    total_timesteps    | 6328000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.423   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 158199   |\n",
            "|    policy_loss        | 0.158    |\n",
            "|    value_loss         | 4.72     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 207      |\n",
            "|    ep_rew_mean        | 199      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1207     |\n",
            "|    iterations         | 158300   |\n",
            "|    time_elapsed       | 5244     |\n",
            "|    total_timesteps    | 6332000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.263   |\n",
            "|    explained_variance | 0.092    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 158299   |\n",
            "|    policy_loss        | -0.192   |\n",
            "|    value_loss         | 1.06e+03 |\n",
            "------------------------------------\n",
            "Num timesteps: 6336000\n",
            "Best mean reward: 235.08 - Last mean reward per episode: 205.78\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 213      |\n",
            "|    ep_rew_mean        | 206      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1207     |\n",
            "|    iterations         | 158400   |\n",
            "|    time_elapsed       | 5246     |\n",
            "|    total_timesteps    | 6336000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.368   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 158399   |\n",
            "|    policy_loss        | -0.91    |\n",
            "|    value_loss         | 7.01     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 218      |\n",
            "|    ep_rew_mean        | 208      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1207     |\n",
            "|    iterations         | 158500   |\n",
            "|    time_elapsed       | 5249     |\n",
            "|    total_timesteps    | 6340000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.269   |\n",
            "|    explained_variance | 0.688    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 158499   |\n",
            "|    policy_loss        | 0.0525   |\n",
            "|    value_loss         | 210      |\n",
            "------------------------------------\n",
            "Num timesteps: 6344000\n",
            "Best mean reward: 235.08 - Last mean reward per episode: 215.08\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 217      |\n",
            "|    ep_rew_mean        | 215      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1208     |\n",
            "|    iterations         | 158600   |\n",
            "|    time_elapsed       | 5251     |\n",
            "|    total_timesteps    | 6344000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.445   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 158599   |\n",
            "|    policy_loss        | -0.262   |\n",
            "|    value_loss         | 5.02     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 218      |\n",
            "|    ep_rew_mean        | 214      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1208     |\n",
            "|    iterations         | 158700   |\n",
            "|    time_elapsed       | 5253     |\n",
            "|    total_timesteps    | 6348000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.515   |\n",
            "|    explained_variance | 0.967    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 158699   |\n",
            "|    policy_loss        | -0.197   |\n",
            "|    value_loss         | 130      |\n",
            "------------------------------------\n",
            "Num timesteps: 6352000\n",
            "Best mean reward: 235.08 - Last mean reward per episode: 212.00\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 228      |\n",
            "|    ep_rew_mean        | 212      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1208     |\n",
            "|    iterations         | 158800   |\n",
            "|    time_elapsed       | 5256     |\n",
            "|    total_timesteps    | 6352000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.399   |\n",
            "|    explained_variance | 0.987    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 158799   |\n",
            "|    policy_loss        | -0.497   |\n",
            "|    value_loss         | 7.83     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 225      |\n",
            "|    ep_rew_mean        | 209      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1208     |\n",
            "|    iterations         | 158900   |\n",
            "|    time_elapsed       | 5258     |\n",
            "|    total_timesteps    | 6356000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.204   |\n",
            "|    explained_variance | 0.976    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 158899   |\n",
            "|    policy_loss        | 0.188    |\n",
            "|    value_loss         | 10.2     |\n",
            "------------------------------------\n",
            "Num timesteps: 6360000\n",
            "Best mean reward: 235.08 - Last mean reward per episode: 203.64\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 225      |\n",
            "|    ep_rew_mean        | 204      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1208     |\n",
            "|    iterations         | 159000   |\n",
            "|    time_elapsed       | 5260     |\n",
            "|    total_timesteps    | 6360000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.473   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 158999   |\n",
            "|    policy_loss        | 0.318    |\n",
            "|    value_loss         | 7.23     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 227      |\n",
            "|    ep_rew_mean        | 204      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1209     |\n",
            "|    iterations         | 159100   |\n",
            "|    time_elapsed       | 5263     |\n",
            "|    total_timesteps    | 6364000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.384   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 159099   |\n",
            "|    policy_loss        | 0.276    |\n",
            "|    value_loss         | 3.6      |\n",
            "------------------------------------\n",
            "Num timesteps: 6368000\n",
            "Best mean reward: 235.08 - Last mean reward per episode: 207.97\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 230      |\n",
            "|    ep_rew_mean        | 208      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1209     |\n",
            "|    iterations         | 159200   |\n",
            "|    time_elapsed       | 5265     |\n",
            "|    total_timesteps    | 6368000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.406   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 159199   |\n",
            "|    policy_loss        | 0.177    |\n",
            "|    value_loss         | 1.44     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 238      |\n",
            "|    ep_rew_mean        | 204      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1209     |\n",
            "|    iterations         | 159300   |\n",
            "|    time_elapsed       | 5268     |\n",
            "|    total_timesteps    | 6372000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.362   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 159299   |\n",
            "|    policy_loss        | 0.102    |\n",
            "|    value_loss         | 2.73     |\n",
            "------------------------------------\n",
            "Num timesteps: 6376000\n",
            "Best mean reward: 235.08 - Last mean reward per episode: 215.00\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 235      |\n",
            "|    ep_rew_mean        | 215      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1209     |\n",
            "|    iterations         | 159400   |\n",
            "|    time_elapsed       | 5270     |\n",
            "|    total_timesteps    | 6376000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.288   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 159399   |\n",
            "|    policy_loss        | -0.0139  |\n",
            "|    value_loss         | 0.586    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 236      |\n",
            "|    ep_rew_mean        | 229      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1209     |\n",
            "|    iterations         | 159500   |\n",
            "|    time_elapsed       | 5272     |\n",
            "|    total_timesteps    | 6380000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.313   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 159499   |\n",
            "|    policy_loss        | -0.281   |\n",
            "|    value_loss         | 5.09     |\n",
            "------------------------------------\n",
            "Num timesteps: 6384000\n",
            "Best mean reward: 235.08 - Last mean reward per episode: 238.21\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 240      |\n",
            "|    ep_rew_mean        | 238      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1210     |\n",
            "|    iterations         | 159600   |\n",
            "|    time_elapsed       | 5275     |\n",
            "|    total_timesteps    | 6384000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.28    |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 159599   |\n",
            "|    policy_loss        | 0.201    |\n",
            "|    value_loss         | 3.19     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 238      |\n",
            "|    ep_rew_mean        | 245      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1210     |\n",
            "|    iterations         | 159700   |\n",
            "|    time_elapsed       | 5277     |\n",
            "|    total_timesteps    | 6388000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.217   |\n",
            "|    explained_variance | 0.478    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 159699   |\n",
            "|    policy_loss        | -0.014   |\n",
            "|    value_loss         | 430      |\n",
            "------------------------------------\n",
            "Num timesteps: 6392000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 235.42\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 232      |\n",
            "|    ep_rew_mean        | 235      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1210     |\n",
            "|    iterations         | 159800   |\n",
            "|    time_elapsed       | 5280     |\n",
            "|    total_timesteps    | 6392000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.476   |\n",
            "|    explained_variance | 0.976    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 159799   |\n",
            "|    policy_loss        | -0.208   |\n",
            "|    value_loss         | 17.1     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 231      |\n",
            "|    ep_rew_mean        | 230      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1210     |\n",
            "|    iterations         | 159900   |\n",
            "|    time_elapsed       | 5282     |\n",
            "|    total_timesteps    | 6396000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.364   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 159899   |\n",
            "|    policy_loss        | -0.451   |\n",
            "|    value_loss         | 2.81     |\n",
            "------------------------------------\n",
            "Num timesteps: 6400000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 213.25\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 226      |\n",
            "|    ep_rew_mean        | 213      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1210     |\n",
            "|    iterations         | 160000   |\n",
            "|    time_elapsed       | 5285     |\n",
            "|    total_timesteps    | 6400000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.37    |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 159999   |\n",
            "|    policy_loss        | -0.361   |\n",
            "|    value_loss         | 2.99     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 222      |\n",
            "|    ep_rew_mean        | 198      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1211     |\n",
            "|    iterations         | 160100   |\n",
            "|    time_elapsed       | 5287     |\n",
            "|    total_timesteps    | 6404000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.228   |\n",
            "|    explained_variance | 0.844    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 160099   |\n",
            "|    policy_loss        | -0.19    |\n",
            "|    value_loss         | 674      |\n",
            "------------------------------------\n",
            "Num timesteps: 6408000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 175.00\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 210      |\n",
            "|    ep_rew_mean        | 175      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1211     |\n",
            "|    iterations         | 160200   |\n",
            "|    time_elapsed       | 5289     |\n",
            "|    total_timesteps    | 6408000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.354   |\n",
            "|    explained_variance | 0.468    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 160199   |\n",
            "|    policy_loss        | 0.00717  |\n",
            "|    value_loss         | 810      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 208      |\n",
            "|    ep_rew_mean        | 167      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1211     |\n",
            "|    iterations         | 160300   |\n",
            "|    time_elapsed       | 5292     |\n",
            "|    total_timesteps    | 6412000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.336   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 160299   |\n",
            "|    policy_loss        | -0.236   |\n",
            "|    value_loss         | 5.49     |\n",
            "------------------------------------\n",
            "Num timesteps: 6416000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 167.73\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 187      |\n",
            "|    ep_rew_mean        | 168      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1211     |\n",
            "|    iterations         | 160400   |\n",
            "|    time_elapsed       | 5294     |\n",
            "|    total_timesteps    | 6416000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.113   |\n",
            "|    explained_variance | 0.98     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 160399   |\n",
            "|    policy_loss        | -0.00939 |\n",
            "|    value_loss         | 4.37     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 190      |\n",
            "|    ep_rew_mean        | 156      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1212     |\n",
            "|    iterations         | 160500   |\n",
            "|    time_elapsed       | 5296     |\n",
            "|    total_timesteps    | 6420000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.423   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 160499   |\n",
            "|    policy_loss        | -0.0357  |\n",
            "|    value_loss         | 1.87     |\n",
            "------------------------------------\n",
            "Num timesteps: 6424000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 154.15\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 189      |\n",
            "|    ep_rew_mean        | 154      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1212     |\n",
            "|    iterations         | 160600   |\n",
            "|    time_elapsed       | 5298     |\n",
            "|    total_timesteps    | 6424000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.348   |\n",
            "|    explained_variance | -0.252   |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 160599   |\n",
            "|    policy_loss        | 0.956    |\n",
            "|    value_loss         | 1.24e+03 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 194      |\n",
            "|    ep_rew_mean        | 159      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1212     |\n",
            "|    iterations         | 160700   |\n",
            "|    time_elapsed       | 5301     |\n",
            "|    total_timesteps    | 6428000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.461   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 160699   |\n",
            "|    policy_loss        | -0.0998  |\n",
            "|    value_loss         | 2.01     |\n",
            "------------------------------------\n",
            "Num timesteps: 6432000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 154.72\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 193      |\n",
            "|    ep_rew_mean        | 155      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1212     |\n",
            "|    iterations         | 160800   |\n",
            "|    time_elapsed       | 5303     |\n",
            "|    total_timesteps    | 6432000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.462   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 160799   |\n",
            "|    policy_loss        | -0.295   |\n",
            "|    value_loss         | 2.02     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 192      |\n",
            "|    ep_rew_mean        | 148      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1213     |\n",
            "|    iterations         | 160900   |\n",
            "|    time_elapsed       | 5305     |\n",
            "|    total_timesteps    | 6436000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.322   |\n",
            "|    explained_variance | 0.919    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 160899   |\n",
            "|    policy_loss        | -8.56    |\n",
            "|    value_loss         | 141      |\n",
            "------------------------------------\n",
            "Num timesteps: 6440000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 150.51\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 186      |\n",
            "|    ep_rew_mean        | 151      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1213     |\n",
            "|    iterations         | 161000   |\n",
            "|    time_elapsed       | 5307     |\n",
            "|    total_timesteps    | 6440000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.299   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 160999   |\n",
            "|    policy_loss        | -0.328   |\n",
            "|    value_loss         | 2.89     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 186      |\n",
            "|    ep_rew_mean        | 150      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1213     |\n",
            "|    iterations         | 161100   |\n",
            "|    time_elapsed       | 5310     |\n",
            "|    total_timesteps    | 6444000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.379   |\n",
            "|    explained_variance | 0.937    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 161099   |\n",
            "|    policy_loss        | -0.235   |\n",
            "|    value_loss         | 93.4     |\n",
            "------------------------------------\n",
            "Num timesteps: 6448000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 152.10\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 193      |\n",
            "|    ep_rew_mean        | 152      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1213     |\n",
            "|    iterations         | 161200   |\n",
            "|    time_elapsed       | 5312     |\n",
            "|    total_timesteps    | 6448000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.352   |\n",
            "|    explained_variance | 0.762    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 161199   |\n",
            "|    policy_loss        | 0.242    |\n",
            "|    value_loss         | 440      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 189      |\n",
            "|    ep_rew_mean        | 140      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1213     |\n",
            "|    iterations         | 161300   |\n",
            "|    time_elapsed       | 5314     |\n",
            "|    total_timesteps    | 6452000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.406   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 161299   |\n",
            "|    policy_loss        | -0.153   |\n",
            "|    value_loss         | 1.57     |\n",
            "------------------------------------\n",
            "Num timesteps: 6456000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 161.61\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 194      |\n",
            "|    ep_rew_mean        | 162      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1214     |\n",
            "|    iterations         | 161400   |\n",
            "|    time_elapsed       | 5317     |\n",
            "|    total_timesteps    | 6456000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.329   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 161399   |\n",
            "|    policy_loss        | -0.0829  |\n",
            "|    value_loss         | 1.98     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 194      |\n",
            "|    ep_rew_mean        | 159      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1214     |\n",
            "|    iterations         | 161500   |\n",
            "|    time_elapsed       | 5319     |\n",
            "|    total_timesteps    | 6460000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.33    |\n",
            "|    explained_variance | 0.863    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 161499   |\n",
            "|    policy_loss        | 0.0595   |\n",
            "|    value_loss         | 203      |\n",
            "------------------------------------\n",
            "Num timesteps: 6464000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 179.73\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 190      |\n",
            "|    ep_rew_mean        | 180      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1214     |\n",
            "|    iterations         | 161600   |\n",
            "|    time_elapsed       | 5321     |\n",
            "|    total_timesteps    | 6464000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.272   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 161599   |\n",
            "|    policy_loss        | 0.106    |\n",
            "|    value_loss         | 2.35     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 191      |\n",
            "|    ep_rew_mean        | 189      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1215     |\n",
            "|    iterations         | 161700   |\n",
            "|    time_elapsed       | 5323     |\n",
            "|    total_timesteps    | 6468000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.284   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 161699   |\n",
            "|    policy_loss        | 0.347    |\n",
            "|    value_loss         | 5.17     |\n",
            "------------------------------------\n",
            "Num timesteps: 6472000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 181.89\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 191      |\n",
            "|    ep_rew_mean        | 182      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1215     |\n",
            "|    iterations         | 161800   |\n",
            "|    time_elapsed       | 5325     |\n",
            "|    total_timesteps    | 6472000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.297   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 161799   |\n",
            "|    policy_loss        | -0.475   |\n",
            "|    value_loss         | 2.22     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 189      |\n",
            "|    ep_rew_mean        | 166      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1215     |\n",
            "|    iterations         | 161900   |\n",
            "|    time_elapsed       | 5327     |\n",
            "|    total_timesteps    | 6476000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.291   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 161899   |\n",
            "|    policy_loss        | 0.0832   |\n",
            "|    value_loss         | 4.56     |\n",
            "------------------------------------\n",
            "Num timesteps: 6480000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 161.11\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 198      |\n",
            "|    ep_rew_mean        | 161      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1215     |\n",
            "|    iterations         | 162000   |\n",
            "|    time_elapsed       | 5329     |\n",
            "|    total_timesteps    | 6480000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.294   |\n",
            "|    explained_variance | 0.864    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 161999   |\n",
            "|    policy_loss        | -0.138   |\n",
            "|    value_loss         | 201      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 197      |\n",
            "|    ep_rew_mean        | 150      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1216     |\n",
            "|    iterations         | 162100   |\n",
            "|    time_elapsed       | 5331     |\n",
            "|    total_timesteps    | 6484000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.423   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 162099   |\n",
            "|    policy_loss        | -0.16    |\n",
            "|    value_loss         | 2.13     |\n",
            "------------------------------------\n",
            "Num timesteps: 6488000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 146.00\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 203      |\n",
            "|    ep_rew_mean        | 146      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1216     |\n",
            "|    iterations         | 162200   |\n",
            "|    time_elapsed       | 5334     |\n",
            "|    total_timesteps    | 6488000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.399   |\n",
            "|    explained_variance | 0.743    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 162199   |\n",
            "|    policy_loss        | 0.0235   |\n",
            "|    value_loss         | 470      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 213      |\n",
            "|    ep_rew_mean        | 162      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1216     |\n",
            "|    iterations         | 162300   |\n",
            "|    time_elapsed       | 5336     |\n",
            "|    total_timesteps    | 6492000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.404   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 162299   |\n",
            "|    policy_loss        | 0.24     |\n",
            "|    value_loss         | 1.17     |\n",
            "------------------------------------\n",
            "Num timesteps: 6496000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 164.43\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 214      |\n",
            "|    ep_rew_mean        | 164      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1216     |\n",
            "|    iterations         | 162400   |\n",
            "|    time_elapsed       | 5339     |\n",
            "|    total_timesteps    | 6496000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.392   |\n",
            "|    explained_variance | 0.632    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 162399   |\n",
            "|    policy_loss        | -0.178   |\n",
            "|    value_loss         | 621      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 208      |\n",
            "|    ep_rew_mean        | 170      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1216     |\n",
            "|    iterations         | 162500   |\n",
            "|    time_elapsed       | 5341     |\n",
            "|    total_timesteps    | 6500000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.373   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 162499   |\n",
            "|    policy_loss        | -0.0536  |\n",
            "|    value_loss         | 0.733    |\n",
            "------------------------------------\n",
            "Num timesteps: 6504000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 160.75\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 212      |\n",
            "|    ep_rew_mean        | 161      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1217     |\n",
            "|    iterations         | 162600   |\n",
            "|    time_elapsed       | 5343     |\n",
            "|    total_timesteps    | 6504000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.22    |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 162599   |\n",
            "|    policy_loss        | -0.0695  |\n",
            "|    value_loss         | 78.4     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 202      |\n",
            "|    ep_rew_mean        | 171      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1217     |\n",
            "|    iterations         | 162700   |\n",
            "|    time_elapsed       | 5345     |\n",
            "|    total_timesteps    | 6508000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.201   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 162699   |\n",
            "|    policy_loss        | -0.397   |\n",
            "|    value_loss         | 9.29     |\n",
            "------------------------------------\n",
            "Num timesteps: 6512000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 158.43\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 192      |\n",
            "|    ep_rew_mean        | 158      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1217     |\n",
            "|    iterations         | 162800   |\n",
            "|    time_elapsed       | 5347     |\n",
            "|    total_timesteps    | 6512000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.336   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 162799   |\n",
            "|    policy_loss        | 0.225    |\n",
            "|    value_loss         | 2.77     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 188      |\n",
            "|    ep_rew_mean        | 145      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1217     |\n",
            "|    iterations         | 162900   |\n",
            "|    time_elapsed       | 5349     |\n",
            "|    total_timesteps    | 6516000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.322   |\n",
            "|    explained_variance | 0.892    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 162899   |\n",
            "|    policy_loss        | -3.57    |\n",
            "|    value_loss         | 178      |\n",
            "------------------------------------\n",
            "Num timesteps: 6520000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 150.43\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 187      |\n",
            "|    ep_rew_mean        | 150      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1218     |\n",
            "|    iterations         | 163000   |\n",
            "|    time_elapsed       | 5352     |\n",
            "|    total_timesteps    | 6520000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.315   |\n",
            "|    explained_variance | 0.515    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 162999   |\n",
            "|    policy_loss        | -0.168   |\n",
            "|    value_loss         | 752      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 186      |\n",
            "|    ep_rew_mean        | 154      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1218     |\n",
            "|    iterations         | 163100   |\n",
            "|    time_elapsed       | 5354     |\n",
            "|    total_timesteps    | 6524000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.341   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 163099   |\n",
            "|    policy_loss        | -0.019   |\n",
            "|    value_loss         | 3.01     |\n",
            "------------------------------------\n",
            "Num timesteps: 6528000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 153.51\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 191      |\n",
            "|    ep_rew_mean        | 154      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1218     |\n",
            "|    iterations         | 163200   |\n",
            "|    time_elapsed       | 5356     |\n",
            "|    total_timesteps    | 6528000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.408   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 163199   |\n",
            "|    policy_loss        | 0.0392   |\n",
            "|    value_loss         | 6.39     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 197      |\n",
            "|    ep_rew_mean        | 175      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1218     |\n",
            "|    iterations         | 163300   |\n",
            "|    time_elapsed       | 5358     |\n",
            "|    total_timesteps    | 6532000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.264   |\n",
            "|    explained_variance | 0.361    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 163299   |\n",
            "|    policy_loss        | -1.13    |\n",
            "|    value_loss         | 2.18e+03 |\n",
            "------------------------------------\n",
            "Num timesteps: 6536000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 179.97\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 198      |\n",
            "|    ep_rew_mean        | 180      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1219     |\n",
            "|    iterations         | 163400   |\n",
            "|    time_elapsed       | 5361     |\n",
            "|    total_timesteps    | 6536000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.369   |\n",
            "|    explained_variance | 0.856    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 163399   |\n",
            "|    policy_loss        | -1.29    |\n",
            "|    value_loss         | 148      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 202      |\n",
            "|    ep_rew_mean        | 194      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1219     |\n",
            "|    iterations         | 163500   |\n",
            "|    time_elapsed       | 5364     |\n",
            "|    total_timesteps    | 6540000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.353   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 163499   |\n",
            "|    policy_loss        | 0.14     |\n",
            "|    value_loss         | 3.74     |\n",
            "------------------------------------\n",
            "Num timesteps: 6544000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 198.58\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 222      |\n",
            "|    ep_rew_mean        | 199      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1219     |\n",
            "|    iterations         | 163600   |\n",
            "|    time_elapsed       | 5367     |\n",
            "|    total_timesteps    | 6544000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.558   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 163599   |\n",
            "|    policy_loss        | 0.684    |\n",
            "|    value_loss         | 3.46     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 222      |\n",
            "|    ep_rew_mean        | 198      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1219     |\n",
            "|    iterations         | 163700   |\n",
            "|    time_elapsed       | 5369     |\n",
            "|    total_timesteps    | 6548000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.209   |\n",
            "|    explained_variance | 0.903    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 163699   |\n",
            "|    policy_loss        | 0.0988   |\n",
            "|    value_loss         | 233      |\n",
            "------------------------------------\n",
            "Num timesteps: 6552000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 199.32\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 221      |\n",
            "|    ep_rew_mean        | 199      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1219     |\n",
            "|    iterations         | 163800   |\n",
            "|    time_elapsed       | 5372     |\n",
            "|    total_timesteps    | 6552000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.356   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 163799   |\n",
            "|    policy_loss        | 0.0562   |\n",
            "|    value_loss         | 1.49     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 230      |\n",
            "|    ep_rew_mean        | 201      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1219     |\n",
            "|    iterations         | 163900   |\n",
            "|    time_elapsed       | 5374     |\n",
            "|    total_timesteps    | 6556000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.368   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 163899   |\n",
            "|    policy_loss        | -0.955   |\n",
            "|    value_loss         | 6.6      |\n",
            "------------------------------------\n",
            "Num timesteps: 6560000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 176.60\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 222      |\n",
            "|    ep_rew_mean        | 177      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1220     |\n",
            "|    iterations         | 164000   |\n",
            "|    time_elapsed       | 5376     |\n",
            "|    total_timesteps    | 6560000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.269   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 163999   |\n",
            "|    policy_loss        | -0.126   |\n",
            "|    value_loss         | 6.76     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 203      |\n",
            "|    ep_rew_mean        | 176      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1220     |\n",
            "|    iterations         | 164100   |\n",
            "|    time_elapsed       | 5379     |\n",
            "|    total_timesteps    | 6564000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.432   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 164099   |\n",
            "|    policy_loss        | -0.545   |\n",
            "|    value_loss         | 5.57     |\n",
            "------------------------------------\n",
            "Num timesteps: 6568000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 178.91\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 207      |\n",
            "|    ep_rew_mean        | 179      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1220     |\n",
            "|    iterations         | 164200   |\n",
            "|    time_elapsed       | 5381     |\n",
            "|    total_timesteps    | 6568000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.325   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 164199   |\n",
            "|    policy_loss        | -0.322   |\n",
            "|    value_loss         | 1.96     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 213      |\n",
            "|    ep_rew_mean        | 189      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1220     |\n",
            "|    iterations         | 164300   |\n",
            "|    time_elapsed       | 5383     |\n",
            "|    total_timesteps    | 6572000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.25    |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 164299   |\n",
            "|    policy_loss        | 0.0764   |\n",
            "|    value_loss         | 1.89     |\n",
            "------------------------------------\n",
            "Num timesteps: 6576000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 180.07\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 199      |\n",
            "|    ep_rew_mean        | 180      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1220     |\n",
            "|    iterations         | 164400   |\n",
            "|    time_elapsed       | 5386     |\n",
            "|    total_timesteps    | 6576000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.31    |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 164399   |\n",
            "|    policy_loss        | -0.432   |\n",
            "|    value_loss         | 4.53     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 210      |\n",
            "|    ep_rew_mean        | 187      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1221     |\n",
            "|    iterations         | 164500   |\n",
            "|    time_elapsed       | 5388     |\n",
            "|    total_timesteps    | 6580000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.267   |\n",
            "|    explained_variance | 0.744    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 164499   |\n",
            "|    policy_loss        | -2.15    |\n",
            "|    value_loss         | 248      |\n",
            "------------------------------------\n",
            "Num timesteps: 6584000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 195.59\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 213      |\n",
            "|    ep_rew_mean        | 196      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1221     |\n",
            "|    iterations         | 164600   |\n",
            "|    time_elapsed       | 5390     |\n",
            "|    total_timesteps    | 6584000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.417   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 164599   |\n",
            "|    policy_loss        | -0.458   |\n",
            "|    value_loss         | 2.59     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 209      |\n",
            "|    ep_rew_mean        | 197      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1221     |\n",
            "|    iterations         | 164700   |\n",
            "|    time_elapsed       | 5393     |\n",
            "|    total_timesteps    | 6588000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.338   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 164699   |\n",
            "|    policy_loss        | 0.339    |\n",
            "|    value_loss         | 1.42     |\n",
            "------------------------------------\n",
            "Num timesteps: 6592000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 189.03\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 222      |\n",
            "|    ep_rew_mean        | 189      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1221     |\n",
            "|    iterations         | 164800   |\n",
            "|    time_elapsed       | 5395     |\n",
            "|    total_timesteps    | 6592000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.309   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 164799   |\n",
            "|    policy_loss        | -0.0829  |\n",
            "|    value_loss         | 4.77     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 220      |\n",
            "|    ep_rew_mean        | 176      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1221     |\n",
            "|    iterations         | 164900   |\n",
            "|    time_elapsed       | 5397     |\n",
            "|    total_timesteps    | 6596000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.321   |\n",
            "|    explained_variance | 0.943    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 164899   |\n",
            "|    policy_loss        | -0.944   |\n",
            "|    value_loss         | 63.7     |\n",
            "------------------------------------\n",
            "Num timesteps: 6600000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 184.80\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 226      |\n",
            "|    ep_rew_mean        | 185      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1222     |\n",
            "|    iterations         | 165000   |\n",
            "|    time_elapsed       | 5400     |\n",
            "|    total_timesteps    | 6600000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.498   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 164999   |\n",
            "|    policy_loss        | -0.139   |\n",
            "|    value_loss         | 0.354    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 217      |\n",
            "|    ep_rew_mean        | 190      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1222     |\n",
            "|    iterations         | 165100   |\n",
            "|    time_elapsed       | 5402     |\n",
            "|    total_timesteps    | 6604000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.424   |\n",
            "|    explained_variance | 0.611    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 165099   |\n",
            "|    policy_loss        | -2.91    |\n",
            "|    value_loss         | 882      |\n",
            "------------------------------------\n",
            "Num timesteps: 6608000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 170.68\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 219      |\n",
            "|    ep_rew_mean        | 171      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1222     |\n",
            "|    iterations         | 165200   |\n",
            "|    time_elapsed       | 5404     |\n",
            "|    total_timesteps    | 6608000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.321   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 165199   |\n",
            "|    policy_loss        | -0.0317  |\n",
            "|    value_loss         | 5.81     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 202      |\n",
            "|    ep_rew_mean        | 180      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1222     |\n",
            "|    iterations         | 165300   |\n",
            "|    time_elapsed       | 5406     |\n",
            "|    total_timesteps    | 6612000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.378   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 165299   |\n",
            "|    policy_loss        | 0.164    |\n",
            "|    value_loss         | 1.24     |\n",
            "------------------------------------\n",
            "Num timesteps: 6616000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 176.05\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 199      |\n",
            "|    ep_rew_mean        | 176      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1223     |\n",
            "|    iterations         | 165400   |\n",
            "|    time_elapsed       | 5408     |\n",
            "|    total_timesteps    | 6616000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.294   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 165399   |\n",
            "|    policy_loss        | -0.0195  |\n",
            "|    value_loss         | 7.31     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 189      |\n",
            "|    ep_rew_mean        | 166      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1223     |\n",
            "|    iterations         | 165500   |\n",
            "|    time_elapsed       | 5411     |\n",
            "|    total_timesteps    | 6620000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.377   |\n",
            "|    explained_variance | 0.474    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 165499   |\n",
            "|    policy_loss        | 0.167    |\n",
            "|    value_loss         | 930      |\n",
            "------------------------------------\n",
            "Num timesteps: 6624000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 172.51\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 182      |\n",
            "|    ep_rew_mean        | 173      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1223     |\n",
            "|    iterations         | 165600   |\n",
            "|    time_elapsed       | 5413     |\n",
            "|    total_timesteps    | 6624000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.329   |\n",
            "|    explained_variance | 0.472    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 165599   |\n",
            "|    policy_loss        | -1.81    |\n",
            "|    value_loss         | 430      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 185      |\n",
            "|    ep_rew_mean        | 174      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1223     |\n",
            "|    iterations         | 165700   |\n",
            "|    time_elapsed       | 5415     |\n",
            "|    total_timesteps    | 6628000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.389   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 165699   |\n",
            "|    policy_loss        | 0.187    |\n",
            "|    value_loss         | 3.66     |\n",
            "------------------------------------\n",
            "Num timesteps: 6632000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 181.84\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 186      |\n",
            "|    ep_rew_mean        | 182      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1224     |\n",
            "|    iterations         | 165800   |\n",
            "|    time_elapsed       | 5417     |\n",
            "|    total_timesteps    | 6632000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.378   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 165799   |\n",
            "|    policy_loss        | 0.0285   |\n",
            "|    value_loss         | 3.37     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 199      |\n",
            "|    ep_rew_mean        | 187      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1224     |\n",
            "|    iterations         | 165900   |\n",
            "|    time_elapsed       | 5419     |\n",
            "|    total_timesteps    | 6636000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.259   |\n",
            "|    explained_variance | 0.95     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 165899   |\n",
            "|    policy_loss        | -0.285   |\n",
            "|    value_loss         | 27.5     |\n",
            "------------------------------------\n",
            "Num timesteps: 6640000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 199.96\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 210      |\n",
            "|    ep_rew_mean        | 200      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1224     |\n",
            "|    iterations         | 166000   |\n",
            "|    time_elapsed       | 5422     |\n",
            "|    total_timesteps    | 6640000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.231   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 165999   |\n",
            "|    policy_loss        | -0.0846  |\n",
            "|    value_loss         | 6.82     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 215      |\n",
            "|    ep_rew_mean        | 201      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1224     |\n",
            "|    iterations         | 166100   |\n",
            "|    time_elapsed       | 5424     |\n",
            "|    total_timesteps    | 6644000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.279   |\n",
            "|    explained_variance | 0.953    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 166099   |\n",
            "|    policy_loss        | -0.0917  |\n",
            "|    value_loss         | 75.9     |\n",
            "------------------------------------\n",
            "Num timesteps: 6648000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 213.24\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 219      |\n",
            "|    ep_rew_mean        | 213      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1224     |\n",
            "|    iterations         | 166200   |\n",
            "|    time_elapsed       | 5427     |\n",
            "|    total_timesteps    | 6648000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.308   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 166199   |\n",
            "|    policy_loss        | 0.109    |\n",
            "|    value_loss         | 1.22     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 228      |\n",
            "|    ep_rew_mean        | 215      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1225     |\n",
            "|    iterations         | 166300   |\n",
            "|    time_elapsed       | 5429     |\n",
            "|    total_timesteps    | 6652000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.256   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 166299   |\n",
            "|    policy_loss        | -0.543   |\n",
            "|    value_loss         | 4.03     |\n",
            "------------------------------------\n",
            "Num timesteps: 6656000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 212.19\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 216      |\n",
            "|    ep_rew_mean        | 212      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1225     |\n",
            "|    iterations         | 166400   |\n",
            "|    time_elapsed       | 5431     |\n",
            "|    total_timesteps    | 6656000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.354   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 166399   |\n",
            "|    policy_loss        | -0.09    |\n",
            "|    value_loss         | 0.76     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 210      |\n",
            "|    ep_rew_mean        | 213      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1225     |\n",
            "|    iterations         | 166500   |\n",
            "|    time_elapsed       | 5433     |\n",
            "|    total_timesteps    | 6660000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.187   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 166499   |\n",
            "|    policy_loss        | -0.0652  |\n",
            "|    value_loss         | 0.977    |\n",
            "------------------------------------\n",
            "Num timesteps: 6664000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 205.20\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 201      |\n",
            "|    ep_rew_mean        | 205      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1225     |\n",
            "|    iterations         | 166600   |\n",
            "|    time_elapsed       | 5436     |\n",
            "|    total_timesteps    | 6664000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.267   |\n",
            "|    explained_variance | 0.956    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 166599   |\n",
            "|    policy_loss        | 0.0655   |\n",
            "|    value_loss         | 35.6     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 198      |\n",
            "|    ep_rew_mean        | 203      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1226     |\n",
            "|    iterations         | 166700   |\n",
            "|    time_elapsed       | 5438     |\n",
            "|    total_timesteps    | 6668000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.42    |\n",
            "|    explained_variance | 0.98     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 166699   |\n",
            "|    policy_loss        | -0.109   |\n",
            "|    value_loss         | 23       |\n",
            "------------------------------------\n",
            "Num timesteps: 6672000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 211.94\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 190      |\n",
            "|    ep_rew_mean        | 212      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1226     |\n",
            "|    iterations         | 166800   |\n",
            "|    time_elapsed       | 5440     |\n",
            "|    total_timesteps    | 6672000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.296   |\n",
            "|    explained_variance | 0.795    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 166799   |\n",
            "|    policy_loss        | -0.185   |\n",
            "|    value_loss         | 289      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 194      |\n",
            "|    ep_rew_mean        | 213      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1226     |\n",
            "|    iterations         | 166900   |\n",
            "|    time_elapsed       | 5442     |\n",
            "|    total_timesteps    | 6676000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.447   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 166899   |\n",
            "|    policy_loss        | -0.0376  |\n",
            "|    value_loss         | 2.32     |\n",
            "------------------------------------\n",
            "Num timesteps: 6680000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 202.90\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 193      |\n",
            "|    ep_rew_mean        | 203      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1226     |\n",
            "|    iterations         | 167000   |\n",
            "|    time_elapsed       | 5444     |\n",
            "|    total_timesteps    | 6680000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.334   |\n",
            "|    explained_variance | 0.61     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 166999   |\n",
            "|    policy_loss        | -3.66    |\n",
            "|    value_loss         | 802      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 198      |\n",
            "|    ep_rew_mean        | 203      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1227     |\n",
            "|    iterations         | 167100   |\n",
            "|    time_elapsed       | 5446     |\n",
            "|    total_timesteps    | 6684000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.363   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 167099   |\n",
            "|    policy_loss        | 0.141    |\n",
            "|    value_loss         | 4.81     |\n",
            "------------------------------------\n",
            "Num timesteps: 6688000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 179.70\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 196      |\n",
            "|    ep_rew_mean        | 180      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1227     |\n",
            "|    iterations         | 167200   |\n",
            "|    time_elapsed       | 5448     |\n",
            "|    total_timesteps    | 6688000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.393   |\n",
            "|    explained_variance | 0.558    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 167199   |\n",
            "|    policy_loss        | 0.0108   |\n",
            "|    value_loss         | 1.41e+03 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 191      |\n",
            "|    ep_rew_mean        | 160      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1227     |\n",
            "|    iterations         | 167300   |\n",
            "|    time_elapsed       | 5450     |\n",
            "|    total_timesteps    | 6692000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.442   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 167299   |\n",
            "|    policy_loss        | 0.196    |\n",
            "|    value_loss         | 0.966    |\n",
            "------------------------------------\n",
            "Num timesteps: 6696000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 147.81\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 184      |\n",
            "|    ep_rew_mean        | 148      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1227     |\n",
            "|    iterations         | 167400   |\n",
            "|    time_elapsed       | 5452     |\n",
            "|    total_timesteps    | 6696000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.319   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 167399   |\n",
            "|    policy_loss        | -0.134   |\n",
            "|    value_loss         | 1.43     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 182      |\n",
            "|    ep_rew_mean        | 151      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1228     |\n",
            "|    iterations         | 167500   |\n",
            "|    time_elapsed       | 5455     |\n",
            "|    total_timesteps    | 6700000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.294   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 167499   |\n",
            "|    policy_loss        | 0.0431   |\n",
            "|    value_loss         | 0.766    |\n",
            "------------------------------------\n",
            "Num timesteps: 6704000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 164.66\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 176      |\n",
            "|    ep_rew_mean        | 165      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1228     |\n",
            "|    iterations         | 167600   |\n",
            "|    time_elapsed       | 5457     |\n",
            "|    total_timesteps    | 6704000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.321   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 167599   |\n",
            "|    policy_loss        | -0.194   |\n",
            "|    value_loss         | 3.83     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 180      |\n",
            "|    ep_rew_mean        | 183      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1228     |\n",
            "|    iterations         | 167700   |\n",
            "|    time_elapsed       | 5459     |\n",
            "|    total_timesteps    | 6708000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.267   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 167699   |\n",
            "|    policy_loss        | -0.207   |\n",
            "|    value_loss         | 1.42     |\n",
            "------------------------------------\n",
            "Num timesteps: 6712000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 172.89\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 178      |\n",
            "|    ep_rew_mean        | 173      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1229     |\n",
            "|    iterations         | 167800   |\n",
            "|    time_elapsed       | 5461     |\n",
            "|    total_timesteps    | 6712000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.456   |\n",
            "|    explained_variance | 0.697    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 167799   |\n",
            "|    policy_loss        | 0.0357   |\n",
            "|    value_loss         | 1.17e+03 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 178      |\n",
            "|    ep_rew_mean        | 174      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1229     |\n",
            "|    iterations         | 167900   |\n",
            "|    time_elapsed       | 5463     |\n",
            "|    total_timesteps    | 6716000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.51    |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 167899   |\n",
            "|    policy_loss        | -0.375   |\n",
            "|    value_loss         | 2.74     |\n",
            "------------------------------------\n",
            "Num timesteps: 6720000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 162.41\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 176      |\n",
            "|    ep_rew_mean        | 162      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1229     |\n",
            "|    iterations         | 168000   |\n",
            "|    time_elapsed       | 5465     |\n",
            "|    total_timesteps    | 6720000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.183   |\n",
            "|    explained_variance | 0.769    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 167999   |\n",
            "|    policy_loss        | -0.0446  |\n",
            "|    value_loss         | 381      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 175      |\n",
            "|    ep_rew_mean        | 153      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1229     |\n",
            "|    iterations         | 168100   |\n",
            "|    time_elapsed       | 5468     |\n",
            "|    total_timesteps    | 6724000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.35    |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 168099   |\n",
            "|    policy_loss        | -0.276   |\n",
            "|    value_loss         | 11.2     |\n",
            "------------------------------------\n",
            "Num timesteps: 6728000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 153.52\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 179      |\n",
            "|    ep_rew_mean        | 154      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1229     |\n",
            "|    iterations         | 168200   |\n",
            "|    time_elapsed       | 5470     |\n",
            "|    total_timesteps    | 6728000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.347   |\n",
            "|    explained_variance | 0.804    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 168199   |\n",
            "|    policy_loss        | -0.583   |\n",
            "|    value_loss         | 726      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 178      |\n",
            "|    ep_rew_mean        | 148      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1230     |\n",
            "|    iterations         | 168300   |\n",
            "|    time_elapsed       | 5472     |\n",
            "|    total_timesteps    | 6732000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.302   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 168299   |\n",
            "|    policy_loss        | -0.241   |\n",
            "|    value_loss         | 2.66     |\n",
            "------------------------------------\n",
            "Num timesteps: 6736000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 148.02\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 180      |\n",
            "|    ep_rew_mean        | 148      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1230     |\n",
            "|    iterations         | 168400   |\n",
            "|    time_elapsed       | 5474     |\n",
            "|    total_timesteps    | 6736000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.308   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 168399   |\n",
            "|    policy_loss        | 0.0188   |\n",
            "|    value_loss         | 1.29     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 194      |\n",
            "|    ep_rew_mean        | 162      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1230     |\n",
            "|    iterations         | 168500   |\n",
            "|    time_elapsed       | 5476     |\n",
            "|    total_timesteps    | 6740000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.406   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 168499   |\n",
            "|    policy_loss        | -0.47    |\n",
            "|    value_loss         | 6.54     |\n",
            "------------------------------------\n",
            "Num timesteps: 6744000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 173.17\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 199      |\n",
            "|    ep_rew_mean        | 173      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1230     |\n",
            "|    iterations         | 168600   |\n",
            "|    time_elapsed       | 5478     |\n",
            "|    total_timesteps    | 6744000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.187   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 168599   |\n",
            "|    policy_loss        | -0.286   |\n",
            "|    value_loss         | 3.93     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 211      |\n",
            "|    ep_rew_mean        | 186      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1231     |\n",
            "|    iterations         | 168700   |\n",
            "|    time_elapsed       | 5481     |\n",
            "|    total_timesteps    | 6748000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.342   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 168699   |\n",
            "|    policy_loss        | -0.239   |\n",
            "|    value_loss         | 1.76     |\n",
            "------------------------------------\n",
            "Num timesteps: 6752000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 202.60\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 212      |\n",
            "|    ep_rew_mean        | 203      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1231     |\n",
            "|    iterations         | 168800   |\n",
            "|    time_elapsed       | 5484     |\n",
            "|    total_timesteps    | 6752000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.327   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 168799   |\n",
            "|    policy_loss        | -0.231   |\n",
            "|    value_loss         | 9.54     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 225      |\n",
            "|    ep_rew_mean        | 212      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1231     |\n",
            "|    iterations         | 168900   |\n",
            "|    time_elapsed       | 5486     |\n",
            "|    total_timesteps    | 6756000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.239   |\n",
            "|    explained_variance | 0.871    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 168899   |\n",
            "|    policy_loss        | -0.564   |\n",
            "|    value_loss         | 23.2     |\n",
            "------------------------------------\n",
            "Num timesteps: 6760000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 213.17\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 225      |\n",
            "|    ep_rew_mean        | 213      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1231     |\n",
            "|    iterations         | 169000   |\n",
            "|    time_elapsed       | 5488     |\n",
            "|    total_timesteps    | 6760000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.381   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 168999   |\n",
            "|    policy_loss        | 0.153    |\n",
            "|    value_loss         | 1.77     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 226      |\n",
            "|    ep_rew_mean        | 216      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1231     |\n",
            "|    iterations         | 169100   |\n",
            "|    time_elapsed       | 5490     |\n",
            "|    total_timesteps    | 6764000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.471   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 169099   |\n",
            "|    policy_loss        | -0.297   |\n",
            "|    value_loss         | 1.1      |\n",
            "------------------------------------\n",
            "Num timesteps: 6768000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 210.59\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 229      |\n",
            "|    ep_rew_mean        | 211      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1232     |\n",
            "|    iterations         | 169200   |\n",
            "|    time_elapsed       | 5492     |\n",
            "|    total_timesteps    | 6768000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.2     |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 169199   |\n",
            "|    policy_loss        | 0.0772   |\n",
            "|    value_loss         | 1.71     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 215      |\n",
            "|    ep_rew_mean        | 194      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1232     |\n",
            "|    iterations         | 169300   |\n",
            "|    time_elapsed       | 5494     |\n",
            "|    total_timesteps    | 6772000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.235   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 169299   |\n",
            "|    policy_loss        | 0.947    |\n",
            "|    value_loss         | 11.1     |\n",
            "------------------------------------\n",
            "Num timesteps: 6776000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 175.76\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 201      |\n",
            "|    ep_rew_mean        | 176      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1232     |\n",
            "|    iterations         | 169400   |\n",
            "|    time_elapsed       | 5497     |\n",
            "|    total_timesteps    | 6776000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.337   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 169399   |\n",
            "|    policy_loss        | -0.213   |\n",
            "|    value_loss         | 1.77     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 196      |\n",
            "|    ep_rew_mean        | 168      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1232     |\n",
            "|    iterations         | 169500   |\n",
            "|    time_elapsed       | 5499     |\n",
            "|    total_timesteps    | 6780000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.252   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 169499   |\n",
            "|    policy_loss        | 0.0171   |\n",
            "|    value_loss         | 1.89     |\n",
            "------------------------------------\n",
            "Num timesteps: 6784000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 165.82\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 196      |\n",
            "|    ep_rew_mean        | 166      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1233     |\n",
            "|    iterations         | 169600   |\n",
            "|    time_elapsed       | 5501     |\n",
            "|    total_timesteps    | 6784000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.232   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 169599   |\n",
            "|    policy_loss        | -0.0845  |\n",
            "|    value_loss         | 1.46     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 192      |\n",
            "|    ep_rew_mean        | 178      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1233     |\n",
            "|    iterations         | 169700   |\n",
            "|    time_elapsed       | 5504     |\n",
            "|    total_timesteps    | 6788000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.37    |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 169699   |\n",
            "|    policy_loss        | -0.0333  |\n",
            "|    value_loss         | 24.2     |\n",
            "------------------------------------\n",
            "Num timesteps: 6792000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 195.34\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 212      |\n",
            "|    ep_rew_mean        | 195      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1233     |\n",
            "|    iterations         | 169800   |\n",
            "|    time_elapsed       | 5506     |\n",
            "|    total_timesteps    | 6792000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.252   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 169799   |\n",
            "|    policy_loss        | -0.144   |\n",
            "|    value_loss         | 0.497    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 213      |\n",
            "|    ep_rew_mean        | 201      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1233     |\n",
            "|    iterations         | 169900   |\n",
            "|    time_elapsed       | 5508     |\n",
            "|    total_timesteps    | 6796000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.327   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 169899   |\n",
            "|    policy_loss        | -0.0422  |\n",
            "|    value_loss         | 1.48     |\n",
            "------------------------------------\n",
            "Num timesteps: 6800000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 197.71\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 213      |\n",
            "|    ep_rew_mean        | 198      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1233     |\n",
            "|    iterations         | 170000   |\n",
            "|    time_elapsed       | 5510     |\n",
            "|    total_timesteps    | 6800000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.476   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 169999   |\n",
            "|    policy_loss        | -0.17    |\n",
            "|    value_loss         | 3.91     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 201      |\n",
            "|    ep_rew_mean        | 197      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1234     |\n",
            "|    iterations         | 170100   |\n",
            "|    time_elapsed       | 5512     |\n",
            "|    total_timesteps    | 6804000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.418   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 170099   |\n",
            "|    policy_loss        | -0.00988 |\n",
            "|    value_loss         | 0.744    |\n",
            "------------------------------------\n",
            "Num timesteps: 6808000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 188.99\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 202      |\n",
            "|    ep_rew_mean        | 189      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1234     |\n",
            "|    iterations         | 170200   |\n",
            "|    time_elapsed       | 5514     |\n",
            "|    total_timesteps    | 6808000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.35    |\n",
            "|    explained_variance | 0.887    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 170199   |\n",
            "|    policy_loss        | 0.66     |\n",
            "|    value_loss         | 124      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 189      |\n",
            "|    ep_rew_mean        | 188      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1234     |\n",
            "|    iterations         | 170300   |\n",
            "|    time_elapsed       | 5516     |\n",
            "|    total_timesteps    | 6812000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.18    |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 170299   |\n",
            "|    policy_loss        | -0.0607  |\n",
            "|    value_loss         | 0.724    |\n",
            "------------------------------------\n",
            "Num timesteps: 6816000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 180.76\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 189      |\n",
            "|    ep_rew_mean        | 181      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1235     |\n",
            "|    iterations         | 170400   |\n",
            "|    time_elapsed       | 5518     |\n",
            "|    total_timesteps    | 6816000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.255   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 170399   |\n",
            "|    policy_loss        | -0.127   |\n",
            "|    value_loss         | 6.61     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 188      |\n",
            "|    ep_rew_mean        | 186      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1235     |\n",
            "|    iterations         | 170500   |\n",
            "|    time_elapsed       | 5520     |\n",
            "|    total_timesteps    | 6820000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.364   |\n",
            "|    explained_variance | 0.964    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 170499   |\n",
            "|    policy_loss        | -1.24    |\n",
            "|    value_loss         | 34.8     |\n",
            "------------------------------------\n",
            "Num timesteps: 6824000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 185.47\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 192      |\n",
            "|    ep_rew_mean        | 185      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1235     |\n",
            "|    iterations         | 170600   |\n",
            "|    time_elapsed       | 5522     |\n",
            "|    total_timesteps    | 6824000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.386   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 170599   |\n",
            "|    policy_loss        | -0.0671  |\n",
            "|    value_loss         | 2.44     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 195      |\n",
            "|    ep_rew_mean        | 193      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1235     |\n",
            "|    iterations         | 170700   |\n",
            "|    time_elapsed       | 5524     |\n",
            "|    total_timesteps    | 6828000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.255   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 170699   |\n",
            "|    policy_loss        | -0.0765  |\n",
            "|    value_loss         | 0.833    |\n",
            "------------------------------------\n",
            "Num timesteps: 6832000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 184.51\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 192      |\n",
            "|    ep_rew_mean        | 185      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1236     |\n",
            "|    iterations         | 170800   |\n",
            "|    time_elapsed       | 5527     |\n",
            "|    total_timesteps    | 6832000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.461   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 170799   |\n",
            "|    policy_loss        | 0.0856   |\n",
            "|    value_loss         | 3.13     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 188      |\n",
            "|    ep_rew_mean        | 171      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1236     |\n",
            "|    iterations         | 170900   |\n",
            "|    time_elapsed       | 5529     |\n",
            "|    total_timesteps    | 6836000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.417   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 170899   |\n",
            "|    policy_loss        | 0.0549   |\n",
            "|    value_loss         | 2.44     |\n",
            "------------------------------------\n",
            "Num timesteps: 6840000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 171.02\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 186      |\n",
            "|    ep_rew_mean        | 171      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1236     |\n",
            "|    iterations         | 171000   |\n",
            "|    time_elapsed       | 5531     |\n",
            "|    total_timesteps    | 6840000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.35    |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 170999   |\n",
            "|    policy_loss        | -0.0268  |\n",
            "|    value_loss         | 6.19     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 181      |\n",
            "|    ep_rew_mean        | 156      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1236     |\n",
            "|    iterations         | 171100   |\n",
            "|    time_elapsed       | 5534     |\n",
            "|    total_timesteps    | 6844000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.405   |\n",
            "|    explained_variance | 0.615    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 171099   |\n",
            "|    policy_loss        | -0.935   |\n",
            "|    value_loss         | 1.64e+03 |\n",
            "------------------------------------\n",
            "Num timesteps: 6848000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 152.03\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 178      |\n",
            "|    ep_rew_mean        | 152      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1236     |\n",
            "|    iterations         | 171200   |\n",
            "|    time_elapsed       | 5536     |\n",
            "|    total_timesteps    | 6848000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.282   |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 171199   |\n",
            "|    policy_loss        | 0.394    |\n",
            "|    value_loss         | 7.73     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 176      |\n",
            "|    ep_rew_mean        | 159      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1237     |\n",
            "|    iterations         | 171300   |\n",
            "|    time_elapsed       | 5538     |\n",
            "|    total_timesteps    | 6852000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.181   |\n",
            "|    explained_variance | 0.728    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 171299   |\n",
            "|    policy_loss        | -0.0832  |\n",
            "|    value_loss         | 419      |\n",
            "------------------------------------\n",
            "Num timesteps: 6856000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 142.38\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 171      |\n",
            "|    ep_rew_mean        | 142      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1237     |\n",
            "|    iterations         | 171400   |\n",
            "|    time_elapsed       | 5540     |\n",
            "|    total_timesteps    | 6856000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.299   |\n",
            "|    explained_variance | 0.972    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 171399   |\n",
            "|    policy_loss        | 0.496    |\n",
            "|    value_loss         | 42.6     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 171      |\n",
            "|    ep_rew_mean        | 149      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1237     |\n",
            "|    iterations         | 171500   |\n",
            "|    time_elapsed       | 5541     |\n",
            "|    total_timesteps    | 6860000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.283   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 171499   |\n",
            "|    policy_loss        | -0.0964  |\n",
            "|    value_loss         | 8.99     |\n",
            "------------------------------------\n",
            "Num timesteps: 6864000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 146.56\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 165      |\n",
            "|    ep_rew_mean        | 147      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1238     |\n",
            "|    iterations         | 171600   |\n",
            "|    time_elapsed       | 5543     |\n",
            "|    total_timesteps    | 6864000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.294   |\n",
            "|    explained_variance | 0.949    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 171599   |\n",
            "|    policy_loss        | -0.143   |\n",
            "|    value_loss         | 38.1     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 162      |\n",
            "|    ep_rew_mean        | 134      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1238     |\n",
            "|    iterations         | 171700   |\n",
            "|    time_elapsed       | 5545     |\n",
            "|    total_timesteps    | 6868000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.444   |\n",
            "|    explained_variance | 0.711    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 171699   |\n",
            "|    policy_loss        | -2.29    |\n",
            "|    value_loss         | 1.15e+03 |\n",
            "------------------------------------\n",
            "Num timesteps: 6872000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 134.65\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 159      |\n",
            "|    ep_rew_mean        | 135      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1238     |\n",
            "|    iterations         | 171800   |\n",
            "|    time_elapsed       | 5547     |\n",
            "|    total_timesteps    | 6872000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.372   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 171799   |\n",
            "|    policy_loss        | -0.0553  |\n",
            "|    value_loss         | 20.4     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 155      |\n",
            "|    ep_rew_mean        | 109      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1238     |\n",
            "|    iterations         | 171900   |\n",
            "|    time_elapsed       | 5549     |\n",
            "|    total_timesteps    | 6876000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.317   |\n",
            "|    explained_variance | 0.98     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 171899   |\n",
            "|    policy_loss        | 0.126    |\n",
            "|    value_loss         | 14.2     |\n",
            "------------------------------------\n",
            "Num timesteps: 6880000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 116.77\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 156      |\n",
            "|    ep_rew_mean        | 117      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1239     |\n",
            "|    iterations         | 172000   |\n",
            "|    time_elapsed       | 5551     |\n",
            "|    total_timesteps    | 6880000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.475   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 171999   |\n",
            "|    policy_loss        | -0.189   |\n",
            "|    value_loss         | 1.36     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 158      |\n",
            "|    ep_rew_mean        | 125      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1239     |\n",
            "|    iterations         | 172100   |\n",
            "|    time_elapsed       | 5554     |\n",
            "|    total_timesteps    | 6884000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.535   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 172099   |\n",
            "|    policy_loss        | 0.0541   |\n",
            "|    value_loss         | 2.4      |\n",
            "------------------------------------\n",
            "Num timesteps: 6888000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 135.81\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 162      |\n",
            "|    ep_rew_mean        | 136      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1239     |\n",
            "|    iterations         | 172200   |\n",
            "|    time_elapsed       | 5556     |\n",
            "|    total_timesteps    | 6888000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.442   |\n",
            "|    explained_variance | 0.511    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 172199   |\n",
            "|    policy_loss        | 0.148    |\n",
            "|    value_loss         | 1.45e+03 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 172      |\n",
            "|    ep_rew_mean        | 169      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1239     |\n",
            "|    iterations         | 172300   |\n",
            "|    time_elapsed       | 5558     |\n",
            "|    total_timesteps    | 6892000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.356   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 172299   |\n",
            "|    policy_loss        | -0.195   |\n",
            "|    value_loss         | 4.37     |\n",
            "------------------------------------\n",
            "Num timesteps: 6896000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 185.66\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 176      |\n",
            "|    ep_rew_mean        | 186      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1240     |\n",
            "|    iterations         | 172400   |\n",
            "|    time_elapsed       | 5560     |\n",
            "|    total_timesteps    | 6896000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.286   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 172399   |\n",
            "|    policy_loss        | -0.0957  |\n",
            "|    value_loss         | 0.82     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 180      |\n",
            "|    ep_rew_mean        | 188      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1240     |\n",
            "|    iterations         | 172500   |\n",
            "|    time_elapsed       | 5562     |\n",
            "|    total_timesteps    | 6900000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.403   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 172499   |\n",
            "|    policy_loss        | -0.234   |\n",
            "|    value_loss         | 2.2      |\n",
            "------------------------------------\n",
            "Num timesteps: 6904000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 173.99\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 183      |\n",
            "|    ep_rew_mean        | 174      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1240     |\n",
            "|    iterations         | 172600   |\n",
            "|    time_elapsed       | 5564     |\n",
            "|    total_timesteps    | 6904000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.388   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 172599   |\n",
            "|    policy_loss        | -0.267   |\n",
            "|    value_loss         | 3.2      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 180      |\n",
            "|    ep_rew_mean        | 163      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1240     |\n",
            "|    iterations         | 172700   |\n",
            "|    time_elapsed       | 5566     |\n",
            "|    total_timesteps    | 6908000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.403   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 172699   |\n",
            "|    policy_loss        | -0.332   |\n",
            "|    value_loss         | 3.78     |\n",
            "------------------------------------\n",
            "Num timesteps: 6912000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 148.84\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 180      |\n",
            "|    ep_rew_mean        | 149      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1241     |\n",
            "|    iterations         | 172800   |\n",
            "|    time_elapsed       | 5569     |\n",
            "|    total_timesteps    | 6912000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.267   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 172799   |\n",
            "|    policy_loss        | -0.124   |\n",
            "|    value_loss         | 0.68     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 176      |\n",
            "|    ep_rew_mean        | 142      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1241     |\n",
            "|    iterations         | 172900   |\n",
            "|    time_elapsed       | 5571     |\n",
            "|    total_timesteps    | 6916000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.297   |\n",
            "|    explained_variance | 0.73     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 172899   |\n",
            "|    policy_loss        | 0.0556   |\n",
            "|    value_loss         | 290      |\n",
            "------------------------------------\n",
            "Num timesteps: 6920000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 156.89\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 191      |\n",
            "|    ep_rew_mean        | 157      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1241     |\n",
            "|    iterations         | 173000   |\n",
            "|    time_elapsed       | 5574     |\n",
            "|    total_timesteps    | 6920000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.217   |\n",
            "|    explained_variance | 0.942    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 172999   |\n",
            "|    policy_loss        | -0.0811  |\n",
            "|    value_loss         | 36.4     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 191      |\n",
            "|    ep_rew_mean        | 176      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1241     |\n",
            "|    iterations         | 173100   |\n",
            "|    time_elapsed       | 5576     |\n",
            "|    total_timesteps    | 6924000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.345   |\n",
            "|    explained_variance | 0.773    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 173099   |\n",
            "|    policy_loss        | 0.0539   |\n",
            "|    value_loss         | 218      |\n",
            "------------------------------------\n",
            "Num timesteps: 6928000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 175.89\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 192      |\n",
            "|    ep_rew_mean        | 176      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1242     |\n",
            "|    iterations         | 173200   |\n",
            "|    time_elapsed       | 5578     |\n",
            "|    total_timesteps    | 6928000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.381   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 173199   |\n",
            "|    policy_loss        | -0.287   |\n",
            "|    value_loss         | 3.9      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 188      |\n",
            "|    ep_rew_mean        | 175      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1242     |\n",
            "|    iterations         | 173300   |\n",
            "|    time_elapsed       | 5580     |\n",
            "|    total_timesteps    | 6932000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.284   |\n",
            "|    explained_variance | 0.45     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 173299   |\n",
            "|    policy_loss        | 2.2      |\n",
            "|    value_loss         | 400      |\n",
            "------------------------------------\n",
            "Num timesteps: 6936000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 150.67\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 167      |\n",
            "|    ep_rew_mean        | 151      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1242     |\n",
            "|    iterations         | 173400   |\n",
            "|    time_elapsed       | 5582     |\n",
            "|    total_timesteps    | 6936000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.304   |\n",
            "|    explained_variance | 0.957    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 173399   |\n",
            "|    policy_loss        | -1.39    |\n",
            "|    value_loss         | 102      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 161      |\n",
            "|    ep_rew_mean        | 140      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1242     |\n",
            "|    iterations         | 173500   |\n",
            "|    time_elapsed       | 5583     |\n",
            "|    total_timesteps    | 6940000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.361   |\n",
            "|    explained_variance | 0.698    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 173499   |\n",
            "|    policy_loss        | -3.43    |\n",
            "|    value_loss         | 777      |\n",
            "------------------------------------\n",
            "Num timesteps: 6944000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 145.03\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 163      |\n",
            "|    ep_rew_mean        | 145      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1243     |\n",
            "|    iterations         | 173600   |\n",
            "|    time_elapsed       | 5586     |\n",
            "|    total_timesteps    | 6944000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0488  |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 173599   |\n",
            "|    policy_loss        | 0.754    |\n",
            "|    value_loss         | 6.05     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 171      |\n",
            "|    ep_rew_mean        | 163      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1243     |\n",
            "|    iterations         | 173700   |\n",
            "|    time_elapsed       | 5588     |\n",
            "|    total_timesteps    | 6948000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.37    |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 173699   |\n",
            "|    policy_loss        | 0.105    |\n",
            "|    value_loss         | 1.25     |\n",
            "------------------------------------\n",
            "Num timesteps: 6952000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 171.41\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 177      |\n",
            "|    ep_rew_mean        | 171      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1243     |\n",
            "|    iterations         | 173800   |\n",
            "|    time_elapsed       | 5590     |\n",
            "|    total_timesteps    | 6952000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.326   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 173799   |\n",
            "|    policy_loss        | -0.4     |\n",
            "|    value_loss         | 4.9      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 186      |\n",
            "|    ep_rew_mean        | 195      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1243     |\n",
            "|    iterations         | 173900   |\n",
            "|    time_elapsed       | 5592     |\n",
            "|    total_timesteps    | 6956000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.422   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 173899   |\n",
            "|    policy_loss        | 0.321    |\n",
            "|    value_loss         | 17.7     |\n",
            "------------------------------------\n",
            "Num timesteps: 6960000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 203.30\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 198      |\n",
            "|    ep_rew_mean        | 203      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1244     |\n",
            "|    iterations         | 174000   |\n",
            "|    time_elapsed       | 5594     |\n",
            "|    total_timesteps    | 6960000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.23    |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 173999   |\n",
            "|    policy_loss        | 0.171    |\n",
            "|    value_loss         | 1.45     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 188      |\n",
            "|    ep_rew_mean        | 190      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1244     |\n",
            "|    iterations         | 174100   |\n",
            "|    time_elapsed       | 5596     |\n",
            "|    total_timesteps    | 6964000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.392   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 174099   |\n",
            "|    policy_loss        | 0.0965   |\n",
            "|    value_loss         | 3.74     |\n",
            "------------------------------------\n",
            "Num timesteps: 6968000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 199.20\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 194      |\n",
            "|    ep_rew_mean        | 199      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1244     |\n",
            "|    iterations         | 174200   |\n",
            "|    time_elapsed       | 5598     |\n",
            "|    total_timesteps    | 6968000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.303   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 174199   |\n",
            "|    policy_loss        | -0.203   |\n",
            "|    value_loss         | 3.76     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 196      |\n",
            "|    ep_rew_mean        | 216      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1244     |\n",
            "|    iterations         | 174300   |\n",
            "|    time_elapsed       | 5600     |\n",
            "|    total_timesteps    | 6972000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.227   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 174299   |\n",
            "|    policy_loss        | -0.0826  |\n",
            "|    value_loss         | 3.14     |\n",
            "------------------------------------\n",
            "Num timesteps: 6976000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 225.29\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 198      |\n",
            "|    ep_rew_mean        | 225      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1245     |\n",
            "|    iterations         | 174400   |\n",
            "|    time_elapsed       | 5602     |\n",
            "|    total_timesteps    | 6976000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.22    |\n",
            "|    explained_variance | 0.797    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 174399   |\n",
            "|    policy_loss        | -0.087   |\n",
            "|    value_loss         | 340      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 192      |\n",
            "|    ep_rew_mean        | 237      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1245     |\n",
            "|    iterations         | 174500   |\n",
            "|    time_elapsed       | 5605     |\n",
            "|    total_timesteps    | 6980000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.303   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 174499   |\n",
            "|    policy_loss        | -0.00699 |\n",
            "|    value_loss         | 0.945    |\n",
            "------------------------------------\n",
            "Num timesteps: 6984000\n",
            "Best mean reward: 238.21 - Last mean reward per episode: 249.05\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 193      |\n",
            "|    ep_rew_mean        | 249      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1245     |\n",
            "|    iterations         | 174600   |\n",
            "|    time_elapsed       | 5607     |\n",
            "|    total_timesteps    | 6984000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.186   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 174599   |\n",
            "|    policy_loss        | -0.219   |\n",
            "|    value_loss         | 9.5      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 185      |\n",
            "|    ep_rew_mean        | 242      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1245     |\n",
            "|    iterations         | 174700   |\n",
            "|    time_elapsed       | 5609     |\n",
            "|    total_timesteps    | 6988000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.131   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 174699   |\n",
            "|    policy_loss        | -0.186   |\n",
            "|    value_loss         | 1.16     |\n",
            "------------------------------------\n",
            "Num timesteps: 6992000\n",
            "Best mean reward: 249.05 - Last mean reward per episode: 230.74\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 183      |\n",
            "|    ep_rew_mean        | 231      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1246     |\n",
            "|    iterations         | 174800   |\n",
            "|    time_elapsed       | 5611     |\n",
            "|    total_timesteps    | 6992000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.225   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 174799   |\n",
            "|    policy_loss        | -0.0019  |\n",
            "|    value_loss         | 4.35     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 177      |\n",
            "|    ep_rew_mean        | 217      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1246     |\n",
            "|    iterations         | 174900   |\n",
            "|    time_elapsed       | 5613     |\n",
            "|    total_timesteps    | 6996000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.3     |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 174899   |\n",
            "|    policy_loss        | -0.0997  |\n",
            "|    value_loss         | 4.18     |\n",
            "------------------------------------\n",
            "Num timesteps: 7000000\n",
            "Best mean reward: 249.05 - Last mean reward per episode: 220.10\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 183      |\n",
            "|    ep_rew_mean        | 220      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1246     |\n",
            "|    iterations         | 175000   |\n",
            "|    time_elapsed       | 5616     |\n",
            "|    total_timesteps    | 7000000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.444   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 174999   |\n",
            "|    policy_loss        | -0.236   |\n",
            "|    value_loss         | 7.38     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 203      |\n",
            "|    ep_rew_mean        | 223      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1246     |\n",
            "|    iterations         | 175100   |\n",
            "|    time_elapsed       | 5619     |\n",
            "|    total_timesteps    | 7004000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.408   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 175099   |\n",
            "|    policy_loss        | 0.0146   |\n",
            "|    value_loss         | 2.86     |\n",
            "------------------------------------\n",
            "Num timesteps: 7008000\n",
            "Best mean reward: 249.05 - Last mean reward per episode: 217.59\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 218      |\n",
            "|    ep_rew_mean        | 218      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1246     |\n",
            "|    iterations         | 175200   |\n",
            "|    time_elapsed       | 5621     |\n",
            "|    total_timesteps    | 7008000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.198   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 175199   |\n",
            "|    policy_loss        | 0.0205   |\n",
            "|    value_loss         | 4.65     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 216      |\n",
            "|    ep_rew_mean        | 209      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1246     |\n",
            "|    iterations         | 175300   |\n",
            "|    time_elapsed       | 5623     |\n",
            "|    total_timesteps    | 7012000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.33    |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 175299   |\n",
            "|    policy_loss        | 0.0468   |\n",
            "|    value_loss         | 1.36     |\n",
            "------------------------------------\n",
            "Num timesteps: 7016000\n",
            "Best mean reward: 249.05 - Last mean reward per episode: 204.52\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 221      |\n",
            "|    ep_rew_mean        | 205      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1246     |\n",
            "|    iterations         | 175400   |\n",
            "|    time_elapsed       | 5626     |\n",
            "|    total_timesteps    | 7016000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.392   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 175399   |\n",
            "|    policy_loss        | 0.0227   |\n",
            "|    value_loss         | 2.5      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 232      |\n",
            "|    ep_rew_mean        | 203      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1247     |\n",
            "|    iterations         | 175500   |\n",
            "|    time_elapsed       | 5628     |\n",
            "|    total_timesteps    | 7020000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.205   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 175499   |\n",
            "|    policy_loss        | -0.0316  |\n",
            "|    value_loss         | 5.63     |\n",
            "------------------------------------\n",
            "Num timesteps: 7024000\n",
            "Best mean reward: 249.05 - Last mean reward per episode: 184.34\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 210      |\n",
            "|    ep_rew_mean        | 184      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1247     |\n",
            "|    iterations         | 175600   |\n",
            "|    time_elapsed       | 5631     |\n",
            "|    total_timesteps    | 7024000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.33    |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 175599   |\n",
            "|    policy_loss        | 0.0469   |\n",
            "|    value_loss         | 1.95     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 189      |\n",
            "|    ep_rew_mean        | 164      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1247     |\n",
            "|    iterations         | 175700   |\n",
            "|    time_elapsed       | 5633     |\n",
            "|    total_timesteps    | 7028000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.254   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 175699   |\n",
            "|    policy_loss        | 0.0314   |\n",
            "|    value_loss         | 1.13     |\n",
            "------------------------------------\n",
            "Num timesteps: 7032000\n",
            "Best mean reward: 249.05 - Last mean reward per episode: 169.67\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 199      |\n",
            "|    ep_rew_mean        | 170      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1247     |\n",
            "|    iterations         | 175800   |\n",
            "|    time_elapsed       | 5635     |\n",
            "|    total_timesteps    | 7032000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.431   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 175799   |\n",
            "|    policy_loss        | 0.493    |\n",
            "|    value_loss         | 3.05     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 203      |\n",
            "|    ep_rew_mean        | 176      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1248     |\n",
            "|    iterations         | 175900   |\n",
            "|    time_elapsed       | 5637     |\n",
            "|    total_timesteps    | 7036000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.286   |\n",
            "|    explained_variance | 0.953    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 175899   |\n",
            "|    policy_loss        | -0.749   |\n",
            "|    value_loss         | 126      |\n",
            "------------------------------------\n",
            "Num timesteps: 7040000\n",
            "Best mean reward: 249.05 - Last mean reward per episode: 184.49\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 191      |\n",
            "|    ep_rew_mean        | 184      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1248     |\n",
            "|    iterations         | 176000   |\n",
            "|    time_elapsed       | 5639     |\n",
            "|    total_timesteps    | 7040000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.383   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 175999   |\n",
            "|    policy_loss        | 0.114    |\n",
            "|    value_loss         | 2.12     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        | 194       |\n",
            "|    ep_rew_mean        | 204       |\n",
            "| time/                 |           |\n",
            "|    fps                | 1248      |\n",
            "|    iterations         | 176100    |\n",
            "|    time_elapsed       | 5641      |\n",
            "|    total_timesteps    | 7044000   |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.234    |\n",
            "|    explained_variance | 0.999     |\n",
            "|    learning_rate      | 0.00083   |\n",
            "|    n_updates          | 176099    |\n",
            "|    policy_loss        | -0.000746 |\n",
            "|    value_loss         | 2.64      |\n",
            "-------------------------------------\n",
            "Num timesteps: 7048000\n",
            "Best mean reward: 249.05 - Last mean reward per episode: 226.12\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 200      |\n",
            "|    ep_rew_mean        | 226      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1248     |\n",
            "|    iterations         | 176200   |\n",
            "|    time_elapsed       | 5644     |\n",
            "|    total_timesteps    | 7048000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.386   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 176199   |\n",
            "|    policy_loss        | 0.0175   |\n",
            "|    value_loss         | 1.49     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 202      |\n",
            "|    ep_rew_mean        | 227      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1248     |\n",
            "|    iterations         | 176300   |\n",
            "|    time_elapsed       | 5646     |\n",
            "|    total_timesteps    | 7052000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.369   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 176299   |\n",
            "|    policy_loss        | -0.0567  |\n",
            "|    value_loss         | 0.866    |\n",
            "------------------------------------\n",
            "Num timesteps: 7056000\n",
            "Best mean reward: 249.05 - Last mean reward per episode: 218.50\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 192      |\n",
            "|    ep_rew_mean        | 219      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1249     |\n",
            "|    iterations         | 176400   |\n",
            "|    time_elapsed       | 5648     |\n",
            "|    total_timesteps    | 7056000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.392   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 176399   |\n",
            "|    policy_loss        | 0.536    |\n",
            "|    value_loss         | 11.7     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 192      |\n",
            "|    ep_rew_mean        | 216      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1249     |\n",
            "|    iterations         | 176500   |\n",
            "|    time_elapsed       | 5650     |\n",
            "|    total_timesteps    | 7060000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.288   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 176499   |\n",
            "|    policy_loss        | -0.0864  |\n",
            "|    value_loss         | 1.89     |\n",
            "------------------------------------\n",
            "Num timesteps: 7064000\n",
            "Best mean reward: 249.05 - Last mean reward per episode: 197.23\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 186      |\n",
            "|    ep_rew_mean        | 197      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1249     |\n",
            "|    iterations         | 176600   |\n",
            "|    time_elapsed       | 5652     |\n",
            "|    total_timesteps    | 7064000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.296   |\n",
            "|    explained_variance | 0.959    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 176599   |\n",
            "|    policy_loss        | 0.232    |\n",
            "|    value_loss         | 62.7     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 171      |\n",
            "|    ep_rew_mean        | 188      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1250     |\n",
            "|    iterations         | 176700   |\n",
            "|    time_elapsed       | 5654     |\n",
            "|    total_timesteps    | 7068000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.22    |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 176699   |\n",
            "|    policy_loss        | 0.0577   |\n",
            "|    value_loss         | 1.59     |\n",
            "------------------------------------\n",
            "Num timesteps: 7072000\n",
            "Best mean reward: 249.05 - Last mean reward per episode: 192.64\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 171      |\n",
            "|    ep_rew_mean        | 193      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1250     |\n",
            "|    iterations         | 176800   |\n",
            "|    time_elapsed       | 5656     |\n",
            "|    total_timesteps    | 7072000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.237   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 176799   |\n",
            "|    policy_loss        | -0.158   |\n",
            "|    value_loss         | 0.852    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 175      |\n",
            "|    ep_rew_mean        | 197      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1250     |\n",
            "|    iterations         | 176900   |\n",
            "|    time_elapsed       | 5658     |\n",
            "|    total_timesteps    | 7076000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.155   |\n",
            "|    explained_variance | 0.517    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 176899   |\n",
            "|    policy_loss        | -0.0622  |\n",
            "|    value_loss         | 614      |\n",
            "------------------------------------\n",
            "Num timesteps: 7080000\n",
            "Best mean reward: 249.05 - Last mean reward per episode: 191.90\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 174      |\n",
            "|    ep_rew_mean        | 192      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1250     |\n",
            "|    iterations         | 177000   |\n",
            "|    time_elapsed       | 5660     |\n",
            "|    total_timesteps    | 7080000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.422   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 176999   |\n",
            "|    policy_loss        | -0.269   |\n",
            "|    value_loss         | 2        |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 192      |\n",
            "|    ep_rew_mean        | 198      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1250     |\n",
            "|    iterations         | 177100   |\n",
            "|    time_elapsed       | 5663     |\n",
            "|    total_timesteps    | 7084000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.383   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 177099   |\n",
            "|    policy_loss        | -0.0329  |\n",
            "|    value_loss         | 1.61     |\n",
            "------------------------------------\n",
            "Num timesteps: 7088000\n",
            "Best mean reward: 249.05 - Last mean reward per episode: 201.97\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 193      |\n",
            "|    ep_rew_mean        | 202      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1251     |\n",
            "|    iterations         | 177200   |\n",
            "|    time_elapsed       | 5665     |\n",
            "|    total_timesteps    | 7088000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.298   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 177199   |\n",
            "|    policy_loss        | 0.0965   |\n",
            "|    value_loss         | 3.92     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 205      |\n",
            "|    ep_rew_mean        | 201      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1251     |\n",
            "|    iterations         | 177300   |\n",
            "|    time_elapsed       | 5668     |\n",
            "|    total_timesteps    | 7092000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.304   |\n",
            "|    explained_variance | 0.847    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 177299   |\n",
            "|    policy_loss        | 0.0979   |\n",
            "|    value_loss         | 112      |\n",
            "------------------------------------\n",
            "Num timesteps: 7096000\n",
            "Best mean reward: 249.05 - Last mean reward per episode: 200.81\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 213      |\n",
            "|    ep_rew_mean        | 201      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1251     |\n",
            "|    iterations         | 177400   |\n",
            "|    time_elapsed       | 5670     |\n",
            "|    total_timesteps    | 7096000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.441   |\n",
            "|    explained_variance | 0.492    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 177399   |\n",
            "|    policy_loss        | 0.583    |\n",
            "|    value_loss         | 480      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 209      |\n",
            "|    ep_rew_mean        | 194      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1251     |\n",
            "|    iterations         | 177500   |\n",
            "|    time_elapsed       | 5672     |\n",
            "|    total_timesteps    | 7100000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.463   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 177499   |\n",
            "|    policy_loss        | 0.19     |\n",
            "|    value_loss         | 0.58     |\n",
            "------------------------------------\n",
            "Num timesteps: 7104000\n",
            "Best mean reward: 249.05 - Last mean reward per episode: 195.79\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 197      |\n",
            "|    ep_rew_mean        | 196      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1251     |\n",
            "|    iterations         | 177600   |\n",
            "|    time_elapsed       | 5674     |\n",
            "|    total_timesteps    | 7104000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.18    |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 177599   |\n",
            "|    policy_loss        | -0.0588  |\n",
            "|    value_loss         | 0.643    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 189      |\n",
            "|    ep_rew_mean        | 198      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1252     |\n",
            "|    iterations         | 177700   |\n",
            "|    time_elapsed       | 5676     |\n",
            "|    total_timesteps    | 7108000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.426   |\n",
            "|    explained_variance | 0.65     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 177699   |\n",
            "|    policy_loss        | -1.86    |\n",
            "|    value_loss         | 833      |\n",
            "------------------------------------\n",
            "Num timesteps: 7112000\n",
            "Best mean reward: 249.05 - Last mean reward per episode: 193.92\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 177      |\n",
            "|    ep_rew_mean        | 194      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1252     |\n",
            "|    iterations         | 177800   |\n",
            "|    time_elapsed       | 5679     |\n",
            "|    total_timesteps    | 7112000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.174   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 177799   |\n",
            "|    policy_loss        | 0.0139   |\n",
            "|    value_loss         | 2.01     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 181      |\n",
            "|    ep_rew_mean        | 192      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1252     |\n",
            "|    iterations         | 177900   |\n",
            "|    time_elapsed       | 5681     |\n",
            "|    total_timesteps    | 7116000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.338   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 177899   |\n",
            "|    policy_loss        | -0.56    |\n",
            "|    value_loss         | 2.06     |\n",
            "------------------------------------\n",
            "Num timesteps: 7120000\n",
            "Best mean reward: 249.05 - Last mean reward per episode: 184.50\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 175      |\n",
            "|    ep_rew_mean        | 185      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1252     |\n",
            "|    iterations         | 178000   |\n",
            "|    time_elapsed       | 5683     |\n",
            "|    total_timesteps    | 7120000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.387   |\n",
            "|    explained_variance | 0.976    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 177999   |\n",
            "|    policy_loss        | -0.719   |\n",
            "|    value_loss         | 60.3     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 177      |\n",
            "|    ep_rew_mean        | 172      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1252     |\n",
            "|    iterations         | 178100   |\n",
            "|    time_elapsed       | 5685     |\n",
            "|    total_timesteps    | 7124000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.371   |\n",
            "|    explained_variance | 0.747    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 178099   |\n",
            "|    policy_loss        | -11.5    |\n",
            "|    value_loss         | 762      |\n",
            "------------------------------------\n",
            "Num timesteps: 7128000\n",
            "Best mean reward: 249.05 - Last mean reward per episode: 160.83\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 170      |\n",
            "|    ep_rew_mean        | 161      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1253     |\n",
            "|    iterations         | 178200   |\n",
            "|    time_elapsed       | 5687     |\n",
            "|    total_timesteps    | 7128000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.405   |\n",
            "|    explained_variance | 0.702    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 178199   |\n",
            "|    policy_loss        | -0.102   |\n",
            "|    value_loss         | 1.52e+03 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 159      |\n",
            "|    ep_rew_mean        | 145      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1253     |\n",
            "|    iterations         | 178300   |\n",
            "|    time_elapsed       | 5690     |\n",
            "|    total_timesteps    | 7132000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.498   |\n",
            "|    explained_variance | 0.908    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 178299   |\n",
            "|    policy_loss        | -1.82    |\n",
            "|    value_loss         | 360      |\n",
            "------------------------------------\n",
            "Num timesteps: 7136000\n",
            "Best mean reward: 249.05 - Last mean reward per episode: 158.23\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 176      |\n",
            "|    ep_rew_mean        | 158      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1253     |\n",
            "|    iterations         | 178400   |\n",
            "|    time_elapsed       | 5692     |\n",
            "|    total_timesteps    | 7136000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.429   |\n",
            "|    explained_variance | 0.962    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 178399   |\n",
            "|    policy_loss        | -0.491   |\n",
            "|    value_loss         | 55.4     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 185      |\n",
            "|    ep_rew_mean        | 167      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1253     |\n",
            "|    iterations         | 178500   |\n",
            "|    time_elapsed       | 5694     |\n",
            "|    total_timesteps    | 7140000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.397   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 178499   |\n",
            "|    policy_loss        | -0.00505 |\n",
            "|    value_loss         | 0.276    |\n",
            "------------------------------------\n",
            "Num timesteps: 7144000\n",
            "Best mean reward: 249.05 - Last mean reward per episode: 179.68\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 186      |\n",
            "|    ep_rew_mean        | 180      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1254     |\n",
            "|    iterations         | 178600   |\n",
            "|    time_elapsed       | 5696     |\n",
            "|    total_timesteps    | 7144000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.288   |\n",
            "|    explained_variance | 0.876    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 178599   |\n",
            "|    policy_loss        | -1.43    |\n",
            "|    value_loss         | 115      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 193      |\n",
            "|    ep_rew_mean        | 191      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1254     |\n",
            "|    iterations         | 178700   |\n",
            "|    time_elapsed       | 5698     |\n",
            "|    total_timesteps    | 7148000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.231   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 178699   |\n",
            "|    policy_loss        | 0.0905   |\n",
            "|    value_loss         | 2.87     |\n",
            "------------------------------------\n",
            "Num timesteps: 7152000\n",
            "Best mean reward: 249.05 - Last mean reward per episode: 199.92\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 190      |\n",
            "|    ep_rew_mean        | 200      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1254     |\n",
            "|    iterations         | 178800   |\n",
            "|    time_elapsed       | 5700     |\n",
            "|    total_timesteps    | 7152000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.451   |\n",
            "|    explained_variance | 0.904    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 178799   |\n",
            "|    policy_loss        | 0.0582   |\n",
            "|    value_loss         | 114      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 183      |\n",
            "|    ep_rew_mean        | 202      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1254     |\n",
            "|    iterations         | 178900   |\n",
            "|    time_elapsed       | 5703     |\n",
            "|    total_timesteps    | 7156000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.499   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 178899   |\n",
            "|    policy_loss        | -0.368   |\n",
            "|    value_loss         | 8.05     |\n",
            "------------------------------------\n",
            "Num timesteps: 7160000\n",
            "Best mean reward: 249.05 - Last mean reward per episode: 198.66\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 193      |\n",
            "|    ep_rew_mean        | 199      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1254     |\n",
            "|    iterations         | 179000   |\n",
            "|    time_elapsed       | 5706     |\n",
            "|    total_timesteps    | 7160000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.405   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 178999   |\n",
            "|    policy_loss        | -0.0221  |\n",
            "|    value_loss         | 0.423    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 199      |\n",
            "|    ep_rew_mean        | 186      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1254     |\n",
            "|    iterations         | 179100   |\n",
            "|    time_elapsed       | 5708     |\n",
            "|    total_timesteps    | 7164000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.402   |\n",
            "|    explained_variance | 0.947    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 179099   |\n",
            "|    policy_loss        | -0.923   |\n",
            "|    value_loss         | 146      |\n",
            "------------------------------------\n",
            "Num timesteps: 7168000\n",
            "Best mean reward: 249.05 - Last mean reward per episode: 187.86\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 204      |\n",
            "|    ep_rew_mean        | 188      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1255     |\n",
            "|    iterations         | 179200   |\n",
            "|    time_elapsed       | 5711     |\n",
            "|    total_timesteps    | 7168000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.337   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 179199   |\n",
            "|    policy_loss        | -0.00615 |\n",
            "|    value_loss         | 0.222    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 200      |\n",
            "|    ep_rew_mean        | 182      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1255     |\n",
            "|    iterations         | 179300   |\n",
            "|    time_elapsed       | 5713     |\n",
            "|    total_timesteps    | 7172000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.475   |\n",
            "|    explained_variance | 0.702    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 179299   |\n",
            "|    policy_loss        | -0.17    |\n",
            "|    value_loss         | 1.18e+03 |\n",
            "------------------------------------\n",
            "Num timesteps: 7176000\n",
            "Best mean reward: 249.05 - Last mean reward per episode: 169.17\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 212      |\n",
            "|    ep_rew_mean        | 169      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1255     |\n",
            "|    iterations         | 179400   |\n",
            "|    time_elapsed       | 5715     |\n",
            "|    total_timesteps    | 7176000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.211   |\n",
            "|    explained_variance | 0.639    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 179399   |\n",
            "|    policy_loss        | 0.0148   |\n",
            "|    value_loss         | 622      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 195      |\n",
            "|    ep_rew_mean        | 167      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1255     |\n",
            "|    iterations         | 179500   |\n",
            "|    time_elapsed       | 5717     |\n",
            "|    total_timesteps    | 7180000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.333   |\n",
            "|    explained_variance | 0.464    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 179499   |\n",
            "|    policy_loss        | 0.551    |\n",
            "|    value_loss         | 2.48e+03 |\n",
            "------------------------------------\n",
            "Num timesteps: 7184000\n",
            "Best mean reward: 249.05 - Last mean reward per episode: 174.72\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 183      |\n",
            "|    ep_rew_mean        | 175      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1256     |\n",
            "|    iterations         | 179600   |\n",
            "|    time_elapsed       | 5719     |\n",
            "|    total_timesteps    | 7184000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.293   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 179599   |\n",
            "|    policy_loss        | 0.0171   |\n",
            "|    value_loss         | 4.94     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 180      |\n",
            "|    ep_rew_mean        | 174      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1256     |\n",
            "|    iterations         | 179700   |\n",
            "|    time_elapsed       | 5721     |\n",
            "|    total_timesteps    | 7188000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.391   |\n",
            "|    explained_variance | 0.723    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 179699   |\n",
            "|    policy_loss        | 0.0684   |\n",
            "|    value_loss         | 309      |\n",
            "------------------------------------\n",
            "Num timesteps: 7192000\n",
            "Best mean reward: 249.05 - Last mean reward per episode: 184.63\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 168      |\n",
            "|    ep_rew_mean        | 185      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1256     |\n",
            "|    iterations         | 179800   |\n",
            "|    time_elapsed       | 5724     |\n",
            "|    total_timesteps    | 7192000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.311   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 179799   |\n",
            "|    policy_loss        | 0.0675   |\n",
            "|    value_loss         | 2.61     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 175      |\n",
            "|    ep_rew_mean        | 206      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1256     |\n",
            "|    iterations         | 179900   |\n",
            "|    time_elapsed       | 5726     |\n",
            "|    total_timesteps    | 7196000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.169   |\n",
            "|    explained_variance | 0.654    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 179899   |\n",
            "|    policy_loss        | 0.256    |\n",
            "|    value_loss         | 432      |\n",
            "------------------------------------\n",
            "Num timesteps: 7200000\n",
            "Best mean reward: 249.05 - Last mean reward per episode: 230.90\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 182      |\n",
            "|    ep_rew_mean        | 231      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1256     |\n",
            "|    iterations         | 180000   |\n",
            "|    time_elapsed       | 5728     |\n",
            "|    total_timesteps    | 7200000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.312   |\n",
            "|    explained_variance | 0.85     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 179999   |\n",
            "|    policy_loss        | -0.172   |\n",
            "|    value_loss         | 489      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 179      |\n",
            "|    ep_rew_mean        | 222      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1257     |\n",
            "|    iterations         | 180100   |\n",
            "|    time_elapsed       | 5730     |\n",
            "|    total_timesteps    | 7204000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.263   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 180099   |\n",
            "|    policy_loss        | -0.0712  |\n",
            "|    value_loss         | 1.14     |\n",
            "------------------------------------\n",
            "Num timesteps: 7208000\n",
            "Best mean reward: 249.05 - Last mean reward per episode: 229.21\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 179      |\n",
            "|    ep_rew_mean        | 229      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1257     |\n",
            "|    iterations         | 180200   |\n",
            "|    time_elapsed       | 5732     |\n",
            "|    total_timesteps    | 7208000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.161   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 180199   |\n",
            "|    policy_loss        | 0.061    |\n",
            "|    value_loss         | 1.61     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 182      |\n",
            "|    ep_rew_mean        | 233      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1257     |\n",
            "|    iterations         | 180300   |\n",
            "|    time_elapsed       | 5734     |\n",
            "|    total_timesteps    | 7212000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.203   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 180299   |\n",
            "|    policy_loss        | -0.119   |\n",
            "|    value_loss         | 1.17     |\n",
            "------------------------------------\n",
            "Num timesteps: 7216000\n",
            "Best mean reward: 249.05 - Last mean reward per episode: 224.27\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 179      |\n",
            "|    ep_rew_mean        | 224      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1257     |\n",
            "|    iterations         | 180400   |\n",
            "|    time_elapsed       | 5736     |\n",
            "|    total_timesteps    | 7216000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.279   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 180399   |\n",
            "|    policy_loss        | 0.272    |\n",
            "|    value_loss         | 11.8     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 176      |\n",
            "|    ep_rew_mean        | 216      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1257     |\n",
            "|    iterations         | 180500   |\n",
            "|    time_elapsed       | 5739     |\n",
            "|    total_timesteps    | 7220000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.337   |\n",
            "|    explained_variance | 0.497    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 180499   |\n",
            "|    policy_loss        | -0.544   |\n",
            "|    value_loss         | 2.01e+03 |\n",
            "------------------------------------\n",
            "Num timesteps: 7224000\n",
            "Best mean reward: 249.05 - Last mean reward per episode: 223.58\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 187      |\n",
            "|    ep_rew_mean        | 224      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1258     |\n",
            "|    iterations         | 180600   |\n",
            "|    time_elapsed       | 5742     |\n",
            "|    total_timesteps    | 7224000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.231   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 180599   |\n",
            "|    policy_loss        | -0.0258  |\n",
            "|    value_loss         | 11       |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 199      |\n",
            "|    ep_rew_mean        | 229      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1258     |\n",
            "|    iterations         | 180700   |\n",
            "|    time_elapsed       | 5744     |\n",
            "|    total_timesteps    | 7228000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.33    |\n",
            "|    explained_variance | 0.981    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 180699   |\n",
            "|    policy_loss        | -0.141   |\n",
            "|    value_loss         | 18.4     |\n",
            "------------------------------------\n",
            "Num timesteps: 7232000\n",
            "Best mean reward: 249.05 - Last mean reward per episode: 202.32\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 191      |\n",
            "|    ep_rew_mean        | 202      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1258     |\n",
            "|    iterations         | 180800   |\n",
            "|    time_elapsed       | 5746     |\n",
            "|    total_timesteps    | 7232000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.226   |\n",
            "|    explained_variance | 0.306    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 180799   |\n",
            "|    policy_loss        | -0.278   |\n",
            "|    value_loss         | 2.35e+03 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 189      |\n",
            "|    ep_rew_mean        | 198      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1258     |\n",
            "|    iterations         | 180900   |\n",
            "|    time_elapsed       | 5748     |\n",
            "|    total_timesteps    | 7236000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.288   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 180899   |\n",
            "|    policy_loss        | 0.104    |\n",
            "|    value_loss         | 2.97     |\n",
            "------------------------------------\n",
            "Num timesteps: 7240000\n",
            "Best mean reward: 249.05 - Last mean reward per episode: 201.66\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 185      |\n",
            "|    ep_rew_mean        | 202      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1258     |\n",
            "|    iterations         | 181000   |\n",
            "|    time_elapsed       | 5751     |\n",
            "|    total_timesteps    | 7240000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.152   |\n",
            "|    explained_variance | 0.701    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 180999   |\n",
            "|    policy_loss        | -0.0551  |\n",
            "|    value_loss         | 451      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 185      |\n",
            "|    ep_rew_mean        | 210      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1259     |\n",
            "|    iterations         | 181100   |\n",
            "|    time_elapsed       | 5753     |\n",
            "|    total_timesteps    | 7244000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.427   |\n",
            "|    explained_variance | 0.986    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 181099   |\n",
            "|    policy_loss        | 0.502    |\n",
            "|    value_loss         | 15.9     |\n",
            "------------------------------------\n",
            "Num timesteps: 7248000\n",
            "Best mean reward: 249.05 - Last mean reward per episode: 229.69\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 184      |\n",
            "|    ep_rew_mean        | 230      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1259     |\n",
            "|    iterations         | 181200   |\n",
            "|    time_elapsed       | 5755     |\n",
            "|    total_timesteps    | 7248000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.223   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 181199   |\n",
            "|    policy_loss        | -0.295   |\n",
            "|    value_loss         | 2.8      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 190      |\n",
            "|    ep_rew_mean        | 250      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1259     |\n",
            "|    iterations         | 181300   |\n",
            "|    time_elapsed       | 5757     |\n",
            "|    total_timesteps    | 7252000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.311   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 181299   |\n",
            "|    policy_loss        | 0.0904   |\n",
            "|    value_loss         | 2.73     |\n",
            "------------------------------------\n",
            "Num timesteps: 7256000\n",
            "Best mean reward: 249.05 - Last mean reward per episode: 266.50\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 202      |\n",
            "|    ep_rew_mean        | 266      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1259     |\n",
            "|    iterations         | 181400   |\n",
            "|    time_elapsed       | 5760     |\n",
            "|    total_timesteps    | 7256000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.317   |\n",
            "|    explained_variance | 0.505    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 181399   |\n",
            "|    policy_loss        | -0.315   |\n",
            "|    value_loss         | 542      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 202      |\n",
            "|    ep_rew_mean        | 271      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1259     |\n",
            "|    iterations         | 181500   |\n",
            "|    time_elapsed       | 5762     |\n",
            "|    total_timesteps    | 7260000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.355   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 181499   |\n",
            "|    policy_loss        | -0.239   |\n",
            "|    value_loss         | 2.13     |\n",
            "------------------------------------\n",
            "Num timesteps: 7264000\n",
            "Best mean reward: 266.50 - Last mean reward per episode: 266.73\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 204      |\n",
            "|    ep_rew_mean        | 267      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1260     |\n",
            "|    iterations         | 181600   |\n",
            "|    time_elapsed       | 5764     |\n",
            "|    total_timesteps    | 7264000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.3     |\n",
            "|    explained_variance | 0.837    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 181599   |\n",
            "|    policy_loss        | -1.35    |\n",
            "|    value_loss         | 155      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 200      |\n",
            "|    ep_rew_mean        | 252      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1260     |\n",
            "|    iterations         | 181700   |\n",
            "|    time_elapsed       | 5767     |\n",
            "|    total_timesteps    | 7268000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.265   |\n",
            "|    explained_variance | 0.37     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 181699   |\n",
            "|    policy_loss        | -0.333   |\n",
            "|    value_loss         | 934      |\n",
            "------------------------------------\n",
            "Num timesteps: 7272000\n",
            "Best mean reward: 266.73 - Last mean reward per episode: 238.94\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 200      |\n",
            "|    ep_rew_mean        | 239      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1260     |\n",
            "|    iterations         | 181800   |\n",
            "|    time_elapsed       | 5769     |\n",
            "|    total_timesteps    | 7272000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.296   |\n",
            "|    explained_variance | 0.983    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 181799   |\n",
            "|    policy_loss        | -1.05    |\n",
            "|    value_loss         | 29.4     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 196      |\n",
            "|    ep_rew_mean        | 241      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1260     |\n",
            "|    iterations         | 181900   |\n",
            "|    time_elapsed       | 5771     |\n",
            "|    total_timesteps    | 7276000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.205   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 181899   |\n",
            "|    policy_loss        | -0.169   |\n",
            "|    value_loss         | 4.68     |\n",
            "------------------------------------\n",
            "Num timesteps: 7280000\n",
            "Best mean reward: 266.73 - Last mean reward per episode: 246.68\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 199      |\n",
            "|    ep_rew_mean        | 247      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1260     |\n",
            "|    iterations         | 182000   |\n",
            "|    time_elapsed       | 5773     |\n",
            "|    total_timesteps    | 7280000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.289   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 181999   |\n",
            "|    policy_loss        | 0.0716   |\n",
            "|    value_loss         | 2.01     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 188      |\n",
            "|    ep_rew_mean        | 241      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1261     |\n",
            "|    iterations         | 182100   |\n",
            "|    time_elapsed       | 5775     |\n",
            "|    total_timesteps    | 7284000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.437   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 182099   |\n",
            "|    policy_loss        | -0.377   |\n",
            "|    value_loss         | 2.87     |\n",
            "------------------------------------\n",
            "Num timesteps: 7288000\n",
            "Best mean reward: 266.73 - Last mean reward per episode: 241.34\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 215      |\n",
            "|    ep_rew_mean        | 241      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1261     |\n",
            "|    iterations         | 182200   |\n",
            "|    time_elapsed       | 5778     |\n",
            "|    total_timesteps    | 7288000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.279   |\n",
            "|    explained_variance | 0.733    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 182199   |\n",
            "|    policy_loss        | 0.369    |\n",
            "|    value_loss         | 523      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 217      |\n",
            "|    ep_rew_mean        | 243      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1261     |\n",
            "|    iterations         | 182300   |\n",
            "|    time_elapsed       | 5781     |\n",
            "|    total_timesteps    | 7292000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.275   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 182299   |\n",
            "|    policy_loss        | -0.189   |\n",
            "|    value_loss         | 1.8      |\n",
            "------------------------------------\n",
            "Num timesteps: 7296000\n",
            "Best mean reward: 266.73 - Last mean reward per episode: 237.60\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 215      |\n",
            "|    ep_rew_mean        | 238      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1261     |\n",
            "|    iterations         | 182400   |\n",
            "|    time_elapsed       | 5783     |\n",
            "|    total_timesteps    | 7296000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.378   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 182399   |\n",
            "|    policy_loss        | -0.0161  |\n",
            "|    value_loss         | 2.02     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 224      |\n",
            "|    ep_rew_mean        | 216      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1261     |\n",
            "|    iterations         | 182500   |\n",
            "|    time_elapsed       | 5785     |\n",
            "|    total_timesteps    | 7300000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.323   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 182499   |\n",
            "|    policy_loss        | -0.68    |\n",
            "|    value_loss         | 2.52     |\n",
            "------------------------------------\n",
            "Num timesteps: 7304000\n",
            "Best mean reward: 266.73 - Last mean reward per episode: 197.47\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 217      |\n",
            "|    ep_rew_mean        | 197      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1261     |\n",
            "|    iterations         | 182600   |\n",
            "|    time_elapsed       | 5787     |\n",
            "|    total_timesteps    | 7304000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.158   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 182599   |\n",
            "|    policy_loss        | -0.48    |\n",
            "|    value_loss         | 3.64     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 189      |\n",
            "|    ep_rew_mean        | 189      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1262     |\n",
            "|    iterations         | 182700   |\n",
            "|    time_elapsed       | 5789     |\n",
            "|    total_timesteps    | 7308000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.136   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 182699   |\n",
            "|    policy_loss        | -0.498   |\n",
            "|    value_loss         | 4.02     |\n",
            "------------------------------------\n",
            "Num timesteps: 7312000\n",
            "Best mean reward: 266.73 - Last mean reward per episode: 189.73\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 183      |\n",
            "|    ep_rew_mean        | 190      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1262     |\n",
            "|    iterations         | 182800   |\n",
            "|    time_elapsed       | 5792     |\n",
            "|    total_timesteps    | 7312000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.196   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 182799   |\n",
            "|    policy_loss        | -0.127   |\n",
            "|    value_loss         | 2.58     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 181      |\n",
            "|    ep_rew_mean        | 188      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1262     |\n",
            "|    iterations         | 182900   |\n",
            "|    time_elapsed       | 5794     |\n",
            "|    total_timesteps    | 7316000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.363   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 182899   |\n",
            "|    policy_loss        | -0.0394  |\n",
            "|    value_loss         | 12.9     |\n",
            "------------------------------------\n",
            "Num timesteps: 7320000\n",
            "Best mean reward: 266.73 - Last mean reward per episode: 184.89\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 180      |\n",
            "|    ep_rew_mean        | 185      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1262     |\n",
            "|    iterations         | 183000   |\n",
            "|    time_elapsed       | 5796     |\n",
            "|    total_timesteps    | 7320000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.235   |\n",
            "|    explained_variance | 0.646    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 182999   |\n",
            "|    policy_loss        | 0.164    |\n",
            "|    value_loss         | 440      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 182      |\n",
            "|    ep_rew_mean        | 191      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1263     |\n",
            "|    iterations         | 183100   |\n",
            "|    time_elapsed       | 5798     |\n",
            "|    total_timesteps    | 7324000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.157   |\n",
            "|    explained_variance | 0.823    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 183099   |\n",
            "|    policy_loss        | -0.0466  |\n",
            "|    value_loss         | 166      |\n",
            "------------------------------------\n",
            "Num timesteps: 7328000\n",
            "Best mean reward: 266.73 - Last mean reward per episode: 185.62\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 180      |\n",
            "|    ep_rew_mean        | 186      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1263     |\n",
            "|    iterations         | 183200   |\n",
            "|    time_elapsed       | 5800     |\n",
            "|    total_timesteps    | 7328000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.252   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 183199   |\n",
            "|    policy_loss        | 0.116    |\n",
            "|    value_loss         | 6.29     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 185      |\n",
            "|    ep_rew_mean        | 193      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1263     |\n",
            "|    iterations         | 183300   |\n",
            "|    time_elapsed       | 5803     |\n",
            "|    total_timesteps    | 7332000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.436   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 183299   |\n",
            "|    policy_loss        | -0.12    |\n",
            "|    value_loss         | 0.373    |\n",
            "------------------------------------\n",
            "Num timesteps: 7336000\n",
            "Best mean reward: 266.73 - Last mean reward per episode: 203.17\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 181      |\n",
            "|    ep_rew_mean        | 203      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1263     |\n",
            "|    iterations         | 183400   |\n",
            "|    time_elapsed       | 5805     |\n",
            "|    total_timesteps    | 7336000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.249   |\n",
            "|    explained_variance | 0.525    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 183399   |\n",
            "|    policy_loss        | 0.144    |\n",
            "|    value_loss         | 715      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 179      |\n",
            "|    ep_rew_mean        | 200      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1263     |\n",
            "|    iterations         | 183500   |\n",
            "|    time_elapsed       | 5807     |\n",
            "|    total_timesteps    | 7340000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.317   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 183499   |\n",
            "|    policy_loss        | -0.0808  |\n",
            "|    value_loss         | 2.4      |\n",
            "------------------------------------\n",
            "Num timesteps: 7344000\n",
            "Best mean reward: 266.73 - Last mean reward per episode: 215.25\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 185      |\n",
            "|    ep_rew_mean        | 215      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1264     |\n",
            "|    iterations         | 183600   |\n",
            "|    time_elapsed       | 5809     |\n",
            "|    total_timesteps    | 7344000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.181   |\n",
            "|    explained_variance | 0.452    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 183599   |\n",
            "|    policy_loss        | -0.385   |\n",
            "|    value_loss         | 358      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 182      |\n",
            "|    ep_rew_mean        | 211      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1264     |\n",
            "|    iterations         | 183700   |\n",
            "|    time_elapsed       | 5811     |\n",
            "|    total_timesteps    | 7348000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.153   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 183699   |\n",
            "|    policy_loss        | 0.25     |\n",
            "|    value_loss         | 10.5     |\n",
            "------------------------------------\n",
            "Num timesteps: 7352000\n",
            "Best mean reward: 266.73 - Last mean reward per episode: 194.06\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 172      |\n",
            "|    ep_rew_mean        | 194      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1264     |\n",
            "|    iterations         | 183800   |\n",
            "|    time_elapsed       | 5813     |\n",
            "|    total_timesteps    | 7352000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.3     |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 183799   |\n",
            "|    policy_loss        | -0.284   |\n",
            "|    value_loss         | 0.902    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 174      |\n",
            "|    ep_rew_mean        | 201      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1264     |\n",
            "|    iterations         | 183900   |\n",
            "|    time_elapsed       | 5815     |\n",
            "|    total_timesteps    | 7356000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.367   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 183899   |\n",
            "|    policy_loss        | -0.0921  |\n",
            "|    value_loss         | 5.71     |\n",
            "------------------------------------\n",
            "Num timesteps: 7360000\n",
            "Best mean reward: 266.73 - Last mean reward per episode: 201.96\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 182      |\n",
            "|    ep_rew_mean        | 202      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1264     |\n",
            "|    iterations         | 184000   |\n",
            "|    time_elapsed       | 5818     |\n",
            "|    total_timesteps    | 7360000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.231   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 183999   |\n",
            "|    policy_loss        | -0.188   |\n",
            "|    value_loss         | 6.16     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 199      |\n",
            "|    ep_rew_mean        | 210      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1265     |\n",
            "|    iterations         | 184100   |\n",
            "|    time_elapsed       | 5820     |\n",
            "|    total_timesteps    | 7364000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.274   |\n",
            "|    explained_variance | 0.979    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 184099   |\n",
            "|    policy_loss        | 0.571    |\n",
            "|    value_loss         | 18.3     |\n",
            "------------------------------------\n",
            "Num timesteps: 7368000\n",
            "Best mean reward: 266.73 - Last mean reward per episode: 222.88\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 204      |\n",
            "|    ep_rew_mean        | 223      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1265     |\n",
            "|    iterations         | 184200   |\n",
            "|    time_elapsed       | 5822     |\n",
            "|    total_timesteps    | 7368000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.253   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 184199   |\n",
            "|    policy_loss        | -0.114   |\n",
            "|    value_loss         | 0.888    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 212      |\n",
            "|    ep_rew_mean        | 237      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1265     |\n",
            "|    iterations         | 184300   |\n",
            "|    time_elapsed       | 5825     |\n",
            "|    total_timesteps    | 7372000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.347   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 184299   |\n",
            "|    policy_loss        | -0.00138 |\n",
            "|    value_loss         | 2.11     |\n",
            "------------------------------------\n",
            "Num timesteps: 7376000\n",
            "Best mean reward: 266.73 - Last mean reward per episode: 242.56\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 216      |\n",
            "|    ep_rew_mean        | 243      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1265     |\n",
            "|    iterations         | 184400   |\n",
            "|    time_elapsed       | 5827     |\n",
            "|    total_timesteps    | 7376000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.368   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 184399   |\n",
            "|    policy_loss        | 0.467    |\n",
            "|    value_loss         | 16.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 210      |\n",
            "|    ep_rew_mean        | 257      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1265     |\n",
            "|    iterations         | 184500   |\n",
            "|    time_elapsed       | 5829     |\n",
            "|    total_timesteps    | 7380000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.179   |\n",
            "|    explained_variance | 0.86     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 184499   |\n",
            "|    policy_loss        | -0.0405  |\n",
            "|    value_loss         | 192      |\n",
            "------------------------------------\n",
            "Num timesteps: 7384000\n",
            "Best mean reward: 266.73 - Last mean reward per episode: 255.23\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 204      |\n",
            "|    ep_rew_mean        | 255      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1266     |\n",
            "|    iterations         | 184600   |\n",
            "|    time_elapsed       | 5832     |\n",
            "|    total_timesteps    | 7384000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.258   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 184599   |\n",
            "|    policy_loss        | -0.0339  |\n",
            "|    value_loss         | 0.286    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 208      |\n",
            "|    ep_rew_mean        | 249      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1266     |\n",
            "|    iterations         | 184700   |\n",
            "|    time_elapsed       | 5835     |\n",
            "|    total_timesteps    | 7388000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.299   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 184699   |\n",
            "|    policy_loss        | -0.225   |\n",
            "|    value_loss         | 0.364    |\n",
            "------------------------------------\n",
            "Num timesteps: 7392000\n",
            "Best mean reward: 266.73 - Last mean reward per episode: 237.53\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 217      |\n",
            "|    ep_rew_mean        | 238      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1266     |\n",
            "|    iterations         | 184800   |\n",
            "|    time_elapsed       | 5837     |\n",
            "|    total_timesteps    | 7392000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.36    |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 184799   |\n",
            "|    policy_loss        | 0.0334   |\n",
            "|    value_loss         | 2.25     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 216      |\n",
            "|    ep_rew_mean        | 235      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1266     |\n",
            "|    iterations         | 184900   |\n",
            "|    time_elapsed       | 5840     |\n",
            "|    total_timesteps    | 7396000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.311   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 184899   |\n",
            "|    policy_loss        | 0.0672   |\n",
            "|    value_loss         | 1.6      |\n",
            "------------------------------------\n",
            "Num timesteps: 7400000\n",
            "Best mean reward: 266.73 - Last mean reward per episode: 206.61\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 209      |\n",
            "|    ep_rew_mean        | 207      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1266     |\n",
            "|    iterations         | 185000   |\n",
            "|    time_elapsed       | 5842     |\n",
            "|    total_timesteps    | 7400000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.319   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 184999   |\n",
            "|    policy_loss        | -0.0908  |\n",
            "|    value_loss         | 1.07     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 198      |\n",
            "|    ep_rew_mean        | 200      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1266     |\n",
            "|    iterations         | 185100   |\n",
            "|    time_elapsed       | 5844     |\n",
            "|    total_timesteps    | 7404000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.372   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 185099   |\n",
            "|    policy_loss        | 0.298    |\n",
            "|    value_loss         | 8.54     |\n",
            "------------------------------------\n",
            "Num timesteps: 7408000\n",
            "Best mean reward: 266.73 - Last mean reward per episode: 196.17\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 201      |\n",
            "|    ep_rew_mean        | 196      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1266     |\n",
            "|    iterations         | 185200   |\n",
            "|    time_elapsed       | 5847     |\n",
            "|    total_timesteps    | 7408000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.338   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 185199   |\n",
            "|    policy_loss        | -0.145   |\n",
            "|    value_loss         | 2.92     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 200      |\n",
            "|    ep_rew_mean        | 202      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1267     |\n",
            "|    iterations         | 185300   |\n",
            "|    time_elapsed       | 5849     |\n",
            "|    total_timesteps    | 7412000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.108   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 185299   |\n",
            "|    policy_loss        | -0.0223  |\n",
            "|    value_loss         | 0.751    |\n",
            "------------------------------------\n",
            "Num timesteps: 7416000\n",
            "Best mean reward: 266.73 - Last mean reward per episode: 215.75\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 197      |\n",
            "|    ep_rew_mean        | 216      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1267     |\n",
            "|    iterations         | 185400   |\n",
            "|    time_elapsed       | 5851     |\n",
            "|    total_timesteps    | 7416000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.269   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 185399   |\n",
            "|    policy_loss        | 0.0631   |\n",
            "|    value_loss         | 8.98     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 203      |\n",
            "|    ep_rew_mean        | 236      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1267     |\n",
            "|    iterations         | 185500   |\n",
            "|    time_elapsed       | 5853     |\n",
            "|    total_timesteps    | 7420000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.335   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 185499   |\n",
            "|    policy_loss        | -0.0542  |\n",
            "|    value_loss         | 1.37     |\n",
            "------------------------------------\n",
            "Num timesteps: 7424000\n",
            "Best mean reward: 266.73 - Last mean reward per episode: 249.30\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 209      |\n",
            "|    ep_rew_mean        | 249      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1267     |\n",
            "|    iterations         | 185600   |\n",
            "|    time_elapsed       | 5855     |\n",
            "|    total_timesteps    | 7424000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.242   |\n",
            "|    explained_variance | 0.917    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 185599   |\n",
            "|    policy_loss        | 0.208    |\n",
            "|    value_loss         | 82.3     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 200      |\n",
            "|    ep_rew_mean        | 254      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1268     |\n",
            "|    iterations         | 185700   |\n",
            "|    time_elapsed       | 5857     |\n",
            "|    total_timesteps    | 7428000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.206   |\n",
            "|    explained_variance | 0.688    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 185699   |\n",
            "|    policy_loss        | -0.124   |\n",
            "|    value_loss         | 507      |\n",
            "------------------------------------\n",
            "Num timesteps: 7432000\n",
            "Best mean reward: 266.73 - Last mean reward per episode: 254.38\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 194      |\n",
            "|    ep_rew_mean        | 254      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1268     |\n",
            "|    iterations         | 185800   |\n",
            "|    time_elapsed       | 5860     |\n",
            "|    total_timesteps    | 7432000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.312   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 185799   |\n",
            "|    policy_loss        | -0.259   |\n",
            "|    value_loss         | 8.7      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 189      |\n",
            "|    ep_rew_mean        | 232      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1268     |\n",
            "|    iterations         | 185900   |\n",
            "|    time_elapsed       | 5862     |\n",
            "|    total_timesteps    | 7436000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.329   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 185899   |\n",
            "|    policy_loss        | -0.111   |\n",
            "|    value_loss         | 7.38     |\n",
            "------------------------------------\n",
            "Num timesteps: 7440000\n",
            "Best mean reward: 266.73 - Last mean reward per episode: 223.82\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 185      |\n",
            "|    ep_rew_mean        | 224      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1268     |\n",
            "|    iterations         | 186000   |\n",
            "|    time_elapsed       | 5864     |\n",
            "|    total_timesteps    | 7440000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.224   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 185999   |\n",
            "|    policy_loss        | 0.0123   |\n",
            "|    value_loss         | 16.7     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 176      |\n",
            "|    ep_rew_mean        | 225      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1268     |\n",
            "|    iterations         | 186100   |\n",
            "|    time_elapsed       | 5866     |\n",
            "|    total_timesteps    | 7444000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.407   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 186099   |\n",
            "|    policy_loss        | -0.359   |\n",
            "|    value_loss         | 1.47     |\n",
            "------------------------------------\n",
            "Num timesteps: 7448000\n",
            "Best mean reward: 266.73 - Last mean reward per episode: 233.26\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 184      |\n",
            "|    ep_rew_mean        | 233      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1268     |\n",
            "|    iterations         | 186200   |\n",
            "|    time_elapsed       | 5869     |\n",
            "|    total_timesteps    | 7448000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.355   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 186199   |\n",
            "|    policy_loss        | 0.0901   |\n",
            "|    value_loss         | 1.2      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 198      |\n",
            "|    ep_rew_mean        | 243      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1269     |\n",
            "|    iterations         | 186300   |\n",
            "|    time_elapsed       | 5871     |\n",
            "|    total_timesteps    | 7452000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.157   |\n",
            "|    explained_variance | -0.491   |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 186299   |\n",
            "|    policy_loss        | 0.25     |\n",
            "|    value_loss         | 1.1e+03  |\n",
            "------------------------------------\n",
            "Num timesteps: 7456000\n",
            "Best mean reward: 266.73 - Last mean reward per episode: 250.55\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 197      |\n",
            "|    ep_rew_mean        | 251      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1269     |\n",
            "|    iterations         | 186400   |\n",
            "|    time_elapsed       | 5873     |\n",
            "|    total_timesteps    | 7456000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.207   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 186399   |\n",
            "|    policy_loss        | -0.0708  |\n",
            "|    value_loss         | 1.14     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 197      |\n",
            "|    ep_rew_mean        | 259      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1269     |\n",
            "|    iterations         | 186500   |\n",
            "|    time_elapsed       | 5876     |\n",
            "|    total_timesteps    | 7460000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.3     |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 186499   |\n",
            "|    policy_loss        | -0.372   |\n",
            "|    value_loss         | 14.4     |\n",
            "------------------------------------\n",
            "Num timesteps: 7464000\n",
            "Best mean reward: 266.73 - Last mean reward per episode: 259.29\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 192      |\n",
            "|    ep_rew_mean        | 259      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1269     |\n",
            "|    iterations         | 186600   |\n",
            "|    time_elapsed       | 5878     |\n",
            "|    total_timesteps    | 7464000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.256   |\n",
            "|    explained_variance | 0.802    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 186599   |\n",
            "|    policy_loss        | 0.0895   |\n",
            "|    value_loss         | 350      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 184      |\n",
            "|    ep_rew_mean        | 254      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1269     |\n",
            "|    iterations         | 186700   |\n",
            "|    time_elapsed       | 5880     |\n",
            "|    total_timesteps    | 7468000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.17    |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 186699   |\n",
            "|    policy_loss        | 0.0647   |\n",
            "|    value_loss         | 2.67     |\n",
            "------------------------------------\n",
            "Num timesteps: 7472000\n",
            "Best mean reward: 266.73 - Last mean reward per episode: 260.71\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 182      |\n",
            "|    ep_rew_mean        | 261      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1270     |\n",
            "|    iterations         | 186800   |\n",
            "|    time_elapsed       | 5882     |\n",
            "|    total_timesteps    | 7472000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.128   |\n",
            "|    explained_variance | 0.7      |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 186799   |\n",
            "|    policy_loss        | -0.495   |\n",
            "|    value_loss         | 965      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 186      |\n",
            "|    ep_rew_mean        | 269      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1270     |\n",
            "|    iterations         | 186900   |\n",
            "|    time_elapsed       | 5885     |\n",
            "|    total_timesteps    | 7476000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.292   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 186899   |\n",
            "|    policy_loss        | -0.128   |\n",
            "|    value_loss         | 1.07     |\n",
            "------------------------------------\n",
            "Num timesteps: 7480000\n",
            "Best mean reward: 266.73 - Last mean reward per episode: 266.73\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 188      |\n",
            "|    ep_rew_mean        | 267      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1270     |\n",
            "|    iterations         | 187000   |\n",
            "|    time_elapsed       | 5887     |\n",
            "|    total_timesteps    | 7480000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.383   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 186999   |\n",
            "|    policy_loss        | -0.0631  |\n",
            "|    value_loss         | 0.99     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 185      |\n",
            "|    ep_rew_mean        | 263      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1270     |\n",
            "|    iterations         | 187100   |\n",
            "|    time_elapsed       | 5889     |\n",
            "|    total_timesteps    | 7484000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.281   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 187099   |\n",
            "|    policy_loss        | -0.0645  |\n",
            "|    value_loss         | 1.36     |\n",
            "------------------------------------\n",
            "Num timesteps: 7488000\n",
            "Best mean reward: 266.73 - Last mean reward per episode: 268.31\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 186      |\n",
            "|    ep_rew_mean        | 268      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1271     |\n",
            "|    iterations         | 187200   |\n",
            "|    time_elapsed       | 5891     |\n",
            "|    total_timesteps    | 7488000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.197   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 187199   |\n",
            "|    policy_loss        | -0.00954 |\n",
            "|    value_loss         | 2.19     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 189      |\n",
            "|    ep_rew_mean        | 272      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1271     |\n",
            "|    iterations         | 187300   |\n",
            "|    time_elapsed       | 5893     |\n",
            "|    total_timesteps    | 7492000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.363   |\n",
            "|    explained_variance | 0.684    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 187299   |\n",
            "|    policy_loss        | 0.394    |\n",
            "|    value_loss         | 659      |\n",
            "------------------------------------\n",
            "Num timesteps: 7496000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 263.13\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 183      |\n",
            "|    ep_rew_mean        | 263      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1271     |\n",
            "|    iterations         | 187400   |\n",
            "|    time_elapsed       | 5896     |\n",
            "|    total_timesteps    | 7496000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.286   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 187399   |\n",
            "|    policy_loss        | -0.0737  |\n",
            "|    value_loss         | 4.65     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 198      |\n",
            "|    ep_rew_mean        | 262      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1271     |\n",
            "|    iterations         | 187500   |\n",
            "|    time_elapsed       | 5898     |\n",
            "|    total_timesteps    | 7500000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.441   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 187499   |\n",
            "|    policy_loss        | 0.0571   |\n",
            "|    value_loss         | 9.02     |\n",
            "------------------------------------\n",
            "Num timesteps: 7504000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 260.72\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 198      |\n",
            "|    ep_rew_mean        | 261      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1271     |\n",
            "|    iterations         | 187600   |\n",
            "|    time_elapsed       | 5900     |\n",
            "|    total_timesteps    | 7504000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.267   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 187599   |\n",
            "|    policy_loss        | 0.204    |\n",
            "|    value_loss         | 2.67     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 200      |\n",
            "|    ep_rew_mean        | 262      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1271     |\n",
            "|    iterations         | 187700   |\n",
            "|    time_elapsed       | 5902     |\n",
            "|    total_timesteps    | 7508000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.215   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 187699   |\n",
            "|    policy_loss        | -0.238   |\n",
            "|    value_loss         | 3.45     |\n",
            "------------------------------------\n",
            "Num timesteps: 7512000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 263.36\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 201      |\n",
            "|    ep_rew_mean        | 263      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1272     |\n",
            "|    iterations         | 187800   |\n",
            "|    time_elapsed       | 5904     |\n",
            "|    total_timesteps    | 7512000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.304   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 187799   |\n",
            "|    policy_loss        | 0.0691   |\n",
            "|    value_loss         | 1        |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 193      |\n",
            "|    ep_rew_mean        | 258      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1272     |\n",
            "|    iterations         | 187900   |\n",
            "|    time_elapsed       | 5906     |\n",
            "|    total_timesteps    | 7516000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.151   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 187899   |\n",
            "|    policy_loss        | 0.0852   |\n",
            "|    value_loss         | 2.97     |\n",
            "------------------------------------\n",
            "Num timesteps: 7520000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 246.19\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 183      |\n",
            "|    ep_rew_mean        | 246      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1272     |\n",
            "|    iterations         | 188000   |\n",
            "|    time_elapsed       | 5908     |\n",
            "|    total_timesteps    | 7520000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.242   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 187999   |\n",
            "|    policy_loss        | -0.131   |\n",
            "|    value_loss         | 2.25     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 182      |\n",
            "|    ep_rew_mean        | 244      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1272     |\n",
            "|    iterations         | 188100   |\n",
            "|    time_elapsed       | 5911     |\n",
            "|    total_timesteps    | 7524000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.486   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 188099   |\n",
            "|    policy_loss        | 0.151    |\n",
            "|    value_loss         | 9.85     |\n",
            "------------------------------------\n",
            "Num timesteps: 7528000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 240.91\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 182      |\n",
            "|    ep_rew_mean        | 241      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1273     |\n",
            "|    iterations         | 188200   |\n",
            "|    time_elapsed       | 5913     |\n",
            "|    total_timesteps    | 7528000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.476   |\n",
            "|    explained_variance | 0.288    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 188199   |\n",
            "|    policy_loss        | 0.272    |\n",
            "|    value_loss         | 274      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 182      |\n",
            "|    ep_rew_mean        | 233      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1273     |\n",
            "|    iterations         | 188300   |\n",
            "|    time_elapsed       | 5915     |\n",
            "|    total_timesteps    | 7532000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.263   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 188299   |\n",
            "|    policy_loss        | 0.0269   |\n",
            "|    value_loss         | 0.924    |\n",
            "------------------------------------\n",
            "Num timesteps: 7536000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 243.02\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 184      |\n",
            "|    ep_rew_mean        | 243      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1273     |\n",
            "|    iterations         | 188400   |\n",
            "|    time_elapsed       | 5917     |\n",
            "|    total_timesteps    | 7536000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.186   |\n",
            "|    explained_variance | 0.932    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 188399   |\n",
            "|    policy_loss        | -0.173   |\n",
            "|    value_loss         | 102      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 179      |\n",
            "|    ep_rew_mean        | 226      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1273     |\n",
            "|    iterations         | 188500   |\n",
            "|    time_elapsed       | 5919     |\n",
            "|    total_timesteps    | 7540000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.205   |\n",
            "|    explained_variance | 0.744    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 188499   |\n",
            "|    policy_loss        | -0.17    |\n",
            "|    value_loss         | 492      |\n",
            "------------------------------------\n",
            "Num timesteps: 7544000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 226.55\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 179      |\n",
            "|    ep_rew_mean        | 227      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1273     |\n",
            "|    iterations         | 188600   |\n",
            "|    time_elapsed       | 5921     |\n",
            "|    total_timesteps    | 7544000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.326   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 188599   |\n",
            "|    policy_loss        | 0.229    |\n",
            "|    value_loss         | 3.45     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 179      |\n",
            "|    ep_rew_mean        | 225      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1274     |\n",
            "|    iterations         | 188700   |\n",
            "|    time_elapsed       | 5923     |\n",
            "|    total_timesteps    | 7548000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.208   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 188699   |\n",
            "|    policy_loss        | -0.0357  |\n",
            "|    value_loss         | 2.21     |\n",
            "------------------------------------\n",
            "Num timesteps: 7552000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 224.87\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 187      |\n",
            "|    ep_rew_mean        | 225      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1274     |\n",
            "|    iterations         | 188800   |\n",
            "|    time_elapsed       | 5926     |\n",
            "|    total_timesteps    | 7552000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.202   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 188799   |\n",
            "|    policy_loss        | 0.175    |\n",
            "|    value_loss         | 7.37     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 187      |\n",
            "|    ep_rew_mean        | 235      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1274     |\n",
            "|    iterations         | 188900   |\n",
            "|    time_elapsed       | 5928     |\n",
            "|    total_timesteps    | 7556000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.376   |\n",
            "|    explained_variance | 0.962    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 188899   |\n",
            "|    policy_loss        | -0.221   |\n",
            "|    value_loss         | 40.1     |\n",
            "------------------------------------\n",
            "Num timesteps: 7560000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 239.92\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 188      |\n",
            "|    ep_rew_mean        | 240      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1274     |\n",
            "|    iterations         | 189000   |\n",
            "|    time_elapsed       | 5930     |\n",
            "|    total_timesteps    | 7560000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.258   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 188999   |\n",
            "|    policy_loss        | 0.113    |\n",
            "|    value_loss         | 2.45     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 186      |\n",
            "|    ep_rew_mean        | 231      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1274     |\n",
            "|    iterations         | 189100   |\n",
            "|    time_elapsed       | 5932     |\n",
            "|    total_timesteps    | 7564000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.389   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 189099   |\n",
            "|    policy_loss        | -0.0279  |\n",
            "|    value_loss         | 1.41     |\n",
            "------------------------------------\n",
            "Num timesteps: 7568000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 228.99\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 188      |\n",
            "|    ep_rew_mean        | 229      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1275     |\n",
            "|    iterations         | 189200   |\n",
            "|    time_elapsed       | 5935     |\n",
            "|    total_timesteps    | 7568000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.452   |\n",
            "|    explained_variance | 0.989    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 189199   |\n",
            "|    policy_loss        | 1.05     |\n",
            "|    value_loss         | 18.7     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 188      |\n",
            "|    ep_rew_mean        | 232      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1275     |\n",
            "|    iterations         | 189300   |\n",
            "|    time_elapsed       | 5937     |\n",
            "|    total_timesteps    | 7572000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.22    |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 189299   |\n",
            "|    policy_loss        | -0.143   |\n",
            "|    value_loss         | 1.34     |\n",
            "------------------------------------\n",
            "Num timesteps: 7576000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 229.87\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 188      |\n",
            "|    ep_rew_mean        | 230      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1275     |\n",
            "|    iterations         | 189400   |\n",
            "|    time_elapsed       | 5939     |\n",
            "|    total_timesteps    | 7576000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.204   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 189399   |\n",
            "|    policy_loss        | 0.166    |\n",
            "|    value_loss         | 4.19     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 194      |\n",
            "|    ep_rew_mean        | 243      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1275     |\n",
            "|    iterations         | 189500   |\n",
            "|    time_elapsed       | 5941     |\n",
            "|    total_timesteps    | 7580000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.33    |\n",
            "|    explained_variance | 0.985    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 189499   |\n",
            "|    policy_loss        | 1.02     |\n",
            "|    value_loss         | 18.1     |\n",
            "------------------------------------\n",
            "Num timesteps: 7584000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 239.03\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 188      |\n",
            "|    ep_rew_mean        | 239      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1275     |\n",
            "|    iterations         | 189600   |\n",
            "|    time_elapsed       | 5943     |\n",
            "|    total_timesteps    | 7584000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.328   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 189599   |\n",
            "|    policy_loss        | 0.045    |\n",
            "|    value_loss         | 0.455    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 181      |\n",
            "|    ep_rew_mean        | 249      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1276     |\n",
            "|    iterations         | 189700   |\n",
            "|    time_elapsed       | 5945     |\n",
            "|    total_timesteps    | 7588000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.143   |\n",
            "|    explained_variance | 0.469    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 189699   |\n",
            "|    policy_loss        | -0.191   |\n",
            "|    value_loss         | 1.82e+03 |\n",
            "------------------------------------\n",
            "Num timesteps: 7592000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 250.41\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 183      |\n",
            "|    ep_rew_mean        | 250      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1276     |\n",
            "|    iterations         | 189800   |\n",
            "|    time_elapsed       | 5947     |\n",
            "|    total_timesteps    | 7592000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.205   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 189799   |\n",
            "|    policy_loss        | -0.0076  |\n",
            "|    value_loss         | 1.54     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 182      |\n",
            "|    ep_rew_mean        | 246      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1276     |\n",
            "|    iterations         | 189900   |\n",
            "|    time_elapsed       | 5950     |\n",
            "|    total_timesteps    | 7596000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.301   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 189899   |\n",
            "|    policy_loss        | -0.284   |\n",
            "|    value_loss         | 3.37     |\n",
            "------------------------------------\n",
            "Num timesteps: 7600000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 250.71\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 191      |\n",
            "|    ep_rew_mean        | 251      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1276     |\n",
            "|    iterations         | 190000   |\n",
            "|    time_elapsed       | 5953     |\n",
            "|    total_timesteps    | 7600000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.293   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 189999   |\n",
            "|    policy_loss        | -0.0933  |\n",
            "|    value_loss         | 2.39     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 207      |\n",
            "|    ep_rew_mean        | 252      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1276     |\n",
            "|    iterations         | 190100   |\n",
            "|    time_elapsed       | 5955     |\n",
            "|    total_timesteps    | 7604000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.252   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 190099   |\n",
            "|    policy_loss        | 0.0977   |\n",
            "|    value_loss         | 1.75     |\n",
            "------------------------------------\n",
            "Num timesteps: 7608000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 242.33\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 209      |\n",
            "|    ep_rew_mean        | 242      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1276     |\n",
            "|    iterations         | 190200   |\n",
            "|    time_elapsed       | 5957     |\n",
            "|    total_timesteps    | 7608000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.287   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 190199   |\n",
            "|    policy_loss        | 0.069    |\n",
            "|    value_loss         | 2.73     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 210      |\n",
            "|    ep_rew_mean        | 242      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1277     |\n",
            "|    iterations         | 190300   |\n",
            "|    time_elapsed       | 5959     |\n",
            "|    total_timesteps    | 7612000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.215   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 190299   |\n",
            "|    policy_loss        | 0.00947  |\n",
            "|    value_loss         | 8.82     |\n",
            "------------------------------------\n",
            "Num timesteps: 7616000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 225.14\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 196      |\n",
            "|    ep_rew_mean        | 225      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1277     |\n",
            "|    iterations         | 190400   |\n",
            "|    time_elapsed       | 5961     |\n",
            "|    total_timesteps    | 7616000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.264   |\n",
            "|    explained_variance | 0.86     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 190399   |\n",
            "|    policy_loss        | 0.127    |\n",
            "|    value_loss         | 165      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 200      |\n",
            "|    ep_rew_mean        | 218      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1277     |\n",
            "|    iterations         | 190500   |\n",
            "|    time_elapsed       | 5963     |\n",
            "|    total_timesteps    | 7620000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.32    |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 190499   |\n",
            "|    policy_loss        | -0.0212  |\n",
            "|    value_loss         | 3.8      |\n",
            "------------------------------------\n",
            "Num timesteps: 7624000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 217.12\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 186      |\n",
            "|    ep_rew_mean        | 217      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1277     |\n",
            "|    iterations         | 190600   |\n",
            "|    time_elapsed       | 5966     |\n",
            "|    total_timesteps    | 7624000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.329   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 190599   |\n",
            "|    policy_loss        | 0.145    |\n",
            "|    value_loss         | 0.544    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 179      |\n",
            "|    ep_rew_mean        | 203      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1277     |\n",
            "|    iterations         | 190700   |\n",
            "|    time_elapsed       | 5968     |\n",
            "|    total_timesteps    | 7628000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.244   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 190699   |\n",
            "|    policy_loss        | -0.105   |\n",
            "|    value_loss         | 3.37     |\n",
            "------------------------------------\n",
            "Num timesteps: 7632000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 204.49\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 191      |\n",
            "|    ep_rew_mean        | 204      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1278     |\n",
            "|    iterations         | 190800   |\n",
            "|    time_elapsed       | 5970     |\n",
            "|    total_timesteps    | 7632000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.277   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 190799   |\n",
            "|    policy_loss        | -0.172   |\n",
            "|    value_loss         | 1.79     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 198      |\n",
            "|    ep_rew_mean        | 223      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1278     |\n",
            "|    iterations         | 190900   |\n",
            "|    time_elapsed       | 5972     |\n",
            "|    total_timesteps    | 7636000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.135   |\n",
            "|    explained_variance | 0.87     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 190899   |\n",
            "|    policy_loss        | -0.0465  |\n",
            "|    value_loss         | 253      |\n",
            "------------------------------------\n",
            "Num timesteps: 7640000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 215.38\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 192      |\n",
            "|    ep_rew_mean        | 215      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1278     |\n",
            "|    iterations         | 191000   |\n",
            "|    time_elapsed       | 5975     |\n",
            "|    total_timesteps    | 7640000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.165   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 190999   |\n",
            "|    policy_loss        | 0.0446   |\n",
            "|    value_loss         | 8.51     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 196      |\n",
            "|    ep_rew_mean        | 229      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1278     |\n",
            "|    iterations         | 191100   |\n",
            "|    time_elapsed       | 5977     |\n",
            "|    total_timesteps    | 7644000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.278   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 191099   |\n",
            "|    policy_loss        | 0.102    |\n",
            "|    value_loss         | 2.31     |\n",
            "------------------------------------\n",
            "Num timesteps: 7648000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 231.39\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 186      |\n",
            "|    ep_rew_mean        | 231      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1279     |\n",
            "|    iterations         | 191200   |\n",
            "|    time_elapsed       | 5979     |\n",
            "|    total_timesteps    | 7648000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.172   |\n",
            "|    explained_variance | 0.613    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 191199   |\n",
            "|    policy_loss        | -2.11    |\n",
            "|    value_loss         | 1.22e+03 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 180      |\n",
            "|    ep_rew_mean        | 212      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1279     |\n",
            "|    iterations         | 191300   |\n",
            "|    time_elapsed       | 5981     |\n",
            "|    total_timesteps    | 7652000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.324   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 191299   |\n",
            "|    policy_loss        | -0.428   |\n",
            "|    value_loss         | 1.21     |\n",
            "------------------------------------\n",
            "Num timesteps: 7656000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 217.04\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 180      |\n",
            "|    ep_rew_mean        | 217      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1279     |\n",
            "|    iterations         | 191400   |\n",
            "|    time_elapsed       | 5983     |\n",
            "|    total_timesteps    | 7656000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.31    |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 191399   |\n",
            "|    policy_loss        | -0.125   |\n",
            "|    value_loss         | 2.07     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 179      |\n",
            "|    ep_rew_mean        | 226      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1279     |\n",
            "|    iterations         | 191500   |\n",
            "|    time_elapsed       | 5985     |\n",
            "|    total_timesteps    | 7660000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.249   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 191499   |\n",
            "|    policy_loss        | 0.335    |\n",
            "|    value_loss         | 5.54     |\n",
            "------------------------------------\n",
            "Num timesteps: 7664000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 231.76\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 185      |\n",
            "|    ep_rew_mean        | 232      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1280     |\n",
            "|    iterations         | 191600   |\n",
            "|    time_elapsed       | 5987     |\n",
            "|    total_timesteps    | 7664000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.254   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 191599   |\n",
            "|    policy_loss        | -0.107   |\n",
            "|    value_loss         | 2.04     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 184      |\n",
            "|    ep_rew_mean        | 243      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1280     |\n",
            "|    iterations         | 191700   |\n",
            "|    time_elapsed       | 5989     |\n",
            "|    total_timesteps    | 7668000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.224   |\n",
            "|    explained_variance | 0.533    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 191699   |\n",
            "|    policy_loss        | -4.09    |\n",
            "|    value_loss         | 1.26e+03 |\n",
            "------------------------------------\n",
            "Num timesteps: 7672000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 251.29\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 193      |\n",
            "|    ep_rew_mean        | 251      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1280     |\n",
            "|    iterations         | 191800   |\n",
            "|    time_elapsed       | 5991     |\n",
            "|    total_timesteps    | 7672000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.427   |\n",
            "|    explained_variance | 0.699    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 191799   |\n",
            "|    policy_loss        | -0.898   |\n",
            "|    value_loss         | 875      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 187      |\n",
            "|    ep_rew_mean        | 238      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1280     |\n",
            "|    iterations         | 191900   |\n",
            "|    time_elapsed       | 5993     |\n",
            "|    total_timesteps    | 7676000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.252   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 191899   |\n",
            "|    policy_loss        | 0.00147  |\n",
            "|    value_loss         | 2.28     |\n",
            "------------------------------------\n",
            "Num timesteps: 7680000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 237.42\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 185      |\n",
            "|    ep_rew_mean        | 237      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1280     |\n",
            "|    iterations         | 192000   |\n",
            "|    time_elapsed       | 5996     |\n",
            "|    total_timesteps    | 7680000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.237   |\n",
            "|    explained_variance | 0.501    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 191999   |\n",
            "|    policy_loss        | -0.119   |\n",
            "|    value_loss         | 344      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 184      |\n",
            "|    ep_rew_mean        | 237      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1281     |\n",
            "|    iterations         | 192100   |\n",
            "|    time_elapsed       | 5997     |\n",
            "|    total_timesteps    | 7684000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.277   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 192099   |\n",
            "|    policy_loss        | -0.02    |\n",
            "|    value_loss         | 4.22     |\n",
            "------------------------------------\n",
            "Num timesteps: 7688000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 240.53\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 181      |\n",
            "|    ep_rew_mean        | 241      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1281     |\n",
            "|    iterations         | 192200   |\n",
            "|    time_elapsed       | 6000     |\n",
            "|    total_timesteps    | 7688000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.215   |\n",
            "|    explained_variance | 0.561    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 192199   |\n",
            "|    policy_loss        | 0.0832   |\n",
            "|    value_loss         | 290      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 186      |\n",
            "|    ep_rew_mean        | 253      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1281     |\n",
            "|    iterations         | 192300   |\n",
            "|    time_elapsed       | 6002     |\n",
            "|    total_timesteps    | 7692000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.214   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 192299   |\n",
            "|    policy_loss        | -0.174   |\n",
            "|    value_loss         | 1.45     |\n",
            "------------------------------------\n",
            "Num timesteps: 7696000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 267.72\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 192      |\n",
            "|    ep_rew_mean        | 268      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1281     |\n",
            "|    iterations         | 192400   |\n",
            "|    time_elapsed       | 6004     |\n",
            "|    total_timesteps    | 7696000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.335   |\n",
            "|    explained_variance | 0.542    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 192399   |\n",
            "|    policy_loss        | -0.00448 |\n",
            "|    value_loss         | 439      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 192      |\n",
            "|    ep_rew_mean        | 269      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1281     |\n",
            "|    iterations         | 192500   |\n",
            "|    time_elapsed       | 6006     |\n",
            "|    total_timesteps    | 7700000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.236   |\n",
            "|    explained_variance | -0.187   |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 192499   |\n",
            "|    policy_loss        | -0.122   |\n",
            "|    value_loss         | 727      |\n",
            "------------------------------------\n",
            "Num timesteps: 7704000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 257.70\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 186      |\n",
            "|    ep_rew_mean        | 258      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1282     |\n",
            "|    iterations         | 192600   |\n",
            "|    time_elapsed       | 6008     |\n",
            "|    total_timesteps    | 7704000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.165   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 192599   |\n",
            "|    policy_loss        | 0.048    |\n",
            "|    value_loss         | 2.43     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 183      |\n",
            "|    ep_rew_mean        | 253      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1282     |\n",
            "|    iterations         | 192700   |\n",
            "|    time_elapsed       | 6011     |\n",
            "|    total_timesteps    | 7708000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.285   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 192699   |\n",
            "|    policy_loss        | 0.00814  |\n",
            "|    value_loss         | 3.42     |\n",
            "------------------------------------\n",
            "Num timesteps: 7712000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 249.68\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 180      |\n",
            "|    ep_rew_mean        | 250      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1282     |\n",
            "|    iterations         | 192800   |\n",
            "|    time_elapsed       | 6012     |\n",
            "|    total_timesteps    | 7712000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.308   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 192799   |\n",
            "|    policy_loss        | -0.0317  |\n",
            "|    value_loss         | 0.642    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 182      |\n",
            "|    ep_rew_mean        | 237      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1282     |\n",
            "|    iterations         | 192900   |\n",
            "|    time_elapsed       | 6015     |\n",
            "|    total_timesteps    | 7716000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.312   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 192899   |\n",
            "|    policy_loss        | 0.0508   |\n",
            "|    value_loss         | 0.96     |\n",
            "------------------------------------\n",
            "Num timesteps: 7720000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 234.41\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 183      |\n",
            "|    ep_rew_mean        | 234      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1282     |\n",
            "|    iterations         | 193000   |\n",
            "|    time_elapsed       | 6017     |\n",
            "|    total_timesteps    | 7720000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.344   |\n",
            "|    explained_variance | 0.992    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 192999   |\n",
            "|    policy_loss        | -0.115   |\n",
            "|    value_loss         | 8.48     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 178      |\n",
            "|    ep_rew_mean        | 222      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1283     |\n",
            "|    iterations         | 193100   |\n",
            "|    time_elapsed       | 6019     |\n",
            "|    total_timesteps    | 7724000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.17    |\n",
            "|    explained_variance | 0.592    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 193099   |\n",
            "|    policy_loss        | -0.102   |\n",
            "|    value_loss         | 204      |\n",
            "------------------------------------\n",
            "Num timesteps: 7728000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 222.82\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 181      |\n",
            "|    ep_rew_mean        | 223      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1283     |\n",
            "|    iterations         | 193200   |\n",
            "|    time_elapsed       | 6022     |\n",
            "|    total_timesteps    | 7728000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.369   |\n",
            "|    explained_variance | 0.706    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 193199   |\n",
            "|    policy_loss        | -1.33    |\n",
            "|    value_loss         | 1.06e+03 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 185      |\n",
            "|    ep_rew_mean        | 230      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1283     |\n",
            "|    iterations         | 193300   |\n",
            "|    time_elapsed       | 6024     |\n",
            "|    total_timesteps    | 7732000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.341   |\n",
            "|    explained_variance | 0.172    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 193299   |\n",
            "|    policy_loss        | 0.158    |\n",
            "|    value_loss         | 605      |\n",
            "------------------------------------\n",
            "Num timesteps: 7736000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 230.41\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 179      |\n",
            "|    ep_rew_mean        | 230      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1283     |\n",
            "|    iterations         | 193400   |\n",
            "|    time_elapsed       | 6026     |\n",
            "|    total_timesteps    | 7736000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.393   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 193399   |\n",
            "|    policy_loss        | 0.106    |\n",
            "|    value_loss         | 2.18     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 189      |\n",
            "|    ep_rew_mean        | 240      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1283     |\n",
            "|    iterations         | 193500   |\n",
            "|    time_elapsed       | 6029     |\n",
            "|    total_timesteps    | 7740000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.3     |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 193499   |\n",
            "|    policy_loss        | -0.336   |\n",
            "|    value_loss         | 4.19     |\n",
            "------------------------------------\n",
            "Num timesteps: 7744000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 251.19\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 193      |\n",
            "|    ep_rew_mean        | 251      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1283     |\n",
            "|    iterations         | 193600   |\n",
            "|    time_elapsed       | 6031     |\n",
            "|    total_timesteps    | 7744000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.219   |\n",
            "|    explained_variance | 0.0296   |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 193599   |\n",
            "|    policy_loss        | 0.00438  |\n",
            "|    value_loss         | 439      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 189      |\n",
            "|    ep_rew_mean        | 253      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1284     |\n",
            "|    iterations         | 193700   |\n",
            "|    time_elapsed       | 6033     |\n",
            "|    total_timesteps    | 7748000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.207   |\n",
            "|    explained_variance | 0.543    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 193699   |\n",
            "|    policy_loss        | -0.414   |\n",
            "|    value_loss         | 220      |\n",
            "------------------------------------\n",
            "Num timesteps: 7752000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 250.93\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 186      |\n",
            "|    ep_rew_mean        | 251      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1284     |\n",
            "|    iterations         | 193800   |\n",
            "|    time_elapsed       | 6035     |\n",
            "|    total_timesteps    | 7752000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.191   |\n",
            "|    explained_variance | 0.876    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 193799   |\n",
            "|    policy_loss        | -0.577   |\n",
            "|    value_loss         | 200      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 180      |\n",
            "|    ep_rew_mean        | 251      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1284     |\n",
            "|    iterations         | 193900   |\n",
            "|    time_elapsed       | 6037     |\n",
            "|    total_timesteps    | 7756000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.362   |\n",
            "|    explained_variance | 0.819    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 193899   |\n",
            "|    policy_loss        | 0.286    |\n",
            "|    value_loss         | 316      |\n",
            "------------------------------------\n",
            "Num timesteps: 7760000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 255.58\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 181      |\n",
            "|    ep_rew_mean        | 256      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1284     |\n",
            "|    iterations         | 194000   |\n",
            "|    time_elapsed       | 6040     |\n",
            "|    total_timesteps    | 7760000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.143   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 193999   |\n",
            "|    policy_loss        | -0.423   |\n",
            "|    value_loss         | 2.67     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 183      |\n",
            "|    ep_rew_mean        | 257      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1284     |\n",
            "|    iterations         | 194100   |\n",
            "|    time_elapsed       | 6042     |\n",
            "|    total_timesteps    | 7764000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.373   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 194099   |\n",
            "|    policy_loss        | -0.0748  |\n",
            "|    value_loss         | 1.1      |\n",
            "------------------------------------\n",
            "Num timesteps: 7768000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 256.45\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 191      |\n",
            "|    ep_rew_mean        | 256      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1285     |\n",
            "|    iterations         | 194200   |\n",
            "|    time_elapsed       | 6045     |\n",
            "|    total_timesteps    | 7768000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.261   |\n",
            "|    explained_variance | 0.984    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 194199   |\n",
            "|    policy_loss        | -0.049   |\n",
            "|    value_loss         | 14.2     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 190      |\n",
            "|    ep_rew_mean        | 256      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1285     |\n",
            "|    iterations         | 194300   |\n",
            "|    time_elapsed       | 6047     |\n",
            "|    total_timesteps    | 7772000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.218   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 194299   |\n",
            "|    policy_loss        | -0.112   |\n",
            "|    value_loss         | 3.58     |\n",
            "------------------------------------\n",
            "Num timesteps: 7776000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 259.45\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 194      |\n",
            "|    ep_rew_mean        | 259      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1285     |\n",
            "|    iterations         | 194400   |\n",
            "|    time_elapsed       | 6049     |\n",
            "|    total_timesteps    | 7776000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.288   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 194399   |\n",
            "|    policy_loss        | 0.0197   |\n",
            "|    value_loss         | 3        |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 194      |\n",
            "|    ep_rew_mean        | 254      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1285     |\n",
            "|    iterations         | 194500   |\n",
            "|    time_elapsed       | 6051     |\n",
            "|    total_timesteps    | 7780000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.343   |\n",
            "|    explained_variance | 0.993    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 194499   |\n",
            "|    policy_loss        | 0.402    |\n",
            "|    value_loss         | 7.06     |\n",
            "------------------------------------\n",
            "Num timesteps: 7784000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 256.04\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 194      |\n",
            "|    ep_rew_mean        | 256      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1285     |\n",
            "|    iterations         | 194600   |\n",
            "|    time_elapsed       | 6054     |\n",
            "|    total_timesteps    | 7784000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.251   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 194599   |\n",
            "|    policy_loss        | 0.148    |\n",
            "|    value_loss         | 1.32     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 185      |\n",
            "|    ep_rew_mean        | 250      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1285     |\n",
            "|    iterations         | 194700   |\n",
            "|    time_elapsed       | 6056     |\n",
            "|    total_timesteps    | 7788000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.309   |\n",
            "|    explained_variance | 0.902    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 194699   |\n",
            "|    policy_loss        | -0.00111 |\n",
            "|    value_loss         | 101      |\n",
            "------------------------------------\n",
            "Num timesteps: 7792000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 245.09\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 181      |\n",
            "|    ep_rew_mean        | 245      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1286     |\n",
            "|    iterations         | 194800   |\n",
            "|    time_elapsed       | 6058     |\n",
            "|    total_timesteps    | 7792000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.33    |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 194799   |\n",
            "|    policy_loss        | -0.204   |\n",
            "|    value_loss         | 6.27     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 191      |\n",
            "|    ep_rew_mean        | 245      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1286     |\n",
            "|    iterations         | 194900   |\n",
            "|    time_elapsed       | 6061     |\n",
            "|    total_timesteps    | 7796000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.163   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 194899   |\n",
            "|    policy_loss        | -0.00212 |\n",
            "|    value_loss         | 1.25     |\n",
            "------------------------------------\n",
            "Num timesteps: 7800000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 241.53\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 188      |\n",
            "|    ep_rew_mean        | 242      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1286     |\n",
            "|    iterations         | 195000   |\n",
            "|    time_elapsed       | 6063     |\n",
            "|    total_timesteps    | 7800000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.182   |\n",
            "|    explained_variance | 0.693    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 194999   |\n",
            "|    policy_loss        | -1.1     |\n",
            "|    value_loss         | 340      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 189      |\n",
            "|    ep_rew_mean        | 247      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1286     |\n",
            "|    iterations         | 195100   |\n",
            "|    time_elapsed       | 6065     |\n",
            "|    total_timesteps    | 7804000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.361   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 195099   |\n",
            "|    policy_loss        | -0.112   |\n",
            "|    value_loss         | 1.24     |\n",
            "------------------------------------\n",
            "Num timesteps: 7808000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 238.08\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 187      |\n",
            "|    ep_rew_mean        | 238      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1286     |\n",
            "|    iterations         | 195200   |\n",
            "|    time_elapsed       | 6067     |\n",
            "|    total_timesteps    | 7808000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.385   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 195199   |\n",
            "|    policy_loss        | 0.0783   |\n",
            "|    value_loss         | 2.81     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 189      |\n",
            "|    ep_rew_mean        | 242      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1286     |\n",
            "|    iterations         | 195300   |\n",
            "|    time_elapsed       | 6069     |\n",
            "|    total_timesteps    | 7812000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.188   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 195299   |\n",
            "|    policy_loss        | -0.0821  |\n",
            "|    value_loss         | 3.51     |\n",
            "------------------------------------\n",
            "Num timesteps: 7816000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 239.72\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 186      |\n",
            "|    ep_rew_mean        | 240      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1287     |\n",
            "|    iterations         | 195400   |\n",
            "|    time_elapsed       | 6072     |\n",
            "|    total_timesteps    | 7816000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.332   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 195399   |\n",
            "|    policy_loss        | -0.188   |\n",
            "|    value_loss         | 1.2      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 205      |\n",
            "|    ep_rew_mean        | 237      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1287     |\n",
            "|    iterations         | 195500   |\n",
            "|    time_elapsed       | 6075     |\n",
            "|    total_timesteps    | 7820000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.313   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 195499   |\n",
            "|    policy_loss        | 0.22     |\n",
            "|    value_loss         | 1.36     |\n",
            "------------------------------------\n",
            "Num timesteps: 7824000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 222.33\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 205      |\n",
            "|    ep_rew_mean        | 222      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1287     |\n",
            "|    iterations         | 195600   |\n",
            "|    time_elapsed       | 6077     |\n",
            "|    total_timesteps    | 7824000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.33    |\n",
            "|    explained_variance | 0.523    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 195599   |\n",
            "|    policy_loss        | -0.159   |\n",
            "|    value_loss         | 1.37e+03 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 210      |\n",
            "|    ep_rew_mean        | 223      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1287     |\n",
            "|    iterations         | 195700   |\n",
            "|    time_elapsed       | 6079     |\n",
            "|    total_timesteps    | 7828000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.253   |\n",
            "|    explained_variance | 0.733    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 195699   |\n",
            "|    policy_loss        | -0.294   |\n",
            "|    value_loss         | 528      |\n",
            "------------------------------------\n",
            "Num timesteps: 7832000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 212.13\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 216      |\n",
            "|    ep_rew_mean        | 212      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1287     |\n",
            "|    iterations         | 195800   |\n",
            "|    time_elapsed       | 6081     |\n",
            "|    total_timesteps    | 7832000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.317   |\n",
            "|    explained_variance | 0.994    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 195799   |\n",
            "|    policy_loss        | -1.08    |\n",
            "|    value_loss         | 9.91     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 192      |\n",
            "|    ep_rew_mean        | 204      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1288     |\n",
            "|    iterations         | 195900   |\n",
            "|    time_elapsed       | 6083     |\n",
            "|    total_timesteps    | 7836000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.284   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 195899   |\n",
            "|    policy_loss        | -0.0907  |\n",
            "|    value_loss         | 1.87     |\n",
            "------------------------------------\n",
            "Num timesteps: 7840000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 183.80\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 176      |\n",
            "|    ep_rew_mean        | 184      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1288     |\n",
            "|    iterations         | 196000   |\n",
            "|    time_elapsed       | 6085     |\n",
            "|    total_timesteps    | 7840000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.199   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 195999   |\n",
            "|    policy_loss        | 0.0861   |\n",
            "|    value_loss         | 7.75     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 168      |\n",
            "|    ep_rew_mean        | 161      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1288     |\n",
            "|    iterations         | 196100   |\n",
            "|    time_elapsed       | 6087     |\n",
            "|    total_timesteps    | 7844000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.247   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 196099   |\n",
            "|    policy_loss        | -0.0507  |\n",
            "|    value_loss         | 2.93     |\n",
            "------------------------------------\n",
            "Num timesteps: 7848000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 164.68\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 163      |\n",
            "|    ep_rew_mean        | 165      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1288     |\n",
            "|    iterations         | 196200   |\n",
            "|    time_elapsed       | 6090     |\n",
            "|    total_timesteps    | 7848000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.262   |\n",
            "|    explained_variance | 0.458    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 196199   |\n",
            "|    policy_loss        | 0.787    |\n",
            "|    value_loss         | 725      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 168      |\n",
            "|    ep_rew_mean        | 172      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1288     |\n",
            "|    iterations         | 196300   |\n",
            "|    time_elapsed       | 6092     |\n",
            "|    total_timesteps    | 7852000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.408   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 196299   |\n",
            "|    policy_loss        | 0.0561   |\n",
            "|    value_loss         | 1.22     |\n",
            "------------------------------------\n",
            "Num timesteps: 7856000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 185.28\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 170      |\n",
            "|    ep_rew_mean        | 185      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1289     |\n",
            "|    iterations         | 196400   |\n",
            "|    time_elapsed       | 6094     |\n",
            "|    total_timesteps    | 7856000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.499   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 196399   |\n",
            "|    policy_loss        | -0.164   |\n",
            "|    value_loss         | 3.07     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 181      |\n",
            "|    ep_rew_mean        | 206      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1289     |\n",
            "|    iterations         | 196500   |\n",
            "|    time_elapsed       | 6096     |\n",
            "|    total_timesteps    | 7860000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.203   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 196499   |\n",
            "|    policy_loss        | -0.121   |\n",
            "|    value_loss         | 2.15     |\n",
            "------------------------------------\n",
            "Num timesteps: 7864000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 218.23\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 181      |\n",
            "|    ep_rew_mean        | 218      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1289     |\n",
            "|    iterations         | 196600   |\n",
            "|    time_elapsed       | 6098     |\n",
            "|    total_timesteps    | 7864000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.249   |\n",
            "|    explained_variance | 0.991    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 196599   |\n",
            "|    policy_loss        | -0.392   |\n",
            "|    value_loss         | 4.3      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 176      |\n",
            "|    ep_rew_mean        | 215      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1289     |\n",
            "|    iterations         | 196700   |\n",
            "|    time_elapsed       | 6100     |\n",
            "|    total_timesteps    | 7868000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.371   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 196699   |\n",
            "|    policy_loss        | -0.338   |\n",
            "|    value_loss         | 1.46     |\n",
            "------------------------------------\n",
            "Num timesteps: 7872000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 220.73\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 179      |\n",
            "|    ep_rew_mean        | 221      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1289     |\n",
            "|    iterations         | 196800   |\n",
            "|    time_elapsed       | 6103     |\n",
            "|    total_timesteps    | 7872000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.191   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 196799   |\n",
            "|    policy_loss        | 0.0616   |\n",
            "|    value_loss         | 1.51     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 178      |\n",
            "|    ep_rew_mean        | 231      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1290     |\n",
            "|    iterations         | 196900   |\n",
            "|    time_elapsed       | 6105     |\n",
            "|    total_timesteps    | 7876000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.271   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 196899   |\n",
            "|    policy_loss        | -0.0206  |\n",
            "|    value_loss         | 0.717    |\n",
            "------------------------------------\n",
            "Num timesteps: 7880000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 226.89\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 178      |\n",
            "|    ep_rew_mean        | 227      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1290     |\n",
            "|    iterations         | 197000   |\n",
            "|    time_elapsed       | 6107     |\n",
            "|    total_timesteps    | 7880000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.251   |\n",
            "|    explained_variance | 0.821    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 196999   |\n",
            "|    policy_loss        | 0.104    |\n",
            "|    value_loss         | 741      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 182      |\n",
            "|    ep_rew_mean        | 230      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1290     |\n",
            "|    iterations         | 197100   |\n",
            "|    time_elapsed       | 6109     |\n",
            "|    total_timesteps    | 7884000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.195   |\n",
            "|    explained_variance | 0.512    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 197099   |\n",
            "|    policy_loss        | 0.757    |\n",
            "|    value_loss         | 571      |\n",
            "------------------------------------\n",
            "Num timesteps: 7888000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 229.84\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 183      |\n",
            "|    ep_rew_mean        | 230      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1290     |\n",
            "|    iterations         | 197200   |\n",
            "|    time_elapsed       | 6111     |\n",
            "|    total_timesteps    | 7888000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.393   |\n",
            "|    explained_variance | 0.972    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 197199   |\n",
            "|    policy_loss        | 0.874    |\n",
            "|    value_loss         | 27.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 194      |\n",
            "|    ep_rew_mean        | 236      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1290     |\n",
            "|    iterations         | 197300   |\n",
            "|    time_elapsed       | 6114     |\n",
            "|    total_timesteps    | 7892000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.488   |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 197299   |\n",
            "|    policy_loss        | 0.199    |\n",
            "|    value_loss         | 1.45     |\n",
            "------------------------------------\n",
            "Num timesteps: 7896000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 238.84\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 196      |\n",
            "|    ep_rew_mean        | 239      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1290     |\n",
            "|    iterations         | 197400   |\n",
            "|    time_elapsed       | 6116     |\n",
            "|    total_timesteps    | 7896000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.308   |\n",
            "|    explained_variance | 0.99     |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 197399   |\n",
            "|    policy_loss        | 0.0414   |\n",
            "|    value_loss         | 5.51     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 202      |\n",
            "|    ep_rew_mean        | 253      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1291     |\n",
            "|    iterations         | 197500   |\n",
            "|    time_elapsed       | 6118     |\n",
            "|    total_timesteps    | 7900000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.252   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 197499   |\n",
            "|    policy_loss        | 0.0533   |\n",
            "|    value_loss         | 1.33     |\n",
            "------------------------------------\n",
            "Num timesteps: 7904000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 261.36\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 203      |\n",
            "|    ep_rew_mean        | 261      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1291     |\n",
            "|    iterations         | 197600   |\n",
            "|    time_elapsed       | 6120     |\n",
            "|    total_timesteps    | 7904000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.183   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 197599   |\n",
            "|    policy_loss        | 0.138    |\n",
            "|    value_loss         | 1.24     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 204      |\n",
            "|    ep_rew_mean        | 257      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1291     |\n",
            "|    iterations         | 197700   |\n",
            "|    time_elapsed       | 6123     |\n",
            "|    total_timesteps    | 7908000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.207   |\n",
            "|    explained_variance | 0.821    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 197699   |\n",
            "|    policy_loss        | 0.396    |\n",
            "|    value_loss         | 355      |\n",
            "------------------------------------\n",
            "Num timesteps: 7912000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 257.57\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 195      |\n",
            "|    ep_rew_mean        | 258      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1291     |\n",
            "|    iterations         | 197800   |\n",
            "|    time_elapsed       | 6125     |\n",
            "|    total_timesteps    | 7912000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.299   |\n",
            "|    explained_variance | 0.615    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 197799   |\n",
            "|    policy_loss        | 0.0707   |\n",
            "|    value_loss         | 317      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 188      |\n",
            "|    ep_rew_mean        | 241      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1291     |\n",
            "|    iterations         | 197900   |\n",
            "|    time_elapsed       | 6127     |\n",
            "|    total_timesteps    | 7916000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.456   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 197899   |\n",
            "|    policy_loss        | -0.264   |\n",
            "|    value_loss         | 2.81     |\n",
            "------------------------------------\n",
            "Num timesteps: 7920000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 232.11\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 194      |\n",
            "|    ep_rew_mean        | 232      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1291     |\n",
            "|    iterations         | 198000   |\n",
            "|    time_elapsed       | 6130     |\n",
            "|    total_timesteps    | 7920000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.326   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 197999   |\n",
            "|    policy_loss        | -0.0792  |\n",
            "|    value_loss         | 2.09     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 191      |\n",
            "|    ep_rew_mean        | 220      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1292     |\n",
            "|    iterations         | 198100   |\n",
            "|    time_elapsed       | 6132     |\n",
            "|    total_timesteps    | 7924000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.216   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 198099   |\n",
            "|    policy_loss        | -0.181   |\n",
            "|    value_loss         | 3.81     |\n",
            "------------------------------------\n",
            "Num timesteps: 7928000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 211.90\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 184      |\n",
            "|    ep_rew_mean        | 212      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1292     |\n",
            "|    iterations         | 198200   |\n",
            "|    time_elapsed       | 6135     |\n",
            "|    total_timesteps    | 7928000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.441   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 198199   |\n",
            "|    policy_loss        | 0.0407   |\n",
            "|    value_loss         | 0.938    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 191      |\n",
            "|    ep_rew_mean        | 202      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1292     |\n",
            "|    iterations         | 198300   |\n",
            "|    time_elapsed       | 6137     |\n",
            "|    total_timesteps    | 7932000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.27    |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 198299   |\n",
            "|    policy_loss        | -0.0646  |\n",
            "|    value_loss         | 0.581    |\n",
            "------------------------------------\n",
            "Num timesteps: 7936000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 204.45\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 193      |\n",
            "|    ep_rew_mean        | 204      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1292     |\n",
            "|    iterations         | 198400   |\n",
            "|    time_elapsed       | 6140     |\n",
            "|    total_timesteps    | 7936000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.283   |\n",
            "|    explained_variance | 1        |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 198399   |\n",
            "|    policy_loss        | -0.0213  |\n",
            "|    value_loss         | 0.27     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 192      |\n",
            "|    ep_rew_mean        | 216      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1292     |\n",
            "|    iterations         | 198500   |\n",
            "|    time_elapsed       | 6142     |\n",
            "|    total_timesteps    | 7940000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.189   |\n",
            "|    explained_variance | 0.879    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 198499   |\n",
            "|    policy_loss        | -0.0568  |\n",
            "|    value_loss         | 128      |\n",
            "------------------------------------\n",
            "Num timesteps: 7944000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 220.39\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 196      |\n",
            "|    ep_rew_mean        | 220      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1292     |\n",
            "|    iterations         | 198600   |\n",
            "|    time_elapsed       | 6145     |\n",
            "|    total_timesteps    | 7944000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.32    |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 198599   |\n",
            "|    policy_loss        | 0.0125   |\n",
            "|    value_loss         | 1.92     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 202      |\n",
            "|    ep_rew_mean        | 230      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1292     |\n",
            "|    iterations         | 198700   |\n",
            "|    time_elapsed       | 6147     |\n",
            "|    total_timesteps    | 7948000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.244   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 198699   |\n",
            "|    policy_loss        | -0.0818  |\n",
            "|    value_loss         | 1.24     |\n",
            "------------------------------------\n",
            "Num timesteps: 7952000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 235.27\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 197      |\n",
            "|    ep_rew_mean        | 235      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1293     |\n",
            "|    iterations         | 198800   |\n",
            "|    time_elapsed       | 6149     |\n",
            "|    total_timesteps    | 7952000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.493   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 198799   |\n",
            "|    policy_loss        | -0.13    |\n",
            "|    value_loss         | 1.61     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 197      |\n",
            "|    ep_rew_mean        | 237      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1293     |\n",
            "|    iterations         | 198900   |\n",
            "|    time_elapsed       | 6152     |\n",
            "|    total_timesteps    | 7956000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.299   |\n",
            "|    explained_variance | 0.619    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 198899   |\n",
            "|    policy_loss        | -0.0256  |\n",
            "|    value_loss         | 1.09e+03 |\n",
            "------------------------------------\n",
            "Num timesteps: 7960000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 231.21\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 197      |\n",
            "|    ep_rew_mean        | 231      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1293     |\n",
            "|    iterations         | 199000   |\n",
            "|    time_elapsed       | 6154     |\n",
            "|    total_timesteps    | 7960000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.235   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 198999   |\n",
            "|    policy_loss        | 0.0419   |\n",
            "|    value_loss         | 1.36     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 199      |\n",
            "|    ep_rew_mean        | 239      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1293     |\n",
            "|    iterations         | 199100   |\n",
            "|    time_elapsed       | 6156     |\n",
            "|    total_timesteps    | 7964000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.299   |\n",
            "|    explained_variance | 0.996    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 199099   |\n",
            "|    policy_loss        | -0.0104  |\n",
            "|    value_loss         | 4.49     |\n",
            "------------------------------------\n",
            "Num timesteps: 7968000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 251.10\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 200      |\n",
            "|    ep_rew_mean        | 251      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1293     |\n",
            "|    iterations         | 199200   |\n",
            "|    time_elapsed       | 6158     |\n",
            "|    total_timesteps    | 7968000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.37    |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 199199   |\n",
            "|    policy_loss        | -0.0797  |\n",
            "|    value_loss         | 1.51     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 203      |\n",
            "|    ep_rew_mean        | 258      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1293     |\n",
            "|    iterations         | 199300   |\n",
            "|    time_elapsed       | 6161     |\n",
            "|    total_timesteps    | 7972000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.352   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 199299   |\n",
            "|    policy_loss        | -0.0617  |\n",
            "|    value_loss         | 0.916    |\n",
            "------------------------------------\n",
            "Num timesteps: 7976000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 267.09\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 199      |\n",
            "|    ep_rew_mean        | 267      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1294     |\n",
            "|    iterations         | 199400   |\n",
            "|    time_elapsed       | 6163     |\n",
            "|    total_timesteps    | 7976000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.39    |\n",
            "|    explained_variance | 0.995    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 199399   |\n",
            "|    policy_loss        | 0.0254   |\n",
            "|    value_loss         | 1.77     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 199      |\n",
            "|    ep_rew_mean        | 269      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1294     |\n",
            "|    iterations         | 199500   |\n",
            "|    time_elapsed       | 6165     |\n",
            "|    total_timesteps    | 7980000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.421   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 199499   |\n",
            "|    policy_loss        | -0.0793  |\n",
            "|    value_loss         | 1.33     |\n",
            "------------------------------------\n",
            "Num timesteps: 7984000\n",
            "Best mean reward: 268.31 - Last mean reward per episode: 269.26\n",
            "Saving new best model to log_dir_A2C/best_model.zip\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 196      |\n",
            "|    ep_rew_mean        | 269      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1294     |\n",
            "|    iterations         | 199600   |\n",
            "|    time_elapsed       | 6167     |\n",
            "|    total_timesteps    | 7984000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.319   |\n",
            "|    explained_variance | 0.988    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 199599   |\n",
            "|    policy_loss        | 0.175    |\n",
            "|    value_loss         | 8.25     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 196      |\n",
            "|    ep_rew_mean        | 269      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1294     |\n",
            "|    iterations         | 199700   |\n",
            "|    time_elapsed       | 6169     |\n",
            "|    total_timesteps    | 7988000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.306   |\n",
            "|    explained_variance | 0.999    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 199699   |\n",
            "|    policy_loss        | 0.042    |\n",
            "|    value_loss         | 1.67     |\n",
            "------------------------------------\n",
            "Num timesteps: 7992000\n",
            "Best mean reward: 269.26 - Last mean reward per episode: 267.13\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 193      |\n",
            "|    ep_rew_mean        | 267      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1294     |\n",
            "|    iterations         | 199800   |\n",
            "|    time_elapsed       | 6171     |\n",
            "|    total_timesteps    | 7992000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.251   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 199799   |\n",
            "|    policy_loss        | 0.315    |\n",
            "|    value_loss         | 2.81     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 192      |\n",
            "|    ep_rew_mean        | 266      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1295     |\n",
            "|    iterations         | 199900   |\n",
            "|    time_elapsed       | 6174     |\n",
            "|    total_timesteps    | 7996000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.166   |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 199899   |\n",
            "|    policy_loss        | 0.0062   |\n",
            "|    value_loss         | 5.24     |\n",
            "------------------------------------\n",
            "Num timesteps: 8000000\n",
            "Best mean reward: 269.26 - Last mean reward per episode: 266.81\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 200      |\n",
            "|    ep_rew_mean        | 267      |\n",
            "| time/                 |          |\n",
            "|    fps                | 1295     |\n",
            "|    iterations         | 200000   |\n",
            "|    time_elapsed       | 6176     |\n",
            "|    total_timesteps    | 8000000  |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.293   |\n",
            "|    explained_variance | 0.998    |\n",
            "|    learning_rate      | 0.00083  |\n",
            "|    n_updates          | 199999   |\n",
            "|    policy_loss        | 0.171    |\n",
            "|    value_loss         | 2.98     |\n",
            "------------------------------------\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<stable_baselines3.a2c.a2c.A2C at 0x7ff148552fd0>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_from_file = False\n",
        "# Hyperparameters are from RL_Zoo\n",
        "# https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/a2c.yml\n",
        "policy = \"MlpPolicy\"\n",
        "n_steps = 5\n",
        "learning_rate = 0.00083\n",
        "# batch_size = 256\n",
        "# n_epochs = 10\n",
        "n_envs = 8\n",
        "n_timesteps = 8e6\n",
        "gamma = 0.995\n",
        "ent_coef = 0.00001\n",
        "\n",
        "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=\"log_dir_A2C/\")\n",
        "\n",
        "# env\n",
        "env = make_vec_env(\"LunarLander-v2\", n_envs=n_envs, monitor_dir=\"log_dir_A2C/\")\n",
        "\n",
        "# instantiate the agent\n",
        "if train_from_file:\n",
        "  model = A2C.load(path=\"log_dir_A2C/best_model.zip\", env=env)\n",
        "else:\n",
        "  model = A2C(policy, env, learning_rate = learning_rate, n_steps = n_steps, ent_coef= ent_coef, tensorboard_log=\"./TensorBoardLog/\", verbose=1)\n",
        "\n",
        "# train the agent\n",
        "model.learn(total_timesteps=n_timesteps, callback=callback)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b1dc8fa",
      "metadata": {
        "id": "5b1dc8fa"
      },
      "source": [
        "# Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "366b80a8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "366b80a8",
        "outputId": "417c672c-1a65-4a71-f550-d10a5c1626ae"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def moving_average(values, window):\n",
        "    \"\"\"\n",
        "    Smooth values by doing a moving average\n",
        "    :param values: (numpy array)\n",
        "    :param window: (int)\n",
        "    :return: (numpy array)\n",
        "    \"\"\"\n",
        "    weights = np.repeat(1.0, window) / window\n",
        "    return np.convolve(values, weights, 'valid')\n",
        "\n",
        "def plot_results(log_folder, title='Learning Curve'):\n",
        "    \"\"\"\n",
        "    plot the results\n",
        "\n",
        "    :param log_folder: (str) the save location of the results to plot\n",
        "    :param title: (str) the title of the task to plot\n",
        "    \"\"\"\n",
        "\n",
        "    x, y = ts2xy(load_results(log_folder), 'timesteps')\n",
        "    y = moving_average(y, window=100)\n",
        "    # Truncate x\n",
        "    x = x[len(x) - len(y):]\n",
        "    fig = plt.figure(title, figsize=(12,5))\n",
        "    plt.plot(x, y)\n",
        "    plt.xlabel('Number of Timesteps')\n",
        "    plt.ylabel('Rewards')\n",
        "    # plt.title(title + \" Smoothed A2C after 8000,000 Timesteps\")\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "plot_results(\"log_dir_A2C\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b00f2a81",
      "metadata": {
        "id": "b00f2a81"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "815393a0",
      "metadata": {
        "id": "815393a0"
      },
      "outputs": [],
      "source": [
        "env = make_vec_env(\"LunarLander-v2\", n_envs=1,monitor_dir=\"evaluate_log_dir_A2C\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "63611e6e",
      "metadata": {
        "id": "63611e6e"
      },
      "outputs": [],
      "source": [
        "model = A2C.load(path=\"log_dir_A2C/best_model.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b06e1a3",
      "metadata": {
        "id": "3b06e1a3"
      },
      "source": [
        "#### Stable Baseline 3 Evaluation Function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "9d4fd326",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d4fd326",
        "outputId": "e75bd790-909d-4ea8-d242-83632d84147e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean & Std Reward after 10 max run is 277.8031373 & 22.651884249193934\n"
          ]
        }
      ],
      "source": [
        "mean_reward, std_reward = evaluate_policy(model, env,n_eval_episodes=10, render=True, deterministic=True)\n",
        "print(\"Mean & Std Reward after {} max run is {} & {}\".format(10,mean_reward, std_reward))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e49c5168",
      "metadata": {
        "id": "e49c5168"
      },
      "source": [
        "# GIF of a Train Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "60cc63dc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60cc63dc",
        "outputId": "9a42d8c7-7a7c-49af-a020-b0d647d5bbfe"
      },
      "outputs": [],
      "source": [
        "env = make_vec_env(\"LunarLander-v2\", n_envs=1)\n",
        "model = A2C.load(path=\"log_dir_A2C/best_model.zip\")\n",
        "\n",
        "images = []\n",
        "obs = env.reset()\n",
        "img = env.render(mode=\"rgb_array\")\n",
        "for i in range(1000):\n",
        "    images.append(img)\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, _, _ ,_ = env.step(action)\n",
        "    img = env.render(mode=\"rgb_array\")\n",
        "\n",
        "imageio.mimsave(\"lunar lander_A2C.gif\", [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3afc060b",
      "metadata": {
        "id": "3afc060b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "857970f990130bbcaee778cf1846f7875676d945310dca1379fe4b5ef3d258a5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
